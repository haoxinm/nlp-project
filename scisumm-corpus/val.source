Sentence Reduction For Automatic Text SummarizationFigure 2: Sample sentence and parse tree we have an input sentence (ABCDEhas a parse tree shown in Figure 2. a human reduces the sentence to which can be translated to a series of decisions made along edges in the sentence parse tree as shown in Figure 3.The symbol &quot;y&quot; along an edge means the node it points to will be kept, and &quot;n&quot; means the node will be removed.Suppose the program reduces sentence to can be translated similarly to the annotated tree shown in Figure 4.Figure 4: Reduced form by the program We can see that along five edges (they are D—)T, D—*G, B-4A, B—>C), both the human and the program made decisions.Two out of the five decisions agree (they are D--÷B and D—>E), so the rate is 2/5 (40%).The rate defined as: man and the program have made the same decision success rate = the total # of edges along which both the human and the progam have made decisions 313 Note that the edges along which only the human or the program has made a decision (e.g., G--F and G—.>F in Figure 3 and Figure 4) are not considered in the computation of success rate, since there is no agreement issue in such cases.3.2 Evaluation result In the evaluation, we used 400 sentences in the corpus to compute the probabilities that a phrase is removed, reduced, or unchanged.We tested the program on the rest 100 sentences.Using five-fold validation (i.e., chose different 100 sentences for testing each time and repeating the experiment five times), The program achieved an average success rate of 81.3%.If we consider the baseline as removing all the prepositional phrases, clauses, to-infinitives and gerunds, the baseline performance is 43.2%.We also computed the success rate of program's decisions on particular types of phrases.For the decisions on removing or keeping a clause, the system has a success rate of 78.1%; for the decisions on removing or keeping a to-infinitive, the system has a success rate of 85.2%.We found out that the system has a low success rate on removing adjectives of noun phrases or removing adverbs of a sentence or a verb phrase.One reason for this is that our probability model can hardly capture the dependencies between a particular adjective and the head noun since the training corpus is not large enough, while the other sources of information, including grammar or context information, provide little evidence on whether an adjective or an adverb should be removed.Given that whether or not an adjective or an adverb is removed does not affect the conciseness of the sentence significantly and the system lacks of reliability in making such decisions, we decide not to remove adjectives and adverbs.On average, the system reduced the length of the 500 sentence by 32.7% (based on the number of words), while humans reduced it by 41.8%.The probabilities we computed from the training corpus covered 58% of instances in the test corpus.When the corpus probability is absent for a case, the system makes decisions based on the other two sources of knowledge.Some of the errors made by the system result from the errors by the syntactic parser.We randomly checked 50 sentences, and found that 8% of the errors made by the system are due to parsing errors.There are two main reasons responsible for this relative low percentage of errors resulted from mistakes in parsing.One reason is that we have taken some special measures to avoid errors introduced by mistakes in parsing.For example, PP attachment is a difficult problem in parsing and it is not rare that a PP is wrongly attached.Therefore, we take this into account when marking the obligatory components using subcategorization knowledge from the lexicon (step 2) — we not only look at the PPs that are attached to a verb phrase, but also PPs that are next to the verb phrase but not attached, in case it is part of the verb phrase.We also wrote a preprocessor to deal with particular structures that the parser often has problems with, such as appositions.The other reason is that parsing errors do not always result in reduction errors.For example, given a sentence &quot;The spokesperson of the University said that ...&quot;, although that-clause in the sentence may have a complicated structure and the parser gets it wrong, the reduction system is not necessarily affected since it may decide in this case to keep that-clause as it is, as humans often do, so the parsing errors will not matter in this example.4 Discussion and related work The reduction algorithm we present assumes generic summarization; that is, we want to generate a summary that includes the most important information in an article.We can tailor the reduction system to queries-based summarization.In that case, the task of the reduction is not to remove phrases that are extraneous in terms of the main topic of an article, but phrases that are not very relevant to users' queries.We extended our sentence reduction program to query-based summarization by adding another step in the algorithm to measure the relevance of users' queries to phrases in the sentence.In the last step of reduction when the system makes the final decision, the relevance of a phrase to the query is taken into account, together with syntactic, context, and corpus information.Ideally, the sentence reduction module should interact with other modules in a summarization system.It should be able to send feedback to the extraction module if it finds that a sentence selected by the extraction module may be inappropriate (for example, having a very low context importance score).It should also be able to interact with the modules that run after it, such as the sentence combination module, so that it can revise reduction decisions according to the feedback from these modules.Some researchers suggested removing phrases or clauses from sentences for certain applications.(Grefenstette, 1998) proposed to remove phrases in sentences to produce a telegraphic text that can be used to provide audio scanning service for the blind.(Corston-Oliver and Dolan, 1999) proposed to remove clauses in sentences before indexing documents for Information Retrieval.Both studies removed phrases based only on their syntactic categories, while the focus of our system is on deciding when it is appropriate to remove a phrase. researchers worked on the text simplifica- 314 tion problem, which usually involves in simplifying text but not removing any phrases.For example, (Carroll et al., 1998) discussed simplifying newspaper text by replacing uncommon words with common words, or replacing complicated syntactic structures with simpler structures to assist people with reading disabilities.(Chandrasekar et al., 1996) discussed text simplification in general.The difference between these studies on text simplification and our system is that a text simplification system usually not from an original sentence, although it may change its structure or words, but our system removes extraneous phrases from the extracted sentences.5 Conclusions and future work We present a novel sentence reduction system which removes extraneous phrases from sentences that are extracted from an article in text summarization.The deleted phrases can be prepositional phrases, clauses, to-infinitives, or gerunds, and multiple phrases can be removed form a single sentence.The focus of this work is on determining, for a sentence in a particular context, which phrases in the sentence are less important and can be removed.Our system makes intelligent reduction decisions based on multiple sources of knowledge, including syntactic knowledge, context, and probabilities computed from corpus analysis.We also created a corpus consisting of 500 sentences and their reduced forms produced by human professionals, and used this corpus for training and testing the system.The evaluation shows that 81.3% of reduction decisions made by the system agreed with those of humans.In the future, we would like to integrate our sentence reduction system with extraction-based summarization systems other than the one we have developed, improve the performance of the system further by introducing other sources of knowledge necessary for reduction, and explore other interesting applications of the reduction system.Acknowledgment This material is based upon work supported by the National Science Foundation under Grant No.IRI 96-19124 and IRI 96-18797.Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do notCurrent automatic summarizers usually rely on sentence extraction to produce summaries.Human professionals also often reuse the input documents to generate summaries; however, rather than simply extracting sentences and stringing them together, as most current summarizers do, humans often &quot;edit&quot; the extracted sentences in some way so that the resulting summary is concise and coherent.We analyzed a set of articles and identified six major operations that can be used for editing the extracted sentences, including removing extraneous phrases from an extracted sentence, combining a reduced sentence with other sentences, syntactic transformation, substituting phrases in an extracted sentence with their paraphrases, substituting phrases with more general or specific descriptions, and reordering the extracted sentences (Jing and McKeown, 1999; Jing and McKeown, 2000).We call the operation of removing extraneous phrases from an extracted sentence sentence reduction.It is one of the most effective operations that can be used to edit the extracted sentences.Reduction can remove material at any granularity: a word, a prepositional phrase, a gerund, a to-infinitive or a clause.We use the term &quot;phrase&quot; here to refer to any of the above components that can be removed in reduction.The following example shows an original sentence and its reduced form written by a human professional: Original sentence: When it arrives sometime next year in new TV sets, the V-chip will give parents a new and potentially revolutionary device to block out programs they don't want their children to see.Reduced sentence by humans: The V-chip will give parents a device to block out programs they don't want their children to see.We implemented an automatic sentence reduction system.Input to the reduction system includes extracted sentences, as well as the original document.Output of reduction are reduced forms of the extracted sentences, which can either be used to produce summaries directly, or be merged with other sentences.The reduction system uses multiple sources of knowledge to make reduction decisions, including syntactic knowledge, context, and statistics computed from a training corpus.We evaluated the system against the output of human professionals.The program achieved a success rate of 81.3%, meaning that 81.3% of reduction decisions made by the system agreed with those of humans.Sentence reduction improves the conciseness of automatically generated summaries, making it concise and on target.It can also improve the coherence of generated summaries, since extraneous phrases that can potentially introduce incoherece are removed.We collected 500 sentences and their corresponding reduced forms written by humans, and found that humans reduced the length of these 500 sentences by 44.2% on average.This indicates that a good sentence reduction system can improve the conciseness of generated summaries significantly.In the next section, we describe the sentence reduction algorithm in details.In Section 3, we introduce the evaluation scheme used to access the performance of the system and present evaluation results.In Section 4, we discuss other applications of sentence reduction, the interaction between reduction and other modules in a summarization system, and related work on sentence simplication.Finally, we The goal of sentence reduction is to &quot;reduce without major loss&quot;; that is, we want to remove as many extraneous phrases as possible from an extracted sentence so that it can be concise, but without detracting from the main idea the sentence conveys.Ideally, we want to remove a phrase from an extracted sentence only if it is irrelevant to the main topic.To achieve this, the system relies on multiple sources of knowledge to make reduction decisions.We first introduce the resources in the system and then describe the reduction algorithm.(1) The corpus.One of the key features of the system is that it uses a corpus consisting of original sentences and their corresponding reduced forms written by humans for training and testing purpose.This corpus was created using an automatic program we have developed to automatically analyze human-written abstracts.The program, called the decomposition program, matches phrases in a human-written summary sentence to phrases in the original document (Jing and McKeown, 1999).The human-written abstracts were collected from the free daily news service &quot;Communicationsrelated headlines&quot;, provided by the Benton Foundation (http://www.benton.org).The articles in the corpus are news reports on telecommunication related issues, but they cover a wide range of topics, such as law, labor, and company mergers. database to date.It provides lexical relations between words, including synonymy, antonymy, meronymy, entailment (e.g., eat —> chew), or causation (e.g., kill --* die).These lexical links are used to identify the focus in the local context.(4) The syntactic parser.We use the English Slot Grammar(ESG) parser developed at IBM (McCord, 1990) to analyze the syntactic structure of an input sentence and produce a sentence parse tree.The ESG parser not only annotates the syntactic category of a phrase (e.g., &quot;np&quot; or &quot;vp&quot;), it also annotates the thematic role of a phrase (e.g., &quot;subject&quot; or &quot;object&quot;).There are five steps in the reduction program: Step 1: Syntactic parsing.We first parse the input sentence using the ESG parser and produce the sentence parse tree.The operations in all other steps are performed based on this parse tree.Each following step annotates each node in the parse tree with additional information, such as syntactic or context importance, which are used later to determine which phrases (they are represented as subtrees in a parse tree) can be considered extraneous and thus removed.Step 2: Grammar checking.In this step, we determine which components of a sentence must not be deleted to keep the sentence grammatical.To do this, we traverse the parse tree produced in the first step in top-down order and mark, for each node in the parse tree, which of its children are grammatically obligatory.We use two sources of knowledge for this purpose.One source includes simple, linguistic-based rules that use the thematic role structure produced by the ESG parser.For instance, for a sentence, the main verb, the subject, and the object(s) are essential if they exist, but a prepositional phrase is not; for a noun phrase, the head noun is essential, but an adjective modifier of the head noun is not.The other source we rely on is the large-scale lexicon we described earlier.The information in the lexicon is used to mark the obligatory arguments of verb phrases.For example, for the verb &quot;convince&quot;, the lexicon has the following entry: This entry indicates that the verb &quot;convince&quot; can be followed by a noun phrase and a prepositional phrase starting with the preposition &quot;of' (e.g., he convinced me of his innocence).It can also be followed by a noun phrase and a to-infinitive phrase (e.g., he convinced me to go to the party).This information prevents the system from deleting the &quot;of&quot; prepositional phrase or the to-infinitive that is part of the verb phrase.At the end of this step, each node in the parse tree — including both leaf nodes and intermediate nodes — is annotated with a value indicating whether it is grammatically obligatory.Note that whether a node is obligatory is relative to its parent node only.For example, whether a determiner is obligatory is relative to the noun phrase it is in; whether a prepositional phrase is obligatory is relative to the sentence or the phrase it is in.Step 3: Context information.In this step, the system decides which components in the sentence are most related to the main topic being discussed.To measure the importance of a phrase in the local context, the system relies on lexical links between words.The hypothesis is that the more connected a word is with other words in the local context, the more likely it is to be the focus of the local context.We link the words in the extracted sentence with words in its local context, if they are repetitions, morphologically related, or linked in WordNet through one of the lexical relations.The system then computes an importance score for each word in the extracted sentence, based on the number of links it has with other words and the types of links.The formula for computing the context importance score for a word w is as follows: Here, i represents the different types of lexical relations the system considered, including repetition, inflectional relation, derivational relation, and the lexical relations from WordNet.We assigned a weight to each type of lexical relation, represented by Li in the formula.Relations such as repetition or inflectional relation are considered more important and are assigned higher weights, while relations such as hypernym are considered less important and assigned lower weights.NU (w) in the formula represents the number of a particular type of lexical links the word w has with words in the local context.After an importance score is computed for each word, each phrase in the 'sentence gets a score by adding up the scores of its children nodes in the parse tree.This score indicates how important the phrase is in the local context.Step 4: Corpus evidence.The program uses a corpus consisting of sentences reduced by human professionals and their corresponding original sentences to compute how likely humans remove a certain phrase.The system first parsed the sentences in the corpus using ESG parser.It then marked which subtrees in these parse trees (i.e., phrases in the sentences) were removed by humans.Using this corpus of marked parse trees, we can compute how likely a subtree is removed from its parent node.For example, we can compute the probability that the &quot;when&quot; temporal clause is removed when the main verb is &quot;give&quot;, represented as Prob(&quot;when-clause is removed&quot; I &quot;v=give&quot;), or the probability that the to-infinitive modifier of the head noun &quot;device&quot; is removed, represented as Prob(&quot;to-infinitive modifier is removed&quot; I&quot;n=device&quot;).These probabilities are computed using Bayes's rule.For example, the probability that the &quot;when&quot; temporal clause is removed when the main verb is &quot;give&quot;, Prob(&quot;when-clause is removed&quot; I &quot;v=give&quot;), is computed as the product of Prob( &quot;v=give&quot; I &quot;when-clause is removed&quot;) (i.e., the probability that the main verb is &quot;give&quot; when the &quot;when&quot; clause is removed) and Prob(&quot;when-clause is removed&quot;) (i.e., the probability that the &quot;when&quot; clause is removed), divided by Prob(&quot;v=give&quot;) (i.e., the probability that the main verb is &quot;give&quot;).Besides computing the probability that a phrase is removed, we also compute two other types of probabilities: the probability that a phrase is reduced (i.e., the phrase is not removed as a whole, but some components in the phrase are removed), and the probability that a phrase is unchanged at all (i.e., neither removed nor reduced).These corpus probabilities help us capture human practice.For example, for sentences like &quot;The agency reported that ...&quot; , &quot;The other source says that ...&quot; , &quot;The new study suggests that ...&quot; , the thatclause following the say-verb (i.e., report, say, and suggest) in each sentence is very rarely changed at all by professionals.The system can capture this human practice, since the probability that that-clause of the verb say or report being unchanged at all will be relatively high, which will help the system to avoid removing components in the that-clause.These corpus probabilities are computed beforehand using a training corpus.They are then stored in a table and loaded at running time.Step 5: Final Decision.The final reduction decisions are based on the results from all the earlier steps.To decide which phrases to remove, the system traverses the sentence parse tree, which now have been annotated with different types of information from earlier steps, in the top-down order and decides which subtrees should be removed, reduced or unchanged.A subtree (i.e., a phrase) is removed only if it is not grammatically obligatory, not the focus of the local context (indicated by a low importance score), and has a reasonable probability of being removed by humans.Figure 1 shows sample output of the reduction program.The reduced sentences produced by humans are also provided for comparison.We define a measure called success rate to evaluate the performance of our sentence reduction program.Original sentence : When it arrives sometime next year in new TV sets, the V-chip will give parents a new and potentially revolutionary device to block out programs they don't want their children to see.The success rate computes the percentage of system's reduction decisions that agree with those of humans.We compute the success rate in the following way.The reduction process can be considered as a series of decision-making process along the edges of a sentence parse tree.At each node of the parse tree, both the human and the program make a decision whether to remove the node or to keep it.If a node is removed, the subtree with that node as the root is removed as a whole, thus no decisions are needed for the descendants of the removed node.If the node is kept, we consider that node as the root and repeat this process.Suppose we have an input sentence (ABCDEFGH), which has a parse tree shown in Figure 2.Suppose a human reduces the sentence to (ABDGH), which can be translated to a series of decisions made along edges in the sentence parse tree as shown in Figure 3.The symbol &quot;y&quot; along an edge means the node it points to will be kept, and &quot;n&quot; means the node will be removed.Suppose the program reduces the sentence to (BCD), which can be translated similarly to the annotated tree shown in Figure 4.We can see that along five edges (they are D—)T, D—*G, B-4A, B—>C), both the human and the program made decisions.Two out of the five decisions agree (they are D--÷B and D—>E), so the success rate is 2/5 (40%).The success rate is defined as: # of edges along which the human and the program have made the same decision success rate = the total # of edges along which both the human and the progam have made decisions Note that the edges along which only the human or the program has made a decision (e.g., G--F and G—.>F in Figure 3 and Figure 4) are not considered in the computation of success rate, since there is no agreement issue in such cases.In the evaluation, we used 400 sentences in the corpus to compute the probabilities that a phrase is removed, reduced, or unchanged.We tested the program on the rest 100 sentences.Using five-fold validation (i.e., chose different 100 sentences for testing each time and repeating the experiment five times), The program achieved an average success rate of 81.3%.If we consider the baseline as removing all the prepositional phrases, clauses, to-infinitives and gerunds, the baseline performance is 43.2%.We also computed the success rate of program's decisions on particular types of phrases.For the decisions on removing or keeping a clause, the system has a success rate of 78.1%; for the decisions on removing or keeping a to-infinitive, the system has a success rate of 85.2%.We found out that the system has a low success rate on removing adjectives of noun phrases or removing adverbs of a sentence or a verb phrase.One reason for this is that our probability model can hardly capture the dependencies between a particular adjective and the head noun since the training corpus is not large enough, while the other sources of information, including grammar or context information, provide little evidence on whether an adjective or an adverb should be removed.Given that whether or not an adjective or an adverb is removed does not affect the conciseness of the sentence significantly and the system lacks of reliability in making such decisions, we decide not to remove adjectives and adverbs.On average, the system reduced the length of the 500 sentence by 32.7% (based on the number of words), while humans reduced it by 41.8%.The probabilities we computed from the training corpus covered 58% of instances in the test corpus.When the corpus probability is absent for a case, the system makes decisions based on the other two sources of knowledge.Some of the errors made by the system result from the errors by the syntactic parser.We randomly checked 50 sentences, and found that 8% of the errors made by the system are due to parsing errors.There are two main reasons responsible for this relative low percentage of errors resulted from mistakes in parsing.One reason is that we have taken some special measures to avoid errors introduced by mistakes in parsing.For example, PP attachment is a difficult problem in parsing and it is not rare that a PP is wrongly attached.Therefore, we take this into account when marking the obligatory components using subcategorization knowledge from the lexicon (step 2) — we not only look at the PPs that are attached to a verb phrase, but also PPs that are next to the verb phrase but not attached, in case it is part of the verb phrase.We also wrote a preprocessor to deal with particular structures that the parser often has problems with, such as appositions.The other reason is that parsing errors do not always result in reduction errors.For example, given a sentence &quot;The spokesperson of the University said that ...&quot;, although that-clause in the sentence may have a complicated structure and the parser gets it wrong, the reduction system is not necessarily affected since it may decide in this case to keep that-clause as it is, as humans often do, so the parsing errors will not matter in this example.The reduction algorithm we present assumes generic summarization; that is, we want to generate a summary that includes the most important information in an article.We can tailor the reduction system to queries-based summarization.In that case, the task of the reduction is not to remove phrases that are extraneous in terms of the main topic of an article, but phrases that are not very relevant to users' queries.We extended our sentence reduction program to query-based summarization by adding another step in the algorithm to measure the relevance of users' queries to phrases in the sentence.In the last step of reduction when the system makes the final decision, the relevance of a phrase to the query is taken into account, together with syntactic, context, and corpus information.Ideally, the sentence reduction module should interact with other modules in a summarization system.It should be able to send feedback to the extraction module if it finds that a sentence selected by the extraction module may be inappropriate (for example, having a very low context importance score).It should also be able to interact with the modules that run after it, such as the sentence combination module, so that it can revise reduction decisions according to the feedback from these modules.Some researchers suggested removing phrases or clauses from sentences for certain applications.(Grefenstette, 1998) proposed to remove phrases in sentences to produce a telegraphic text that can be used to provide audio scanning service for the blind.(Corston-Oliver and Dolan, 1999) proposed to remove clauses in sentences before indexing documents for Information Retrieval.Both studies removed phrases based only on their syntactic categories, while the focus of our system is on deciding when it is appropriate to remove a phrase.Other researchers worked on the text simplification problem, which usually involves in simplifying text but not removing any phrases.For example, (Carroll et al., 1998) discussed simplifying newspaper text by replacing uncommon words with common words, or replacing complicated syntactic structures with simpler structures to assist people with reading disabilities.(Chandrasekar et al., 1996) discussed text simplification in general.The difference between these studies on text simplification and our system is that a text simplification system usually does not remove anything from an original sentence, although it may change its structure or words, but our system removes extraneous phrases from the extracted sentences.We present a novel sentence reduction system which removes extraneous phrases from sentences that are extracted from an article in text summarization.The deleted phrases can be prepositional phrases, clauses, to-infinitives, or gerunds, and multiple phrases can be removed form a single sentence.The focus of this work is on determining, for a sentence in a particular context, which phrases in the sentence are less important and can be removed.Our system makes intelligent reduction decisions based on multiple sources of knowledge, including syntactic knowledge, context, and probabilities computed from corpus analysis.We also created a corpus consisting of 500 sentences and their reduced forms produced by human professionals, and used this corpus for training and testing the system.The evaluation shows that 81.3% of reduction decisions made by the system agreed with those of humans.In the future, we would like to integrate our sentence reduction system with extraction-based summarization systems other than the one we have developed, improve the performance of the system further by introducing other sources of knowledge necessary for reduction, and explore other interesting applications of the reduction system.This material is based upon work supported by the National Science Foundation under Grant No.IRI 96-19124 and IRI 96-18797.Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.
Does Baum-Welch Re-Estimation Help Taggers?In part of speech tagging by Hidden Markov Model, a statistical model is used to assign grammatical categories to words in a text.Early work in the field relied on a corpus which had been tagged by a human annotator to train the model.recently, Cutting al. suggest that training can be achieved with a minimal lexicon and a limited amount priori about probabilities, by using an Baum-Welch re-estimation to automatically refine the model.In this paper, I report two experiments designed to determine how much manual training information is needed.The first experiment suggests that initial biasing of either lexical or transition probabilities is essential to achieve a good accuracy.The second experiment reveals that there are three distinct patterns of Baum-Welch reestimation.In two of the patterns, the re-estimation ultimately reduces the accuracy of the tagging rather than improving it.The pattern which is applicable can be predicted from the quality of the initial model and the similarity between the tagged training corpus (if any) and the corpus to be tagged.Heuristics for deciding how to use re-estimation in an effective manner are given.The conclusions are broadly in agreement with those of Merialdo (1994), but give greater detail about the contributions of different parts of the model.1 Background Part-of-speech tagging is the process of assigning grammatical categories to individual words in a corpus.One widely used approach makes use of a statistical technique called a Hidden Markov Model (HMM).The model is defined by two collections of the probabilities, express the probability that a tag follows the preceding (or two for a second order model); and the the probability that a word has a given tag without regard to words on either side of it.To tag a text, the tags with non-zero probability are hypothesised for each word, and the most probable sequence of tags given the sequence of words is determined from the probabilities.Two algorithms are commonly used, known as the Forward-Backward (FB) and Viterbi algorithms.FB assigns a probability to every tag on every word, while Viterbi prunes tags which cannot be chosen because their probability is lower than the ones of competing hypotheses, with a corresponding gain in computational efPart-of-speech tagging is the process of assigning grammatical categories to individual words in a corpus.One widely used approach makes use of a statistical technique called a Hidden Markov Model (HMM).The model is defined by two collections of parameters: the transition probabilities, which express the probability that a tag follows the preceding one (or two for a second order model); and the lexical probabilities, giving the probability that a word has a given tag without regard to words on either side of it.To tag a text, the tags with non-zero probability are hypothesised for each word, and the most probable sequence of tags given the sequence of words is determined from the probabilities.Two algorithms are commonly used, known as the Forward-Backward (FB) and Viterbi algorithms.FB assigns a probability to every tag on every word, while Viterbi prunes tags which cannot be chosen because their probability is lower than the ones of competing hypotheses, with a corresponding gain in computational efficiency.For an introduction to the algorithms, see Cutting et at.(1992), or the lucid description by Sharman (1990).There are two principal sources for the parameters of the model.If a tagged corpus prepared by a human annotator is available, the transition and lexical probabilities can be estimated from the frequencies of pairs of tags and of tags associated with words.Alternatively, a procedure called BaumWelch (BW) re-estimation may be used, in which an untagged corpus is passed through the FB algorithm with some initial model, and the resulting probabilities used to determine new values for the lexical and transition probabilities.By iterating the algorithm with the same corpus, the parameters of the model can be made to converge on values which are locally optimal for the given text.The degree of convergence can be measured using a perplexity measure, the sum of plog2p for hypothesis probabilities p, which gives an estimate of the degree of disorder in the model.The algorithm is again described by Cutting et ad. and by Sharman, and a mathematical justification for it can be found in Huang et at.(1990).The first major use of HMMs for part of speech tagging was in CLAWS (Garside et a/., 1987) in the 1970s.With the availability of large corpora and fast computers, there has been a recent resurgence of interest, and a number of variations on and alternatives to the FB, Viterbi and BW algorithms have been tried; see the work of, for example, Church (Church, 1988), Brill (Brill and Marcus, 1992; Brill, 1992), DeRose (DeRose, 1988) and Kupiec (Kupiec, 1992).One of the most effective taggers based on a pure HMM is that developed at Xerox (Cutting et al., 1992).An important aspect of this tagger is that it will give good accuracy with a minimal amount of manually tagged training data.96% accuracy correct assignment of tags to word token, compared with a human annotator, is quoted, over a 500000 word corpus.The Xerox tagger attempts to avoid the need for a hand-tagged training corpus as far as possible.Instead, an approximate model is constructed by hand, which is then improved by BW re-estimation on an untagged training corpus.In the above example, 8 iterations were sufficient.The initial model set up so that some transitions and some tags in the lexicon are favoured, and hence having a higher initial probability.Convergence of the model is improved by keeping the number of parameters in the model down.To assist in this, low frequency items in the lexicon are grouped together into equivalence classes, such that all words in a given equivalence class have the same tags and lexical probabilities, and whenever one of the words is looked up, then the data common to all of them is used.Re-estimation on any of the words in a class therefore counts towards re-estimation for all of them'.The results of the Xerox experiment appear very encouraging.Preparing tagged corpora either by hand is labour-intensive and potentially error-prone, and although a semi-automatic approach can be used (Marcus et al., 1993), it is a good thing to reduce the human involvement as much as possible.However, some careful examination of the experiment is needed.In the first place, Cutting et a/. do not compare the success rate in their work with that achieved from a hand-tagged training text with no re-estimation.Secondly, it is unclear how much the initial biasing contributes the success rate.If significant human intervention is needed to provide the biasing, then the advantages of automatic training become rather weaker, especially if such intervention is needed on each new text domain.The kind of biasing Cutting et a/. describe reflects linguistic insights combined with an understanding of the predictions a tagger could reasonably be expected to make and the ones it could not.The aim of this paper is to examine the role that training plays in the tagging process, by an experimental evaluation of how the accuracy of the tagger varies with the initial conditions.The results suggest that a completely unconstrained initial model does not produce good quality results, and that one 'The technique was originally developed by Kupiec (Kupiec, 1989). accurately trained from a hand-tagged corpus will generally do better than using an approach based on re-estimation, even when the training comes from a different source.A second experiment shows that there are different patterns of re-estimation, and that these patterns vary more or less regularly with a broad characterisation of the initial conditions.The outcome of the two experiments together points to heuristics for making effective use of training and reestimation, together with some directions for further research.Work similar to that described here has been carried out by Merialdo (1994), with broadly similar conclusions.We will discuss this work below.The principal contribution of this work is to separate the effect of the lexical and transition parameters of the model, and to show how the results vary with different degree of similarity between the training and test data.The experiments were conducted using two taggers, one written in C at Cambridge University Computer Laboratory, and the other in C++ at Sharp Laboratories.Both taggers implement the FB, Viterbi and BW algorithms.For training from a hand-tagged corpus, the model is estimated by counting the number of transitions from each tag i to each tag j, the total occurrence of each tag i, and the total occurrence of word w with tag i.Writing these as f(i, j), f(i) and f(i, w) respectively, the transition probability from tag i to tag j is estimated as f (i, j)/ f (i) and the lexical probability as f(i, w)/ f (i).Other estimation formulae have been used in the past.For example, CLAWS (Garside et al., 1987) normalises the lexical probabilities by the total frequency of the word rather than of the tag.Consulting the BaumWelch re-estimation formulae suggests that the approach described is more appropriate, and this is confirmed by slightly greater tagging accuracy.Any transitions not seen in the training corpus are given a small, non-zero probability.The lexicon lists, for each word, all of tags seen in the training corpus with their probabilities.For words not found in the lexicon, all open-class tags are hypothesised. with equal probabilities.These words are added to the lexicon at the end of first iteration when re-estimation is being used, so that the probabilities of their hypotheses subsequently diverge from being uniform.To measure the accuracy of the tagger, we compare the chosen tag with one provided by a human annotator.Various methods of quoting accuracy have been used in the literature, the most common being the proport ion of words (tokens) receiving the correct tag.A better measure is the proportion of ambiguous words which are given the correct tag, where by ambiguous we mean that more than one tag was hypothesised.The former figure looks more impressive, but the latter gives a better measure of how well the tagger is doing, since it factors out the trivial assignment of tags to non-ambiguous words.For a corpus in which a fraction a of the words are ambiguous, and p is the accuracy on ambiguous words, the overall accuracy can be recovered from 1 — a + pa. All of the accuracy figures quoted below are for ambiguous words only.The training and test corpora were drawn from the LOB corpus and the Penn treebank.The hand tagging of these corpora is quite different.For example, the LOB tagset used 134 tags, while the Penn treebank tagset has 48.The general pattern of the results presented does not vary greatly with the corpus and tagset used.The first experiment concerned the effect of the initial conditions on the accuracy using Baum-Welch re-estimation.A model was trained from a handtagged corpus in the manner described above, and then degraded in various ways to simulate the effect of poorer training, as follows:DO Un-degraded lexical probabilities, calculated from f (i, w) / f (i).D1 Lexical probabilities are correctly ordered, so that the most frequent tag has the highest lexical probability and so on, but the absolute values are otherwise unreliable.D2 Lexical probabilities are proportional to the overall tag frequencies, and are hence independent of the actual occurrence of the word in the training corpus.D3 All lexical probabilities have the same value, so that the lexicon contains no information other than the possible tags for each word.TO Un-degraded transition probabilities, calculated from f (i, j)/ f (i).Ti All transition probabilities have the same value.We could expect to achieve D1 from, say, a printed dictionary listing parts of speech in order of frequency.Perfect training is represented by case DO+TO.The Xerox experiments (Cutting et a/., 1992) correspond to something between D1 and D2, and between TO and Ti, in that there is some initial biasing of the probabilities.For the test, four corpora were constructed from the LOB corpus: LOB-B from part B, LOB-L from part L, LOB-B-G from parts B to G inclusive and LOB-l3-3 from parts B to J inclusive.Corpus LOBB-J was used to train the model, and LOB-B, LOBL and LOB-B-G were passed through thirty iterations of the BW algorithm as untagged data.In each case, the best accuracy (on ambiguous words, as usual) from the FB algorithm was noted.As an additional test, we tried assigning the most probable tag from the DO lexicon, completely ignoring tag-tag transitions.The results are summarised in table 1, for various corpora, where F denotes the &quot;most frequent tag&quot; test.As an example of how these figures relate to overall accuracies, LOB-B contains 32.35% ambiguous tokens with respect to the lexicon from LOB-B-J, and the overall accuracy in the DO+TO case is hence 98.69%.The general pattern of the results is similar across the three test corpora, with the only difference of interest being that case D3+TO does better for LOB-L than for the other two cases, and in particular does better than cases DO+T1 and Dl+Tl.A possible explanation is that in this case the test data does not overlap with the training data, and hence the good quality lexicons (DO and D1) have less of an influence.It is also interesting that D3+T1 does better than D2-FT1.The reasons for this are unclear, and the results are not always the same with other corpora, which suggests that they are not statistically significant.Several follow-up experiments were used to confirm the results: using corpora from the Penn treebank, using equivalence classes to ensure that all lexical entries have a total relative frequency of at least 0.01, and using larger corpora.The specific accuracies were different in the various tests, but the overall patterns remained much the same, suggesting that they are not an artifact of the tagset or of details of the text.The observations we can make about these results are as follows.Firstly, two of the tests, D2+T1 and D3-1-T1, give very poor performance.Their accuracy is not even as good as that achieved by picking the most frequent tag (although this of course implies a lexicon of DO or D1 quality).It follows that if BaumWelch re-estimation is to be an effective technique, the initial data must have either biasing in the transitions (the TO cases) or in the lexical probabilities (cases DO+T1 and D1-FT1), but it is not necessary to have both (D2/D3+TO and DO/Did-T1).Secondly, training from a hand-tagged corpus (case DO+TO) always does best, even when the test data is from a different source to the training data, as it is for LOB-L.So perhaps it is worth investing effort in hand-tagging training corpora after all, rather than just building a lexicon and letting reestimation sort out the probabilities.But how can we ensure that re-estimation will produce a good quality model?We look further at this issue in the next section.During the first experiment, it became apparent that Baum-Welch re-estimation sometimes decreases the accuracy as the iteration progresses.A second experiment was conducted to decide when it is appropriate to use Baum-Welch re-estimation at all.There seem to be three patterns of behaviour: Classical A general trend of rising accuracy on each iteration, with any falls in accuracy being local.It indicates that the model is converging towards an optimum which is better than its starting point.Initial maximum Highest accuracy on the first iteration, and falling thereafter.In this case the initial model is of better quality than BW can achieve.That is, while BW will converge on an optimum, the notion of optimality is with respect to the HMM rather than to the linguistic judgements about correct tagging.Early maximum Rising accuracy for a small number of iterations (2-4), and then falling as in initial maximum.An example of each of the three behaviours is shown in figure 1.The values of the accuracies and the test conditions are unimportant here; all we want to show is the general patterns.The second experiment had the aim of trying to discover which pattern applies under which circumstances, in order to help decide how to train the model.Clearly, if the expected pattern is initial maximum, we should not use BW at all, if early maximum, we should halt the process after a few iterations, and if classical, we should halt the process in a &quot;standard&quot; way, such as comparing the perplexity of successive models.The tests were conducted in a similar manner to those of the first experiment, by building a lexicon and transitions from a hand tagged training corpus, and then applying them to a test corpus with varying degrees of degradation.Firstly, four different degrees of degradation were used: no degradation at all, D2 degradation of the lexicon, Ti degradation of the transitions, and the two together.Secondly, we selected test corpora with varying degrees of similarity to the training corpus: the same text, text from a similar domain, and text which is significantly different.Two tests were conducted with each combination of the degradation and similarity, using different corpora (from the Penn treebank) ranging in size from approximately 50000 words to 500000 words.The re-estimation was allowed to run for ten iterations.The results appear in table 2, showing the best accuracy achieved (on ambiguous words). the iteration at which it occurred, and the pattern of re-estimation (I = initial maximum, E = early maximum, C = classical).The patterns are summarised in table 3, each entry in the table showing the patterns for the two tests under the given conditions.Although there is some variations in the readings, for example in the &quot;similar/DO+TO&quot; case, we can draw some general conclusions about the patterns obtained from different sorts of data.When the lexicon is degraded (D2), the pattern is always classical.With a good lexicon but either degraded transitions or a test corpus differing from the training corpus, the pattern tends to be early maximum.When the test corpus is very similar to the model, then the pattern is initial maximum.Furthermore, examining the accuracies in table 2, in the cases of initial maximum and early maximum, the accuracy tends to be significantly higher than with classical behaviour.It seems likely that what is going on is that the model is converging to towards something of similar &quot;quality&quot; in each case, but when the pattern is classical, the convergence starts from a lower quality model and improves, and in the other cases, it starts from a higher quality one and deteriorates.In the case of early maximum, the few iterations where the accuracy is improving correspond to the creation of entries for unknown words and the fine tuning of ones for known ones, and these changes outweigh those produced by the re-estimation.From the observations in the previous section, we propose the following guidelines for how to train a HMM for use in tagging: able, use BW re-estimation with standard convergence tests such as perplexity.Without a lexicon, some initial biasing of the transitions is needed if good results are to be obtained.Similar results are presented by Merialdo (1994), who describes experiments to compare the effect of training from a hand-tagged corpora and using the Baum-Welch algorithm with various initial conditions.As in the experiments above, BW reestimation gave a decrease in accuracy when the starting point was derived from a significant amount of hand-tagged text.In addition, although Merialdo does not highlight the point, BW re-estimation starting from less than 5000 words of hand-tagged text shows early maximum behaviour.Merialdo's conclusion is that taggers should be trained using as much hand-tagged text as possible to begin with, and only then applying BW re-estimation with untagged text.The step forward taken in the work here is to show that there are three patterns of reestimation behaviour, with differing guidelines for how to use BW effectively, and that to obtain a good starting point when a hand-tagged corpus is not available or is too small, either the lexicon or the transitions must be biased.While these may be useful heuristics from a practical point of view, the next step forward is to look for an automatic way of predicting the accuracy of the tagging process given a corpus and a model.Some preliminary experiments with using measures such as perplexity and the average probability of hypotheses show that, while they do give an indication of convergence during re-estimation, neither shows a strong correlation with the accuracy.Perhaps what is needed is a &quot;similarity measure&quot; between two models M and M', such that if a corpus were tagged with model M, M' is the model obtained by training from the output corpus from the tagger as if it were a hand-tagged corpus.However, preliminary experiments using such measures as the Kullback-Liebler distance between the initial and new models have again showed that it does not give good predictions of accuracy.In the end it may turn out there is simply no way of making the prediction without a source of information extrinsic to both model and corpus.The work described here was carried out at the Cambridge University Computer Laboratory as part of Esprit BR Project 7315 &quot;The Acquisition of Lexical Knowledge&quot; (Acquilex-II).The results were confirmed and extended at Sharp Laboratories of Europe.I thank Ted Briscoe for his guidance and advice, and the AN LP referees for their comments.
The LinGO Redwoods Treebank Motivation and Preliminary Applications Stephan Oepen, Kristina Toutanova, Stuart Shieber, Christopher Manning, Dan Flickinger, and Thorsten Brants {oe |kristina |manning |dan}@csli.stanford.edu, shieber@deas.harvard.edu, brants@parc.xerox.com Abstract The LinGO Redwoods initiative is a seed activity in the de- sign and development of a new type of treebank.While sev- eral medium- to large-scale treebanks exist for English (and for other major languages), pre-existing publicly available re- sources exhibit the following limitations: (i) annotation is mono-stratal, either encoding topological (phrase structure) or tectogrammatical (dependency) information, (ii) the depth of linguistic information recorded is comparatively shallow, (iii) the design and format of linguistic representation in the tree- bank hard-wires a small, predefined range of ways in which information can be extracted from the treebank, and (iv) rep- resentations in existing treebanks are static and over the (often year- or decade-long) evolution of a large-scale treebank tend to fall behind the development of the field.LinGO Redwoods aims at the development of a novel treebanking methodology, rich in nature and dynamic both in the ways linguistic data can be retrieved from the treebank in varying granularity and in the constant evolution and regular updating of the treebank itself.Since October 2001, the project is working to build the foun- dations for this new type of treebank, to develop a basic set of tools for treebank construction and maintenance, and to con- struct an initial set of 10,000 annotated trees to be distributed together with the tools under an open-source license.1 Why Another (Type of) Treebank?For the past decade or more, symbolic, linguistically ori- ented methods and statistical or machine learning ap- proaches to NLP have often been perceived as incompat- ible or even competing paradigms.While shallow and probabilistic processing techniques have produced use- ful results in many classes of applications, they have not met the full range of needs for NLP, particularly where precise interpretation is important, or where the variety of linguistic expression is large relative to the amount of training data available.On the other hand, deep approaches to NLP have only recently achieved broad enough grammatical coverage and sufficient processing efficiency to allow the use of precise linguistic grammars in certain types of real-world applications.In particular, applications of broad-coverage analyti- cal grammars for parsing or generation require the use of sophisticated statistical techniques for resolving ambigu- ities; the transfer of Head-Driven Phrase Structure Gram- mar (HPSG) systems into industry, for example, has am- plified the need for general parse ranking, disambigua- tion, and robust recovery techniques.We observe general consensus on the necessity for bridging activities, com- bining symbolic and stochastic approaches to NLP.But although we find promising research in stochastic pars- ing in a number of frameworks, there is a lack of appro- priately rich and dynamic language corpora for HPSG.Likewise, stochastic parsing has so far been focussed on information-extraction-type applications and lacks any depth of semantic interpretation.The Redwoods initia- tive is designed to fill in this gap.In the next section, we present some of the motivation for the LinGO Redwoods project as a treebank develop- ment process.Although construction of the treebank is in its early stages, we present in Section 3 some prelim- inary results of using the treebank data already acquired on concrete applications.We show, for instance, that even simple statistical models of parse ranking trained on the Redwoods corpus built so far can disambiguate parses with close to 80% accuracy.2 A Rich and Dynamic Treebank The Redwoods treebank is based on open-source HPSG resources developed by a broad consortium of re- search groups including researchers at Stanford (USA), Saarbru?cken (Germany), Cambridge, Edinburgh, and Sussex (UK), and Tokyo (Japan).Their wide distribution and common acceptance make the HPSG framework and resources an excellent anchor point for the Redwoods treebanking initiative.The key innovative aspect of the Redwoods ap- proach to treebanking is the anchoring of all linguis- tic data captured in the treebank to the HPSG frame- work and a generally-available broad-coverage gram- mar of English, the LinGO English Resource Grammar (Flickinger, 2000) as implemented with the LKB gram- mar development environment (Copestake, 2002).Un- like existing treebanks, there is no need to define a (new) form of grammatical representation specific to the tree- bank.Instead, the treebank records complete syntacto- semantic analyses as defined by the LinGO ERG and pro- vide tools to extract different types of linguistic informa- tion at varying granularity.The treebanking environment, building on the [incr tsdb()] profiling environment (Oepen & Callmeier, 2000), presents annotators, one sentence at a time, with the full set of analyses produced by the grammar.Using a pre-existing tree comparison tool in the LKB (similar in kind to the SRI Cambridge TreeBanker; Carter, 1997), annotators can quickly navigate through the parse for- est and identify the correct or preferred analysis in the current context (or, in rare cases, reject all analyses pro- posed by the grammar).The tree selection tool presents users, who need little expert knowledge of the underly- ing grammar, with a range of basic properties that distin- guish competing analyses and that are relatively easy to judge.All disambiguating decisions made by annotators are recorded in the [incr tsdb()] database and thus become available for (i) later dynamic extraction from the anno- tated profile or (ii) dynamic propagation into a more re- cent profile obtained from re-running a newer version of the grammar on the same corpus.Important innovative research aspects in this approach to treebanking are (i) enabling users of the treebank to extract information of the type they need and to trans- form the available representation into a form suited to their needs and (ii) the ability to update the treebank with an enhanced version of the grammar in an automated fashion, viz.by re-applying the disambiguating decisions on the corpus with an updated version of the grammar.Depth of Representation and Transformation of In- formation Internally, the [incr tsdb()] database records analyses in three different formats, viz.(i) as a deriva- tion tree composed of identifiers of lexical items and con- structions used to build the analysis, (ii) as a traditional phrase structure tree labeled with an inventory of some fifty atomic labels (of the type ?S?, ?NP?, ?VP?), and (iii) as an underspecified MRS (Copestake, Lascarides, & Flickinger, 2001) meaning representation.While rep- resentation (ii) will in many cases be similar to the rep- resentation found in the Penn Treebank, representation (iii) subsumes the functor ?argument (or tectogrammati- cal) structure advocated in the Prague Dependency Tree- bank or the German TiGer corpus.Most importantly, however, representation (i) provides all the information required to replay the full HPSG analysis (using the orig- inal grammar and one of the open-source HPSG process- ing environments, e.g., the LKB or PET, which already have been interfaced to [incr tsdb()]).Using the latter ap- proach, users of the treebank are enabled to extract infor- mation in whatever representation they require, simply by reconstructing full analyses and adapting the exist- ing mappings (e.g., the inventory of node labels used for phrase structure trees) to their needs.Likewise, the ex- isting [incr tsdb()] facilities for comparing across compe- tence and performance profiles can be deployed to evalu- ate results of a (stochastic) parse disambiguation system, essentially using the preferences recorded in the treebank as a ?gold standard?target for comparison.Automating Treebank Construction Although a pre- cise HPSG grammar like the LinGO ERG will typically assign a small number of analyses to a given sentence, choosing among a few or sometimes a few dozen read- ings is time-consuming and error-prone.The project is exploring two approaches to automating the disambigua- tion task, (i) seeding lexical selection from a part-of- speech (POS) tagger and (ii) automated inter-annotator comparison and assisted resolution of conflicts.Treebank Maintenance and Evolution One of the challenging research aspects of the Redwoods initiative is about developing a methodology for automated up- dates of the treebank to reflect the continuous evolution of the underlying linguistic framework and of the LinGO grammar.Again building on the notion of elementary linguistic discriminators, we expect to explore the semi- automatic propagation of recorded disambiguating deci- sions into newer versions of the parsed corpus.While it can be assumed that the basic phrase structure inven- tory and granularity of lexical distinctions have stabilized to a certain degree, it is not guaranteed that one set of discriminators will always fully disambiguate a more re- cent set of analyses for the same utterance (as the gram- mar may introduce new ambiguity), nor that re-playing a history of disambiguating decisions will necessarily identify the correct, preferred analysis for all sentences.A better understanding of the nature of discriminators and relations holding among them is expected to provide the foundations for an update procedure that, ultimately, should be mostly automated, with minimal manual in- spection, and which can become part of the regular re- gression test cycle for the grammar.Scope and Current State of Seeding Initiative The first 10,000 trees to be hand-annotated as part of the kick-off initiative are taken from a domain for which the English Resource Grammar is known to exhibit broad and accurate coverage, viz.transcribed face-to-face dia- logues in an appointment scheduling and travel arrange- ment domain.1 For the follow-up phase of the project, it is expected to move into a second domain and text genre, presumably more formal, edited text taken from newspa- per text or another widely available on-line source.As of June 2002, the seeding initiative is well underway.The integrated treebanking environment, combining [incr tsdb()] and the LKB tree selection tool, has been estab- lished and has been deployed in a first iteration of anno- tating the VerbMobil utterances.The approach to parse selection through minimal discriminators turned out to be not hard to learn for a second-year Stanford under- graduate in linguistics, and allowed completion of the first iteration in less than ten weeks.Table 1 summarizes the current Redwoods status.1Corpora of some 50,000 such utterances are readily available from the VerbMobil project (Wahlster, 2000) and have already been studied extensively among researchers world-wide.2Of the four data sets only VM32 has been double-checked by an expert grammarian and (almost) completely disambiguated to date; therefore it exhibits an interestingly higher degree of phrasal ambiguity in the ?active = 1?total active = 0 active = 1 active > 1 unannotated corpus ] ?VM6 2422 7?7 4?2 32?9 218 8?0 4?4 9?7 1910 7?0 4?0 7?5 80 10?0 4?8 23?8 214 14?9 4?3 287?5 VM13 1984 8?5 4?0 37?9 175 8?5 4?1 9?9 1491 7?2 3?9 7?5 85 9?9 4?5 22?1 233 14?1 4?2 22?1 VM31 1726 6?2 4?5 22?4 164 7?9 4?6 8?0 1360 6?6 4?5 5?9 61 10?1 4?2 14?5 141 13?5 4?7 201?5 VM32 608 7?4 4?3 25?6 51 10?7 4?3 54?4 551 7?9 4?4 19?0 5 12?2 3?9 27?2 1 21?0 6?1 2220?0 Table 1: Redwoods development status as of June 2002: four sets of transcribed and hand-segmented VerbMobil dialogues have been annotated.The columns are, from left to right, the total number of sentences (excluding fragments) for which the LinGO grammar has at least one analysis (?]?), average length (???), lexical and structural ambiguity (??, respectively), followed by the last four metrics broken down for the following subsets: sentences (i) for which the annotator rejected all analyses (no active trees), (ii) where annotation resulted in exactly one preferred analysis (one active tree), (iii) those where full disambiguation was not accomplished through the first round of annotation (more than one active tree), and (iv) massively ambiguous sentences that have yet to be annotated.2 3 Early Experimental Results Development of the treebank has just started.Nonethe- less, we have performed some preliminary experiments on concrete applications to motivate the utility of the re- source being developed.In this section, we describe ex- periments using the Redwoods treebank to build and test systems for parse disambiguation.As a component, we build a tagger for the HPSG lexical tags in the treebank, and report results on this application as well.Any linguistic system that allows multiple parses of strings must address the problem of selecting from among the admitted parses the preferred one.A variety of approaches for building statistical models of parse se- lection are possible.At the simplest end, we might look only at the lexical type sequence assigned to the words by each parse and rank the parse based on the likelihood of that sequence.These lexical types ?the preterminals in the derivation ?are essentially part-of-speech tags, but encode considerably finer-grained information about the words.Well-understood statistical part-of-speech tag- ging technology is sufficient for this approach.In order to use more information about the parse, we might examine the entire derivation of the string.Most probabilistic parsing research ?including, for ex- ample, work by by Collins (1997), and Charniak (1997) ?is based on branching process models (Harris, 1963).The HPSG derivations that the treebank makes available can be viewed as just such a branching process, and a stochastic model of the trees can be built as a prob- abilistic context-free grammar (PCFG) model.Abney (1997) notes important problems with the soundness of the approach when a unification-based grammar is ac- tually determining the derivations, motivating the use of log-linear models (Agresti, 1990) for parse ranking that Johnson and colleagues further developed (Johnson, Geman, Canon, Chi, & Riezler, 1999).These models can deal with the many interacting dependencies and the structural complexity found in constraint-based or unification-based theories of syntax.Nevertheless, the naive PCFG approach has the advan- tage of simplicity, so we pursue it and the tagging ap- proach to parse ranking in these proof-of-concept exper- iments (more recently, we have begun work on building log-linear models over HPSG signs (Toutanova & Man- ning, 2002)).The learned models were used to rank possible parses of unseen test sentences according to the probabilities they assign to them.We report parse se- lection performance as percentage of test sentences for which the correct parse was highest ranked by the model.(We restrict attention in the test corpus to sentences that are ambiguous according to the grammar, that is, for which the parse selection task is nontrivial.)We examine four models: an HMM tagging model, a simple PCFG, a PCFG with grandparent annotation, and a hybrid model that combines predictions from the PCFG and the tagger.These models will be described in more detail presently.The tagger that we have implemented is a standard tri- gram HMM tagger, defining a joint probability distribu- tion over the preterminal sequences and yields of these trees.Trigram probabilities are smoothed by linear in- terpolation with lower-order models.For comparison, we present the performance of a unigram tagger and an upper-bound oracle tagger that knows the true tag se- quence and scores highest the parses that have the correct preterminal sequence.The PCFG models define probability distributions over the trees of derivational types corresponding to the HPSG analyses of sentences.A PCFG model has parame- ters ?i, j for each rule Ai ?j in the corresponding con- text free grammar.3 In our application, the nonterminals in the PCFG Ai are rules of the HPSG grammar used to build the parses (such as HEAD-COMPL or HEAD-ADJ).We set the parameters to maximize the likelihood of the set of derivation trees for the preferred parses of the sen- tences in a training set.As noted above, estimating prob- abilities from local tree counts in the treebank does not provide a maximum likelihood estimate of the observed data, as the grammar rules further constrain the possible derivations.Essentially, we are making an assumption of context-freeness of rule application that does not hold in the case of the HPSG grammar.Nonetheless, we can still build the model and use it to rank parses.3For an introduction to PCFG grammars see, for example, Manning & Schu?tze (1999).As previously noted by other researchers (Charniak & Caroll, 1994), extending a PCFG with grandparent an- notation improves the accuracy of the model.We imple- mented an extended PCFG that conditions each node?s expansion on its parent in the phrase structure tree.The extended PCFG (henceforth PCFG-GP) has parameters P(Ak Ai ?The resulting grammar can be viewed as a PCFG whose nonterminals are pairs of the nonterminals of the original PCFG.The combined model scores possible parses using probabilities from the PCFG-GP model together with the probability of the preterminal sequence of the parse tree according to a trigram tag sequence model.More specif- ically, for a tree T , Score(t) = log(PPCFG-GP(T )) + ?log(PTRIG(tags(T )) where PTRIG(tags(T )) is the probability of the sequence of preterminals t1 ?tn in T according to a trigram tag model: PTRIG(t1 ?tn) = ?n i=1 P(ti |ti?1, ti?2) with appropriate treatment of boundaries.The trigram probabilities are smoothed as for the HMM tagger.The combined model is relatively insensitive to the relative weights of the two component models, as specified by ?; in any case, exact optimization of this parameter was not performed.We refer to this model as Combined.The Combined model is not a sound probabilistic model as it does not define a probability distribution over parse trees.It does however provide a crude way to combine ancestor and left context information.The second column in Table 2 shows the accuracy of parse selection using the models described above.For comparison, a baseline showing the expected perfor- mance of choosing parses randomly according to a uni- form distribution is included as the first row.The accu- racy results are averaged over a ten-fold cross-validation on the data set summarized in Table 1.The data we used for this experiment was the set of disambiguated sen- tences that have exactly one preferred parse (comprising a total of 5312 sentences).Often the stochastic models we are considering give the same score to several differ- ent parses.When a model ranks a set of m parses highest with equal scores and one of those parses is the preferred parse in the treebank, we compute the accuracy on this sentence as 1/m.Since our approach of defining the probability of anal- yses using derivation trees is different from the tradi- tional approach of learning PCFG grammars from phrase structure trees, a comparison of the two is probably in order.We tested the model PCFG-GP defined over the corresponding phrase structure trees and its average ac- curacy was 65.65% which is much lower than the accu- racy of the same model over derivation trees (71.73%).This result suggests that the information about grammar constructions is very helpful for parse disambiguation.Method Task tag sel.Random 90.13% 25.81% Tagger unigram 96.75% 44.15% trigram 97.87% 47.74% oracle 100.00% 54.59% PCFG simple 97.40% 66.26% grandparent 97.43% 71.73% combined 98.08% 74.03% Table 2: Performance of the HMM and PCFG models for the tag and parse selection tasks (accuracy).The results in Table 2 indicate that high disambigua- tion accuracy can be achieved using very simple statisti- cal models.The performance of the perfect tagger shows that, informally speaking, roughly half of the information necessary to disambiguate parses is available in the lexi- cal types alone.About half of the remaining information is recovered by our best method, Combined.An alternative (more primitive) task is the tagging task itself.It is interesting to know how much the tagging task can be improved by perfecting parse disambigua- tion.With the availability of a parser, we can examine the accuracy of the tag sequence of the highest scoring parse, rather than trying to tag the word sequence directly.We refer to this problem as the tag selection problem, by analogy with the relation between the parsing problem and the parse selection problem.The first column of Ta- ble 2 presents the performance of the models on the tag selection problem.The results are averaged accuracies over 10 cross-validation splits of the same corpus as the previous experiment, and show that parse disambigua- tion using information beyond the lexical type sequence slightly improves tag selection performance.Note that in these experiments, the models are used to rank the tag sequences of the possible parses and not to find the most probable tag sequence.Therefore tagging accuracy re- sults are higher than they would be in the latter case.Since our corpus has relatively short sentences and low ambiguity it is interesting to see how much the perfor- mance degrades as we move to longer and more highly ambiguous sentences.For this purpose, we report in Ta- ble 3 the parse ranking accuracy of the Combined model as a function of the number of possible analyses for sen- tences.Each row corresponds to a set of sentences with number of possible analyses greater or equal to the bound shown in the first column.For example, the first row con- tains information for the sentences with ambiguity ?2, which is all ambiguous sentences.The columns show the total number of sentences in the set, the expected accu- racy of guessing at random, and the accuracy of the Com- bined model.We can see that the parse ranking accuracy is decreasing quickly and more powerful models will be needed to achieve good accuracy for highly ambiguous sentences.Despite several differences in corpus size and compo- Analyses Sentences Random Combined ?2 3824 25.81% 74.03% ?5 1789 9.66% 59.64% ?10 1027 5.33% 51.61% ?20 525 3.03% 45.33% Table 3: Parse ranking accuracy by number of possible parses.sition, it is perhaps nevertheless useful to compare this work with other work on parse selection for unification- based grammars.(1999) estimate a Stochastic Unification Based Grammar (SUBG) using a log-linear model.The features they include in the model are not limited to production rule features but also ad- junct and argument and other linguistically motivated features.On a dataset of 540 sentences (total training and test set) from a Verbmobil corpus they report parse disambiguation accuracy of 58.7% given a baseline accu- racy for choosing at random of 9.7%.The random base- line is much lower than ours for the full data set, but it is comparable for the random baseline for sentences with more than 5 analyses.The accuracy of our Combined model for these sentences is 59.64%, so the accuracies of the two models seem fairly similar.4 Related Work To the best of our knowledge, no prior research has been conducted exploring the linguistic depth, flexibil- ity in available information, and dynamic nature of tree- banks that we have proposed.Earlier work on building corpora of hand-selected analyses relative to an exist- ing broad-coverage grammar was carried out at Xerox PARC, SRI Cambridge, and Microsoft Research.As all these resources are tuned to proprietary grammars and analysis engines, the resulting treebanks are not publicly available, nor have reported research results been repro- ducible.Yet, especially in light of the successful LinGO open-source repository, it seems vital that both the tree- bank and associated processing schemes and stochastic models be available to the general (academic) public.An on-going initiative at Rijksuniversiteit Groningen (NL) is developing a treebank of dependency structures (Mullen, Malouf, & Noord, 2001), derived from an HPSG-like grammar of Dutch (Bouma, Noord, & Malouf, 2001).The general approach resembles the Redwoods initiative (specifically the discriminator-based method of tree se- lection; the LKB tree comparison tool was originally de- veloped by Malouf, after all), but it provides only a sin- gle stratum of representation, and has no provision for evolving analyses in tandem with the grammar.Dipper (2000) presents the application of a broad-coverage LFG grammar for German to constructing tectogrammatical structures for the TiGer corpus.The approach is similar to the Groningen framework, and shares its limitations.References Abney, S. P. (1997).Stochastic attribute-value grammars.Computational Linguistics, 23, 597 ?Categorical data analysis.John Wiley & Sons.Bouma, G., Noord, G. van, & Malouf, R. (2001).Wide-coverage computational analysis of Dutch.In W. Daelemans, K. Sima-an, J. Veenstra, & J. Zavrel (Eds.), Computational linguistics in the Netherlands (pp.Amsterdam, The Netherlands: Rodopi.Carter, D. (1997).The TreeBanker.A tool for supervised training of parsed corpora.In Proceedings of the Workshop on Computational Environments for Grammar Development and Linguistic Engineering.Charniak, E. (1997).Statistical parsing with a context-free grammar and word statistics.In Proceedings of the Four- teenth National Conference on Artificial Intelligence (pp.Providence, RI.Charniak, E., & Caroll, G. (1994).Context-sensitive statistics for improved grammatical language models.In Proceedings of the Twelth National Conference on Artificial Intelligence (pp.Three generative, lexicalised models for statistical parsing.In Proceedings of the 35th Meeting of the Association for Computational Linguistics and the 7th Conference of the European Chapter of the ACL (pp.Implementing typed feature structure grammars.Stanford, CA: CSLI Publications.Copestake, A., Lascarides, A., & Flickinger, D. (2001).An algebra for semantic construction in constraint-based gram- mars.In Proceedings of the 39th Meeting of the Association for Computational Linguistics.Toulouse, France.Dipper, S. (2000).Grammar-based corpus annotation.In Workshop on linguistically interpreted corpora LINC-2000 (pp.Flickinger, D. (2000).On building a more efficient grammar by exploiting types.Natural Language Engineering, 6 (1) (Special Issue on Efficient Processing with HPSG), 15 ?Harris, T. E. (1963).The theory of branching processes.Berlin, Germany: Springer.Johnson, M., Geman, S., Canon, S., Chi, Z., & Riezler, S. (1999).Estimators for stochastic ?unification-based?In Proceedings of the 37th Meeting of the Associa- tion for Computational Linguistics (pp.College Park, MD.Manning, C. D., & Schu?tze, H. (1999).Foundations of statis- tical Natural Language Processing.Cambridge, MA: MIT Press.Mullen, T., Malouf, R., & Noord, G. van.Statistical parsing of Dutch using Maximum Entropy models with fea- ture merging.In Proceedings of the Natural Language Pro- cessing Pacific Rim Symposium.Oepen, S., & Callmeier, U.Measure for mea- sure: Parser cross-fertilization.Towards increased compo- nent comparability and exchange.In Proceedings of the 6th International Workshop on Parsing Technologies (pp.Toutanova, K., & Manning, C. D. (2002).Feature selection for a rich HPSG grammar using decision trees.In Proceed- ings of the sixth conference on natural language learning (CoNLL-2002).Foundations of speech- to-speech translation.Berlin, Germany: Springer.
ORANGE: A Method For Evaluating Automatic Evaluation Metrics For Machine TranslationComparisons of automatic evaluation metrics for machine translation are usually conducted on corpus level using correlation statistics such as Pearson?s product moment correlation coefficient or Spearman?s rank order correlation coefficient between human scores and automatic scores.However, such comparisons rely on human judgments of translation qualities such as adequacy and fluency.Unfortunately, these judgments are often inconsistent and very expensive to acquire.In this paper, we introduce a new evaluation method, ORANGE, for evaluating automatic machine translation evaluation metrics automatically without extra human involvement other than using a set of reference translations.We also show the results of comparing several existing automatic metrics and three new automatic metrics using ORANGE.To automatically evaluate machine translations, the machine translation community recently adopted an n-gram co-occurrence scoring procedure BLEU (Papineni et al 2001).A similar metric, NIST, used by NIST (NIST 2002) in a couple of machine translation evaluations in the past two years is based on BLEU.The main idea of BLEU is to measure the translation closeness between a candidate translation and a set of reference translations with a numerical metric.Although the idea of using objective functions to automatically evaluate machine translation quality is not new (Su et al 1992), the success of BLEU prompts a lot of interests in developing better automatic evaluation metrics.For example, Akiba et al (2001) proposed a metric called RED based on edit distances over a set of multiple references.Nie?en et al (2000) calculated the length normalized edit distance, called word error rate (WER), between a candidate and multiple reference translations.Leusch et al (2003) proposed a related measure called position independent word error rate (PER) that did not consider word position, i.e. using bag-of-words instead.Turian et al (2003) introduced General Text Matcher (GTM) based on accuracy measures such as recall, precision, and F-measure.With so many different automatic metrics available, it is necessary to have a common and objective way to evaluate these metrics.Comparison of automatic evaluation metrics are usually conducted on corpus level using correlation analysis between human scores and automatic scores such as BLEU, NIST, WER, and PER.However, the performance of automatic metrics in terms of human vs. system correlation analysis is not stable across different evaluation settings.For example, Table 1 shows the Pearson?s linear correlation coefficient analysis of 8 machine translation systems from 2003 NIST Chinese English machine translation evaluation.The Pearson?correlation coefficients are computed according to different automatic evaluation methods vs. human assigned adequacy and fluency.BLEU1, 4, and 12 are BLEU with maximum n-gram lengths of 1, 4, and 12 respectively.GTM10, 20, and 30 are GTM with exponents of 1.0, 2.0, and 3.0 respectively.95% confidence intervals are estimated using bootstrap resampling (Davison and Hinkley 1997).From the BLEU group, we found that shorter BLEU has better adequacy correlation while longer BLEU has better fluency correlation.GTM with smaller exponent has better adequacy correlation and GTM with larger exponent has better fluency correlation.NIST is very good in adequacy correlation but not as good as GTM30 in fluency correlation.Based on these observations, we are not able to conclude which metric is the best because it depends on the manual evaluation criteria.This results also indicate that high correlation between human and automatic scores in both adequacy and fluency cannot always been achieved at the same time.The best performing metrics in fluency according to Table 1 are BLEU12 and GTM30 (dark/green cells).However, many metrics are statistically equivalent (gray cells) to them when we factor in the 95% confidence intervals.For example, even PER is as good as BLEU12 in adequacy.One reason for this might be due to data sparseness since only 8 systems are available.The other potential problem for correlation analysis of human vs. automatic framework is that high corpus-level correlation might not translate to high sentence-level correlation.However, high sentence-level correlation is often an important property that machine translation researchers look for.For example, candidate translations shorter than 12 words would have zero BLEU12 score but BLEU12 has the best correlation with human judgment in fluency as shown in Table 1.In order to evaluate the ever increasing number of automatic evaluation metrics for machine translation objectively, efficiently, and reliably, we introduce a new evaluation method: ORANGE.We describe ORANGE in details in Section 2 and briefly introduce three new automatic metrics that will be used in comparisons in Section 3.The results of comparing several existing automatic metrics and the three new automatic metrics using ORANGE will be presented in Section 4.We conclude this paper and discuss future directions in Section 5.Intuitively a good evaluation metric should give higher score to a good translation than a bad one.Therefore, a good translation should be ranked higher than a bad translation based their scores.One basic assumption of all automatic evaluation metrics for machine translation is that reference translations are good translations and the more a machine translation is similar to its reference translations the better.We adopt this assumption and add one more assumption that automatic translations are usually worst than their reference translations.Therefore, reference translations should be ranked higher than machine translations on average if a good automatic evaluation metric is used.Based on these assumptions, we propose a new automatic evaluation method for evaluation of automatic machine translation metrics as follows: Given a source sentence, its machine translations, and its reference translations, we compute the average rank of the reference translations within the combined machine and reference translation list.For example, a statistical machine translation system such as ISI?s AlTemp SMT system (Och 2003) can generate a list of n-best alternative translations given a source sentence.We compute the automatic scores for the n-best translations and their reference translations.We then rank these translations, calculate the average rank of the references in the n-best list, and compute the ratio of the average reference rank to the length of the n-best list.We call this ratio ?ORANGE?(Oracle1 Ranking for Gisting Evaluation) and the smaller the ratio is, the better the automatic metric is. There are several advantages of the proposed ORANGE evaluation method: ? No extra human involvement ? ORANGE uses the existing human references but not human evaluations.Applicable on sentence-level ? Diagnostic error analysis on sentence-level is naturally provided.This is a feature that many machine translation researchers look for.Many existing data points ? Every sentence is a data point instead of every system (corpus-level).For example, there are 919 sentences vs. 8 systems in the 2003 NIST Chinese-English machine translation evaluation.Only one objective function to optimize ? Minimize a single ORANGE score instead of maximize Pearson?s correlation coefficients between automatic scores and human judgments in adequacy, fluency, or other quality metrics.A natural fit to the existing statistical machine translation framework ? A metric that ranks a good translation high in an n best list could be easily integrated in a minimal error rate statistical machine translation training framework (Och 2003).The overall system performance in terms of 1 Oracles refer to the reference translations used in.the evaluation procedure.Method Pearson 95%L 95%U Pearson 95%L 95%U BLEU1 0.86 0.83 0.89 0.81 0.75 0.86 BLEU4 0.77 0.72 0.81 0.86 0.81 0.90 BLEU12 0.66 0.60 0.72 0.87 0.76 0.93 NIST 0.89 0.86 0.92 0.81 0.75 0.87 WER 0.47 0.41 0.53 0.69 0.62 0.75 PER 0.67 0.62 0.72 0.79 0.74 0.85 GTM10 0.82 0.79 0.85 0.73 0.66 0.79 GTM20 0.77 0.73 0.81 0.86 0.81 0.90 GTM30 0.74 0.70 0.78 0.87 0.81 0.91 Adequacy Fluency Table 1.Pearson's correlation analysis of 8 machine translation systems in 2003 NIST Chinese-English machine translation evaluation.generating more human like translations should also be improved.Before we demonstrate how to use ORANGE to evaluate automatic metrics, we briefly introduce three new metrics in the next section.ROUGE-L and ROUGE-S are described in details in Lin and Och (2004).Since these two metrics are relatively new, we provide short summaries of them in Section 3.1 and Section 3.3 respectively.ROUGE-W, an extension of ROUGE-L, is new and is explained in details in Section 3.2.3.1 ROUGE-L: Longest Common Sub-.sequence Given two sequences X and Y, the longest common subsequence (LCS) of X and Y is a common subsequence with maximum length (Cormen et al 1989).To apply LCS in machine translation evaluation, we view a translation as a sequence of words.The intuition is that the longer the LCS of two translations is, the more similar the two translations are.We propose using LCS-based F-measure to estimate the similarity between two translations X of length m and Y of length n, assuming X is a reference translation and Y is a candidate translation, as follows: Rlcs m YXLCS ),( = (1) Plcs n YXLCS ),( = (2) Flcs lcslcs lcslcs PR PR 2 2 )1( ? ?+ + = (3) Where LCS(X,Y) is the length of a longest common subsequence of X and Y, and ? = Plcs/Rlcs when ?Flcs/?Rlcs_=_?Flcs/?Plcs.We call the LCS based F-measure, i.e. Equation 3, ROUGE-L.Notice that ROUGE-L is 1 when X = Y since LCS(X,Y) = m or n; while ROUGE-L is zero when LCS(X,Y) = 0, i.e. there is nothing in common between X and Y. One advantage of using LCS is that it does not require consecutive matches but in-sequence matches that reflect sentence level word order as n grams.The other advantage is that it automatically includes longest in-sequence common n-grams, therefore no predefined n-gram length is necessary.By only awarding credit to in-sequence unigram matches, ROUGE-L also captures sentence level structure in a natural way.Consider the following example: S1.police killed the gunman S2.police kill the gunman S3.the gunman kill police Using S1 as the reference translation, S2 has a ROUGE-L score of 3/4 = 0.75 and S3 has a ROUGE L score of 2/4 = 0.5, with ? = 1.Therefore S2 is better than S3 according to ROUGE-L.This example illustrated that ROUGE-L can work reliably at sentence level.However, LCS suffers one disadvantage: it only counts the main in sequence words; therefore, other alternative LCSes and shorter sequences are not reflected in the final score.In the next section, we introduce ROUGE-W.3.2 ROUGE-W: Weighted Longest Common.Subsequence LCS has many nice properties as we have described in the previous sections.Unfortunately, the basic LCS also has a problem that it does not differentiate LCSes of different spatial relations within their embedding sequences.For example, given a reference sequence X and two candidate sequences Y1 and Y2 as follows: X: [A B C D E F G] Y1: [A B C D H I K] Y2: [A H B K C I D] Y1 and Y2 have the same ROUGE-L score.However, in this case, Y1 should be the better choice than Y2 because Y1 has consecutive matches.To improve the basic LCS method, we can simply remember the length of consecutive matches encountered so far to a regular two dimensional dynamic program table computing LCS.We call this weighted LCS (WLCS) and use k to indicate the length of the current consecutive matches ending at words xi and yj.Given two sentences X and Y, the recurrent relations can be written as follows: (1) If xi = yj Then // the length of consecutive matches at // position i-1 and j-1 k = w(i-1,j-1) c(i,j) = c(i-1,j-1) + f(k+1) ? f(k) // remember the length of consecutive // matches at position i, j w(i,j) = k+1 (2) Otherwise If c(i-1,j) > c(i,j-1) Then c(i,j) = c(i-1,j) w(i,j) = 0 // no match at i, j Else c(i,j) = c(i,j-1) w(i,j) = 0 // no match at i, j (3) WLCS(X,Y) = c(m,n) Where c is the dynamic programming table, 0 <= i <= m, 0 <= j <= n, w is the table storing the length of consecutive matches ended at c table position i and j, and f is a function of consecutive matches at the table position, c(i,j).Notice that by providing different weighting function f, we can parameterize the WLCS algorithm to assign different credit to consecutive in-sequence matches.The weighting function f must have the property that f(x+y) > f(x) + f(y) for any positive integers x and y. In other words, consecutive matches are awarded more scores than non-consecutive matches.For example, f(k)-=-?k ? ?when k >= 0, and ?, ? > 0.This function charges a gap penalty of ??for each non-consecutive n-gram sequences.Another possible function family is the polynomial family of the form k?where -?> 1.However, in order to normalize the final ROUGE-W score, we also prefer to have a function that has a close form inverse function.For example, f(k)-=-k2 has a close form inverse function f -1(k)-=-k1/2.F-measure based on WLCS can be computed as follows, given two sequences X of length m and Y of length n: Rwlcs ???= ? )( ),(1 mf YXWLCSf (4) Pwlcs ???= ? )( ),(1 nf YXWLCSf (5) Fwlcs wlcswlcs wlcswlcs PR PR 2 2 )1( ? ?+ + = (6) f -1 is the inverse function of f. We call the WLCS-based F-measure, i.e. Equation 6, ROUGE W. Using Equation 6 and f(k)-=-k2 as the weighting function, the ROUGE-W scores for sequences Y1 and Y2 are 0.571 and 0.286 respectively.Therefore, Y1 would be ranked higher than Y2 using WLCS.We use the polynomial function of the form k?in the experiments described in Section 4 with the weighting factor ? varying from 1.1 to 2.0 with 0.1 increment.ROUGE-W is the same as ROUGE-L when ? is set to 1.In the next section, we introduce the skip-bigram co-occurrence statistics.3.3 ROUGE-S: Skip-Bigram Co-Occurrence.Statistics Skip-bigram is any pair of words in their sentence order, allowing for arbitrary gaps.Skip-bigram cooccurrence statistics measure the overlap of skip bigrams between a candidate translation and a set of reference translations.Using the example given in Section 3.1: S1.police killed the gunman S2.police kill the gunman S3.the gunman kill police S4.the gunman police killed each sentence has C(4,2)2 = 6 skip-bigrams.For example, S1 has the following skip-bigrams: (?police killed?, ?police the?, ?police gunman?, ?killed the?, ?killed gunman?, ?the gunman?)Given translations X of length m and Y of length n, assuming X is a reference translation and Y is a candidate translation, we compute skip-bigram based F-measure as follows: Rskip2 )2,( ),(2 mC YXSKIP = (7) Pskip2 )2,( ),(2 nC YXSKIP = (8) Fskip2 2 2 2 22 2 )1( skipskip skipskip PR PR ? ?+ + = (9) Where SKIP2(X,Y) is the number of skip-bigram matches between X and Y, ? = Pskip2/Rskip2 when ?Fskip2/?Rskip2_=_?Fskip2/?Pskip2, and C is the combination function.We call the skip-bigram based F-measure, i.e. Equation 9, ROUGE-S.Using Equation 9 with ? = 1 and S1 as the reference, S2?s ROUGE-S score is 0.5, S3 is 0.167, and S4 is 0.333.Therefore, S2 is better than S3 and S4, and S4 is better than S3.One advantage of skip-bigram vs. BLEU is that it does not require consecutive matches but is still sensitive to word order.Comparing skip-bigram with LCS, skip-bigram counts all in-order matching word pairs while LCS only counts one longest common subsequence.We can limit the maximum skip distance, between two in-order words to control the admission of a skip-bigram.We use skip distances of 1 to 9 with increment of 1 (ROUGE-S1 to 9) and without any skip distance constraint (ROUGE-S*).In the next section, we present the evaluations of BLEU, NIST, PER, WER, ROUGE-L, ROUGE-W, and ROUGE-S using the ORANGE evaluation method described in Section 2.2 Combinations: C(4,2) = 4!/(2!*2!) = 6..Comparing automatic evaluation metrics using the ORANGE evaluation method is straightforward.To simulate real world scenario, we use n-best lists from ISI?s state-of-the-art statistical machine translation system, AlTemp (Och 2003), and the 2002 NIST Chinese-English evaluation corpus as.the test corpus.There are 878 source sentences in Chinese and 4 sets of reference translations provided by LDC3.For exploration study, we generate 1024-best list using AlTemp for 872 source sentences.AlTemp generates less than 1024 alternative translations for 6 out of the 878 source 3 Linguistic Data Consortium prepared these manual.translations as part of the DARPA?s TIDES project.sentences.These 6 source sentences are excluded from the 1024-best set.In order to compute BLEU at sentence level, we apply the following smoothing technique: Add one count to the n-gram hit and total n gram count for n > 1.Therefore, for candidate translations with less than n words, they can still get a positive smoothed BLEU score from shorter n-gram matches; however if nothing matches then they will get zero scores.We call the smoothed BLEU: BLEUS.For each candidate translation in the 1024-best list and each reference, we compute the following scores: 1.BLEUS1 to 9.2. NIST, PER, and WER.3.ROUGE-L.4.ROUGE-W with weight ranging from 1.1.to 2.0 with increment of 0.1ranging from 0 to 9 (ROUGE-S0 to S9) and without any skip distance limit (ROUGE-S*) We compute the average score of the references and then rank the candidate translations and the references according to these automatic scores.The ORANGE score for each metric is calculated as the average rank of the average reference (oracle) score over the whole corpus (872 sentences) divided by the length of the n-best list plus 1.Assuming the length of the n-best list is N and the size of the corpus is S (in number of sentences), we compute Orange as follows: ORANGE = )1( )( 1 + ???= NS OracleRank S i i (10) Rank(Oraclei) is the average rank of source sentence i?s reference translations in n-best list i. Table 2 shows the results for BLEUS1 to 9.To assess the reliability of the results, 95% confidence intervals (95%-CI-L for lower bound and CI-U for upper bound) of average rank of the oracles are Method ORANGE Avg Rank 95%-CI-L 95%-CI-U BLEUS1 35.39% 363 337 387 BLEUS2 25.51% 261 239 283 BLEUS3 23.74% 243 221 267 BLEUS4 23.13% 237 215 258 BLEUS5 23.13% 237 215 260 BLEUS6 22.91% 235 211 257 BLEUS7 22.98% 236 213 258 BLEUS8 23.20% 238 214 261 BLEUS9 23.56% 241 218 265 Table 2.ORANGE scores for BLEUS1 to 9.Method Pearson 95%L 95%U Pearson 95%L 95%U BLEUS1 0.87 0.84 0.90 0.83 0.77 0.88 BLEUS2 0.84 0.81 0.87 0.85 0.80 0.90 BLEUS3 0.80 0.76 0.84 0.87 0.82 0.91 BLEUS4 0.76 0.72 0.80 0.88 0.83 0.92 BLEUS5 0.73 0.69 0.78 0.88 0.83 0.91 BLEUS6 0.70 0.65 0.75 0.87 0.82 0.91 BLEUS7 0.65 0.60 0.70 0.85 0.80 0.89 BLEUS8 0.58 0.52 0.64 0.82 0.76 0.86 BLEUS9 0.50 0.44 0.57 0.76 0.70 0.82 Adequacy Fluency Table 3.Pearson's correlation analysis BLEUS1 to 9 vs. adequacy and fluency of 8 machine translation systems in 2003 NIST Chinese-English machine translation evaluation.Method ORANGE Avg Rank 95%-CI-L 95%-CI-U ROUGE-L 20.56% 211 190 234 ROUGE-W-1.1 20.45% 210 189 232 ROUGE-W-1.2 20.47% 210 186 230 ROUGE-W-1.3 20.69% 212 188 234 ROUGE-W-1.4 20.91% 214 191 238 ROUGE-W-1.5 21.17% 217 196 241 ROUGE-W-1.6 21.47% 220 199 242 ROUGE-W-1.7 21.72% 223 200 245 ROUGE-W-1.8 21.88% 224 204 246 ROUGE-W-1.9 22.04% 226 203 249 ROUGE-W-2.0 22.25% 228 206 250 Table 4.ORANGE scores for ROUGE-L and ROUGE-W-1.1 to 2.0.Method ORANGE Avg Rank 95%-CI-L 95%-CI-U ROUGE-S0 25.15% 258 234 280 ROUGE-S1 22.44% 230 209 253 ROUGE-S2 20.38% 209 186 231 ROUGE-S3 19.81% 203 183 226 ROUGE-S4 19.66% 202 177 224 ROUGE-S5 19.95% 204 184 226 ROUGE-S6 20.32% 208 187 230 ROUGE-S7 20.77% 213 191 236 ROUGE-S8 21.42% 220 198 242 ROUGE-S9 21.92% 225 204 247 ROUGE-S* 27.43% 281 259 304 Table 5.ORANGE scores for ROUGE-S1 to 9 and ROUGE-S*.estimated using bootstrap resampling (Davison and Hinkley).According to Table 2, BLEUS6 (dark/green cell) is the best performer among all BLEUSes, but it is statistically equivalent to BLEUS3, 4, 5, 7, 8, and 9 with 95% of confidence.Table 3 shows Pearson?s correlation coefficient for BLEUS1 to 9 over 8 participants in 2003 NIST Chinese-English machine translation evaluation.According to Table 3, we find that shorter BLEUS has better correlation with adequacy.However, correlation with fluency increases when longer n gram is considered but decreases after BLEUS5.There is no consensus winner that achieves best correlation with adequacy and fluency at the same time.So which version of BLEUS should we use?A reasonable answer is that if we would like to optimize for adequacy then choose BLEUS1; however, if we would like to optimize for fluency then choose BLEUS4 or BLEUS5.According to Table 2, we know that BLEUS6 on average places reference translations at rank 235 in a 1024-best list machine translations that is significantly better than BLEUS1 and BLEUS2.Therefore, we have better chance of finding more human-like translations on the top of an n-best list by choosing BLEUS6 instead of BLEUS2.To design automatic metrics better than BLEUS6, we can carry out error analysis over the machine translations that are ranked higher than their references.Based on the results of error analysis, promising modifications can be identified.This indicates that the ORANGE evaluation method provides a natural automatic evaluation metric development cycle.Table 4 shows the ORANGE scores for ROUGE-L and ROUGE-W-1.1 to 2.0.ROUGE-W 1.1 does have better ORANGE score but it is equivalent to other ROUGE-W variants and ROUGE-L.Table 5 lists performance of different ROUGE-S variants.ROUGE-S4 is the best performer but is only significantly better than ROUGE-S0 (bigram), ROUGE-S1, ROUGE-S9 and ROUGE-S*.The relatively worse performance of ROUGE-S* might to due to spurious matches such as ?the the?or ?the of?.Table 6 summarizes the performance of 7 different metrics.ROUGE-S4 (dark/green cell) is the best with an ORANGE score of 19.66% that is statistically equivalent to ROUGE-L and ROUGE W-1.1 (gray cells) and is significantly better than BLEUS6, NIST, PER, and WER.Among them PER is the worst.To examine the length effect of n-best lists on the relative performance of automatic metrics, we use the AlTemp SMT system to generate a 16384 best list and compute ORANGE scores for BLEUS4, PER, WER, ROUGE-L, ROUGE-W-1.2, and ROUGE-S4.Only 474 source sentences that have more than 16384 alternative translations are used in this experiment.Table 7 shows the results.It confirms that when we extend the length of the n best list to 16 times the size of the 1024-best, the relative performance of each automatic evaluation metric group stays the same.ROUGE-S4 is still the best performer.Figure 1 shows the trend of ORANGE scores for these metrics over N-best list of N from 1 to 16384 with length increment of 64.It is clear that relative performance of these metrics stay the same over the entire range.5 Conclusion.In this paper we introduce a new automatic evaluation method, ORANGE, to evaluate automatic evaluation metrics for machine translations.We showed that the new method can be easily implemented and integrated with existing statistical machine translation frameworks.ORANGE assumes a good automatic evaluation metric should assign high scores to good translations and assign low scores to bad translations.Using reference translations as examples of good translations, we measure the quality of an automatic evaluation metric based on the average rank of the references within a list of alternative machine translations.Comparing with traditional approaches that require human judgments on adequacy or fluency, ORANGE requires no extra human involvement other than the availability of reference translations.It also streamlines the process of design and error analysis for developing new automatic metrics.Using ORANGE, we have only one parameter, i.e. ORANGE itself, to optimize vs. two in correlation analysis using human assigned adequacy and fluency.By examining the rank position of the Method ORANGE Avg Rank 95%-CI-L 95%-CI-U BLEUS6 22.91% 235 211 257 NIST 29.70% 304 280 328 PER 36.84% 378 350 403 WER 23.90% 245 222 268 ROUGE-L 20.56% 211 190 234 ROUGE-W-1.1 20.45% 210 189 232 ROUGE-S4 19.66% 202 177 224 Table 6.Summary of ORANGE scores for 7 automatic evaluation metrics.Method ORANGE Avg Rank 95%-CI-L 95%-CI-U BLEUS4 18.27% 2993 2607 3474 PER 28.95% 4744 4245 5292 WER 19.36% 3172 2748 3639 ROUGE-L 16.22% 2657 2259 3072 ROUGE-W-1.2 15.87% 2600 2216 2989 ROUGE-S4 14.92% 2444 2028 2860 Table 7.Summary of ORANGE scores for 6 automatic evaluation metrics (16384-best list).references, we can easily identify the confusion set of the references and propose new features to improve automatic metrics.One caveat of the ORANGE method is that what if machine translations are as good as reference translations?To rule out this scenario, we can sample instances where machine translations are ranked higher than human translations.We then check the portion of the cases where machine translations are as good as the human translations.If the portion is small then the ORANGE method can be confidently applied.We conjecture that this is the case for the currently available machine translation systems.However, we plan to conduct the sampling procedure to verify this is indeed the case.
Learning Entailment Rules for Unary TemplatesMost work on unsupervised entailment rule acquisition focused on rules between templates with two variables, ignoring unary rules - entailment rules betweentemplates with a single variable.In this paper we investigate two approaches for unsupervised learning of such rules and com pare the proposed methods with a binary rule learning method.The results show that the learned unary rule-sets outperform the binary rule-set.In addition, a novel directional similarity measure for learning entailment, termed Balanced-Inclusion, is the best performing measure.In many NLP applications, such as Question An swering (QA) and Information Extraction (IE), it is crucial to recognize whether a specific target meaning is inferred from a text.For example, a QA system has to deduce that ?SCO sued IBM?is inferred from ?SCO won a lawsuit against IBM?to answer ?Whom did SCO sue??.This type of reasoning has been identified as a core semanticinference paradigm by the generic Textual Entail ment framework (Giampiccolo et al, 2007).An important type of knowledge needed for such inference is entailment rules.An entailmentrule specifies a directional inference relation be tween two templates, text patterns with variables, such as ?X win lawsuit against Y ? X sue Y ?.Applying this rule by matching ?X win lawsuit against Y ? in the above text allows a QA system to c ? 2008.Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.infer ?X sue Y ? and identify ?IBM?, Y ?s instantiation, as the answer for the above question.Entail ment rules capture linguistic and world-knowledge inferences and are used as an important building block within different applications, e.g.(Romano et al, 2006).One reason for the limited performance of generic semantic inference systems is the lack of broad-scale knowledge-bases of entailment rules (in analog to lexical resources such as WordNet).Supervised learning of broad coverage rule-sets is an arduous task.This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.(Lin and Pantel, 2001; Szpektor et al, 2004; Sekine, 2005).Most unsupervised entailment rule acquisitionmethods learn binary rules, rules between tem plates with two variables, ignoring unary rules, rules between unary templates (templates withonly one variable).However, a predicate quite of ten appears in the text with just a single variable(e.g. intransitive verbs or passives), where infer ence requires unary rules, e.g. ?X take a nap?X sleep?(further motivations in Section 3.1).In this paper we focus on unsupervised learning of unary entailment rules.Two learning ap proaches are proposed.In our main approach, rules are learned by measuring how similar the variable instantiations of two templates in a corpusare.In addition to adapting state-of-the-art similar ity measures for unary rule learning, we propose a new measure, termed Balanced-Inclusion, which balances the notion of directionality in entailment with the common notion of symmetric semantic similarity.In a second approach, unary rules arederived from binary rules learned by state-of-the art binary rule learning methods.We tested the various unsupervised unary rule 849learning methods, as well as a binary rule learn ing method, on a test set derived from a standard IE benchmark.This provides the first comparisonbetween the performance of unary and binary rule sets.Several results rise from our evaluation: (a) while most work on unsupervised learning ignored unary rules, all tested unary methods outperformed the binary method; (b) it is better to learn unary rules directly than to derive them from a binary rule-base; (c) our proposed Balanced-Inclusion measure outperformed all other tested methods interms of F1 measure.Moreover, only BalancedInclusion improved F1 score over a baseline infer ence that does not use entailment rules at all .This section reviews relevant distributional simi larity measures, both symmetric and directional, which were applied for either lexical similarity or unsupervised entailment rule learning.Distributional similarity measures follow the Distributional Hypothesis, which states that words that occur in the same contexts tend to have similar meanings (Harris, 1954).Various measures wereproposed in the literature for assessing such simi larity between two words, u and v. Given a word q, its set of features F q and feature weights w q (f) for f ? F q , a common symmetric similarity measure is Lin similarity (Lin, 1998a): Lin(u, v) = ? f?F u ?F v [w u (f) + w v (f)] ? f?F u w u (f) + ? f?F v w v (f) where the weight of each feature is the pointwise mutual information (pmi) between the word and the feature: w q (f) = log[ Pr(f |q) Pr(f) ].Weeds and Weir (2003) proposed to measure thesymmetric similarity between two words by av eraging two directional (asymmetric) scores: the coverage of each word?s features by the other.The coverage of u by v is measured by: Cover(u, v) = ? f?F u ?F v w u (f) ? f?F u w u (f) The average can be arithmetic or harmonic: WeedsA(u, v) = 1 2 [Cover(u, v) + Cover(v, u)] WeedsH(u, v) = 2 ? Cover(u, v) ? Cover(v, u) Cover(u, v) + Cover(v, u) Weeds et al also used pmi for feature weights.Binary rule learning algorithms adopted suchlexical similarity approaches for learning rules between templates, where the features of each tem plate are its variable instantiations in a corpus, such as {X=?SCO?, Y =?IBM?}for the example in Section 1.Some works focused on learningrules from comparable corpora, containing com parable documents such as different news articles from the same date on the same topic (Barzilay and Lee, 2003; Ibrahim et al, 2003).Such corpora are highly informative for identifying variations of the same meaning, since, typically, when variableinstantiations are shared across comparable docu ments the same predicates are described.However,it is hard to collect broad-scale comparable cor pora, as the majority of texts are non-comparable.A complementary approach is learning from the abundant regular, non-comparable, corpora.Yet,in such corpora it is harder to recognize varia tions of the same predicate.The DIRT algorithm(Lin and Pantel, 2001) learns non-directional binary rules for templates that are paths in a depen dency parse-tree between two noun variables X and Y . The similarity between two templates t and t ? is the geometric average: DIRT (t, t ? ) = ? Lin x (t, t ? ) ? Lin y (t, t ? ) where Lin xis the Lin similarity between X?s in stantiations of t and X?s instantiations of t ? in a corpus (equivalently for Lin y ).Some workstake the combination of the two variable instantiations in each template occurrence as a single complex feature, e.g. {X-Y =?SCO-IBM?}, and com pare between these complex features of t and t ?(Ravichandran and Hovy, 2002; Szpektor et al, 2004; Sekine, 2005).Directional Measures Most rule learning meth ods apply a symmetric similarity measure between two templates, viewing them as paraphrasing eachother.However, entailment is in general a direc tional relation.For example, ?X acquire Y ? X own Y ? and ?countersuit against X ? lawsuit against X?.(Weeds and Weir, 2003) propose a directional measure for learning hyponymy between twowords, ?l? r?, by giving more weight to the cov erage of the features of l by r (with ? > 1 2 ): WeedsD(l, r)=?Cover(l, r)+(1??)Cover(r, l) When ?=1, this measure degenerates into Cover(l, r), termed Precision(l, r).With 850 Precision(l, r) we obtain a ?soft?version of the inclusion hypothesis presented in (Geffet and Dagan, 2005), which expects l to entail r if the ?important?features of l appear also in r. Similarly, the LEDIR algorithm (Bhagat et al, 2007) identifies the entailment direction between two binary templates, l and r, which participate in a relation learned by (the symmetric) DIRT, by measuring the proportion of instantiations of l that are covered by the instantiations of r. As far as we know, only (Shinyama et al, 2002)and (Pekar, 2006) learn rules between unary tem plates.However, (Shinyama et al, 2002) relies on comparable corpora for identifying paraphrasesand simply takes any two templates from comparable sentences that share a named entity instan tiation to be paraphrases.Such approach is notfeasible for non-comparable corpora where statis tical measurement is required.(Pekar, 2006) learnsrules only between templates related by local dis course (information from different documents is ignored).In addition, their template structure islimited to only verbs and their direct syntactic ar guments, which may yield incorrect rules, e.g. forlight verbs (see Section 5.2).To overcome this limitation, we use a more expressive template struc ture.3.1 Motivations.Most unsupervised rule learning algorithms focused on learning binary entailment rules.How ever, using binary rules for inference is not enough.First, a predicate that can have multiple arguments may still occur with only one of its arguments.For example, in ?The acquisition of TCA was successful?, ?TCA?is the only argument of ?acqui sition?.Second, some predicate expressions are unary by nature.For example, modifiers, such as ?the elected X?, or intransitive verbs.In addition, it appears more tractable to learn all variations for each argument of a predicate separately than to learn them for combinations of argument pairs.For these reasons, it seems that unary rule learn ing should be addressed in addition to binary rule learning.We are further motivated by the fact thatsome (mostly supervised) works in IE found learn ing unary templates useful for recognizing relevant named entities (Riloff, 1996; Sudo et al, 2003; Shinyama and Sekine, 2006), though they did notattempt to learn generic knowledge bases of entail ment rules.This paper investigates acquisition of unary entailment rules from regular non-comparable cor pora.We first describe the structure of unarytemplates and then explore two conceivable approaches for learning unary rules.The first ap proach directly assesses the relation between twogiven templates based on the similarity of their in stantiations in the corpus.The second approach,which was also mentioned in (Iftene and Balahur Dobrescu, 2007), derives unary rules from learned binary rules.3.2 Unary Template Structure.To learn unary rules we first need to define theirstructure.In this paper we work at the syntac tic representation level.Texts are represented by dependency parse trees (using the Minipar parser (Lin, 1998b)) and templates by parse sub-trees.Given a dependency parse tree, any sub-tree can be a candidate template, setting some of its nodesas variables (Sudo et al, 2003).However, the num ber of possible templates is exponential in the sizeof the sentence.In the binary rule learning litera ture, the main solution for exhaustively learning allrules between any pair of templates in a given corpus is to restrict the structure of templates.Typi cally, a template is restricted to be a path in a parse tree between two variable nodes (Lin and Pantel, 2001; Ibrahim et al, 2003).Following this approach, we chose the structure of unary templates to be paths as well, where oneend of the path is the template?s variable.How ever, paths with one variable have more expressive power than paths between two variables, since the combination of two unary paths may generate a binary template that is not a path.For example, the combination of ?X call indictable?and ?call Y indictable?is the template ?X call Y indictable?, which is not a path between X and Y . For every noun node v in a parsed sentence, we generate templates with v as a variable as follows: 1.Traverse the path from v towards the root of.the parse tree.Whenever a candidate pred icate is encountered (any noun, adjective or verb) the path from that node to v is taken as a template.We stop when the first verb orclause boundary (e.g. a relative clause) is encountered, which typically represent the syn tactic boundary of a specific predicate.851 2.To enable templates with control verbs and.light verbs, e.g. ?X help preventing?, ?Xmake noise?, whenever a verb is encountered we generate templates that are paths between v and the verb?s modifiers, either ob jects, prepositional complements or infinite or gerund verb forms (paths ending at stop words, e.g. pronouns, are not generated).3.To capture noun modifiers that act as predi-.cates, e.g. ?the losingX?, we extract template paths between v and each of its modifiers, nouns or adjectives, that are derived from a verb.We use the Catvar database to identify verb derivations (Habash and Dorr, 2003).As an example for the procedure, the templates extracted from the sentence ?The losing party played it safe?with ?party?as the variable are: ?losing X?, ?X play?and ?X play safe?.3.3 Direct Learning of Unary Rules.We applied the lexical similarity measures pre sented in Section 2 for unary rule learning.Each argument instantiation of template t in the corpus is taken as a feature f , and the pmi between t and f is used for the feature?s weight.We first adaptedDIRT for unary templates (unary-DIRT, apply ing Lin-similarity to the single feature vector), as well as its output filtering by LEDIR.The various Weeds measures were also applied 1 : symmetric arithmetic average, symmetric harmonic average, weighted arithmetic average and Precision.After initial analysis, we found that given a right hand side template r, symmetric measures such as Lin (in DIRT) generally tend to prefer (score higher) relations ?l, r?in which l and r are related but do not necessarily participate in an entailment or equivalence relation, e.g. the wrong rule ?kill X ? injure X?.On the other hand, directional measures such as Weeds Precision tend to prefer directional rules inwhich the entailing template is infrequent.If an in frequent template has common instantiations with another template, the coverage of its features istypically high, whether or not an entailment relation exists between the two templates.This behav ior generates high-score incorrect rules.Based on this analysis, we propose a new measure that balances the two behaviors, termed 1We applied the best performing parameter values pre sented in (Bhagat et al, 2007) and (Weeds and Weir, 2003).Balanced-Inclusion (BInc).BInc identifies entail ing templates based on a directional measure but penalizes infrequent templates using a symmetric measure: BInc(l, r) = ? Lin(l, r) ? Precision(l, r) 3.4 Deriving Unary Rules From Binary Rules.An alternative way to learn unary rules is to first learn binary entailment rules and then derive unary rules from them.We derive unary rules from a given binary rule-base in two steps.First, for each binary rule, we generate all possible unary rules that are part of that rule (each unary template is extracted following the same procedure describedin Section 3.2).For example, from ?X find solu tion to Y ? X solve Y ? we generate the unary rules ?X find?X solve?, ?X find solution?Xsolve?, ?solution to Y ? solve Y ? and ?find solu tion to Y ? solve Y ?.The score of each generated rule is set to be the score of the original binary rule.The same unary rule can be derived from different binary rules.For example, ?hire Y ? employ Y ? is derived both from ?X hire Y ? X em ploy Y ? and ?hire Y for Z ? employ Y for Z?, having a different score from each original binary rule.The second step of the algorithm aggregates the different scores yielded for each derived rule to produce the final rule score.Three aggregationfunctions were tested: sum (Derived-Sum), aver age (Derived-Avg) and maximum (Derived-Max).We want to evaluate learned unary and binary rule bases by their utility for NLP applications throughassessing the validity of inferences that are per formed in practice using the rule base.To perform such experiments, we need a test set of seed templates, which correspond to a set of target predicates, and a corpus annotated with allargument mentions of each predicate.The evaluation assesses the correctness of all argument ex tractions, which are obtained by matching in the corpus either the seed templates or templates that entail them according to the rule-base (the latter corresponds to rule-application).Following (Szpektor et al, 2008), we found the ACE 2005 event training set 2useful for this pur pose.This standard IE dataset includes 33 types of event predicates such as Injure, Sue and Divorce.2 http://projects.ldc.upenn.edu/ace/ 852All event mentions are annotated in the corpus, in cluding the instantiated arguments of the predicate.ACE guidelines specify for each event its possible arguments, each associated with a semantic role.For instance, some of the Injure event arguments are Agent, Victim and Time.To utilize the ACE dataset for evaluating entail ment rule applications, we manually represented each ACE event predicate by unary seed templates.For example, the seed templates for Injure are ?A injure?, ?injure V ? and ?injure in T ?.We mapped each event role annotation to the corresponding seed template variable, e.g. ?Agent?to A and ?Victim?to V in the above example.Templatesare matched using a syntactic matcher that han dles simple morpho-syntactic phenomena, as in (Szpektor and Dagan, 2007).A rule application is considered correct if the matched argument is annotated by the corresponding ACE role.For testing binary rule-bases, we automatically generated binary seed templates from any twounary seeds that share the same predicate.For ex ample, for Injure the binary seeds ?A injure V ?, ?A injure in T ? and ?injure V in T ? were automatically generated from the above unary seeds.We performed two adaptations to the ACE dataset to fit it better to our evaluation needs.First, our evaluation aims at assessing the correctness of inferring a specific target semantic meaning, which is denoted by a specific predicate, using rules.Thus, four events that correspond ambiguously tomultiple distinct predicates were ignored.For instance, the Transfer-Money event refers to both do nating and lending money, and thus annotations ofthis event cannot be mapped to a specific seed tem plate.We also omitted 3 events with less than 10mentions, and were left with 26 events (6380 argu ment mentions).Additionally, we regard all entailing mentions under the textual entailment definition as correct.However, event mentions are annotated as correct in ACE only if they explicitly describe the targetevent.For instance, a Divorce mention does entail a preceding marriage event but it does not ex plicitly describe it, and thus it is not annotated as a Marry event.To better utilize the ACE dataset, we considered for a target event the annotations of other events that entail it as being correct as well.We note that each argument was considered sep arately.For example, we marked a mention of a divorced person as entailing the marriage of that person, but did not consider the place and time of the divorce act to be those of the marriage .We implemented the unary rule learning algo rithms described in Section 3 and the binary DIRT algorithm (Lin and Pantel, 2001).We executed each method over the Reuters RCV1 corpus 3 , learning for each template r in the corpus the top100 rules in which r is entailed by another tem plate l, ?l? r?.All rules were learned in canonical form (Szpektor and Dagan, 2007).The rule-base learned by binary DIRT was taken as the input for deriving unary rules from binary rules.The performance of each acquired rule-base was measured for each ACE event.We measured the percentage of correct argument mentions extracted out of all correct argument mentions annotated for the event (recall) and out of all argument mentionsextracted for the event (precision).We also mea sured F1, their harmonic average, and report macro average Recall, Precision and F1 over the 26 event types.No threshold setting mechanism is suggested inthe literature for the scores of the different algo rithms, especially since rules for different right hand side templates have different score ranges.Thus, we follow common evaluation practice (Lin and Pantel, 2001; Geffet and Dagan, 2005) and test each learned rule-set by taking the top K rules for each seed template, whereK ranges from 0 to 100.WhenK=0, no rules are used and mentions are ex tracted only by direct matching of seed templates.Our rule application setting provides a rather simplistic IE system (for example, no named entity recognition or approximate template matching).It is thus useful for comparing different rule-bases,though the absolute extraction figures do not re flect the full potential of the rules.In Secion 5.2 we analyze the full-system?s errors to isolate the rules?contribution to overall system performance.5.1 Results.In this section we focus on the best performing variations of each algorithm type: binary DIRT, unary DIRT, unary Weeds Harmonic, BInc and Derived-Avg.We omitted the results of methods that were clearly inferior to others: (a) WeedsA, WeedsD and Weeds-Precision did not increase 3 http://about.reuters.com/researchandstandards/corpus/ 853Recall over not using rules because rules with in frequent templates scored highest and arithmetic averaging could not balance well these high scores; (b) out of the methods for deriving unary rules from binary rule-bases, Derived-Avg performed best; (c) filtering with (the directional) LEDIR did not improve the performance of unary DIRT.Figure 1 presents Recall, Precision and F1 of themethods for different cutoff points.First, we observe that even when matching only the seed tem plates (K=0), unary seeds outperform the binary seeds in terms of both Precision and Recall.This surprising behavior is consistent through all rulecutoff points: all unary learning algorithms per form better than binary DIRT in all parameters.The inferior behavior of binary DIRT is analyzed in Section 5.2.The graphs show that symmetric unary approaches substantially increase recall, but dramati cally decrease precision already at the top 10 rules.As a result, F1 only decreases for these methods.Lin similarity (DIRT) and Weeds-Harmonic show similar behaviors.They consistently outperform Derived-Avg.One reason for this is that incorrectunary rules may be derived even from correct bi nary rules.For example, from ?X gain seat on Y ? elect X to Y ? the incorrect unary rule ?X gain?electX?is also generated.This problem is less frequent when unary rules are directly scored based on their corpus statistics.The directional measure of BInc yields a more accurate rule-base, as can be seen by the much slower precision reduction rate compared to theother algorithms.As a result, it is the only algo rithm that improves over the F1 baseline of K=0,with the best cutoff point at K=20.BInc?s re call increases moderately compared to other unarylearning approaches, but it is still substantially bet ter than not using rules (a relative recall increase of 50% already at K=10).We found that many of the correct mentions missed by BInc but identified by other methods are due to occasional extractions of incorrect frequent rules, such as partial templates (see Section 5.2).This is reflected in the very low precision of the other methods.On the other hand, some correct rules were only learned by BInc, e.g. ?countersuit againstX?X sue?and ?X take wife ? X marry?.When only one argument is annotated for a specific event mention (28% of ACE predicate mentions, which account for 15% of all annotated arFigure 1: Average Precision, Recall and F1 at dif ferent top K rule cutoff points.guments), binary rules either miss that mention, orextract both the correct argument and another in correct one.To neutralize this bias, we also testedthe various methods only on event mentions an notated with two or more arguments and obtained similar results to those presented for all mentions.This further emphasizes the general advantage of using unary rules over binary rules.854 5.2 Analysis.Binary-DIRT We analyzed incorrect rules both for binary-DIRT and BInc by randomly sampling,for each algorithm, 200 rules that extracted incor rect mentions.We manually classified each rule ?l ? r? as either: (a) Correct - the rule is valid insome contexts of the event but extracted some in correct mentions; (b) Partial Template - l is only apart of a correct template that entails r. For exam ple, learning ?X decide?X meet?instead of ?X decide to meet ? X meet?; (e) Incorrect - other incorrect rules, e.g. ?charge X ? convict X?.Table 1 summarizes the analysis and demonstrates two problems of binary-DIRT.First, rela tive to BInc, it tends to learn incorrect rules for high frequency templates, and therefore extractedmany more incorrect mentions for the same num ber of incorrect rules.Second, a large percentage of incorrect mentions extracted are due to partial templates at the rule left-hand-side.Such rules are leaned because many binary templates have a more complex structure than paths between arguments.As explained in Section 3.2 the unary template structure we use is more expressive, enabling to learn the correct rules.For example, BInc learned?take Y into custody ? arrest Y ? while binary DIRT learned ?X take Y ? X arrest Y ?.System Level Analysis We manually analyzedthe reasons for false positives (incorrect extrac tions) and false negatives (missed extractions) of BInc, at its best performing cutoff point (K=20), by sampling 200 extractions of each type.From the false positives analysis (Table 2) we see that 39% of the errors are due to incorrect rules.The main reasons for learning such rules are those discussed in Section 3.3: (a) related templates that are not entailing; (b) infrequent templates.All learning methods suffer from these issues.As wasshown by our results, BInc provides a first step to wards reducing these problems.Yet, these issues require further research.Apart from incorrectly learned rules, incorrect template matching (e.g. due to parse errors) and context mismatch contribute together 46% of theerrors.Context mismatches occur when the entail ing template is matched in inappropriate contexts.For example, ?slam X ? attack X?should not be applied when X is a ball, only when it is a person.The rule-set net effect on system precision is better estimated by removing these errors and fixing the annotation errors, which yields 72% precision.Binary DIRT Balanced Inclusion Correct 16 (70) 38 (91) Partial Template 27 (2665) 6 (81) Incorrect 157 (2584) 156 (787) Total 200 (5319) 200 (959) Table 1: Rule type distribution of a sample of 200rules that extracted incorrect mentions.The corre sponding numbers of incorrect mentions extracted by the sampled rules is shown in parentheses.Reason % mentions Incorrect Rule learned 39.0 Context mismatch 27.0 Match error 19.0 Annotation problem 15.0 Table 2: Distribution of reasons for false positives (incorrect argument extractions) by BInc at K=20.Reason % mentions Rule not learned 61.5 Match error 25.0 Discourse analysis needed 12.0 Argument is predicative 1.5 Table 3: Distribution of reasons for false negatives (missed argument mentions) by BInc at K=20.Table 3 presents the analysis of false negatives.First, we note that 12% of the arguments cannotbe extracted by rules alone, due to necessary discourse analysis.Thus, a recall upper bound for en tailment rules is 88%.Many missed extractions aredue to rules that were not learned (61.5%).How ever, 25% of the mentions were missed because of incorrect syntactic matching of correctly learned rules.By assuming correct matches in these cases we isolate the recall of the rule-set (along with the seeds), which yields 39% recall.We presented two approaches for unsupervised ac quisition of unary entailment rules from regular (non-comparable) corpora.In the first approach, rules are directly learned based on distributionalsimilarity measures.The second approach de rives unary rules from a given rule-base of binary rules.Under the first approach we proposed a novel directional measure for scoring entailment rules, termed Balanced-Inclusion.We tested the different approaches utilizing a standard IE test-set and compared them to binary rule learning.Our results suggest the advantage of learning unary rules: (a) unary rule-bases perform 855 better than binary rules; (b) it is better to directly learn unary rules than to derive them from binary rule-bases.In addition, the Balanced-Inclusion measure outperformed all other tested methods.In future work, we plan to explore additional unary template structures and similarity scores, and to improve rule application utilizing context matching methods such as (Szpektor et al, 2008).Acknowledgements This work was partially supported by ISF grant 1095/05, the IST Programme of the EuropeanCommunity under the PASCAL Network of Ex cellence IST-2002-506778 and the NEGEV project (www.negev-initiative.org).
The Ups and Downs of Preposition Error Detection in ESL WritingIn this paper we describe a methodologyfor detecting preposition errors in the writ ing of non-native English speakers.Our system performs at 84% precision andclose to 19% recall on a large set of stu dent essays.In addition, we address the problem of annotation and evaluation inthis domain by showing how current ap proaches of using only one rater can skew system evaluation.We present a sampling approach to circumvent some of the issuesthat complicate evaluation of error detec tion systems.The long-term goal of our work is to develop asystem which detects errors in grammar and us age so that appropriate feedback can be given to non-native English writers, a large and growing segment of the world?s population.Estimates arethat in China alone as many as 300 million people are currently studying English as a second lan guage (ESL).Usage errors involving prepositions are among the most common types seen in thewriting of non-native English speakers.For ex ample, (Izumi et al, 2003) reported error rates for English prepositions that were as high as 10% ina Japanese learner corpus.Errors can involve incorrect selection (?we arrived to the station?), ex traneous use (?he went to outside?), and omission (?we are fond null beer?).What is responsiblefor making preposition usage so difficult for non native speakers?c ? 2008.Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.At least part of the difficulty seems to be due tothe great variety of linguistic functions that prepositions serve.When a preposition marks the argument of a predicate, such as a verb, an adjective, or a noun, preposition selection is con strained by the argument role that it marks, thenoun which fills that role, and the particular predi cate.Many English verbs also display alternations (Levin, 1993) in which an argument is sometimes marked by a preposition and sometimes not (e.g., ?They loaded the wagon with hay?/ ?They loaded hay on the wagon?).When prepositions introduceadjuncts, such as those of time or manner, selec tion is constrained by the object of the preposition (?at length?, ?in time?, ?with haste?).Finally, the selection of a preposition for a given context also depends upon the intended meaning of the writer (?we sat at the beach?, ?on the beach?, ?near the beach?, ?by the beach?).With so many sources of variation in Englishpreposition usage, we wondered if the task of se lecting a preposition for a given context might prove challenging even for native speakers.To investigate this possibility, we randomly selected200 sentences from Microsoft?s Encarta Encyclopedia, and, in each sentence, we replaced a ran domly selected preposition with a blank line.We then asked two native English speakers to perform a cloze task by filling in the blank with the best preposition, given the context provided by the rest of the sentence.Our results showed only about75% agreement between the two raters, and be tween each of our raters and Encarta.The presence of so much variability in prepo sition function and usage makes the task of thelearner a daunting one.It also poses special chal lenges for developing and evaluating an NLP error detection system.This paper addresses both the 865 development and evaluation of such a system.First, we describe a machine learning system that detects preposition errors in essays of ESL writers.To date there have been relatively few attempts to address preposition error detection,though the sister task of detecting determiner errors has been the focus of more research.Our system performs comparably with other leading sys tems.We extend our previous work (Chodorow etal., 2007) by experimenting with combination fea tures, as well as features derived from the Google N-Gram corpus and Comlex (Grishman et al, 1994).Second, we discuss drawbacks in current meth ods of annotating ESL data and evaluating errordetection systems, which are not limited to prepo sition errors.While the need for annotation by multiple raters has been well established in NLP tasks (Carletta, 1996), most previous work in error detection has surprisingly relied on only one raterto either create an annotated corpus of learner errors, or to check the system?s output.Some grammatical errors, such as number disagreement be tween subject and verb, no doubt show very highreliability, but others, such as usage errors involv ing prepositions or determiners are likely to be much less reliable.Our results show that relyingon one rater for system evaluation can be problem atic, and we provide a sampling approach which can facilitate using multiple raters for this task.In the next section, we describe a system that automatically detects errors involving incorrect preposition selection (?We arrived to the station?)and extraneous preposition usage (?He went to outside?).In sections 3 and 4, we discuss theproblem of relying on only one rater for exhaus tive annotation and show how multiple raters can be used more efficiently with a sampling approach.Finally, in section 5 we present an analysis of com mon preposition errors that non-native speakers make.2.1 Model.We have used a Maximum Entropy (ME) classi fier (Ratnaparkhi, 1998) to build a model of correctpreposition usage for 34 common English prepo sitions.The classifier was trained on 7 million preposition contexts extracted from parts of the MetaMetrics Lexile corpus that contain textbooks and other materials for high school students.Each context was represented by 25 features consisting of the words and part-of-speech (POS) tags found in a local window of +/- two positions around the preposition, plus the head verb of the preceding verb phrase (PV), the head noun of the precedingnoun phrase (PN), and the head noun of the following noun phrase (FH), among others.In analyzing the contexts, we used only tagging and heuris tic phrase-chunking, rather than parsing, so as to avoid problems that a parser might encounter with ill-formed non-native text 1 . In test mode, the clas-.sifier was given the context in which a preposition occurred, and it returned a probability for each of the 34 prepositions.2.2 Other Components.While the ME classifier constitutes the core of thesystem, it is only one of several processing com ponents that refines or blocks the system?s output.Since the goal of an error detection system is to provide diagnostic feedback to a student, typically a system?s output is heavily constrained so that it minimizes false positives (i.e., the system tries toavoid saying a writer?s preposition is used incor rectly when it is actually right), and thus does not mislead the writer.Pre-Processing Filter: A pre-processing pro gram skips over preposition contexts that contain spelling errors.Classifier performance is poor in such cases because the classifier was trained on well-edited text, i.e., without misspelled words.Inthe context of a diagnostic feedback and assess ment tool for writers, a spell checker would first highlight the spelling errors and ask the writer tocorrect them before the system analyzed the prepo sitions.Post-Processing Filter: After the ME clas sifier has output a probability for each of the 34prepositions but before the system has made its fi nal decision, a series of rule-based post-processingfilters block what would otherwise be false posi tives that occur in specific contexts.The first filter prevents the classifier from marking as an error acase where the classifier?s most probable preposi tion is an antonym of what the writer wrote, such as ?with/without?and ?from/to?.In these cases, resolution is dependent on the intent of the writerand thus is outside the scope of information cap 1 For an example of a common ungrammatical sentence from our corpus, consider: ?In consion, for some reasons,museums, particuraly known travel place, get on many peo ple.?866 tured by the current feature set.Another problem for the classifier involves differentiating between certain adjuncts and arguments.For example, in the sentence ?They described a part for a kid?, thesystem?s top choices were of and to.The benefac tive adjunct introduced by for is difficult for theclassifier to learn, perhaps because it so freely occurs in many locations within a sentence.A post processing filter prevents the system from marking as an error a prepositional phrase that begins with for and has an object headed by a human noun (a WordNet hyponym of person or group).Extraneous Use Filter: To cover extraneous use errors, we developed two rule-based filters: 1) Plural Quantifier Constructions, to handle casessuch as ?some of people?and 2) Repeated Prepo sitions, where the writer accidentally repeated the same preposition two or more times, such as ?canfind friends with with?.We found that extrane ous use errors usually constituted up to 18% of all preposition errors, and our extraneous use filters handle a quarter of that 18%.Thresholding: The final step for the preposi tion error detection system is a set of thresholds that allows the system to skip cases that are likely to result in false positives.One of these is wherethe top-ranked preposition and the writer?s prepo sition differ by less than a pre-specified amount.This was also meant to avoid flagging cases where the system?s preposition has a score only slightly higher than the writer?s preposition score, such as: ?My sister usually gets home around 3:00?(writer: around = 0.49, system: by = 0.51).In these cases, the system?s and the writer?s prepositions both fit the context, and it would be inappropriate to claimthe writer?s preposition was used incorrectly.Another system threshold requires that the probability of the writer?s preposition be lower than a pre specified value in order for it to be flagged as anerror.The thresholds were set so as to strongly fa vor precision over recall due to the high number offalse positives that may arise if there is no thresh olding.This is a tactic also used for determiner selection in (Nagata et al, 2006) and (Han et al, 2006).Both thresholds were empirically set on a development corpus.2.3 Combination Features.ME is an attractive choice of machine learning al gorithm for a problem as complex as preposition error detection, in no small part because of theavailability of ME implementations that can han dle many millions of training events and features.However, one disadvantage of ME is that it does not automatically model the interactions amongfeatures as some other approaches do, such as sup port vector machines (Jurafsky and Martin, 2008).To overcome this, we have experimented with aug menting our original feature set with ?combinationfeatures?which represent richer contextual struc ture in the form of syntactic patterns.Table 1 (first column) illustrates the four com bination features used for the example context ?take our place in the line?.The p denotes a preposition, so N-p-N denotes a syntactic context where the preposition is preceded and followed by a noun phrase.We use the preceding noun phrase (PN) and following head (FH) from the original feature set for the N-p-N feature.Column 3 shows one instantiation of combination features:Combo:word.For the N-p-N feature, the corresponding Combo:word instantiation is ?place line?since ?place?is the PN and ?line?is theFH.We also experimented with using combinations of POS tags (Combo:tag) and word+tag com binations (Combo:word+tag).So for the example, the Combo:tag N-p-N feature would be ?NN-NN?, and the Combo:word+tag N-p-N feature would beplace NN+line NN (see the fourth column of Ta ble 1).The intuition with the Combo:tag features is that the Combo:word features have the potentialto be sparse, and these capture more general pat terns of usage.We also experimented with other features such as augmenting the model with verb-preposition preferences derived from Comlex (Grishman et al, 1994), and querying the Google Terabyte N-gramcorpus with the same patterns used in the combina tion features.The Comlex-based features did not improve the model, and though the Google N-gram corpus represents much more information than our7 million event model, its inclusion improved per formance only marginally.2.4 Evaluation.In our initial evaluation of the system we col lected a corpus of 8,269 preposition contexts,error-annotated by two raters using the scheme de scribed in Section 3 to serve as a gold standard.In this study, we focus on two of the three types of preposition errors: using the incorrect preposition and using an extraneous preposition.We compared 867 Class Components Combo:word Features Combo:tag Features p-N FH line NN N-p-N PN-FH place-line NN-NN V-p-N PV-PN take-line VB-NN V-N-p-N PV-PN-FH take-place-line VB-NN-NN Table 1: Feature Examples for take our place in the line different models: the baseline model of 25 features and baseline with combination features added.Theprecision and recall for the top performing models are shown in Table 2.These results do not in clude the extraneous use filter; this filter generally increased precision by as much as 2% and recall by as much as 5%.Evaluation Metrics In the tasks of determiner and preposition selection in well-formed, nativetexts (such as (Knight and Chander, 1994), (Min nen et al, 2000), (Turner and Charniak, 2007) and (Gamon et al, 2008)), the evaluation metric most commonly used is accuracy.In these tasks, one compares the system?s output on a determiner or preposition to the gold standard of what the writeroriginally wrote.However, in the tasks of deter miner and preposition error detection, precision and recall are better metrics to use because oneis only concerned with a subset of the preposi tions (or determiners), those used incorrectly, as opposed to all of them in the selection task.In essence, accuracy has the problem of distorting system performance.Results The baseline system (described in(Chodorow et al, 2007)) performed at 79.8% precision and 11.7% recall.Next we tested the differ ent combination models: word, tag, word+tag, andall three.Surprisingly, three of the four combina tion models: tag, word+tag, all, did not improve performance of the system when added to the model, but using just the +Combo:word features improved recall by 1%.We use the +Combo:word model to test our sampling approach in section 4.As a final test, we tuned our training corpus of 7 million events by removing any contexts with unknown or misspelled words, and then retrained the model.This ?purge?resulted in a removal of nearly 200,000 training events.With this new training corpus, the +Combo:tag feature showed the biggest improvement over the baseline, withan improvement in both precision (+2.3%) and re call (+2.4%) to 82.1% and 14.1% respectively (last line of Table 2.While this improvement may seemsmall, it is in part due to the difficulty of the prob lem, but also the high baseline system score that was established in our prior work (Chodorow et al., 2007).It should be noted that with the inclusion of the extraneous use filter, performance of the +Combo:tag rose to 84% precision and close to 19% recall.Model Precision Recall Baseline 79.8% 11.7% +Combo:word 79.8% 12.8% +Combo:tag (with purge) 82.1% 14.1%Table 2: Best System Results on Incorrect Selec tion Task 2.5 Related Work.Currently there are only a handful of approachesthat tackle the problem of preposition error detec tion in English learner texts.(Gamon et al, 2008)used a language model and decision trees to de tect preposition and determiner errors in the CLEC corpus of learner essays.Their system performs at 79% precision (which is on par with our system),however recall figures are not presented thus making comparison difficult.In addition, their eval uation differs from ours in that they also include errors of omission, and their work focuses on the top twelve most frequent prepositions, while ours has greater coverage with the top 34.(Izumi etal., 2003) and (Izumi et al, 2004) used an ME ap proach to classify different grammatical errors in transcripts of Japanese interviews.They do not present performance of prepositions specifically, but overall performance for the 13 error types they target reached 25% precision and 7% recall.(Eeg-Olofsson and Knuttson, 2003) created a rule based approach to detecting preposition errors in Swedish language learners (unlike the approaches presented here, which focus on English languagelearners), and their system performed at 25% ac curacy.(Lee and Seneff, 2006) used a language model to tackle the novel problem of prepositionselection in a dialogue corpus.While their perfor mance results are quite high, 88% precision and 868 78% recall, it should be noted that their evaluation was on a small corpus with a highly constraineddomain, and focused on a limited number of prepo sitions, thus making direct comparison with our approach difficult.Although our recall figures may seem low, es pecially when compared to other NLP tasks such as parsing and anaphora resolution, this is really a reflection of how difficult the task is. For example, in the problem of preposition selection in native text, a baseline using the most frequent preposition(of) results in precision and recall of 26%.In addi tion, the cloze tests presented earlier indicate thateven in well-formed text, agreement between na tive speakers on preposition selection is only 75%.In texts written by non-native speakers, rater dis agreement increases, as will be shown in the next section.While developing an error detection system forprepositions is certainly challenging, given the re sults from our work and others, evaluation also poses a major challenge.To date, single human annotation has typically been the gold standard for grammatical error detection, such as in the work of (Izumi et al, 2004), (Han et al, 2006), (Nagata et al, 2006), (Eeg-Olofsson and Knuttson, 2003) 2 .Another method for evaluation is verification ((Ga mon et al, 2008), where a human rater checks over a system?s output.The drawbacks of this approach are: 1.every time the system is changed, a rater is needed to re-check the output, and 2.it is very hard to estimate recall.What these two evaluation methods have in common is that they side-step the issue of annotator reliability.In this section, we show how relying on only onerater can be problematic for difficult error detec tion tasks, and in section 4, we propose a method(?the sampling approach?)for efficiently evaluat ing a system that does not require the amount ofeffort needed in the standard approach to annota tion.3.1 Annotation.To create a gold-standard corpus of error annotations for system evaluation, and also to deter mine whether multiple raters are better than one, 2(Eeg-Olofsson and Knuttson, 2003) had a small evaluation on 40 preposition contexts and it is unclear whether mul tiple annotators were used.we trained two native English speakers with prior NLP annotation experience to annotate prepositionerrors in ESL text.The training was very extensive: both raters were trained on 2000 preposition contexts and the annotation manual was it eratively refined as necessary.To summarize the procedure, the two raters were shown sentences randomly selected from student essays with each preposition highlighted in the sentence.They marked each context (?2-word window around thepreposition, plus the commanding verb) for gram mar and spelling errors, and then judged whether the writer used an incorrect preposition, a correct preposition, or an extraneous preposition.Finally, the raters suggested prepositions that would best fit the context, even if there were no error (some contexts can license multiple prepositions).3.2 Reliability.Each rater judged approximately 18,000 prepo sitions contexts, with 18 sets of 100 contextsjudged by both raters for purposes of comput ing kappa.Despite the rigorous training regimen, kappa ranged from 0.411 to 0.786, with an overall combined value of 0.630.Of the prepositions that Rater 1 judged to be errors, Rater 2 judged 30.2% to be acceptable.Conversely, of the prepositions Rater 2 judged to be erroneous, Rater 1 found 38.1% acceptable.The kappa of 0.630 shows the difficulty of this task and also shows how two highly trained raters can produce very different judgments.Details on our annotation and human judgment experiments can be found in (Tetreault and Chodorow, 2008).Variability in raters?judgments translates to variability of system evaluation.For instance, in our previous work (Chodorow et al, 2007), wefound that when our system?s output was com pared to judgments of two different raters, therewas a 10% difference in precision and a 5% differ ence in recall.These differences are problematicwhen evaluating a system, as they highlight the potential to substantially over- or under-estimate per formance.The results from the previous section motivate theneed for a more refined evaluation.They sug gest that for certain error annotation tasks, such as preposition usage, it may not be appropriate to use only one rater and that if one uses multiple raters 869for error annotation, there is the possibility of cre ating an adjudicated set, or at least calculating the variability of the system?s performance.However,annotation with multiple raters has its own disadvantages as it is much more expensive and time consuming.Even using one rater to produce a sizeable evaluation corpus of preposition errors is extremely costly.For example, if we assume that500 prepositions can be annotated in 4 hours us ing our annotation scheme, and that the base rate for preposition errors is 10%, then it would take atleast 80 hours for a rater to find and mark 1000 er rors.In this section, we propose a more efficient annotation approach to circumvent this problem.4.1 Methodology.Figure 1: Sampling Approach ExampleThe sampling procedure outlined here is inspired by the one described in (Chodorow and Lea cock, 2000) for the task of evaluating the usage of nouns, verbs and adjectives.The central idea is to skew the annotation corpus so that it contains a greater proportion of errors.Here are the steps in the procedure: 1.Process a test corpus of sentences so that each.preposition in the corpus is labeled ?OK? or ?Error?by the system.2.Divide the processed corpus into two sub-.corpora, one consisting of the system?s ?OK? prepositions and the other of the system?s ?Error?prepositions.For the hypotheticaldata in Figure 1, the ?OK? sub-corpus con tains 90% of the prepositions, and the ?Error?sub-corpus contains the remaining 10%.3.Randomly sample cases from each sub-.corpus and combine the samples into an an notation set that is given to a ?blind?human rater.We generally use a higher sampling rate for the ?Error?sub-corpus because we want to ?enrich?the annotation set with a larger proportion of errors than is found in the test corpus as a whole.In Figure 1, 75% of the ?Error?sub-corpus is sampled while only 16% of the ?OK? sub-corpus is sampled.4.For each case that the human rater judges to.be an error, check to see which sub-corpus itcame from.If it came from the ?OK? sub corpus, then the case is a Miss (an error that the system failed to detect).If it came from the ?Error?sub-corpus, then the case is a Hit (an error that the system detected).If the rater judges a case to be a correct usage and it came from the ?Error?sub-corpus, then it is a False Positive (FP).the sample from the ?Error?sub-corpus.Forthe hypothetical data in Figure 1, these val ues are 600/750 = 0.80 for Hits, and 150/750 = 0.20 for FPs.Calculate the proportion ofMisses in the sample from the ?OK? sub corpus.For the hypothetical data, this is 450/1500 = 0.30 for Misses.6.The values computed in step 5 are conditional.proportions based on the sub-corpora.To calculate the overall proportions in the test cor pus, it is necessary to multiply each value by the relative size of its sub-corpus.This is shown in Table 3, where the proportion ofHits in the ?Error?sub-corpus (0.80) is multiplied by the relative size of the ?Error?sub corpus (0.10) to produce an overall Hit rate (0.08).Overall rates for FPs and Misses are calculated in a similar manner.7.Using the values from step 6, calculate Preci-.sion (Hits/(Hits + FP)) and Recall (Hits/(Hits + Misses)).These are shown in the last two rows of Table 3.Estimated Overall Rates Sample Proportion * Sub-Corpus Proportion Hits 0.80 * 0.10 = 0.08 FP 0.20 * 0.10 = 0.02 Misses 0.30 * 0.90 = 0.27 Precision 0.08/(0.08 + 0.02) = 0.80 Recall 0.08/(0.08 + 0.27) = 0.23 Table 3: Sampling Calculations (Hypothetical) 870 This method is similar in spirit to active learning ((Dagan and Engelson, 1995) and (Engelson and Dagan, 1996)), which has been used to iteratively build up an annotated corpus, but it differs fromactive learning applications in that there are no it erative loops between the system and the human annotator(s).In addition, while our methodology is used for evaluating a system, active learning is commonly used for training a system.4.2 Application.Next, we tested whether our proposed sampling approach provides good estimates of a sys tem?s performance.For this task, we used the +Combo:word model to separate a large corpusof student essays into the ?Error?and ?OK? sub corpora.The original corpus totaled over 22,000 prepositions which would normally take several weeks for two raters to double annotate and thenadjudicate.After the two sub-corpora were propor tionally sampled, this resulted in an annotation set of 752 preposition contexts (requiring roughly 6 hours for annotation), which is substantially more manageable than the full corpus.We had both raters work together to make judgments for each preposition.It is important to note that while these are notthe exact same essays used in the previous evalua tion of 8,269 preposition contexts, they come from the same pool of student essays and were on the same topics.Given these strong similarities, we feel that one can compare scores between the two approaches.The precision and recall scores forboth approaches are shown in Table 4 and are ex tremely similar, thus suggesting that the samplingapproach can be used as an alternative to exhaus tive annotation.Precision Recall Standard Approach 80% 12% Sampling Approach 79% 14% Table 4: Sampling Results It is important with the sampling approach to use appropriate sample sizes when drawing from the sub-corpora, because the accuracy of the estimatesof hits and misses will depend upon the propor tion of errors in each sub-corpus as well as on the sample sizes.The OK sub-corpus is expected to have even fewer errors than the overall base rate, so it is especially important to have a relativelylarge sample from this sub-corpus.The compari son study described above used an OK sub-corpussample that was twice as large as the Error subcorpus sample (about 500 contexts vs. 250 con texts).In short, the sampling approach is intended to alleviate the burden on annotators when faced with the task of having to rate several thousand errors of a particular type in order to produce a sizeable error corpus.On the other hand, one advantage that exhaustive annotation has over the sampling method is that it makes possible the comparison of multiple systems.With the sampling approach, one would have to resample and annotate for each system, thus multiplying the work needed.One aspect of automatic error detection that usu ally is under-reported is an analysis of the errors that learners typically make.The obvious benefit of this analysis is that it can focus development of the system.From our annotated set of preposition errors, we found that the most common prepositions that learners used incorrectly were in (21.4%), to (20.8%) and of (16.6%).The top ten prepositions accounted for 93.8% of all preposition errors in our learner corpus.Next, we ranked the common preposition ?con fusions?, the common mistakes made for each preposition.The top ten most common confusions are listed in Table 5, where null refers to cases where no preposition is licensed (the writer usedan extraneous preposition).The most common of fenses were actually extraneous errors (see Table5): using to and of when no preposition was li censed accounted for 16.8% of all errors.It is interesting to note that the most common usage errors by learners overwhelmingly involved the ten most frequently occurring prepositions in native text.This suggests that our effort to handle the 34 most frequently occurring prepositions maybe overextended and that a system that is specifically trained and refined on the top ten preposi tions may provide better diagnostic feedback to a learner.This paper has two contributions to the field of error detection in non-native writing.First, we discussed a system that detects preposition errors with high precison (up to 84%) and is competitive 871 Writer?s Prep.Rater?s Prep.Frequency to null 9.5% of null 7.3% in at 7.1% to for 4.6% in null 3.2% of for 3.1% in on 3.1% of in 2.9% at in 2.7% for to 2.5% Table 5: Common Preposition Confusions with other leading methods.We used an ME approach augmented with combination features and a series of thresholds.This system is currently incorporated in the Criterion writing evaluationservice.Second, we showed that the standard ap proach to evaluating NLP error detection systems (comparing a system?s output with a gold-standard annotation) can greatly skew system results when the annotation is done by only one rater.However, one reason why a single rater is commonly used is that building a corpus of learner errors can be extremely costly and time consuming.To address this efficiency issue, we presented a sampling approach that produces results comparable to exhaustive annotation.This makes using multiple raters possible since less time is required to assess the system?s performance.While the work presented here has focused on prepositions, the arguments against using only one rater, and for using a sampling approach generalize to other error types, such as determiners and collocations.Acknowledgements We would first like to thank our two annotators Sarah Ohls and Waverly VanWinkle for their hours of hard work.We wouldalso like to acknowledge the three anonymous reviewers and Derrick Higgins for their helpful com ments and feedback.
HMM-Based Word Alignment In Statistical TranslationIn this paper, we describe a new model for word alignment in statistical trans- lation and present experimental results.The idea of the model is to make the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions.To achieve this goal, the approach us- es a first-order Hidden Markov model (HMM) for the word alignment problem as they are used successfully in speech recognition for the time alignment prob- lem.The difference to the time align- ment HMM is that there is no monotony constraint for the possible word order- ings.We describe the details of the mod- el and test the model on several bilingual corpora.In this paper, we address the problem of word alignments for a bilingual corpus.In the recent years, there have been a number of papers con- sidering this or similar problems: (Brown et al, 1990), (Dagan et al, 1993), (Kay et al, 1993), (Fung et al, 1993).In our approach, we use a first-order Hidden Markov model (HMM) (aelinek, 1976), which is similar, but not identical to those used in speech recognition.The key component of this approach is to make the alignment probabilities dependent not on the absolute position of the word align- ment, but on its relative position; i.e. we consider the differences in the index of the word positions rather than the index itself.The organization of the paper is as follows.After reviewing the statistical approach to ma- chine translation, we first describe the convention- al model (mixture model).We then present our first-order HMM approach in lull detail.Finally we present some experimental results and compare our model with the conventional model.The goal is the translation of a text given in some language F into a target language E. For conve- nience, we choose for the following exposition as language pair French and English, i.e. we are giv- en a French string f~ = fx ...fj...fJ, which is to be translated into an English string e / = el...ei...cl. Among all possible English strings, we will choose the one with the highest probability which is given by Bayes' decision rule: a{ = argmax{P,.(c{lAa)} q = argmax {Pr(ejt) . l ' r ( f  le\[)} el ~ Pr(e{) is the language model of the target lan- guage, whereas Pr(fJle{) is the string translation model.The argmax operation denotes the search problem.In this paper, we address the problem of introducing structures into the probabilistic de- pendencies in order to model the string translation probability Pr(f~ le{).A key issne in modeling the string translation probability Pr(J'~le I) is the question of how we define the correspondence b tween the words of the English sentence and the words of the French sentence.In typical cases, we can assume a sort of pairwise dependence by considering all word pairs (fj, ei) for a given sentence pair I.-/1\[~'J', elqlj' We fur- ther constrain this model by assigning each French word to exactly one English word.Models describ- ing these types of dependencies are referred to as alignment models.In this section, we describe two models for word alignrnent in detail: ,.a mixture-based alignment model, which was introduced in (Brown et al, 1990); ? an HMM-based alignment model.In this paper, we address the question of how to define specific models for the alignment probabil- ities.The notational convention will be as fol- lows.We use the symbol Pr(.)to denote general 836 probability distributions with (nearly) no Sl)eeitic asSUml)tions.In contrast, for modcl-t)ased prol)-- ability distributions, we use the generic symbol v(.).3.1 Al ignment w i th M ix ture D is t r i |mt ion.Here, we describe the mixture-based alignment model in a fornmlation which is different fronl the original formulation ill (Brown el, a\[., 1990).We will ,is(: this model as reference tbr the IIMM- based alignments to lie 1)resented later.The model is based on a decomposition of the joint probability \[br ,l'~ into a product over the probabilities for each word J): a j=l wheFe~ fo\[' norll-la\]iz;i, t on 17(~/SOllS~ the 8elltC\]\[ce length probability p(J\] l) has been included.The next step now is to assutne a sort O\['l,airwise inter- act, ion between tim French word f j an(l each, F,n- glish word ci, i = 1, ... l . These dep('ndencies are captured in the lbrm of a rnixtnre distritmtion: 1 p(J)le{) = ~_.p(i, fjlc I) i=1 I = ~_~p(ilj, l).p(fjle~) i=1 Putting everything together, we have the following mixture-based ntodel: J l r,'(fi!l~I) = p(JIO ' H ~_~ \[~,(ilJ, l).~,(j)led\] (1) j= l i=t with the following ingredients: ? sentence length prob~d)ility: P(J l l); ? mixture alignment probability: p( i l j , I); ? translation probM)ility: p(f\[e).Assuming a tmifornl ~flignment prol)ability 1 .p(ilj, 1) = 7 we arrive at the lh'st model proposed t)y (Brown et al, 1990).This model will be referred to as IB M 1 model.To train the translation probabilities p(J'fc), we use a bilingual (;orpus consisting of sentence pairs \[:/ ';4"1 : ', . , s Using the ,,laxin,ul , like- lihood criterion, we ol)tain the following iterative L a equation (Brown et al, 1990): / ) ( f ie) = ~ - will, $' A(f,e) = ~ 2 ~5(f,J).~) }~ a(e,e~.~) For unilbrm alignment probabilities, it can be shown (Brown et al, 1990), that there is only one optinnnn and therefore the I,',M algorithm (Baum, 1!)72) always tinds the global optimum.For mixture alignment model with nonunilbrm alignment probabilities (subsequently referred to as IBM2 model), there ~tre to() many alignrnent parameters Pill j , I) to be estimated for smMl co l pora.Therefore, a specific model tbr tile Mign- ment in:obabilities i used: r ( i - j~- ) (~) p( i l j , 1) = l . I E i ' : l "( it --" J J-) This model assumes that the position distance rel- ative to the diagonal ine of the (j, i) plane is the dominating factor (see Fig.1).'lb train this mod- el, we use the ,naximutn likelihood criterion in the so-called ulaximmn al)proximation, i.e. the likeli- hood criterion covers only tile most lik(-.ly align: inch, rather than the set of all alignm(,nts: d P,'(f(I,:I) ~ I I ~"IU HO, ~)v(J} I,:~)\] (a) j=l In training, this criterion amounts to a sequence of iterations, each of which consists of two steps: * posi l ion al ignmcnl: (riven the model parame- ters, deLerlniim the mosL likely position align- \]lient.paramc, lcr cst imal ion: Given the position alignment, i.e. goiug along the alignment paths for all sentence pairs, perform maxi- tnulu likelihood estimation of the model pa- rameters; for model-De(' distributions, these estimates result in relative frequencies.l)ue to the natnre of tile nfixture tnod(:l, there is no interaction between d jacent word positions.Theretbre, the optimal position i for each posi- tion j can be determined in(lependently of the neighbouring positions.Thus l.he resulting train- ing procedure is straightforward.a.2 Al ignment w i th HMM We now propose all HMM-based alignment model.'\['he motivation is that typicMly we have a strong localization effect in aligning the words in parallel texts (for language pairs fi:om \]ndoeuropean lan- guages): the words are not distrilmted arbitrarily over the senteuce \])ositions, but tend to form clus- ters.Fig.1 illustrates this effect for the language pair German- 15'nglish.Each word of the German sentence is assigned to a word of the English sentence.The alignments have a strong tendency to preserve the local neigh- borhood when going from the one langnage to the other language.In mm,y cases, although not al~ ways, there is an even stronger restriction: the differeuce in the position index is smMler than 3.837 DAYS BOTH ON EIGHT AT IT MAKE CAN WE IF THINK I WELL + + + + + + + + +j~ + + + + + + + + + ~J ~+ + +++++++/+?+.- . + + + + + + +/+ + + + + + + + + + ~x~ + + + + + + + + + +/+ D + + + + + ++ + + ~ + + + + + + + + + + _~ + + + + + + + + + +~ + + ++ ++++ + +jg + + + + + + + + + +~ +++ + + + + + + + g + + ++ + ++ + + + + z aa Figure 1: Word alignment for a German- English sentence pair.To describe these word-by-word aligmnents, we introduce the mapping j ---+ aj, which assigns a word f j in position j to a word el in position { = aj.The concept of these alignments i similar to the ones introduced by (Brown et al, 1990), but we wilt use another type of dependence in the probability distributions.Looking at such align- ments produced by a hmnan expert, it is evident that the mathematical model should try to cap- ture the strong dependence of aj on the previous aligmnent.Therefore the probability of alignment aj for position j should have a dependence on the previous alignment aj _ 1 : p(a j ia j_ l , i ) , where we have inchided the conditioning on the total length \[ of the English sentence for normal- ization reasons.A sinfilar approach as been cho- sen by (Da.gan et al, 1993).Thus the problem formulation is similar to that of the time align- ment problem in speech recognition, where the so-called IIidden Markov models have been suc- cessfully used for a long time (Jelinek, 1976).Us- ing the same basic principles, we can rewrite the probability by introducing the 'hidden' alignments af := al. . .aj . . .aa for a sentence pair If,a; e{\]: Pr(f~al es) = ~_,Vr(fal, aT\[ eI't, a7 ,1 = ~ 1-IP"(k,"stfT-',"{ -*,e/) a I j=l So far there has been no basic restriction of the approach.We now assume a first-order depen- dence on the alignments aj only: Vr(fj,aslf{ -~, J-* a I , e l ) where, in addition, we have assmned that tile translation probability del)ends only oil aj and not oil aj-:l. Putting everything together, we have the ibllowing llMM-based model: a Pr(f:i'le{) = ~ I-I \[p(ajlaj - ' , l).p(Y)lea,)\] (4) af J=, with the following ingredients: ? IlMM alignment probability: p(i\]i', I) or p(a j la j _ l , I ) ; ? translation probabflity: p(f\]e).In addition, we assume that the t{MM align- ment probabilities p(i\[i', \[) depend only on the jump width (i - i').Using a set of non-negative parameters {s ( i - i')}, we can write the IIMM alignment probabilities in the form: 4 i - i') (5) p(ili', i ) = E ' s(1 - i') 1=1 This form ensures that for each word position i', i' = 1, ..., I, the ItMM alignment probabilities satisfy the normMization constraint.Note the similarity between Equations (2) and (5).The mixtm;e model can be interpreted as a zeroth-order model in contrast to the first-order tlMM model.As with the IBM2 model, we use again the max- imum approximation: J Pr(fiSle~) "~ max\]--\[ \[p(asl<*j-1, z)p(fj l<~,)\] (6) a ' / .ll.j,,, j= l In th is case, the task o f f ind ing the opt ima l alignment is more involved than in the case of the mixture model (lBM2).Thereibre, we have to re- sort to dynainic programming for which we have the following typical reeursion formula: Q(i, j ) = p(f j lel) ,nvax \[p(ili', 1) . Q(i', j - 1)\] i =l , . , , I Here, Q(i, j ) is a sort of partial probability as in time alignment for speech recognition (Jelinek, 197@.The models were tested on several tasks: ? the Avalanche Bulletins published by the Swiss Federal Institute for Snow and Avalanche Research (SHSAR) in Davos, Switzerland and made awtilable by the Eu- p "q I ropean Corpus Initiative (I,CI/MCI, 1994); ? the Verbmobil Corpus consisting of sponta- neously spoken dialogs in the domain of ap- pointment scheduling (Wahlster, 1993); 838 ,, the EuTrans C, orpus which contains tyl)ical phrases from the tourists and t.ravel docnain.(EuTrans, 1996).' l 'able \] gives the details on the size of tit<; cor- pora a, ud t;\]t<'it' vocal>ulary.It shottld I>e noted that in a.ll thes(; three ca.ses the ratio el' vocal)t,- \]ary size a.ml numl)er of running words is not very faw)rable.Tall)le, I: (,orpol :L (,o~pt s l,angua.ge Words Voc.Size AvalancJte \] A\[ \ [ ra i l s Verlmlobil Frolt ch (~('~ l lal l Spanish I,;nglish ( le 11 an English 62849 ,\]4805 --1:77@- 15888 150279 25,\] 27 1993 2265 2008 t 63(} dO 17 2`\]/13 For several years 1)et;weeu 83 and !)2, the Avalanche Bulletins are awdlabte for I>oth Get- ntan and I!'ren(;\]l. The following is a tyl)ical sen-- t<;nce t>air fS;onl the <;or:IreS: Bei zu('.rst recht holnm, Sl)~i.tev tM'eren 'l'em- l)eraJ, uren sind vou Samsta.g his 1)ienstag tno f gett auf <l<'~t; All>ennor(ls<'.ite un</ am All>en-.ha.uptkanml oberhalb 2000 m 60 his 80 cm Neuschnee gel'aJlen.l)ar des temp&'atures d' abord dlevdes, puis plus basses, 60 h 8(1 cm de neige sent tombs de samedi h. mardi matin sur le versant herd el; la eft're des Alpes au-dessus de 2000 l\[1.An exa,nq)le fi'om the Vet%mobil corpus is given in Figure 1.l,;ach of the three COrlJora.were ttsed to train 1)oth al ignnmnt models, the mixture-I>ased al ignment model in Eq.(1) and the llMM-base<l a.lignntent mod('l in Eq.(d).ltere, we will consider the ex- p<'.rimenta.l tesl;s on tit<'.Avalanche corpus in more detail.The traii, ing procedure consiste(l of the following steps: ? , Init ial ization training: IBMI model trahted for t0 iterations of the i';M algorithm.,, l{,efinement traiuiug: The translation pcoba- 1)ilities Dotn the initialization training wet'(; use+d to initialize both the IBM2 model and the I I M M-based nligntnent mo<t<'+l IBM2 Model: 5 iteratious using Lit(" max- i lnum a.I)proximatiolt (Eq+(3)) I IMM Model: 5 iterations usiug l le max-.imum al)l)roximation (Fq.(6)) 'l'h(, resulting perl>h:'~xity (inverse g<~olu(;l.ric av- era,ge of the likelihoods) for the dilferent lno(lels ave given iu tim Tal>\[es 2 and 3 for the Awdanehe <:<)rims.In adclitiou t;o the total i>erl>lexity, whi<'.h is the' globa.l opt imizat ion criterion, the tables al- so show the perplexities of the translation prob- abilities and of the al ignment probabil it ies.The last line in Table 2 gives the perplexity measures wh(m a.lJplying the rtlaxilnun| approximat ion and COml>uting the perph'~xity in t;\]lis approximation.These values are equal to the ones after initializing the IBM2 and HMM models, as they should be.From Ta,ble 3, we can see.that the mixture align- ment gives slightly better perplexity values for the translation l)roba.1)ilities, whereas the I IMM mod- el produces a smaller perplexity for the al ignment l>rohal)ilities.In the calculatiot, of the, perplexi- ties, th<' seld;en(;e length probal)ility was not in= eluded.Tahle 2: IBM I: Translation, a, l igmnent and total pert)h'~xil.y as a. fimction of' the iteration.Iteration Tra,nslatiotl.Alignrnent Total 0 1 2 9 10 99.36 3.72 2.67 t.87 1.86 20.07 20.07 20.07 20.07 20.07 1994.00 7/1.57 53.62 37.55 37.36 Max.3.88 20.07 77.!)5 'l'able 3: '1 rans\] ~+tion, aligmn en t and totaJ perplex- ity as a function of the itcra.tion for the IBM2 (A) and the I IMM model (13) Iter.Tratmlat;i(m A 0 l 2 3 ,\] 5 1~ 0 1 3 4 5 A ligniN.elJ t 3.88- 20.07 3.17 10.82 3.25 10.15 3.22 10.10 3.20 \] 0.06 3.18 10.05 3.88 20.07 3.37 7.99 3.46 6.17 ;{./17 5.90 "Ld6 5.85 3.`\]5 5.8,\] ' l 'otal 77.95 34.27 33.03 32.48 32.18 32.00 77.95 26.98 2t.36 20.48 20.2/1 20.18 Anoth<2r inl;crc:sting question is whether the IIMM alignntent model helps in finding good and sharply fo('usscd word+to-word (-orres\]Jondences.As an (;xamf,1o, Table 4 gives a COmlm+rison of the translatioJ~ probabil it ies p(f l e) bctweett the mixture and the IIMM alignnw+nt model For the (,e, u +l word Alpensiidhang.The counts of the words a.re given in brackets.The, re is virLually no ,:lilfc~rc~nce between the translation l.al>les for the two nn)dels (1BM2 and I IMM).But+ itt general, the tl M M model seems to giw'.slightly better re- suits in the cases of (;, ttna t COml+olmd words like Alpcus'iidha'n,(I vcrsant sud des Alpes which re- quire \['u,tction words in the trattslation.839 Table 4: Alpens/idhang.IBM1 Alpes (684) 0.171 des (1968) 0.035 le (1419) 0.039 sud (416) 0.427 sur (769) 0.040 versant (431) 0.284 IBM2 Alpes (684) 0.276 sud (41.6) 0.371 versant (431) 0.356 HMM Alpes (684) 0.284 des (1968) 0.028 sud (416) 0.354 versant (431) 0.333 This is a result of the smoother position align- ments produced by the HMM model.A pro- nounced example is given in Figure 2.'She prob- lem of the absolute position alignment can he demonstrated at the positions (a) and (c): both Schneebretlgefahr und Schneeverfrachtungen have a high probability on neige.The IBM2 models chooses the position near the diagonal, as this is the one with the higher probability.Again, Schneebrettgefahr generates de which explains the wrong alignment near the diagonal in (c).However, this strength of the HMM model can also be a weakness as in the case of est developpe ist ... entstanden (see (b) in Figure 2.The required two large jumps are correctly found by the mixture model, but not by the HMM mod- el.These cases suggest an extention to the HMM model.In general, there are only a small number of big jumps in the position alignments in a given sentence pair.Therefore a model could be useful that distinguishes between local and big jumps.The models have also been tested on the Verb- mobil Translation Corpus as well as on a small Corpus used in the EuTrans project.The sen- tences in the EuTrans corpus are in general short phrases with simple grammatical structures.However, the training corpus is very small and the produced alignments are generally of poor quality.There is no marked difference for the two align- ment models.Table 5: Perplexity results (b) Verbmobil Corpus.for (a) EuTrans and Model Iter.Transl.Align.Total IBM1 10 2.610 6.233 16.267 IBM2 5 2.443 4.003 9.781 HMM 5 2.461 3.934 9.686 IBM1 10 4.373 10.674 46.672 IBM2 5 4.696 6.538 30.706 ItMM 5 4.859 5.452 26.495 The Verbmobil Corpus consists of spontaneous- ly spoken dialogs in the domain of appointment scheduling.The assumption that every word in the source language is aligned to a word in the target language breaks down for many sentence pairs, resulting in poor alignment.This in turn affects the quality of the translation probabilities.Several extensions to the current IIMM based model could be used to tackle these problems: * The results presented here did not use the concept of the empty word.For the HMM- based model this, however, requires a second- order rather than a first-order model.We could allow for multi-word phrases in.both languages.In addition to the absolute or relative align- ment positions, the alignment probabilities can be assumed to depend on part of speech tags or on the words themselves.(confer model 4 in (Brown et al, 1990)).5 Conclusion.In this paper, we have presented an itMM-based approach for rnodelling word aligmnents in par- allel texts.The characteristic feature of this ap- proach is to make the alignment probabilities ex- plicitly dependent on the alignment position of the previous word.We have tested the model suc- cessfully on real data.The HMM-based approach produces translation probabilities comparable to the mixture alignment model.When looking at the position alignments those generated by the ItMM model are in general much smoother.This could be especially helpful for languages uch as German, where compound words are matched to several words in the source language.On the oth- er hand, large jumps due to different word order- ings in the two languages are successfully modeled.We are presently studying and testing a nmltilevel HMM model that allows only a small number of large jumps.The ultimate test of the different alignment and translation models can only be car- ried out in the framework of a fully operational translation system.6 Acknowledgement.This research was partly supported by the (\]er- man Federal Ministery of Education, Science, t{e- search and Technology under the Contract Num- ber 01 IV 601 A (Verbmobil) and under the Esprit Research Project 20268 'EuTrans).
Online Learning of Relaxed CCG Grammars for Parsing to Logical FormWe consider the problem of learning toparse sentences to lambda-calculus repre sentations of their underlying semantics and present an algorithm that learns a weighted combinatory categorial grammar (CCG).A key idea is to introduce non-standard CCG combinators that relax certain parts of the grammar?for example allowing flexible word order, or insertion of lexical items?with learned costs.We also present a new, online algorithm for inducing a weighted CCG.Results for the approach on ATIS data show 86% F-measure in recovering fully correct semantic analyses and 95.9% F-measure by a partial-match criterion, a more than 5% improvement over the 90.3% partial-match figure reported by He and Young (2006).Recent work (Mooney, 2007; He and Young, 2006;Zettlemoyer and Collins, 2005) has developed learn ing algorithms for the problem of mapping sentences to underlying semantic representations.In one such approach (Zettlemoyer and Collins, 2005) (ZC05), the input to the learning algorithm is a training set consisting of sentences paired with lambda-calculus expressions.For instance, the training data might contain the following example: Sentence: list flights to boston Logical Form: ?x.flight(x) ? to(x, boston) In this case the lambda-calculus expression denotes the set of all flights that land in Boston.In ZC05 it is assumed that training examples do not include additional information, for example parse trees or a) on may four atlanta to denver delta flight 257 ?x.month(x,may) ? day number(x, fourth)?from(x, atlanta) ? to(x, denver)?airline(x, delta air lines) ? flight(x)?flight number(x, 257) b) show me information on american airlines from fort worth texas to philadelphia ?x.airline(x, american airlines)?from(x, fort worth) ? to(x, philadelphia) c) okay that one?s great too now we?re going to go on april twenty second dallas to washington the latest nighttime departure one way argmax(?x.flight(x) ? from(x, dallas)?to(x,washington) ? month(x, april)?day number(x, 22) ? during(x, night)?one way(x), ?y.depart time(y)) Figure 1: Three sentences from the ATIS domain.other derivations.The output from the learning algo rithm is a combinatory categorial grammar (CCG),together with parameters that define a log-linear distribution over parses under the grammar.Experi ments show that the approach gives high accuracy on two database-query problems, introduced by Zelle and Mooney (1996) and Tang and Mooney (2000).The use of a detailed grammatical formalism such as CCG has the advantage that it allows a system tohandle quite complex semantic effects, such as co ordination or scoping phenomena.In particular, it allows us to leverage the considerable body of work on semantics within these formalisms, for example see Carpenter (1997).However, a grammar based on a formalism such as CCG can be somewhat rigid, and this can cause problems when a system is faced with spontaneous, unedited natural language input, as is commonly seen in natural language interface applications.For example, consider the sentences shown in figure 1, which were taken from the ATIS travel-planning domain (Dahl et al, 1994).Thesesentences exhibit characteristics which present significant challenges to the approach of ZC05.For ex 678 ample, the sentences have quite flexible word order, and include telegraphic language where some words are effectively omitted.In this paper we describe a learning algorithm that retains the advantages of using a detailed grammar, but is highly effective in dealing with phenomenaseen in spontaneous natural language, as exempli fied by the ATIS domain.A key idea is to extendthe approach of ZC05 by allowing additional nonstandard CCG combinators.These combinators relax certain parts of the grammar?for example al lowing flexible word order, or insertion of lexical items?with learned costs for the new operations.This approach has the advantage that it can be seam lessly integrated into CCG learning algorithms such as the algorithm described in ZC05.A second contribution of the work is a new, online algorithm for CCG learning.The approach in volves perceptron training of a model with hidden variables.In this sense it is related to the algorithmof Liang et al (2006).However it has the addi tional twist of also performing grammar induction(lexical learning) in an online manner.In our experiments, we show that the new algorithm is consid erably more efficient than the ZC05 algorithm; this is important when training on large training sets, for example the ATIS data used in this paper.Results for the approach on ATIS data show 86%F-measure accuracy in recovering fully correct semantic analyses, and 95.9% F-measure by a partial match criterion described by He and Young (2006).The latter figure contrasts with a figure of 90.3% for the approach reported by He and Young (2006).1Results on the Geo880 domain also show an im provement in accuracy, with 88.9% F-measure for the new approach, compared to 87.0% F-measure for the method in ZC05.2.1 Semantics.Training examples in our approach consist of sen tences paired with lambda-calculus expressions.We use a version of the lambda calculus that is closely related to the one presented by Carpenter (1997).There are three basic types: t, the type of truth val 1He and Young (2006) do not give results for recovering fully correct parses.ues; e, the type for entities; and r, the type for realnumbers.Functional types are defined by specify ing their input and output types, for example ?e, t?is the type of a function from entities to truth val ues.In general, declarative sentences have a logical form of type t. Question sentences generally have functional types.2 Each expression is constructed from constants, logical connectors, quantifiers and lambda functions.2.2 Combinatory Categorial Grammars.Combinatory categorial grammar (CCG) is a syn tactic theory that models a wide range of linguistic phenomena (Steedman, 1996; Steedman, 2000).The core of a CCG grammar is a lexicon ?.For example, consider the lexicon flights := N : ?x.flight(x) to := (N\N)/NP : ?y.?f.?x.f(x) ? to(x, y) boston := NP : boston Each entry in the lexicon is a pair consisting of aword and an associated category.The category con tains both syntactic and semantic information.For example, the first entry states that the word flightscan have the category N : ?x.flight(x).This cat egory consists of a syntactic type N , together withthe semantics ?x.flight(x).In general, the seman tic entries for words in the lexicon can consist of anylambda-calculus expression.Syntactic types can ei ther be simple types such as N , NP , or S, or can be more complex types that make use of slash notation, for example (N\N)/NP . CCG makes use of a set of combinators which are used to combine categories to form larger pieces of syntactic and semantic structure.The simplest such rules are the functional application rules: A/B : f B : g ? A : f(g) (>) B : g A\B : f ? A : f(g) (<) The first rule states that a category with syntactic type A/B can be combined with a category to the right of syntactic type B to create a new category of type A. It also states that the new semantics will be formed by applying the function f tothe expression g. The second rule handles argu ments to the left.Using these rules, we can parse the 2For example, many question sentences have semantics of type ?e, t?, as in ?x.flight(x) ? to(x, boston).679 following phrase to create a new category of typeN : flights to boston N (N\N)/NP NP ?x.flight(x) ?y.?f.?x.f(x) ? to(x, y) boston > (N\N) ?f.?x.f(x) ? to(x, boston) < N ?x.flight(x) ? to(x, boston) The top-most parse operations pair each word with a corresponding category from the lexicon.The later steps are labeled ?> (for each instance of forward application) or ?< (for backward application).A second set of combinators in CCG grammars are the rules of functional composition: A/B : f B/C : g ? A/C : ?x.f(g(x)) (> B) B\C : g A\B : f ? A\C : ?x.f(g(x)) (< B)These rules allow for an unrestricted notion of con stituency that is useful for modeling coordination and other linguistic phenomena.As we will see, theyalso turn out to be useful when modeling construc tions with relaxed word order, as seen frequently in domains such as ATIS.In addition to the application and compositionrules, we will also make use of type raising and co ordination combinators.A full description of these combinators goes beyond the scope of this paper.Steedman (1996; 2000) presents a detailed descrip tion of CCG.2.3 Log-Linear CCGs.We can generalize CCGs to weighted, or probabilis tic, models as follows.Our models are similar to several other approaches (Ratnaparkhi et al, 1994; Johnson et al, 1999; Lafferty et al, 2001; Collins,2004; Taskar et al, 2004).We will write x to de note a sentence, and y to denote a CCG parse for asentence.We use GEN(x; ?) to refer to all possi ble CCG parses for x under some CCG lexicon ?.We will define f(x, y) ? Rd to be a d-dimensional feature?vector that represents a parse tree y pairedwith an input sentence x. In principle, f could include features that are sensitive to arbitrary sub structures within the pair (x, y).We will define w ? Rd to be a parameter vector.The optimal parse for a sentence x under parameters w and lexicon ? is then defined as y?(x) = arg max y?GEN(x;?)w ? f(x, y) . Assuming sufficiently local features3 in f , search fory?can be achieved using dynamic-programming style algorithms, typically with some form of beam search.4 Training a model of this form involves learning the parameters w and potentially also thelexicon ?.This paper focuses on a method for learn ing a (w,?)pair from a training set of sentences paired with lambda-calculus expressions.2.4 Zettlemoyer and Collins 2005.We now give a description of the approach of Zettle moyer and Collins (2005).This method will form the basis for our approach, and will be one of the baseline models for the experimental comparisons.The input to the ZC05 algorithm is a set of train ing examples (xi, zi) for i = 1 . . .n. Each xi isa sentence, and each zi is a corresponding lambda expression.The output from the algorithm is a pair (w,?)specifying a set of parameter values, and a CCG lexicon.Note that for a given training example (xi, zi), there may be many possible parses y which lead to the correct semantics zi.5 For this reason the training problem is a hidden-variable problem,where the training examples contain only partial information, and the CCG lexicon and parse deriva tions must be learned without direct supervision.A central part of the ZC05 approach is a function GENLEX(x, z) which maps a sentence x together with semantics z to a set of potential lexical entries.The function GENLEX is defined through a set of rules?see figure 2?that consider the expression z, and generate a set of categories that may help in building the target semantics z. An exhaustive setof lexical entries is then generated by taking all categories generated by the GENLEX rules, and pair ing themwith all possible sub-strings of the sentencex.Note that our lexicon can contain multi-word en tries, where a multi-word string such as New Yorkcan be paired with a CCG category.The final out 3For example, features which count the number of lexical entries of a particular type, or features that count the number of applications of a particular CCG combinator.4In our experiments we use a parsing algorithm that is simi lar to a CKY-style parser with dynamic programming.Dynamic programming is used but each entry in the chart maintains a full semantic expression, preventing a polynomial-time algorithm; beam search is used to make the approach tractable.5This problem is compounded by the fact that the lexicon is unknown, so that many of the possible hidden derivations involve completely spurious lexical entries.680 Rules Example categories produced from the logical form Input Trigger Output Category argmax(?x.flight(x) ? from(x, boston), ?x.cost(x)) constant c NP : c NP : boston arity one predicate p N : ?x.p(x) N : ?x.flight(x) arity one predicate p S\NP : ?x.p(x) S\NP : ?x.flight(x) arity two predicate p2 (S\NP )/NP : ?x.?y.p2(y, x) (S\NP )/NP : ?x.?y.from(y, x) arity two predicate p2 (S\NP )/NP : ?x.?y.p2(x, y) (S\NP )/NP : ?x.?y.from(x, y) arity one predicate p1 N/N : ?g.?x.p1(x) ? g(x) N/N : ?g.?x.flight(x) ? g(x) literal with arity two predicate p2 and constant second argument c N/N : ?g.?x.p2(x, c) ? g(x) N/N : ?g.?x.from(x, boston) ? g(x) arity two predicate p2 (N\N)/NP : ?y.?g.?x.p2(x, y) ? g(x) (N\N)/NP : ?y.?g.?x.from(x, y) ? g(x) an argmax /min with second argument arity one function f NP/N : ?g. argmax /min(g, ?x.f(x)) NP/N : ?g. argmax(g, ?x.cost(x)) arity one function f S/NP : ?x.f(x) S/NP : ?x.cost(x) arity one function f (N\N)/NP : ?y.?f.?x.g(x) ? f(x) >/< y (N\N)/NP : ?y.?f.?x.g(x) ? cost(x) > y no trigger S/NP : ?x.x, S/N : ?f.?x.f(x) S/NP : ?x.x, S/N : ?f.?x.f(x) Figure 2: Rules used in GENLEX.Each row represents a rule.The first column lists the triggers that identify some sub-structure within a logical form.The second column lists the category that is created.The third column lists categories that are created when the rule is applied to the logical form at the top of this column.We use the 10 rules described in ZC05 and add two new rules, listed in the last two rows above.This first new rule is instantiated for greater than (>) and less than (<) comparisions.The second new rule has no trigger; it is always applied.It generates categories that are used to learn lexical entries for semantically vacuous sentence prefixes such as the phrase show me information on in the example in figure 1(b).put from GENLEX(x, z) is a large set of potentiallexical entries, with the vast majority of those en tries being spurious.The algorithm in ZC05 embeds GENLEX within an overall learning approach that simultaneously selects a small subset of all entriesgenerated by GENLEX and estimates parameter val uesw.Zettlemoyer and Collins (2005) present more complete details.In section 4.2 we describe a new, online algorithm that uses GENLEX.This section describes a set of CCG combinators which we add to the conventional CCG combinatorsdescribed in section 2.2.These additional combinators are natural extensions of the forward appli cation, forward composition, and type-raising rulesseen in CCG.We first describe a set of combinators that allow the parser to significantly relax con straints on word order.We then describe a set of type-raising rules which allow the parser to copewith telegraphic input (in particular, missing func tion words).In both cases these additional rules lead to significantly more parses for any sentence x given a lexicon ?.Many of these parses will be suspect from a linguistic perspective; broadening theset of CCG combinators in this way might be con sidered a dangerous move.However, the learning algorithm in our approach can learn weights for the new rules, effectively allowing the model to learn touse them only in appropriate contexts; in the exper iments we show that the rules are highly effective additions when used within a weighted CCG.3.1 Application and Composition Rules.The first new combinators we consider are the relaxed functional application rules: A\B : f B : g ? A : f(g) (&) B : g A/B : f ? A : f(g) (.)These are variants of the original applicationrules, where the slash direction on the principal categories (A/B or A\B) is reversed.6 These rules allow simple reversing of regular word order, for ex ample flights one way N N/N ?x.flight(x) ?f.?x.f(x) ? one way(x) . N ?x.flight(x) ? one way(x) Note that we can recover the correct analysis for this fragment, with the same lexical entries as those used for the conventional word order, one-way flights.A second set of new combinators are the relaxed functional composition rules: A\B : f B/C : g ? A/C : ?x.f(g(x)) (& B) B\C : g A/B : f ? A\C : ?x.f(g(x)) (.B)These rules are variantions of the standard func tional composition rules, where the slashes of the principal categories are reversed.6Rules of this type are non-standard in the sense that theyviolate Steedman?s Principle of Consistency (2000); this princi ple states that rules must be consistent with the slash direction of the principal category.Steedman (2000) only considers rulesthat do not violate this principle?for example, crossed compo sition rules, which we consider later, and which Steedman also considers, do not violate this principle.681An important point is that that these new composition and application rules can deal with quite flex ible word orders.For example, take the fragment to washington the latest flight.In this case the parse is to washington the latest flight N\N NP/N N ?f.?x.f(x)??f. argmax(f, ?x.flight(x) to(x,washington) ?y.depart time(y)) .B NP\N ?f. argmax(?x.f(x)?to(x,washington), ?y.depart time(y)) & NP argmax(?x.flight(x) ? to(x,washington), ?y.depart time(y))Note that in this case the substring the latest has cat egory NP/N , and this prevents a naive parse wherethe latest first combines with flight, and to washington then combines with the latest flight.The func tional composition rules effectively allow the latest to take scope over flight and to washington, in spite of the fact that the latest appears between the twoother sub-strings.Examples like this are quite fre quent in domains such as ATIS.We add features in the model which track the oc currences of each of these four new combinators.Specifically, we have four new features in the def inition of f; each feature tracks the number of times one of the combinators is used in a CCG parse.Themodel learns parameter values for each of these fea tures, allowing it to learn to penalise these rules to the correct extent.3.2 Additional Rules of Type-Raising.We now describe new CCG operations designed todeal with cases where words are in some sense miss ing in the input.For example, in the string flights Boston to New York, one style of analysis would assume that the preposition from had been deleted from the position before Boston.The first set of rules is generated from the follow ing role-hypothesising type shifting rules template: NP : c ? N\N : ?f.?x.f(x) ? p(x, c) (TR) This rule can be applied to any NP with semantics c, and any arity-two function p such that the secondargument of p has the same type as c. By ?any?aritytwo function, we mean any of the arity-two func tions seen in training data.We define features within the feature-vector f that are sensitive to the number of times these rules are applied in a parse; a separate feature is defined for each value of p. In practice, in our experiments most rules of this form have p as the semantics of some preposition, for example from or to.A typical example of a use of this rule would be the following: flights boston to new york N NP N\N ?x.flight(x) bos ?f.?x.f(x) ?to(x, new york) TR N\N ?f.?x.f(x) ? from(x, bos) < N ?f.?x.flight(x) ? from(x, bos) < N ?x.flight(x) ? to(x, new york) ? from(x, bos) The second rule we consider is the null-head type shifting rule: N\N : f ? N : f(?x.true) (TN)This rule allows parses of fragments such as Amer ican Airlines from New York, where there is again aword that is in some sense missing (it is straightfor ward to derive a parse for American Airlines flights from New York).The analysis would be as follows: American Airlines from New York N/N N\N ?f.?x.f(x) ? airline(x, aa) ?f.?x.f(x) ? from(x, new york) TN N ?x.from(x, new york) > N ?x.airline(x, aa) ? from(x, new york)The new rule effectively allows the preposi tional phrase from New York to type-shift to an entry with syntactic type N and semantics ?x.from(x, new york), representing the set of all things from New York.7 We introduce a single additional feature which counts the number of times this rule is used.3.3 Crossed Composition Rules.Finally, we include crossed functional composition rules: A/B : f B\C : g ? A\C : ?x.f(g(x)) (>B?) B/C : g A\B : f ? A/C : ?x.f(g(x)) (<B?) These rules are standard CCG operators but they were not used by the parser described in ZC05.When used in unrestricted contexts, they can sig nificantly relax word order.Again, we address this 7Note that we do not analyze this prepositional phrase as having the semantics ?x.flight(x) ? from(x, new york)?although in principle this is possible?as the flight(x) predi cate is not necessarily implied by this utterance.682 dallas to washington the latest on friday NP (N\N)/NP NP NP/N (N\N)/NP NP dallas ?y.?f.?x.f(x) washington ?f. argmax(f, ?y.?f.?x.f(x) friday ?to(x, y) ?y.depart time(y)) ?day(x, y) TR > > N\N N\N N\N ?f.?x.f(x) ? from(x, dallas) ?f.?x.f(x) ? to(x,washington) ?f.?x.f(x) ? day(x, friday) <B TN N\N N ?f.?x.f(x) ? from(x, dallas) ? to(x,washington) ?x.day(x, friday) .B NP\N ?f. argmax(?x.f(x) ? from(x, dallas) ? to(x,washington), ?y.depart time(y)) & NP argmax(?x.day(x, friday) ? from(x, dallas) ? to(x,washington), ?y.depart time(y)) Figure 3: A parse with the flexible parser.problem by introducing features that count the num ber of times they are used in a parse.8 3.4 An Example.As a final point, to see how these rules can interact in practice, see figure 3.This example demonstrates the use of the relaxed application and composition rules, as well as the new type-raising rules.This section describes an approach to learning in ourmodel.We first define the features used and then de scribe a new online learning algorithm for the task.4.1 Features in the Model.Section 2.3 described the use of a function f(x, y) which maps a sentence x together with a CCG parse y to a feature vector.As described in section 3,we introduce features for the new CCG combinators.In addition, we follow ZC05 in defining fea tures which track the number of times each lexical item in ? is used.For example, we would have one feature tracking the number of times the lexical entry flights := N : ?x.flights(x) is used in a parse, and similar features for all other members of ?.Finally, we introduce new features which directly consider the semantics of a parse.For each predicate f seen in training data, we introduce a feature that counts the number of times f is conjoined with itself at some level in the logical form.For example, the expression ?x.flight(x) ? from(x, new york) ? from(x, boston) would trigger the new feature for 8In general, applications of the crossed composition rules can be lexically governed, as described in work on Multi-ModalCCG (Baldridge, 2002).In the future we would like to incorpo rate more fine-grained lexical distinctions of this type.the from predicate signaling that the logical-form describes flights with more than one origin city.We introduce similar features which track disjunction as opposed to conjunction.4.2 An Online Learning Algorithm.Figure 4 shows a learning algorithm that takes a training set of (xi, zi) pairs as input, and returns a weighted CCG (i.e., a pair (w,?)) as its output.The algorithm is online, in that it visits each example in turn, and updates both w and ? if neces sary.In Step 1 on each example, the input xi isparsed.If it is parsed correctly, the algorithm im mediately moves to the next example.In Step 2,the algorithm temporarily introduces all lexical en tries seen in GENLEX(xi, zi), and finds the highest scoring parse that leads to the correct semantics zi.A small subset of GENLEX(xi, zi)?namely, only those lexical entries that are contained in the highest scoring parse?are added to ?.In Step 3, a simple perceptron update (Collins, 2002) is performed.The hypothesis is parsed again with the new lexicon, andan update to the parameters w is made if the result ing parse does not have the correct logical form.This algorithm differs from the approach in ZC05in a couple of important respects.First, the ZC05 al gorithm performed learning of the lexicon ? at each iteration in a batch method, requiring a pass over the entire training set.The new algorithm is fully online, learning both ? and w in an example-by-example fashion.This has important consequences for the efficiency of the algorithm.Second, the parameter estimation method in ZC05 was based on stochasticgradient descent on a log-likelihood objective func tion.The new algorithm makes use of perceptron 683 Inputs: Training examples {(xi, zi) : i = 1 . . .n} where each xi is a sentence, each zi is a logical form.An initial lexicon ?0.Number of training iterations, T . Definitions: GENLEX(x, z) takes as input a sentence x anda logical form z and returns a set of lexical items as de scribed in section 2.4.GEN(x; ?) is the set of all parses for x with lexicon ?.GEN(x, z; ?) is the set of all parses for x with lexicon ?, which have logical form z. Thefunction f(x, y) represents the features described in sec tion 4.1.The function L(y) maps a parse tree y to its associated logical form.Initialization: Set parameters w to initial values described in section 6.2.Set ? = ?0.Algorithm: ? For t = 1 . . .T, i = 1 . . .n : Step 1: (Check correctness) ? Let y?= argmaxy?GEN(xi;?)w ? f(xi, y) . ? If L(y?) = zi, go to the next example.Step 2: (Lexical generation) ? Set ? = ? ?GENLEX(xi, zi) . ? Let y?= argmaxy?GEN(xi,zi;?)w ? f(xi, y) . ? Define ?i to be the set of lexical entries in y?.Set lexicon to ? = ? ??i . Step 3: (Update parameters) ? Let y?= argmaxy?GEN(xi;?)w ? f(xi, y) . ? If L(y?) 6= zi : ? Set w = w + f(xi, y?)f(xi, y?)Output: Lexicon ? together with parameters w. Figure 4: An online learning algorithm.updates, which are simpler and cheaper to compute.As in ZC05, the algorithm assumes an initial lex icon ?0 that contains two types of entries.First, we compile entries such as Boston := NP : boston for entities such as cities, times and month-names that occur in the domain or underlying database.In practice it is easy to compile a list of these atomic entities.Second, the lexicon has entries for some function words such as wh-words, and determiners.9There has been a significant amount of previous work on learning to map sentences to under lying semantic representations.A wide variety 9Our assumption is that these entries are likely to be domain independent, so it is simple enough to compile a list that can be reused in new domains.Another approach, which we may consider in the future, would be to annotate a small subset of the training examples with full CCG derivations, from which these frequently occurring entries could be learned.of techniques have been considered including ap proaches based on machine translation techniques (Papineni et al, 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006), parsing techniques(Miller et al, 1996; Ge and Mooney, 2006), tech niques that use inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2002; Tang and Mooney, 2000; Kate et al, 2005), andideas from string kernels and support vector ma chines (Kate and Mooney, 2006; Nguyen et al, 2006).In our experiments we compare to He and Young (2006) on the ATIS domain and Zettlemoyerand Collins (2005) on the Geo880 domain, because these systems currently achieve the best per formance on these problems.The approach of Zettlemoyer and Collins (2005) was presented in section 2.4.He and Young (2005) describe an algorithm that learns a probabilisticpush-down automaton that models hierarchical de pendencies but can still be trained on a data set that does not have full treebank-style annotations.Thisapproach has been integrated with a speech recog nizer and shown to be robust to recognition errors (He and Young, 2006).There is also related work in the CCG litera ture.Clark and Curran (2003) present a method forlearning the parameters of a log-linear CCG pars ing model from fully annotated normal?form parse trees.Watkinson and Manandhar (1999) present an unsupervised approach for learning CCG lexiconsthat does not represent the semantics of the training sentences.Bos et al (2004) present an al gorithm that learns CCG lexicons with semantics but requires fully?specified CCG derivations in thetraining data.Bozsahin (1998) presents work on us ing CCG to model languages with free word order.In addition, there is related work that focuses on modeling child language learning.Siskind (1996) presents an algorithm that learns word-to-meaning mappings from sentences that are paired with a set of possible meaning representations.Villavicencio (2001) describes an approach that learns a categorial grammar with syntactic and semantic information.Both of these approaches use sentences from child directed speech, which differ significantly from the natural language interface queries we consider.Finally, there is work on manually developing parsing techniques to improve robustness (Carbonell 684and Hayes, 1983; Seneff, 1992).In contrast, our ap proach is integrated into a learning framework.The main focus of our experiments is on the ATIS travel planning domain.For development, we used4978 sentences, split into a training set of 4500 ex amples, and a development set of 478 examples.Fortest, we used the ATIS NOV93 test set which con tains 448 examples.To create the annotations, wecreated a script that maps the original SQL annotations provided with the data to lambda-calculus ex pressions.He and Young (2006) previously reported results on the ATIS domain, using a learning approachwhich also takes sentences paired with semantic annotations as input.In their case, the semantic struc tures resemble context-free parses with semantic (asopposed to syntactic) non-terminal labels.In our experiments we have used the same split into training and test data as He and Young (2006), ensur ing that our results are directly comparable.He and Young (2006) report partial match figures for their parser, based on precision and recall in recovering attribute-value pairs.(For example, the sentence flights to Boston would have a single attribute-valueentry, namely destination = Boston.)It is sim ple for us to map from lambda-calculus expressions to attribute-value entries of this form; for example, the expression to(x,Boston) would be mapped to destination = Boston.He and Young (2006) gave us their data and annotations, so we can directly compare results on the partial-match criterion.Wealso report accuracy for exact matches of lambda calculus expressions, which is a stricter criterion.In addition, we report results for the method on the Geo880 domain.This allows us to compare directly to the previous work of Zettlemoyer and Collins (2005), using the same split of the data intotraining and test sets of sizes 600 and 280 respec tively.We use cross-validation of the training set, asopposed to a separate development set, for optimiza tion of parameters.6.1 Improving Recall.The simplest approach to the task is to train the parser and directly apply it to test sentences.In our experiments we will see that this produces resultswhich have high precision, but somewhat lower recall, due to some test sentences failing to parse (usu ally due to words in the test set which were neverobserved in training data).A simple strategy to alle viate this problem is as follows.If the sentence failsto parse, we parse the sentence again, this time al lowing parse moves which can delete words at some cost.The cost of this deletion operation is optimizedon development data.This approach can significantly improve F-measure on the partial-match cri terion in particular.We report results both with and without this second pass strategy.6.2 Parameters in the Approach.The algorithm in figure 4 has a number of param eters, the set {T, ?, ?, ?}, which we now describe.The values of these parameters were chosen to op timize the performance on development data.T is the number of passes over the training set, and was set to be 4.Each lexical entry in the initial lexicon?0 has an associated feature which counts the num ber of times this entry is seen in a parse.The initial parameter value in w for all features of this form was chosen to be some value ?.Each of the newCCG rules?the application, composition, crossedcomposition, and type-raising rules described in sec tion 3?has an associated parameter.We set al of these parameters to the same initial value ?.Finally, when new lexical entries are added to ?(in step 2 of the algorithm), their initial weight is set to some value ?.In practice, optimization on developmentdata led to a positive value for ?, and negative val ues for ? and ?.6.3 Results.Table 1 shows accuracy for the method by the exact match criterion on the ATIS test set.The two passstrategy actually hurts F-measure in this case, al though it does improve recall of the method.Table 2 shows results under the partial-match cri terion.The results for our approach are higher than those reported by He and Young (2006) even without the second, high-recall, strategy.With the two-pass strategy our method has more than halved the F-measure error rate, giving improvements from 90.3% F-measure to 95.9% F-measure.Table 3 shows results on the Geo880 domain.The 685 Precision Recall F1 Single-Pass Parsing 90.61 81.92 86.05 Two-Pass Parsing 85.75 84.6 85.16 Table 1: Exact-match accuracy on the ATIS test set.Precision Recall F1 Single-Pass Parsing 96.76 86.89 91.56 Two-Pass Parsing 95.11 96.71 95.9 He and Young (2006) ? ?90.3 Table 2: Partial-credit accuracy on the ATIS test set.new method gives improvements in performance both with and without the two pass strategy, showingthat the new CCG combinators, and the new learn ing algorithm, give some improvement on even this domain.The improved performance comes from aslight drop in precision which is offset by a large in crease in recall.Table 4 shows ablation studies on the ATIS data, where we have selectively removed various aspectsof the approach, to measure their impact on performance.It can be seen that accuracy is seriously de graded if the new CCG rules are removed, or if the features associated with these rules (which allow the model to penalize these rules) are removed.Finally, we report results concerning the effi ciency of the new online algorithm as compared to the ZC05 algorithm.We compared running times for the new algorithm, and the ZC05 algorithm, on the geography domain, with both methods making 4 passes over the training data.The new algorithm took less than 4 hours, compared to over 12 hours for the ZC05 algorithm.The main explanation for this improved performance is that on many trainingexamples,10 in step 1 of the new algorithm a cor rect parse is found, and the algorithm immediately moves on to the next example.Thus GENLEX is not required, and in particular parsing the example with the large set of entries generated by GENLEX is not required.We presented a new, online algorithm for learning a combinatory categorial grammar (CCG), together with parameters that define a log-linear pars ing model.We showed that the use of non-standardCCG combinators is highly effective for parsing sen 10Measurements on the Geo880 domain showed that in the 4 iterations, 83.3% of all parses were successful at step 1.Precision Recall F1 Single-Pass Parsing 95.49 83.2 88.93 Two-Pass Parsing 91.63 86.07 88.76 ZC05 96.25 79.29 86.95 Table 3: Exact-match accuracy on the Geo880 test set.Precision Recall F1 Full Online Method 87.26 74.44 80.35 Without control features 70.33 42.45 52.95 Without relaxed word order 82.81 63.98 72.19 Without word insertion 77.31 56.94 65.58 Table 4: Exact-match accuracy on the ATIS development setfor the full algorithm and restricted versions of it.The sec ond row reports results of the approach without the featuresdescribed in section 3 that control the use of the new combi nators.The third row presents results without the combinators from section 3.1 that relax word order.The fourth row reports experiments without the type-raising combinators presented in section 3.2.tences with the types of phenomena seen in spontaneous, unedited natural language.The resulting sys tem achieved significant accuracy improvements in both the ATIS and Geo880 domains.Acknowledgements Wewould like to thank Yulan He and Steve Young for their help with obtaining the ATIS data set.We also acknowledge the support for this research.Luke Zettlemoyer was funded by a Microsoft graduateresearch fellowship and Michael Collins was sup ported by the National Science Foundation under grants 0347631 and DMS-0434222.
Tree Kernel-Based Relation Extraction with Context-Sensitive Structured Parse Tree InformationThis paper proposes a tree kernel with contextsensitive structured parse tree information for re lation extraction.It resolves two critical problems in previous tree kernels for relation extraction in two ways.First, it automatically determines a dynamic context-sensitive tree span for relation ex traction by extending the widely-used Shortest Path-enclosed Tree (SPT) to include necessary context information outside SPT.Second, it pro poses a context-sensitive convolution tree kernel, which enumerates both context-free and context sensitive sub-trees by considering their ancestor node paths as their contexts.Moreover, this paper evaluates the complementary nature between our tree kernel and a state-of-the-art linear kernel.Evaluation on the ACE RDC corpora shows that our dynamic context-sensitive tree span is much more suitable for relation extraction than SPT and our tree kernel outperforms the state-of-the-art Collins and Duffy?s convolution tree kernel.It also shows that our tree kernel achieves much bet ter performance than the state-of-the-art linear kernels . Finally, it shows that feature-based and tree kernel-based methods much complement each other and the composite kernel can well integrate both flat and structured features.Relation extraction is to find various predefined se mantic relations between pairs of entities in text.The research in relation extraction has been promoted by the Message Understanding Conferences (MUCs) (MUC, 1987-1998) and the NIST Automatic Content Extraction (ACE) program (ACE, 2002-2005).Ac cording to the ACE Program, an entity is an object or a set of objects in the world and a relation is an explicitly or implicitly stated relationship among enti ties.For example, the sentence ?Bill Gates is the chairman and chief software architect of Microsoft Corporation.?conveys the ACE-style relation ?EMPLOYMENT.exec?between the entities ?Bill Gates?(person name) and ?Microsoft Corporation?(organization name).Extraction of semantic relations between entities can be very useful in many applica tions such as question answering, e.g. to answer the query ?Who is the president of the United States??, and information retrieval, e.g. to expand the query ?George W. Bush?with ?the president of the United States?via his relationship with ?the United States?.Many researches have been done in relation extraction.Among them, feature-based methods (Kamb hatla 2004; Zhou et al, 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entityrelated information to syntactic parse trees, depend ency trees and semantic information.However, it is difficult for them to effectively capture structured parse tree information (Zhou et al2005), which is critical for further performance improvement in rela tion extraction.As an alternative to feature-based methods, tree kernel-based methods provide an elegant solution to explore implicitly structured features by directly computing the similarity between two trees.Although earlier researches (Zelenko et al2003; Culotta and Sorensen 2004; Bunescu and Mooney 2005a) only achieve success on simple tasks and fail on complex tasks, such as the ACE RDC task, tree kernel-based methods achieve much progress recently.As the state-of-the-art, Zhang et al(2006) applied the convo lution tree kernel (Collins and Duffy 2001) and achieved comparable performance with a state-of-the art linear kernel (Zhou et al2005) on the 5 relation types in the ACE RDC 2003 corpus.However, there are two problems in Collins and Duffy?s convolution tree kernel for relation extraction.The first is that the sub-trees enumerated in the tree kernel computation are context-free.That is, each sub-tree enumerated in the tree kernel computation 728 does not consider the context information outside the sub-tree.The second is to decide a proper tree span in relation extraction.Zhang et al(2006) explored five tree spans in relation extraction and it was a bit sur prising to find that the Shortest Path-enclosed Tree (SPT, i.e. the sub-tree enclosed by the shortest path linking two involved entities in the parse tree) performed best.This is contrast to our intuition.For ex ample, ?got married?is critical to determine the relationship between ?John?and ?Mary?in the sen tence ?John and Mary got married?as shown in Figure 1(e).It is obvious that the information con tained in SPT (?John and Marry?)is not enough to determine their relationship.This paper proposes a context-sensitive convolu tion tree kernel for relation extraction to resolve the above two problems.It first automatically determines a dynamic context-sensitive tree span for relation ex traction by extending the Shortest Path-enclosed Tree (SPT) to include necessary context information outside SPT.Then it proposes a context-sensitive convolution tree kernel, whic h not only enumerates context free sub-trees but also context-sensitive sub-trees by considering their ancestor node paths as their contexts.Moreover, this paper evaluates the complementary nature of different linear kernels and tree kernels via a composite kernel.The layout of this paper is as follows.In Section 2, we review related work in more details.Then, the dynamic context-sensitive tree span and the contextsensitive convolution tree kernel are proposed in Sec tion 3 while Section 4 shows the experimental results.Finally, we conclude our work in Sec tion 5.The relation extraction task was first introduced as part of the Template Element task in MUC6 and then formulated as the Template Relation task in MUC7.Since then, many methods, such as feature-based (Kambhatla 2004; Zhou et al2005, 2006), tree ker nel-based (Zelenko et al2003; Culotta and Sorensen 2004; Bunescu and Mooney 2005a; Zhang et al2006) and composite kernel-based (Zhao and Gris hman 2005; Zhang et al2006), have been proposed in lit erature.For the feature-based methods, Kambhatla (2004) employed Maximum Entropy models to combine diverse lexical, syntactic and semantic features in rela tion extraction, and achieved the F-measure of 52.8 on the 24 relation subtypes in the ACE RDC 2003 corpus.Zhou et al(2005) further systematically ex plored diverse features through a linear kernel and Support Vector Machines, and achieved the F measures of 68.0 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 cor pus respectively.One problem with the feature-based methods is that they need extensive feature engineering.Another problem is that, although they can ex plore some structured information in the parse tree (e.g. Kambhatla (2004) used the non-terminal path connecting the given two entities in a parse tree while Zhou et al (2005) introduced additional chunking features to enhance the performance), it is found dif ficult to well preserve structured information in the parse trees using the feature-based methods.Zhou et al (2006) further improved the performance by ex ploring the commonality among related classes in a class hierarchy using hierarchical learning strategy.As an alternative to the feature-based methods, the kernel-based methods (Haussler, 1999) have been proposed to implicitly explore various features in a high dimensional space by employing a kernel to cal culate the similarity between two objects directly.In particular, the kernel-based methods could be very effective at reducing the burden of feature engineer ing for structured objects in NLP researches, e.g. the tree structure in relation extraction.Zelenko et al (2003) proposed a kernel between two parse trees, which recursively matches nodes from roots to leaves in a top-down manner.For each pair of matched nodes, a subsequence kernel on their child nodes is invoked.They achieved quite success on two simple relation extraction tasks.Culotta and Sorensen (2004) extended this work to estimate simi larity between augmented dependency trees and achieved the F-measure of 45.8 on the 5 relation types in the ACE RDC 2003 corpus.One problem with the above two tree kernels is that matched nodes must be at the same height and have the same path to the root node.Bunescu and Mooney (2005a) pro posed a shortest path dependency tree kernel, which just sums up the number of common word classes at each position in the two paths, and achieved the F-measure of 52.5 on the 5 relation types in the ACE RDC 2003 corpus.They argued that the information to model a relationship between two entities can be typically captured by the shortest path between them in the dependency graph.While the shortest path may not be able to well preserve structured de pendency tree information, another problem with their kernel is that the two paths should have same length.This makes it suffer from the similar behavior with that of Culotta and Sorensen (2004): high preci sion but very low recall.As the state-of-the-art tree kernel-based method, Zhang et al(2006) explored various structured feature 729 spaces and used the convolution tree kernel over parse trees (Collins and Duffy 2001) to model syntac tic structured information for relation extraction.They achieved the F-measures of 61.9 and 63.6 on the 5 relation types of the ACE RDC 2003 corpus and the 7 relation types of the ACE RDC 2004 corpus respectively without entity-related information while the F measure on the 5 relation types in the ACE RDC 2003 corpus reached 68.7 when entity-related infor mation was included in the parse tree.One problem with Collins and Duffy?s convolution tree kernel is that the sub-trees involved in the tree kernel computa tion are context-free, that is, they do not consider the information outside the sub-trees.This is different from the tree kernel in Culota and Sorensen (2004), where the sub-trees involved in the tree kernel com putation are context-sensitive (that is, with the path from the tree root node to the sub-tree root node in consideration).Zhang et al(2006) also showed that the widely-used Shortest Path-enclosed Tree (SPT) performed best.One problem with SPT is that it fails to capture the contextual information outside the shortest path, which is important for relation extraction in many cases.Our random selection of 100 pos i tive training instances from the ACE RDC 2003 training corpus shows that ~25% of the cases need contextual information outside the shortest path.Among other kernels, Bunescu and Mooney (2005b) proposed a subsequence kernel and applied it in pro tein interaction and ACE relation extraction tasks.In order to integrate the advantages of featurebased and tree kernel-based methods, some research ers have turned to composite kernel-based methods.Zhao and Grishman (2005) defined several feature based composite kernels to integrate diverse features for relation extraction and achieved the F-measure of 70.4 on the 7 relation types of the ACE RDC 2004 corpus.Zhang et al(2006) proposed two composite kernels to integrate a linear kernel and Collins and Duffy?s convolution tree kernel.It achieved the Fmeasure of 70.9/57.2 on the 5 relation types/24 rela tion subtypes in the ACE RDC 2003 corpus and the F-measure of 72.1/63.6 on the 7 relation types/23 relation subtypes in the ACE RDC 2004 corpus.The above discussion suggests that structured in formation in the parse tree may not be fully utilized in the previous works, regardless of feature-based, tree kernel-based or composite kernel-based methods.Compared with the previous works, this paper pro poses a dynamic context-sensitive tree span trying to cover necessary structured information and a context sensitive convolution tree kernel considering both context-free and context-sensitive sub-trees.Further more, a composite kernel is applied to combine our tree kernel and a state-of-the-art linear kernel for in tegrating both flat and structured features in relation extraction as well as validating their complementary nature.Kernel for Relation Extraction In this section, we first propose an algorithm to dy namically determine a proper context-sensitive tree span and then a context-sensitive convolution tree kernel for relation extraction.3.1 Dynamic Context-Sensitive Tree Span in.Relation Extraction A relation instance between two entities is encaps u lated by a parse tree.Thus, it is critical to understand which portion of a parse tree is important in the tree kernel calculation.Zhang et al(2006) systematically explored seven different tree spans, including the Shortest Path-enclosed Tree (SPT) and a Context Sensitive Path-enclosed Tree1 (CSPT), and found that SPT per formed best.That is, SPT even outperforms CSPT.This is contrary to our intuition.For example, ?got married?is critical to determine the relationship between ?John?and ?Mary?in the sentence ?John and Mary got married?as shown in Figure 1(e), and the information contained in SPT (?John and Mary?)is not enough to determine their relationship.Obviously, context-sensitive tree spans should have the potential for better performance.One problem with the context-sensitive tree span explored in Zhang et al(2006) is that it only considers the availability of entities?siblings and fails to consider following two factors: 1) Whether is the information contained in SPT enough to determine the relationship between two entities?It depends.In the embedded cases, SPT is enough.For example, ?John?s wife?is enough to determine the relationship between ?John?and ?John?s wife?in the sentence ?John?s wife got a good job?as shown in Figure 1(a) . However, SPT is not enough in the coordinated cases, e.g. to determine the relationship between ?John?and ?Mary?in the sentence ?John and Mary got married?as shown in Figure 1(e).1 CSPT means SPT extending with the 1st left sibling of.the node of entity 1 and the 1st right sibling of the node of entity 2.In the case of no available sibling, it moves to the parent of current node and repeat the same proc ess until a sibling is available or the root is reached.730 2) How can we extend SPT to include necessary context information if there is no enough infor mation in SPT for relation extraction?To answer the above two questions, we randomly chose 100 positive instances from the ACE RDC 2003 training data and studied their necessary tree spans.It was observed that we can classify them into 5 categories: 1) embedded (37 instances), where one entity is embedded in another entity, e.g. ?John?and ?John?s wife?as shown in Figure 1(a); 2) PP-linked (21 instances), where one entity is linked to another entity via PP attachment, e.g. ?CEO?and ?Microsoft?in the sentence ?CEO of Microsoft announced ? ?as shown in Figure 1(b); 3) semi-structured (15 in stances), where the sentence consists of a sequence of noun phrases (including the two given entities), e.g. ?Jane?and ?ABC news?in the sentence ?Jane, ABC news, California.?as shown in Figure 1(c); 4) de scriptive (7 instances), e.g. the citizenship between ?his mother?and ?Lebanese?in the sentence ?his mother Lebanese landed at ??as shown in Figure 1(d); 5) predicate-linked and others (19 instances, including coordinated cases), where the predicate information is necessary to determine the relationship between two entities, e.g. ?John?and ?Mary?in the sentence ?John and Mary got married??as shown in Figure 1(e); Based on the above observations, we implement an algorithm to determine the necessary tree span for the relation extract task.The idea behind the algorithm is that the necessary tree span for a relation should be determined dynamically according to its tree span category and context.Given a parsed tree and two entities in consideration, it first determin es the tree span category and then extends the tree span accordingly.By default, we adopt the Shortest Pathenclosed Tree (SPT) as our tree span.We only ex pand the tree span when the tree span belongs to the ?predicate-linked?category.This is based on our observation that the tree spans belonging to the ?predi cate-linked?category vary much syntactically and majority (~70%) of them need information outside SPT while it is quite safe (>90%) to use SPT as the tree span for the remaining categories.In our algo rithm, the expansion is done by first moving up until a predicate-headed phrase is found and then moving down along the predicated-headed path to the predi cate terminal node.Figure 1(e) shows an example for the ?predicate-linked?category where the lines with arrows indicate the expansion path.e) predicate-linked: SPT and the dynamic context-sensitive tree span Figure 1: Different tree span categories with SPT (dotted circle) and an ex ample of the dynamic context-sensitive tree span (solid circle) Figure 2: Examples of contextfree and context-sensitive sub trees related with Figure 1(b).Note: the bold node is the root for a sub-tree.A problem with our algorithm is how to determine whether an entity pair belongs to the ?predi cate-linked?category.In this paper, a simple method is applied by regarding the ?predicate linked?category as the default category.That is, those entity pairs, which do not belong to the four well defined and easily detected categories (i.e. embedded, PP-liked, semi-structured and descriptive), are classified into the ?predicate-linked?cate gory.His mother Lebanese landed PRP$ NNP VBD IN NP-E1-PER NP-E2-GPE PP S d) descriptive NP NN at ? VP Jane ABC news , NNP , NNP NNS , NNP . NP NP-E1-PER NP-E2-ORG NP c) semi-structured California . . , , , NP(NN) of Microsoft IN NNP NP-E2-ORG PP(IN)-subroot b) context -sensitive NP(NN) of Microsoft IN NNP NP-E2-ORG S(VBD) PP(IN)-subroot c) context -sensitive PP(IN)-subtoot NP-E2-ORG of Microsoft IN NNP a) context -free ? NP John and Mary got NNP CC NNP VBD married NP-E1-PER NP-E2-PER VP S VP VBN ? John and Mary got NNP CC NNP VBD married NP-E1-PER NP-E2-PER VP NP VP ? NP CEO of Microsoft announced NN IN NNP VBD ? NP-E1-PER NP-E2-ORG VP S b) PP -linked PP ? John ?s wife found a job NNP POS NN VBD DT JJ NN NP NP-E1-PER NP-E2-PER VP S a) embedded good 731 Since ?predicate -linked?instances only occupy ~20% of cases, this explains why SPT performs better than the Context-Sensitive Path-enclosed Tree (CSPT) as described in Zhang et al(2006): consistently adopting CSPT may introduce too much noise/unnecessary information in the tree kernel.3.2 Context-Sensitive Convolution Tree Kernel.Given any tree span, e.g. the dynamic context sensitive tree span in the last subsection, we now study how to measure the similarity between two trees, using a convolution tree kernel.A convolution kernel (Haussler D., 1999) aims to capture structured information in terms of substructures . As a specialized convolution kernel, Collins and Duffy?s convolu tion tree kernel ),( 21 TTKC (?C? for convolution) counts the number of common sub-trees (substructures) as the syntactic structure similarity be tween two parse trees T1 and T2 (Collins and Duffy 2001): ? ??D= 2211 , 2121 ),(),( NnNn C nnTTK (1) where Nj is the set of nodes in tree Tj , and 1 2( , )n nD evaluates the common sub-trees rooted at n1 and n2 2 and is computed recursively as follows: 1) If the context-free productions (Context-Free Grammar(CFG) rules) at 1n and 2n are different, 1 2( , ) 0n nD = ; Otherwise go to 2.2) If both 1n and 2n are POS tags, 1 2( , ) 1n n lD = ? ; Otherwise go to 3.3) Calculate 1 2( , )n nD recursively as: ? = D+=D )(# 1 2121 1 )),(),,((1(),( nch k knchknchnn l (2) where )(# nch is the number of children of node n , ),( knch is the k th child of node n andl (0< l <1) is the decay factor in order to make the kernel value less variable with respect to different sub-tree sizes.This convolution tree kernel has been successfully applied by Zhang et al(2006) in relation extraction.However, there is one problem with this tree kernel: the sub-trees involved in the tree kernel computation are context-free (That is, they do not consider the information outside the sub-trees).This is contrast to 2 That is, each node n encodes the identity of a sub-.tree rooted at n and, if there are two nodes in the tree with the same label, the summation will go over both of them.the tree kernel proposed in Culota and Sorensen (2004) which is context-sensitive, that is, it considers the path from the tree root node to the sub-tree root node.In order to integrate the advantages of both tree kernels and resolve the problem in Collins and Duffy?s convolution tree kernel, this paper proposes a context-sensitive convolution tree kernel.It works by taking ancestral information (i.e. the root node path) of sub-trees into consideration: ? ?D= m i NnNn ii C iiii nnTTK 1 ]2[]2[],1[]1[ 11 1111 ])2[],1[(])2[],1[( (3) Where ? ][1 jN i is the set of root node paths with length i in tree T[j] while the maximal length of a root node path is defined by m. ? ])[...(][ 211 jnnnjn ii = is a root node path with length i in tree T[j] , which takes into account the i-1 ancestral nodes in2 [j] of 1n [j] in T[j].Here, ][1 jn k+ is the parent of ][ jn k and ][1 jn is the root node of a context-free sub-tree in T[j].For better differentiation, the label of each ancestral node in in1 [j] is augmented with the POS tag of its head word.])2[],1[( 11 ii nnD measures the common context sensitive sub-trees rooted at root node paths ]1[1in and ]2[1in 3.In our tree kernel, a sub-tree.becomes context-sensitive with its dependence on the root node path instead of the root node itself.Figure 2 shows a few examples of contextsensitive sub-trees with comparison to context free sub-trees.Similar to Collins and Duffy (2001), our tree ker nel computes ])2[],1[( 11 ii nnD recursively as follows: 1) If the context-sensitive productions (Context Sensitive Grammar (CSG) rules with root node paths as their left hand sides) rooted at ]1[1in and ]2[1 in are different, return ])2[],1[( 11 ii nnD =0; Otherwise go to Step 2.2) If both ]1[1n and ]2[1n are POS tags, l=D ])2[],1[( 11 ii nn ; Otherwise go to Step 3.3 That is, each root node path in1 encodes the identity.of a context-sensitive sub-tree rooted at in1 and, if there are two root node paths in the tree with the same label sequence, the summation will go over both of them.732 3) Calculate ])2[],1[( 11 ii nnD recursively as: ? = D+= D ])1[(# 1 11 11 1 ))],2[(),],1[((1( ])2[],1[( inch k ii ii knchknch nn l (4) where ])],[( 1 kjnch i is the k th context-sensitive child of the context-sensitive sub-tree rooted at ][1 jn i with ])[(# 1 jnch i the number of the con text-sensitive children.Here, l (0< l <1) is the decay factor in order to make the kernel value less variable with respect to different sizes of the context-sensitive sub-trees.It is worth comparing our tree kernel with previous tree kernels.Obviously, our tree kernel is an exten sion of Collins and Duffy?s convolution tree kernel, which is a special case of our tree kernel (if m=1 in Equation (3)).Our tree kernel not only counts the occurrence of each context-free sub-tree, which does not consider its ancestors, but also counts the occurrence of each context-sensitive sub-tree, which con siders its ancestors.As a result, our tree kernel is not limited by the constraints in previous tree kernels (as discussed in Section 2), such as Collins and Duffy (2001), Zhang et al(2006), Culotta and Sorensen (2004) and Bunescu and Mooney (2005a).Finally, let?s study the computational issue with our tree kernel.Although our tree kernel takes the context sensitive sub-trees into consideration, it only slightly increases the computational burden, compared with Collins and Duffy?s convolution tree kernel.This is due to that 0])2[],1[( 11 =D nn holds for the major ity of context-free sub-tree pairs (Collins and Duffy 2001) and that computation for context-sensitive sub tree pairs is necessary only when 0])2[],1[( 11 ?D nn and the context-sensitive sub tree pairs have the same root node path(i.e. ]2[]1[ 11 ii nn = in Equation (3)).This paper uses the ACE RDC 2003 and 2004 cor pora provided by LDC in all our experiments.4.1 Experimental Setting.The ACE RDC corpora are gathered from various newspapers, newswire and broadcasts.In the 2003 corpus , the training set consists of 674 documents and 9683 positive relation instances w hile the test set consists of 97 documents and 1386 positive relation in stances.The 2003 corpus defines 5 entity types, 5 major relation types and 24 relation subtypes.All the reported performances in this paper on the ACE RDC 2003 corpus are evaluated on the test data.The 2004 corpus contains 451 documents and 5702 positive relation instances.It redefines 7 entity types, 7 major relation types and 23 relation subtypes.For compari son, we use the same setting as Zhang et al(2006) by applying a 5-fold cross-validation on a subset of the 2004 data, containing 348 documents and 4400 rela tion instances.That is, all the reported performances in this paper on the ACE RDC 2004 corpus are evalu ated using 5-fold cross validation on the entire corpus . Both corpora are parsed using Charniak?s parser (Charniak, 2001) with the boundaries of all the entity mentions kept 4 . We iterate over all pairs of entity mentions occurring in the same sentence to generate potential relation instances5.In our experimentation, SVM (SVMLight, Joachims(1998)) is selected as our classifier.For efficiency, we apply the one vs. others strategy, which builds K classifiers so as to separate one class from all others.The training parameters are chosen using cross-validation on the ACE RDC 2003 training data.In particular, l in our tree kernel is fine-tuned to 0.5.This suggests that about 50% dis count is done as our tree kernel moves down one level in computing ])2[],1[( 11 ii nnD . 4.2 Experimental Results.First, we systematically evaluate the context-sensitive convolution tree kernel and the dynamic context sensitive tree span proposed in this paper.Then, we evaluate the complementary nature between our tree kernel and a state-of-the-art linear ker nel via a composite kernel.Generally different feature-based methods and tree kernel-based methods have their own merits.It is usually easy to build a system using a feature-based method and achieve the state-of-the-art performance, while tree kernel-based methods hold the potential for further performance improvement.Therefore, it is always a good idea to integrate them via a composite kernel.4 This can be done by first representing all entity men-.tions with their head words and then restoring all the entity mentions after parsing.Moreover, please note that the final performance of relation extraction may change much with different range of parsing errors.We will study this issue in the near future.tion extraction on ?true?mentions with ?true?chain ing of co-reference (i.e. as annotated by LDC annotators ).Moreover, we only model explicit relations and explicitly model the argument order of the two mentions in volved.733Finally, we compare our system with the state-of the-art systems in the literature.Context-Sensitive Convolution Tree Kernel In this paper, the m parameter of our context-sensitive convolution tree kernel as shown in Equation (3) indicates the maximal length of root node paths and is optimized to 3 using 5-fold cross validation on the ACE RDC 2003 training data.Table 1 compares the impact of different m in context-sensitive convolution tree kernels using the Shortest Path-enclosed Tree (SPT) (as described in Zhang et al(2006)) on the major relation types of the ACE RDC 2003 and 2004 corpora, in details.It also shows that our tree kernel achieves best performance on the test data using SPT with m = 3, which outperforms the one with m = 1 by ~2.3 in F-measure.This suggests the parent and grandparent nodes of a sub-tree contains much information for relation extraction while considering more ancestral nodes may not help.This may be due to that, although our experimentation on the training data indicates that more than 80% (on average) of subtrees has a root node path longer than 3 (since most of the subtrees are deep from the root node and more than 90% of the parsed trees in the training data are deeper than 6 levels), including a root node path longer than 3 may be vulnerable to the full parsing errors and have negative impact.Table 1 also evaluates the impact of entity-related information in our tree kernel by attaching entity type information (e.g. ?PER?in the entity node 1 of Figure 1(b)) into both entity nodes.It shows that such information can significantly improve the performance by ~6.0 in F-measure.In all the following experiments, we will apply our tree kernel with m=3 and entity-related information by default.Table 2 compares the dynamic context-sensitive tree span with SPT using our tree kernel.It shows that the dynamic tree span can futher improve the performance by ~1.2 in F-measure6.This suggests the usefulness of extending the tree span beyond SPT for the ?predicate-linked?tree span category.In the future work, we will further explore expanding the dynamic tree span beyond SPT for the remaining tree span categories.6 Significance test shows that the dynamic tree span per-.forms s tatistically significantly better than SPT with p values smaller than 0.05.m P(%) R(%) F 1 72.3(72.7) 56.6(53.8) 63.5(61.8) 2 74.9(75.2) 57.9(54.7) 65.3(63.5) 3 75.7(76.1) 58.3(55.1) 65.9(64.0) 4 76.0(75.9) 58.3(55.3) 66.0(63.9) a) without entity-related information m P(%) R(%) F 1 77.2(76.9) 63.5(60.8) 69.7(67.9) 2 79.1(78.6) 65.0(62.2) 71.3(69.4) 3 79.6(79.4) 65.6(62.5) 71.9(69.9) 4 79.4(79.1) 65.6(62.3) 71.8(69.7) b) with entity-related information Table 1: Evaluation of context-sensitive convolution tree kernels using SPT on the major relation types of the ACE RDC 2003 (inside the parentheses) and 2004 (outside the parentheses) corpora.Tree Span P(%) R(%) F Shortest Path- enclosed Tree 79.6 (79.4) 65.6 (62.5) 71.9 (69.9) Dynamic Context- Sensitive Tee 81.1 (80.1) 66.7 (63.8) 73.2 (71.0) Table 2: Comparison of dynamic context-sensitive tree span with SPT using our context-sensitive convolution tree kernel on the major relation types of the ACE RDC 2003 (inside the parentheses) and 2004 (outside the parentheses) corpora.18% of positive instances in the ACE RDC 2003 test data belong to the predicate-linked category.Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al(2006), is ap plied to integrate the proposed context-sensitive convolution tree kernel with a state-of-the-art linear kernel (Zhou et al2005) 7: ),()1(),(),(1 ???-+???=??CPL KKK aa (5) Here, ),( ??LK and ),( ??CK indicates the normal ized linear kernel and context-sensitive convolution tree kernel respectively while ( , )pK ? ?is the poly nomial expansion of ( , )K ? ?with degree d=2, i.e. 2( , ) ( ( , ) 1)pK K??= + and a is the coefficient (a is set to 0.3 using cross-validation).7 Here, we use the same set of flat features (i.e. word,.entity type, mention level, overlap, base phrase chunking, dependency tree, parse tree and semantic informa tion) as Zhou et al(2005).734 Table 3 evaluates the performance of the composite kernel.It shows that the composite kernel much further improves the performance beyond that of either the state-of-the-art linear kernel or our tree kernel and achieves the F-measures of 74.1 and 75.8 on the major relation types of the ACE RDC 2003 and 2004 corpora respectively.This suggests that our tree kernel and the state-of-the-art linear kernel are quite complementary, and that our composite kernel can effectively integrate both flat and structured features.System P(%) R(%) F Linear Kernel 78.2 (77.2) 63.4 (60.7) 70.1 (68.0) Context-Sensitive Con volution Tree Kernel 81.1 (80.1) 66.7 (63.8) 73.2 (71.0) Composite Kernel 82.2 (80.8) 70.2 (68.4) 75.8 (74.1) Table 3: Performance of the compos ite kernel via polynomial interpolation on the major relation types of the ACE RDC 2003 (inside the parentheses) and 2004 (outside the parentheses) corpora Comparison with Other Systems ACE RDC 2003 P(%) R(%) F Ours: composite kernel 80.8 (65.2) 68.4 (54.9) 74.1 (59.6) Zhang et al(2006): composite kernel 77.3 (64.9) 65.6 (51.2) 70.9 (57.2) Ours: context-sensitive convolution tree kernel 80.1 (63.4) 63.8 (51.9) 71.0 (57.1) Zhang et al(2006): convolution tree kernel 76.1 (62.4) 62.6 (48.5) 68.7 (54.6) Bunescu et al(2005): shortest path dependency kernel 65.5 (-) 43.8 (-) 52.5 (-) Culotta et al(2004): dependency kernel 67.1 (-) 35.0 (-) 45.8 (-) Zhou et al (2005): feature-based 77.2 (63.1) 60.7 (49.5) 68.0 (55.5) Kambhatla (2004): feature-based - (63.5) - (45.2) - (52.8) Table 4: Comparison of difference systems on the ACE RDC 2003 corpus over both 5 types (outside the parentheses) and 24 subtypes (inside the parentheses) ACE RDC 2004 P(%) R(%) F Ours: composite kernel 82.2 (70.3) 70.2 (62.2) 75.8 (66.0) Zhang et al(2006): composite kernel 76.1 (68.6) 68.4 (59.3) 72.1 (63.6) Zhao et al(2005):8 composite kernel 69.2 (-) 70.5 (-) 70.4 (-) Ours: context-sensitive convolution tree kernel 81.1 (68.8) 66.7 (60.3) 73.2 (64.3) Zhang et al(2006): convolution tree kernel 72.5 (-) 56.7 (-) 63.6 (-) Table 5: Comparison of difference systems on the ACE RDC 2004 corpus over both 7 types (outside the parentheses) and 23 subtypes (inside the parentheses) Finally, Tables 4 and 5 compare our system with other state-of-the-art systems9 on the ACE RDC 2003 and 2004 corpora, respectively.They show that our tree kernel-based system outperforms previous tree kernel-based systems.This is largely due to the con text-sensitive nature of our tree kernel which resolves the limitations of the previous tree kernels.They also show that our tree kernel-based system outperforms the state-of-the-art feature-based system.This proves the great potential inherent in the parse tree structure for relation extraction and our tree kernel takes a big stride towards the right direction.Finally, they also show that our composite kernel-based system outper forms other composite kernel-based systems.Structured parse tree information holds great potential for relation extraction.This paper proposes a contextsensitive convolution tree kernel to resolve two critical problems in previous tree kernels for relation ex traction by first automatically determining a dynamic context-sensitive tree span and then applying a con text-sensitive convolution tree kernel.Moreover, this paper evaluates the complementary nature between our tree kernel and a state-of-the-art linear kernel.Evaluation on the ACE RDC corpora shows that our dynamic context-sensitive tree span is much more suitable for relation extraction than the widely -used Shortest Path-enclosed Tree and our tree kernel outperforms the state-of-the-art Collins and Duffy?s con volution tree kernel.It also shows that feature-based 8 There might be some typing errors for the performance.reported in Zhao and Grishman(2005) since P, R and F do not match.9 All the state-of-the-art systems apply the entity-related.information.It is not supervising: our experiments show that using the entity-related information gives a large performance improvement.735 and tree kernel-based methods well complement each other and the composite kernel can effectively inte grate both flat and structured features.To our knowledge, this is the first research to dem onstrate that, without extensive feature engineer ing, an individual tree kernel can achieve much better performance than the state-of-the-art linear kernel in re lation extraction.This shows the great potential of structured parse tree information for relation extrac tion and our tree kernel takes a big stride towards the right direction.For the future work, we will focus on improving the context-sensitive convolution tree kernel by ex ploring more useful context information.Moreover, we will explore more entity-related information in the parse tree.Our preliminary work of including the entity type information significantly improves the per formance.Finally, we will study how to resolve the data imbalance and sparseness issues from the learn ing algorithm viewpoint.Acknowledgement This research is supported by Project 60673041 under the National Natural Science Foundation of China and Project 2006AA01Z147 under the ?863?National High-Tech Research and Development of China.We would also like to thank the critical and insightful comments from the four anonymous reviewers.
Dependency Parsing by Belief PropagationWe formulate dependency parsing as a graphical model with the novel ingredient of global constraints.We show how to apply loopy belief propagation (BP), a simple and tool for and inference.As a parsing algorithm, BP is both asymptotically and empirically efficient.Even with second-order features or latent variables, which would make exact parsing considerslower or NP-hard, BP needs only with a small constant factor.Furthermore, such features significantly improve parse accuracy over exact first-order methods.Incorporating additional features would increase the runtime additively rather than multiplicatively.Computational linguists worry constantly about runtime.Sometimes we oversimplify our models, trading linguistic nuance for fast dynamic programming.Alternatively, we write down a better but intractable model and then use approximations.The CL community has often approximated using heavy pruning or reranking, but is beginning to adopt other methods from the machine learning community, such as Gibbs sampling, rejection sampling, and certain variational approximations.We propose borrowing a different approximation technique from machine learning, namely, loopy belief propagation (BP).In this paper, we show that BP can be used to train and decode complex parsing models.Our approach calls a simpler parser as a subroutine, so it still exploits the useful, well-studied combinatorial structure of the parsing problem.1We wish to make a dependency parse’s score depend on higher-order features, which consider arbitrary interactions among two or more edges in the parse (and perhaps also other latent variables such as part-of-speech tags or edge labels).Such features can help accuracy—as we show.Alas, they raise the polynomial runtime of projective parsing, and render non-projective parsing NP-hard.Hence we seek approximations.We will show how BP’s “message-passing” discipline offers a principled way for higher-order features to incrementally adjust the numerical edge weights that are fed to a fast first-order parser.Thus the first-order parser is influenced by higher-order interactions among edges—but not asymptotically slowed down by considering the interactions itself.BP’s behavior in our setup can be understood intuitively as follows.Inasmuch as the first-order parser finds that edge e is probable, the higher-order features will kick in and discourage other edges e' to the extent that they prefer not to coexist with e.2 Thus, the next call to the first-order parser assigns lower probabilities to parses that contain these e'.(The method is approximate because a first-order parser must equally penalize all parses containing e', even those that do not in fact contain e.) This behavior is somewhat similar to parser stacking (Nivre and McDonald, 2008; Martins et al., 2008), in which a first-order parser derives some of its input features from the full 1-best output of another parser.In our method, a first-order parser derives such input features from its own previous full output (but probabilistic output rather than just 1best).This circular process is iterated to convergence.Our method also permits the parse to interact cheaply with other variables.Thus first-order parsing, part-of-speech tagging, and other tasks on a common input could mutually influence one another.Our method and its numerical details emerge naturally as an instance of the well-studied loopy BP algorithm, suggesting several potential future improvements to accuracy (Yedidia et al., 2004; Braunstein et al., 2005) and efficiency (Sutton and McCallum, 2007).Loopy BP has occasionally been used before in NLP, with good results, to handle non-local features (Sutton and McCallum, 2004) or joint decoding (Sutton et al., 2004).However, our application to parsing requires an innovation to BP that we explain in §5—a global constraint to enforce that the parse is a tree.The tractability of some such global constraints points the way toward applying BP to other computationally intensive NLP problems, such as syntax-based alignment of parallel text.To apply BP, we must formulate dependency parsing as a search for an optimal assignment to the variables of a graphical model.We encode a parse using the following variables: Sentence.The n-word input sentence W is fully observed (not a lattice).Let W = W0W1 · · · Wn, where W0 is always the special symbol ROOT.Tags.If desired, the variables T = T1T2 · · · Tn may specify tags on the n words, drawn from some tagset T (e.g., parts of speech).These variables are needed iff the tags are to be inferred jointly with the parse.Links.The O(n2) boolean variables {Lij : 0 < i < n,1 < j < n, i =� j} correspond to the possible links in the dependency parse.3 Lij = true is interpreted as meaning that there exists a dependency link from parent i —* child j.4 Link roles, etc.It would be straightforward to add other variables, such as a binary variable Lij that is true iff there is a link i � j labeled with role r (e.g., AGENT, PATIENT, TEMPORAL ADJUNCT).We wish to define a probability distribution over all configurations, i.e., all joint assignments A to these variables.Our distribution is simply an undirected graphical model, or Markov random field (MRF):5 specified by the collection of factors Fm : A H R[ 'O.Each factor is a function that consults only a subset of A.We say that the factor has degree d if it depends on the values of d variables in A, and that it is unary, binary, ternary, or global if d is respectively 1, 2, 3, or unbounded (grows with n).A factor function Fm(A) may also depend freely on the observed variables—the input sentence W and a known (learned) parameter vector 0.For notational simplicity, we suppress these extra arguments when writing and drawing factor functions, and when computing their degree.In this treatment, these observed variables are not specified by A, but instead are absorbed into the very definition of Fm.In defining a factor Fm, we often define the circumstances under which it fires.These are the only circumstances that allow Fm(A) =� 1.When Fm does not fire, Fm(A) = 1 and does not affect the product in equation (1).A hard factor Fm fires only on parses A that violate some specified condition.It has value 0 on those parses, acting as a hard constraint to rule them out.TREE.A hard global constraint on all the Lij variables at once.It requires that exactly n of these variables be true, and that the corresponding links form a directed tree rooted at position 0.PTREE.This stronger version of TREE requires further that the tree be projective.That is, it prohibits Lij and LH from both being true if i —* j crosses k —* E. (These links are said to cross if one of k, E is strictly between i and j while the other is strictly outside that range.)EXACTLY1.A family of O(n) hard global constraints, indexed by 1 < j < n. EXACTLY1j requires that j have exactly one parent, i.e., exactly one of the Lij variables must be true.Note that EXACTLY1 is implied by TREE or PTREE.ATMOST1.A weaker version.ATMOST1j requires j to have one or zero parents.NAND.A family of hard binary constraints.NANDij,kt requires that Lij and Lkt may not both be true.We will be interested in certain subfamilies.NOT2.Shorthand for the family of O(n3) binary constraints {NANDij,kj}.These are collectively equivalent to ATMOST1, but expressed via a larger number of simpler constraints, which can make the BP approximation less effective (footnote 30).NO2CYCLE.Shorthand for the family of O(n2) binary constraints {NANDij,ji}.A soft factor Fm acts as a soft constraint that prefers some parses to others.In our experiments, it is always a log-linear function returning positive values: where θ is a learned, finite collection of weights and f is a corresponding collection of feature functions, some of which are used by Fm.(Note that fh is permitted to consult the observed input W. It also sees which factor Fm it is scoring, to support reuse of a single feature function fh and its weight θh by unboundedly many factors in a model.)LINK.A family of unary soft factors that judge the links in a parse A individually.LINKij fires iff Lij = true, and then its value depends on (i, j), W, and θ.Our experiments use the same features as McDonald et al. (2005).A first-order (or “edge-factored”) parsing model (McDonald et al., 2005) contains only LINK factors, along with a global TREE or PTREE factor.Though there are O(n2) link factors (one per Lij), only n of them fire on any particular parse, since the global factor ensures that exactly n are true.We’ll consider various higher-order soft factors: PAIR.The binary factor PAIRij,kt fires with some value iff Lij and Lkt are both true.Thus, it penalizes or rewards a pair of links for being simultaneously present.This is a soft version of NAND.GRAND.Shorthand for the family of O(n3) binary factors {PAIRij,jk}, which evaluate grandparentparent-child configurations, i —* j —* k. For example, whether preposition j attaches to verb i might depend on its object k. In non-projective parsing, we might prefer (but not require) that a parent and child be on the same side of the grandparent.SIB.Shorthand for the family of O(n3) binary factors {PAIRij,ik}, which judge whether two children of the same parent are compatible.E.g., a given verb may not like to have two noun children both to its left.6 The children do not need to be adjacent.CHILDSEQ.A family of O(n) global factors.CHILDSEQi scores i’s sequence of children; hence it consults all variables of the form Lij.The scoring follows the parametrization of a weighted split head-automaton grammar (Eisner and Satta, 1999).If 5 has children 2, 7, 9 under A, then CHILDSEQi is a product of subfactors of the form PAIR5#,57, PAIR57,59, PAIR59,5# (right child sequence) and PAIR5#,52, PAIR52,5# (left child sequence).NOCROSS.A family of O(n2) global constraints.If the parent-to-j link crosses the parent-to-` link, then NOCROSSjt fires with a value that depends only on j and `.(If j and ` do not each have exactly one parent, NOCROSSjt fires with value 0; i.e., it incorporates EXACTLY1j and EXACTLY1t.)7 TAGi is a unary factor that evaluates whether Ti’s value is consistent with W (especially Wi).TAGLINKij is a ternary version of the LINKij factor whose value depends on Lij, Ti and Tj (i.e., its feature functions consult the tag variables to decide whether a link is likely).One could similarly enrich the other features above to depend on tags and/or link roles; TAGLINK is just an illustrative example.TRIGRAM is a global factor that evaluates the tag sequence T according to a trigram model.It is a product of subfactors, each of which scores a trigram of adjacent tags Ti_2, Ti_1, Ti, possibly also considering the word sequence W (as in CRFs).MacKay (2003, chapters 16 and 26) provides an excellent introduction to belief propagation, a generalization of the forward-backward algorithm that is deeply studied in the graphical models literature (Yedidia et al., 2004, for example).We briefly sketch the method in terms of our parsing task.The basic BP idea is simple.Variable L34 maintains a distribution over values true and false—a “belief”—that is periodically recalculated based on the current distributions at other variables.8 Readers familiar with Gibbs sampling can regard this as a kind of deterministic approximation.In Gibbs sampling, L34’s value is periodically resampled based on the current values of other variables.Loopy BP works not with random samples but their expectations.Hence it is approximate but tends to converge much faster than Gibbs sampling will mix.It is convenient to visualize an undirected factor graph (Fig.1), in which each factor is connected to the variables it depends on.Many factors may connect to—and hence influence—a given variable such as L34.If X is a variable or a factor, N(X) denotes its set of neighbors.Given an input sentence W and a parameter vector θ, the collection of factors Fm defines a probability distribution (1).The parser should determine the values of the individual variables.In other words, we would like to marginalize equation (1) to obtain the distribution p(L34) over L34 = true vs. false, the distribution p(T4) over tags, etc.If the factor graph is acyclic, then BP computes these marginal distributions exactly.Given 8Or, more precisely—this is the tricky part—based on versions of those other distributions that do not factor in L34’s reciprocal influence on them.This prevents (e.g.)L34 and T3 from mutually reinforcing each other’s existing beliefs. an HMM, for example, BP reduces to the forwardbackward algorithm.BP’s estimates of these distributions are called beliefs about the variables.BP also computes beliefs about the factors, which are useful in learning θ (see §7).E.g., if the model includes the factor TAGLINKij, which is connected to variables Lij, Ti, Tj, then BP will estimate the marginal joint distribution p(Lij, Ti, Tj) over (boolean, tag, tag) triples.When the factor graph has loops, BP’s beliefs are usually not the true marginals of equation (1) (which are in general intractable to compute).Indeed, BP’s beliefs may not be the true marginals of any distribution p(A) over assignments, i.e., they may be globally inconsistent.All BP does is to incrementally adjust the beliefs till they are at least locally consistent: e.g., the beliefs at factors TAGLINKij and TAGLINKik must both imply9 the same belief about variable Ti, their common neighbor.This iterated negotiation among the factors is handled by message passing along the edges of the factor graph.A message to or from a variable is a (possibly unnormalized) probability distribution over the values of that variable.The variable V sends a message to factor F, saying “My other neighboring factors G jointly suggest that I have posterior distribution qV ,F (assuming that they are sending me independent evidence).” Meanwhile, factor F sends messages to V , saying, “Based on my factor function and the messages received from my other neighboring variables U about their values (and assuming that those messages are independent), I suggest you have posterior distribution rF,V over your values.” To be more precise, BP at each iteration k (until convergence) updates two kinds of messages: from factors to variables.Each message is a probability distribution over values v of V , normalized by a scaling constant n. Alternatively, messages may be left as unnormalized distributions, choosing n =� 1 only as needed to prevent over- or underflow.Messages are initialized to uniform distributions.Whenever we wish, we may compute the beliefs at V and F: These beliefs do not truly characterize the expected behavior of Gibbs sampling (§4.1), since the products in (5)–(6) make conditional independence assumptions that are valid only if the factor graph is acyclic.Furthermore, on cyclic (“loopy”) graphs, BP might only converge to a local optimum (Weiss and Freedman, 2001), or it might not converge at all.Still, BP often leads to good, fast approximations.One iteration of standard BP simply updates all the messages as in equations (3)–(4): one message per edge of the factor graph.Therefore, adding new factors to the model increases the runtime per iteration additively, by increasing the number of messages to update.We believe this is a compelling advantage over dynamic programming—in which new factors usually increase the runtime and space multiplicatively by exploding the number of distinct items.10 But how long does updating each message take?The runtime of summing over all assignments EA in 10For example, with unknown tags T, a model with PTREE+TAGLINK will take only O(n3 + n2g2) time for BP, compared to O(n3g2) time for dynamic programming (Eisner & Satta 1999).Adding TRIGRAM, which is string-local rather than tree-local, will increase this only to O(n3 + n2g2 + ng3), compared to O(n3g6) for dynamic programming.Even more dramatic, adding the SIB family of O(n3) PAIRij,ik factors will add only O(n3) to the runtime of BP (Table 1).By contrast, the runtime of dynamic programming becomes exponential, because each item must record its headword’s full set of current children. equation (4) may appear prohibitive.Crucially, however, F(A) only depends on the values in A of F’s its neighboring variables N(F).So this sum is proportional to a sum over restricted assignments to just those variables.11 For example, computing a message from TAGLINKij —* Ti only requires iterating over all (boolean, tag, tag) triples.12 The runtime to update that message is therefore O(2 · |T  |· |T |).The above may be tolerable for a ternary factor.But how about global factors?EXACTLY1j has n neighboring boolean variables: surely we cannot iterate over all 2n assignments to these!TREE is even worse, with 2O(n2) assignments to consider.We will give specialized algorithms for handling these summations more efficiently.A historical note is in order.Traditional constraint satisfaction corresponds to the special case of (1) where all factors Fm are hard constraints (with values in {0,1}).In that case, loopy BP reduces to an algorithm for generalized arc consistency (Mackworth, 1977; Bessi`ere and R´egin, 1997; Dechter, 2003), and updating a factor’s outgoing messages is known as constraint propagation.R´egin (1994) famously introduced an efficient propagator for a global constraint, ALLDIFFERENT, by adapting combinatorial bipartite matching algorithms.In the same spirit, we will demonstrate efficient propagators for our global constraints, e.g. by adapting combinatorial algorithms for weighted parsing.We are unaware of any previous work on global factors in sum-product BP, although for max-product BP,13 Duchi et al. (2007) independently showed that a global 1-to-1 alignment constraint—a kind of weighted ALLDIFFERENT—permits an efficient propagator based on weighted bipartite matching.Table 1 shows our asymptotic runtimes for all factors in §§3.3–3.4.Remember that if several of these factors are included, the total runtime is additive.14 Propagating the local factors is straightforward (§5.1).We now explain how to handle the global factors.Our main trick is to work backwards from marginal beliefs.Let F be a factor and V be one of its neighboring variables.At any time, F has a marginal belief about V (see footnote 9), A s.t.A[V]=v a sum over (6)’s products of incoming messages.By the definition of rF→V in (4), and distributivity, we can also express the marginal belief (7) as a pointwise product of outgoing and incoming messages15 up to a constant.If we can quickly sum up the marginal belief (7), then (8) says we can divide out each particular incoming message q��) V →F to obtain its corresponding outgoing message r���1) 14We may ignore the cost of propagators at the variables.Each outgoing message from a variable can be computed in time proportional to its size, which may be amortized against the cost of generating the corresponding incoming message.15E.g., the familiar product of forward and backward messages that is used to extract posterior marginals from an HMM.Note that the marginal belief and both messages are unnormalized distributions over values v of V .F and k are clear from context below, so we simplify the notation so that (7)–(8) become TRIGRAM must sum over assignments to the tag sequence T. The belief (6) in a given assignment is a product of trigram scores (which play the role of transition weights) and incoming messages qTj (playing the role of emission weights).The marginal belief (7) needed above, b(Ti = t), is found by summing over assignments where Ti = t. All marginal beliefs are computed together in O(ng3) total time by the forward-backward algorithm.16 EXACTLY1j is a sparse hard constraint.Even though there are 2n assignments to its n neighboring variables {Lij}, the factor function returns 1 on only n assignments and 0 on the rest.In fact, for a given i, b(Lij = true) in (7) is defined by (6) to have exactly one non-zero summand, in which A puts Lij = true and all other Ligj = false.We compute the marginal beliefs for all i together in O(n) total time: TREE and PTREE must sum over assignments to the O(n2) neighboring variables {Lij}.There are now exponentially many non-zero summands, those in which A corresponds to a valid tree.Nonetheless, 16Which is itself an exact BP algorithm, but on a different graph—a junction tree formed from the graph of TRIGRAM subfactors.Each variable in the junction tree is a bigram.If we had simply replaced the global TRIGRAM factor with its subfactors in the full factor graph, we would have had to resort to Generalized BP (Yedidia et al., 2004) to obtain the same exact results.17But taking it = 1 gives the same results, up to a constant.18As a matter of implementation, this odds ratio qL,, can be used to represent the incoming message qL., everywhere. we can follow the same approach as for EXACTLY1.Steps 1 and 4 are modified to iterate over all i, j such that Lij is a variable.In step 3, the partition function PA b(A) is now 7r times the total weight of all trees, where the weight of a given tree is the product of the gLij values of its n edges.In step 2, the marginal belief b(Lij = true) is now 7r times the total weight of all trees having edge i → j.We perform these combinatorial sums by calling a first-order parsing algorithm, with edge weights qij.Thus, as outlined in §2, a first-order parser is called each time we propagate through the global TREE or PTREE constraint, using edge weights that include the first-order LINK factors but also multiply in any current messages from higher-order factors.The parsing algorithm simultaneously computes the partition function b(), and all O(n2) marginal beliefs b(Lij = true).For PTREE (projective), it is the inside-outside version of a dynamic programming algorithm (Eisner, 1996).For TREE (nonprojective), Koo et al. (2007) and Smith and Smith (2007) show how to employ the matrix-tree theorem.In both cases, the total time is O(n3).19 NOCROSSj` must sum over assignments to O(n) neighboring variables {Lij} and {Lk`}.The nonzero summands are assignments where j and E each have exactly one parent.At step 1, 7r def = Qi qLij(false) · Qk qLke(false).At step 2, the marginal belief b(Lij = true) sums over the n nonzero assignments containing i → j.It is 7r · gLij · Pk �qLke · PAIRij,k`, where PAIRij,k` is xj` if i → j crosses k → E and is 1 otherwise. xj` is some factor value defined by equation (2) to penalize or reward the crossing.Steps 3–4 are just as in EXACTLY1j.The question is how to compute b(Lij = true) for each i in only O(1) time,20 so that we can propagate each of the O(n2) NOCROSSj` in O(n) time.This is why we allowed xj` to depend only on j, E. We can rewrite the sum b(Lij = true) as crossing k noncrossing k 19A dynamic algorithm could incrementally update the outgoing messages if only a few incoming messages have changed (as in asynchronous BP).In the case of TREE, dynamic matrix inverse allows us to update any row or column (i.e., messages from all parents or children of a given word) and find the new inverse in O(n2) time (Sherman and Morrison, 1950).20Symmetrically, we compute b(Lke = true) for each k. To find this in O(1) time, we precompute for each E an array of partial sums Q`[s, t] def = Ps<k<t �qLke.Since Q`[s, t] = Q`[s, t−1]+�qLte, we can compute each entry in O(1) time.The total precomputation time over all E, s, t is then O(n3), with the array Q` shared across all factors NOCROSSjq.The crossing sum is respectively Q`[0, i−1]+Q`[j+1, n], Q`[i+ 1, j − 1], or 0 according to whether E ∈ (i, j), E ∈� [i, j], or E = i.21 The non-crossing sum is Q`[0, n] minus the crossing sum.CHILDSEQi , like TRIGRAM, is propagated by a forward-backward algorithm.In this case, the algorithm is easiest to describe by replacing CHILDSEQi in the factor graph by a collection of local subfactors, which pass messages in the ordinary way.22 Roughly speaking,23 at each j ∈ [1, n], we introduce a new variable Cij—a hidden state whose value is the position of i’s previous child, if any (so 0 ≤ Cij < j).So the ternary subfactor on (Cij, Lij, Ci,j+1) has value 1 if Lij = false and Ci,j+1 = Ci,j; a sibling-bigram score (PAIRiCij,iCi,j+1) if Lij = true and Ci,j+1 = j; and 0 otherwise.The sparsity of this factor, which is 0 almost everywhere, is what gives CHILDSEQi a total runtime of O(n2) rather than O(n3).It is equivalent to forward-backward on an HMM with n observations (the Lij) and n states per observation (the Cj), with a deterministic (thus sparse) transition function.BP computes local beliefs, e.g. the conditional probability that a link Lij is present.But if we wish to output a single well-formed dependency tree, we need to find a single assignment to all the {Lij} that satisfies the TREE (or PTREE) constraint.Our final belief about the TREE factor is a distribution over such assignments, in which a tree’s probability is proportional to the probability of its edge weights gLij (incoming messages).We could simply return the mode of this distribution (found by using a 1-best first-order parser) or the k-best trees, or take samples.21There are no NOCROSSje factors with f = j.22We still treat CHILDSEQi as a global factor and compute all its correct outgoing messages on a single BP iteration, via serial forward and backward sweeps through the subfactors.Handling the subfactors in parallel, (3)–(4), would need O(n) iterations.23Ignoring the treatment of boundary symbols “#” (see §3.4).In our experiments, we actually take the edge weights to be not the messages �qLij from the links, def �bLij = log bLij(true)/bLij(false)).These are passed into a fast algorithm for maximum spanning tree (Tarjan, 1977) or maximum projective spanning tree (Eisner, 1996).This procedure is equivalent to minimum Bayes risk (MBR) parsing (Goodman, 1996) with a dependency accuracy loss function.Notice that the above decoding approaches do not enforce any hard constraints other than TREE in the final output.In addition, they only recover values of the Lij variables.They marginalize over other variables such as tags and link roles.This solves the problem of “nuisance” variables (which merely fragment probability mass among refinements of a parse).On the other hand, it may be undesirable for variables whose values we desire to recover.24Our training method also uses beliefs computed by BP, but at the factors.We choose the weight vector 0 by maximizing the log-probability of training data 24An alternative is to attempt to find the most probable (“MAP”) assignment to all variables—using the max-product algorithm (footnote 13) or one of its recent variants.The estimated marginal beliefs become “max marginals,” which assess the 1-best assignment consistent with each value of the variable.We can indeed build max-product propagators for our global constraints.PTREE still propagates in O(n3) time: simply change the first-order parser’s semiring (Goodman, 1999) to use max instead of sum.TREE requires O(n4) time: it seems that the O(n2) max marginals must be computed separately, each requiring a separate call to an O(n2) maximum spanning tree algorithm (Tarjan, 1977).If max-product BP converges, we may simply output each variable’s favorite value (according to its belief), if unique.However, max-product BP tends to be unstable on loopy graphs, and we may not wish to wait for full convergence in any case.A more robust technique for extracting an assignment is to mimic Viterbi decoding, and “follow backpointers” of the max-product computation along some spanning subtree of the factor graph.A slower but potentially more stable alternative is deterministic annealing.Replace each factor Fm(A) with Fm(A)1/T , where T > 0 is a temperature.As T --+ 0 (“quenches”), the distribution (1) retains the same mode (the MAP assignment), but becomes more sharply peaked at the mode, and sum-product BP approaches max-product BP.Deterministic annealing runs sum-product BP while gradually reducing T toward 0 as it iterates.By starting at a high T and reducing T slowly, it often manages in practice to find a good local optimum.We may then extract an assignment just as we do for max-product. under equation (1), regularizing only by early stopping.If all variables are observed in training, this objective function is convex (as for any log-linear model).The difficult step in computing the gradient of our objective is finding Vθ log Z, where Z in equation (1) is the normalizing constant (partition function) that sums over all assignments A.(Recall that Z, like each Fm, depends implicitly on W and 0.)As usual for log-linear models, Since VθFm(A) only depends on the assignment A’s values for variables that are connected to Fm in the factor graph, its expectation under p(A) depends only on the marginalization of p(A) to those variables jointly.Fortunately, BP provides an estimate of that marginal distribution, namely, its belief about the factor Fm, given W and 0 (§4.2).25 Note that the hard constraints do not depend on 0 at all; so their summands in equation (10) will be 0.We employ stochastic gradient descent (Bottou, 2003), since this does not require us to compute the objective function itself but only to (approximately) estimate its gradient as explained above.Alternatively, given any of the MAP decoding procedures from §6, we could use an error-driven learning method such as the perceptron or MIRA.26We asked: (1) For projective parsing, where higherorder factors have traditionally been incorporated into slow but exact dynamic programming (DP), what are the comparative speed and quality of the BP approximation?(2) How helpful are such higherorder factors—particularly for non-projective parsing, where BP is needed to make them tractable?(3) Do our global constraints (e.g., TREE) contribute to the goodness of BP’s approximation?We built a first-order projective parser—one that uses only factors PTREE and LINK—and then compared the cost of incorporating second-order factors, GRAND and CHILDSEQ, by BP versus DP.28 Under DP, the first-order runtime of O(n3) is increased to O(n4) with GRAND, and to O(n5) when we add CHILDSEQ as well.BP keeps runtime down to O(n3)—although with a higher constant factor, since it takes several rounds to converge, and since it computes more than just the best parse.29 Figures 2–3 compare the empirical runtimes for various input sentence lengths.With only the GRAND factor, exact DP can still find the Viterbi parse (though not the MBR parse29) faster than ten iterations of the asymptotically better BP (Fig.2), at least for sentences with n < 75.However, once we add the CHILDSEQ factor, BP is always faster— dramatically so for longer sentences (Fig.3).More complex models would widen BP’s advantage.Fig.4 shows the tradeoff between runtime and search error of BP in the former case (GRAND only).To determine BP’s search error at finding the MBR parse, we measured its dependency accuracy not against the gold standard, but against the optimal MBR parse under the model, which DP is able to find.After 10 iterations, the overall macro-averaged search error compared to O(n4) DP MBR is 0.4%; compared to O(n5) (not shown), 2.4%.More BP iterations may help accuracy.In future work, we plan to compare BP’s speed-accuracy curve on more complex projective models with the speed-accuracy curve of pruned or reranked DP.The BP approximation can be used to improve the accuracy of non-projective parsing by adding higher-order features.These would be NP-hard to incorporate exactly; DP cannot be used.We used BP with a non-projective TREE factor to train conditional log-linear parsing models of two highly non-projective languages, Danish and Dutch, as well as slightly non-projective English (§8.1).In all three languages, the first-order non-projective parser greatly overpredicts the number of crossing links.We thus added NOCROSS factors, as well as GRAND and CHILDSEQ as before.All of these significantly improve the first-order baseline, though not necessarily cumulatively (Table 2).Finally, Table 2 compares loopy BP to a previously proposed “hill-climbing” method for approximate inference in non-projective parsing McDonald and Pereira (2006).Hill-climbing decodes our richest non-projective model by finding the best projective parse under that model—using slow, higherorder DP—and then greedily modifies words’ parents until the parse score (1) stops improving. with TREE, decoding it with weaker constraints is asymptotically faster (except for NOT2) but usually harmful.(Parenthetical numbers show that the harm is compounded if the weaker constraints are used in training as well; even though this matches training to test conditions, it may suffer more from BP’s approximate gradients.)Decoding the TREE model with the even stronger PTREE constraint can actually be helpful for a more projective language.All results use 5 iterations of BP.BP for non-projective languages is much faster and more accurate than the hill-climbing method.Also, hill-climbing only produces an (approximate) 1-best parse, but BP also obtains (approximate) marginals of the distribution over all parses.Given the BP architecture, do we even need the hard TREE constraint?Or would it suffice for more local hard constraints to negotiate locally via BP?We investigated this for non-projective first-order parsing.Table 3 shows that global constraints are indeed important, and that it is essential to use TREE during training.At test time, the weaker but still global EXACTLY1 may suffice (followed by MBR decoding to eliminate cycles), for total time O(n2).Table 3 includes NOT2, which takes O(n3) time, merely to demonstrate how the BP approximation becomes more accurate for training and decoding when we join the simple NOT2 constraints into more global ATMOST1 constraints.This does not change the distribution (1), but makes BP enforce stronger local consistency requirements at the factors, relying less on independence assumptions.In general, one can get better BP approximations by replacing a group of factors F,,t(A) with their product.30 The above experiments concern gold-standard 30In the limit, one could replace the product (1) with a single all-purpose factor; then BP would be exact—but slow.(In constraint satisfaction, joining constraints similarly makes arc consistency slower but better at eliminating impossible values.) accuracy under a given first-order, non-projective model.Flipping all three of these parameters for Danish, we confirmed the pattern by instead measuring search error under a higher-order, projective model (PTREE+LINK+GRAND), when PTREE was weakened during decoding.Compared to the MBR parse under that model, the search errors from decoding with weaker hard constraints were 2.2% for NOT2, 2.1% for EXACTLY1, 1.7% for EXACTLY1 + NO2CYCLE, and 0.0% for PTREE.Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable.For projective parsing, it is significantly faster than exact dynamic programming, at the cost of small amounts of search error, We are interested in extending these ideas to phrase-structure and lattice parsing, and in trying other higher-order features, such as those used in parse reranking (Charniak and Johnson, 2005; Huang, 2008) and history-based parsing (Nivre and McDonald, 2008).We could also introduce new variables, e.g., nonterminal refinements (Matsuzaki et al., 2005), or secondary links Mid (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc.(Sleator and Temperley, 1993; Buch-Kromann, 2006).Other parsing-like problems that could be attacked with BP appear in syntax-based machine translation.Decoding is very expensive with a synchronous grammar composed with an n-gram language model (Chiang, 2007)—but our footnote 10 suggests that BP might incorporate a language model rapidly.String alignment with synchronous grammars is quite expensive even for simple synchronous formalisms like ITG (Wu, 1997)—but Duchi et al. (2007) show how to incorporate bipartite matching into max-product BP.Finally, we can take advantage of improvements to BP proposed in the context of other applications.For example, instead of updating all messages in parallel at every iteration, it is empirically faster to serialize updates using a priority queue (Elidan et al., 2006; Sutton and McCallum, 2007).31 31These methods need alteration to handle our global propagators, which do update all their outgoing messages at once.
Fast Cheap and Creative: Evaluating Translation Quality Using Amazon&rsquo;s Mechanical TurkManual evaluation of translation quality is generally thought to be excessively time consuming and expensive.We explore a fast and inexpensive way of doing it using Amazon’s Mechanical Turk to pay small sums to a large number of non-expert annotators.For $10 we redundantly recreate judgments from a WMT08 translation task.We find that when combined non-expert judgments have a high-level of agreement with the existing gold-standard judgments of machine translation quality, and correlate more strongly with expert judgments than Bleu does.We go on to show that Mechanical Turk can be used to calculate human-mediated translation edit rate (HTER), to conduct reading comprehension experiments with machine translation, and to create high quality reference translations.Conventional wisdom holds that manual evaluation of machine translation is too time-consuming and expensive to conduct.Instead, researchers routinely use automatic metrics like Bleu (Papineni et al., 2002) as the sole evidence of improvement to translation quality.Automatic metrics have been criticized for a variety of reasons (Babych and Hartley, 2004; Callison-Burch et al., 2006; Chiang et al., 2008), and it is clear that they only loosely approximate human judgments.Therefore, having people evaluate translation output would be preferable, if it were more practical.In this paper we demonstrate that the manual evaluation of translation quality is not as expensive or as time consuming as generally thought.We use Amazon’s Mechanical Turk, an online labor market that is designed to pay people small sums of money to complete human intelligence tests – tasks that are difficult for computers but easy for people.We show that:Snow et al. (2008) examined the accuracy of labels created using Mechanical Turk for a variety of natural language processing tasks.These tasks included word sense disambiguation, word similarity, textual entailment, and temporal ordering of events, but not machine translation.Snow et al. measured the quality of non-expert annotations by comparing them against labels that had been previously created by expert annotators.They report inter-annotator agreement between expert and non-expert annotators, and show that the average of many non-experts converges on performance of a single expert for many of their tasks.Although it is not common for manual evaluation results to be reported in conference papers, several large-scale manual evaluations of machine translation quality take place annually.These include public forums like the NIST MT Evaluation Workshop, IWSLT and WMT, as well as the project-specific Go/No Go evaluations for the DARPA GALE program.Various types of human judgments are used.NIST collects 5-point fluency and adequacy scores (LDC, 2005), IWSLT and WMT collect relative rankings (Callison-Burch et al., 2008; Paul, 2006), and DARPA evaluates using HTER (Snover et al., 2006).The details of these are provided later in the paper.Public evaluation campaigns provide a ready source of goldstandard data that non-expert annotations can be compared to.Amazon describes its Mechanical Turk web service1 as artificial artificial intelligence.The name and tag line refer to a historical hoax from the 18th century where an automaton appeared to be able to beat human opponents at chess using a clockwork mechanism, but was, in fact, controlled by a person hiding inside the machine.The Mechanical Turk web site provides a way to pay people small amounts of money to perform tasks that are simple for humans but difficult for computers.Examples of these Human Intelligence Tasks (or HITs) range from labeling images to moderating blog comments to providing feedback on relevance of results for a search query.Anyone with an Amazon account can either submit HITs or work on HITs that were submitted by others.Workers are sometimes referred to as “Turkers” and people designing the HITs are “Requesters.” Requesters can specify the amount that they will pay for each item that is completed.Payments are frequently as low as $0.01.Turkers are free to select whichever HITs interest them.Amazon provides three mechanisms to help ensure quality: First, Requesters can have each HIT be completed by multiple Turkers, which allows higher quality labels to be selected, for instance, by taking the majority label.Second, the Requester can require that all workers meet a particular set of qualications, such as sufficient accuracy on a small test set or a minimum percentage of previously accepted submissions.Finally, the Requester has the option of rejecting the work of individual workers, in which case they are not paid.The level of good-faith participation by Turkers is surprisingly high, given the generally small nature of the payment.2 For complex undertakings like creating data for NLP tasks, Turkers do not have a specialized background in the subject, so there is an obvious tradeoff between hiring individuals from this non-expert labor pool and seeking out annotators who have a particular expertise.We use Mechanical Turk as an inexpensive way of evaluating machine translation.In this section, we measure the level of agreement between expert and non-expert judgments of translation quality.To do so, we recreate an existing set of goldstandard judgments of machine translation quality taken from the Workshop on Statistical Machine Translation (WMT), which conducts an annual large-scale human evaluation of machine translation quality.The experts who produced the goldstandard judgments are computational linguists who develop machine translation systems.We recreated all judgments from the WMT08 German-English News translation task.The output of the 11 different machine translation systems that participated in this task was scored by ranking translated sentences relative to each other.To collect judgements, we reproduced the WMT08 web interface in Mechanical Turk and provided these instructions: Evaluate machine translation quality Rank each translation from Best to Worst relative to the other choices (ties are allowed).If you do not know the source language then you can read the reference translation, which was created by a professional human translator.The web interface displaced 5 different machine translations of the same source sentence, and had radio buttons to rate them.Turkers were paid a grand total of $9.75 to complete nearly 1,000 HITs.These HITs exactly replicated the 200 screens worth of expert judgments that were collected for the WMT08 German-English News translation task, with each screen being completed by five different Turkers.The Turkers were shown a source sentence, a reference translation, and translations from five MT systems.They were asked to rank the translations relative to each other, assigning scores from best to worst and allowing ties.We evaluate non-expert Turker judges by measuring their inter-annotator agreement with the WMT08 expert judges, and by comparing the correlation coefficient across the rankings of the machine translation systems produced by the two sets of judges. equal.The quality of their works varies.Figure 2 shows the agreement of individual Turkers with expert annotators, plotted against the number of HITs they completed.The figure shows that their agreement varies considerably, and that Turker who completed the most judgments was among the worst performing.To avoid letting careless annotators drag down results, we experimented with weighted voting.We weighted votes in two ways: Turker agreed with the rest of the Turkers over the whole data set.This does not require any gold standard calibration data.It goes beyond simple voting, because it looks at a Turker’s performance over the entire set, rather than on an item-by-item basis.Figure 1 shows that these weighting mechanisms perform similarly well.For this task, deriving weights from agreement with other non-experts is as effective as deriving weights from experts.Moreover, by weighting the votes of five Turkers, non-expert judgments perform at the upper bound of expert-expert correlation.All correlate more strongly than Bleu. we are able to achieve the same rate of agreement with experts as they achieve with each other.Correlation when ranking systems In addition to measuring agreement with experts at the sentence-level, we also compare non-expert system-level rankings with experts.Following Callison-Burch et al. (2008), we assigned a score to each of the 11 MT systems based on how often its translations were judged to be better than or equal to any other system.These scores were used to rank systems and we measured Spearman’s ρ against the system-level ranking produced by experts.Figure 3 shows how well the non-expert rankings correlate with expert rankings.An upper bound is indicated by the expert-expert bar.This was created using a five-fold cross validation where we used 20% of the expert judgments to rank the systems and measured the correlation against the rankings produced by the other 80% of the judgments.This gave a ρ of 0.78.All ways of combining the non-expert judgments resulted in nearly identical correlation, and all produced correlation within the range of with what we would experts to.The rankings produced using Mechanical Turk had a much stronger correlation with the WMT08 expert rankings than the Blue score did.It should be noted that the WMT08 data set does not have multiple reference translations.If multiple references were used that Bleu would likely have stronger correlation.However, it is clear that the cost of hiring professional translators to create multiple references for the 2000 sentence test set would be much greater than the $10 cost of collecting manual judgments on Mechanical Turk.In this section we report on a number of creative uses of Mechanical Turk to do more sophisticated tasks.We give evidence that Turkers can create high quality translations for some languages, which would make creating multiple reference translations for Bleu less costly than using professional translators.We report on experiments evaluating translation quality with HTER and with reading comprehension tests.In addition to evaluating machine translation quality, we also investigated the possibility of using Mechanical Turk to create additional reference translations for use with automatic metrics like Bleu.Before trying this, we were skeptical that Turkers would have sufficient language skills to produce translations.Our translation HIT had the following instructions: We solicited translations for 50 sentences in French, German, Spanish, Chinese and Urdu, and designed the HIT so that five Turkers would translate each sentence.Filtering machine translation Upon inspecting the Turker’s translations it became clear that many had ignored the instructions, and had simply cutand-paste machine translation rather then translating the text themselves.We therefore set up a second HIT to filter these out.After receiving the score when one LDC translator is compared against the other 10 translators (or the other 2 translators in the case of Urdu).This gives an upper bound on the expected quality.The Turkers’ translation quality falls within a standard deviation of LDC translators for Spanish, German and Chinese.For all languages, Turkers produce significantly better translations than an online machine translation system. translations, we had a second group of Turkers clean the results.We automatically excluded Turkers whose translations were flagged 30% of the time or more.Quality of Turkers’ translations Our 50 sentence test sets were selected so that we could compare the translations created by Turkers to translations commissioned by the Linguistics Data Consortium.For the Chinese, French, Spanish, and German translations we used the the MultipleTranslation Chinese Corpus.3 This corpus has 11 reference human translations for each Chinese source sentence.We had bilingual graduate students translate the first 50 English sentences of that corpus into French, German and Spanish, so that we could re-use the multiple English reference translations.The Urdu sentences were taken from the NIST MT Eval 2008 Urdu-English Test Set4 which includes three distinct English translations for every Urdu source sentence.Figure 4 shows the Turker’s translation quality in terms of the Bleu metric.To establish an upper bound on expected quality, we determined what the Bleu score would be for a professional translator when measured against other professionals.We calculated a Bleu score for each of the 11 LDC translators using the other 10 translators as the reference set.The average Bleu score for LDC2002T01 was 0.54, with a standard deviation of 0.07.The average Bleu for the Urdu test set is lower because it has fewer reference translations.To measure the Turkers’ translation quality, we randomly selected translations of each sentence from Turkers who passed the Detect MT HIT, and compared them against the same sets of 10 reference translations that the LDC translators were compared against.We randomly sampled the Turkers 10 times, and calculated averages and standard deviations for each source language.Figure 4 the Bleu scores for the Turkers’ translations of Spanish, German and Chinese are within the range of the LDC translators.For all languages, the quality is significantly higher than an online machine translation system.We used Yahoo’s Babelfish for Spanish, German, French and Chinese,5 was likely and Babylon for Urdu.Demographics We collected demographic information about the Turkers who completed the translation task.We asked how long they had spoken the source language, how long they had spostatistics on the left are for people who appeared to do the task honestly.The statistics on the right are for people who appeared to be using MT (marked as using it 20% or more in the Detect MT HIT). ken English, what their native language was, and where they lived.Table 1 gives their replies.Cost and speed We paid Turkers $0.10 to translate each sentence, and $0.006 to detect whether a sentence was machine translated.The cost is low enough that we could create a multiple reference set quite cheaply; it would cost less than $1,000 to create 4 reference translations for 2000 sentences.The time it took for the 250 translations to be completed for each language varied.It took less than 4 hours for Spanish, 20 hours for French, 22.5 hours for German, 2 days for Chinese, and nearly 4 days for Urdu.Human-mediated translation edit rate (HTER) is the official evaluation metric of the DARPA GALE program.The evaluation is conducted annually by the Linguistics Data Consortium, and it is used to determine whether the teams participating the program have met that year’s benchmarks.These evaluations are used as a “Go / No Go” determinant of whether teams will continue to receive funding.Thus, each team have a strong incentive to get as good a result as possible under the metric.Each of the three GALE teams encompasses multiple sites and each has a collection of machine translation systems.A general strategy employed by all teams is to perform system combination over these systems to produce a synthetic translation that is better than the sum of its parts (Matusov et al., 2006; Rosti et al., 2007).The contribution of each component system is weighted by the expectation that it will produce good output.To our knowledge, none of the teams perform their own HTER evaluations in order to set these weights.We evaluated the feasibility of using Mechanical Turk to perform HTER.We simplified the official GALE post-editing guidelines (NIST and LDC, 2007).We provided these instructions: Edit Machine Translation Your task is to edit the machine translation making as few changes as possible so that it matches the meaning of the human translation and is good English.Please follow these guidelines: edit rate decreases as the number of editors increases from zero (where HTER is simply the TER score between the MT output and the reference translation) and five.We displayed 10 sentences from a news article.In one column was the reference English translation, in the other column were text boxes containing the MT output to be edited.To minimize the edit rate, we collected edits from five different Turkers for every machine translated segment.We verified these with a second HIT were we prompted Turkers to: For the final score, we choose the edited segment which passed the criteria and which minimized the edit distance to the unedited machine translation output.If none of the five edits was deemed to be acceptable, then we used the edit distance between the MT and the reference.Setup We evaluated five machine translation systems using HTER.These systems were selected from WMT09 (Callison-Burch et al., 2009).We wanted a spread in quality, so we took the top two and bottom two systems from the GermanEnglish task, and the top system from the FrenchEnglish task (which significantly outperformed everything else).Based on the results of the WMT09 evaluation we would expect the see the following ranking from the least edits to the most edits: google.fr-en, google.de-en, rbmt5.de-en, geneva.de-en and tromble.de-en.Results Table 2 gives the HTER scores for the five systems.Their ranking is as predicted, indicating that the editing is working as expected.The table reports averaged scores when the five annotators are subsampled.This gives a sense of how much each additional editor is able to minimize the score for each system.The difference between the TER score with zero editors, and the HTER five editors is greatest for the rmbt5 system, which has a delta of .29 and is smallest for jhu-tromble with .07.One interesting technique for evaluating machine translation quality is through reading comprehension questions about automatically translated text.The quality of machine translation systems can be quantified based on how many questions are answered correctly.Jones et al. (2005) evaluated translation quality using a reading comprehension test the Defense Language Proficiency Test (DLPT), which is administered to military translators.The DLPT contains a collection of foreign articles of varying levels of difficulties, and a set of short answer questions.Jones et al used the Arabic DLPT to do a study of machine translation quality, by automatically translating the Arabic documents into English and seeing how many human subjects could successfully pass the exam.The advantage of this type of evaluation is that the results have a natural interpretation.They indicate how understandable the output of a machine translation system is better than Bleu does, and better than other manual evaluation like the relative ranking.Despite this advantage, evaluating MT through reading comprehension hasn’t caught on, due to the difficulty of administering it and due to the fact that the DLPT or similar tests are not publicly available.We conducted a reading comprehension evaluation using Mechanical Turk.Instead of simply administering the test on Mechanical Turk, we used it for all aspects from test creation to answer grading.Our procedure was as follows: Test creation We posted human translations of foreign news articles, and ask Tukers to write three questions and provide sample answers.We gave simple instructions on what qualifies as a good reading comprehension question.System google.fr-en google.de-en rbmt5.de-en geneva.de-en tromble.de-en Question selection We posted the questions for each article back to Mechanical Turk, and asked other Turkers to vote on whether each question was a good and to indicate if it was redundant with any other questions in the set.We sorted questions to maximize the votes and minimized redundancies using a simple perl script, which discarded questions below a threshold, and eliminated all redundancies.Taking the test We posted machine translated versions of the foreign articles along with the questions, and had Turkers answer them.We ensured that no one would see multiple translations of the same article.Grading the answers We aggregated the answers and used Mechanical Turk to grade them.We showed the human translation of the article, one question, the sample answer, and displayed all answers to it.After the Turkers graded the answers, we calculated the percentage of questions that were answered correctly for each system.Turkers created 90 questions for 10 articles, which were subsequently filtered down to 47 good questions, ranging from 3–6 questions per article.25 Turkers answered questions about each translated article.To avoid them answering the questions multiple times, we randomly selected which system’s translation was shown to them.Each system’s translation was displayed an average of 5 reference 0.94 google.fr-en 0.85 google.de-en 0.80 rbmt5.de-en 0.77 geneva.de-en 0.63 jhu-tromble.de-en 0.50 times per article.As a control, we had three Turkers answer the reading comprehension questions using the reference translation.Table 3 gives the percent of questions that were correctly answered using each of the different systems’ outputs and using the reference translation.The ranking is exactly what we would expect, based on the HTER scores and on the human evaluation of the systems in WMT09.This again helps to validate that the reading comprehension methodology.The scores are more interpretable than Blue scores and than the WMT09 relative rankings, since it gives an indication of how understandable the MT output is.Appendix A shows some sample questions and answers for an article.Mechanical Turk is an inexpensive way of gathering human judgments and annotations for a wide variety of tasks.In this paper we demonstrate that it is feasible to perform manual evaluations of machine translation quality using the web service.The low cost of the non-expert labor found on Mechanical Turk is cheap enough to collect redundant annotations, which can be utilized to ensure translation quality.By combining the judgments of many non-experts we are able to achieve the equivalent quality of experts.The suggests that manual evaluation of translation quality could be straightforwardly done to validate performance improvements reported in conference papers, or even for mundane tasks like tracking incremental system updates.This challenges the conventional wisdom which has long held that automatic metrics must be used since manual evaluation is too costly and timeconsuming.We have shown that Mechanical Turk can be used creatively to produce quite interesting things.We showed how a reading comprehension test could be created, administered, and graded, with only very minimal intervention.We believe that it is feasible to use Mechanical Turk for a wide variety of other machine translated tasks like creating word alignments for sentence pairs, verifying the accuracy of document- and sentence-alignments, performing non-simulated active learning experiments for statistical machine translation, even collecting training data for low resource languages like Urdu.The cost of using Mechanical Turk is low enough that we might consider attempting quixotic things like human-in-the-loop minimum error rate training (Zaidan and Callison-Burch, 2009), or doubling the amount of training data available for Urdu.This research was supported by the EuroMatrixPlus project funded by the European Commission, and by the US National Science Foundation under grant IIS-0713448.The views and findings are the author’s alone.The actress Heather Locklear, Amanda on the popular series Melrose Place, was arrested this weekend in Santa Barbara (California) after driving under the influence of drugs.A witness saw her performing inappropriate maneuvers while trying to take her car out of a parking space in Montecito, as revealed to People magazine by a spokesman for the Californian Highway Police.The witness stated that around 4.30pm Ms. Locklear “hit the accelerator very roughly, making excessive noise and trying to take the car out from the parking space with abrupt back and forth maneuvers.While reversing, she passed several times in front of his sunglasses.” Shortly after, the witness, who at first, apparently had not recognized the actress, saw Ms. Locklear stopping in a nearby street and leaving the vehicle.It was this person who alerted the emergency services, because “he was concerned about Ms. Locklear’s life.” When the patrol arrived, the police found the actress sitting inside her car, which was partially blocking the road.“She seemed confused,” so the policemen took her to a specialized centre for drugs and alcohol and submitted her a test.According to a spokesman for the police, the actress was cooperative and excessive alcohol was ruled out from the beginning, even if “as the officers initially observed, we believe Ms. Locklear was under the influences drugs.” Ms. Locklear was arrested under suspicion of driving under the influence of some - unspecified substance, and imprisoned in the local jail at 7.00pm, to be released some hours later.Two months ago, Ms. Locklear was released from a specialist clinic in Arizona where she was treated after an episode of anxiety and depression.4 questions were selected She was arested on suspicion of driving under the influence of drugs.She was cured for anxiety and depression.Answers to Where was Ms. Locklear two months ago? that were judged to be correct: Arizona hospital for treatment of depression; at a treatmend clinic in Arizona; in the Arizona clinic being treated for nervous breakdown; a clinic in Arizona; Arizona, under treatment for depression; She was a patient in a clinic in Arizona undergoing treatment for anxiety and depression; In an Arizona mental health facility ; A clinic in Arizona.; In a clinic being treated for anxiety and depression.; at an Arizona clinic These answers were judged to be incorrect: Locklear was retired in Arizona; Arizona; Arizona; in Arizona; Ms.Locklaer were laid off after a treatment out of the clinic in Arizona.
Parser Adaptation and Projection with Quasi-Synchronous Grammar FeaturesWe connect two scenarios in structured parser trained on one corpus to another annotation style, and annotations from one to another.We propose quasigrammar features for these structured learning tasks.That is, we score a aligned pair of source and target trees based on local features of the trees and the alignment.Our quasi-synchronous model assigns positive probability to any alignment of any trees, in contrast to a synchronous grammar, which would insist on some form of structural parallelism.In monolingual dependency parser adaptation, we achieve high accuracy in translating among multiple annotation styles for the same sentence.On the more difficult problem of cross-lingual parser projection, we learn a dependency parser for a target language by using bilingual text, an English parser, and automatic word alignments.Our experiments show that unsupervised QG projection improves on parses trained using only highprecision projected annotations and far outperforms, by more than 35% absolute dependency accuracy, learning an unsupervised parser from raw target-language text alone.When a few target-language parse trees are available, projection gives a boost equivalent to doubling the number of target-language trees. first author would like to thank the Center for Intelligent Information Retrieval at UMass Amherst.We would also like to thank Noah Smith and Rebecca Hwa for helpful discussions and the anonymous reviewers for their suggestions for improving the paper.Consider the problem of learning a dependency parser, which must produce a directed tree whose vertices are the words of a given sentence.There are many differing conventions for representing syntactic relations in dependency trees.Say that we wish to output parses in the Prague style and so have annotated a small target corpus—e.g., 100 sentences—with those conventions.A parser trained on those hundred sentences will achieve mediocre dependency accuracy (the proportion of words that attach to their correct parent).But what if we also had a large number of trees in the CoNLL style (the source corpus)?Ideally they should help train our parser.But unfortunately, a parser that learned to produce perfect CoNLL-style trees would, for example, get both links “wrong” when its coordination constructions were evaluated against a Prague-style gold standard (Figure 1).If it were just a matter of this one construction, the obvious solution would be to write a few rules by hand to transform the large source training corpus into the target style.Suppose, however, that there were many more ways that our corpora differed.Then we would like to learn a statistical model to transform one style of tree into another.We may not possess hand-annotated training data for this tree-to-tree transformation task.That would require the two corpora to annotate some of the same sentences in different styles.But fortunately, we can automatically obtain a noisy form of the necessary paired-tree training data.A parser trained on the source corpus can parse the sentences in our target corpus, yielding trees (or more generally, probability distributions over trees) in the source style.We will then learn a tree transformation model relating these noisy source trees to our known trees in the target style.This model should enable us to convert the original large source corpus to target style, giving us additional training data in the target style.For many target languages, however, we do not have the luxury of a large parsed “source corpus” in the language, even one in a different style or domain as above.Thus, we may seek other forms of data to augment our small target corpus.One option would be to leverage unannotated text (McClosky et al., 2006; Smith and Eisner, 2007).But we can also try to transfer syntactic information from a parsed source corpus in another language.This is an extreme case of out-of-domain data.This leads to the second task of this paper: learning a statistical model to transform a syntactic analysis of a sentence in one language into an analysis of its translation.Tree transformations are often modeled with synchronous grammars.Suppose we are given a sentence w' in the “source” language and its translation w into the “target” language.Their syntactic parses t' and t are presumably not independent, but will tend to have some parallel or at least correlated structure.So we could jointly model the parses t', t and the alignment a between them, with a model of the form p(t, a, t' I w, w').Such a joint model captures how t, a, t' mutually constrain each other, so that even partial knowledge of some of these three variables can help us to recover the others when training or decoding on bilingual text.This idea underlies a number of recent papers on syntax-based alignment (using t and t' to better recover a), grammar induction from bitext (using a to better recover t and t'), parser projection (using t' and a to better recover t), as well as full joint parsing (Smith and Smith, 2004; Burkett and Klein, 2008).In this paper, we condition on the 1-best source tree t'.As for the alignment a, our models either condition on the 1-best alignment or integrate the alignment out.Our models are thus of the form p(t w, w', t', a) or, in the generative case, p(w, t, a w', t').We intend to consider other formulations in future work.So far, this is very similar to the monolingual parser adaptation scenario, but there are a few key differences.Since the source and target sentences in the bitext are in different languages, there is no longer a trivial alignment between the words of the source and target trees.Given word alignments, we could simply try to project dependency links in the source tree onto the target text.A link-by-link projection, however, could result in invalid trees on the target side, with cycles or disconnected words.Instead, our models learn the necessary transformations that align and transform a source tree into a target tree by means of quasisynchronous grammar (QG) features.Figure 2 shows an example of bitext helping disambiguation when a parser is trained with only a small number of Chinese trees.With the help of the English tree and alignment, the parser is able to recover the correct Chinese dependencies using QG features.Incorrect edges from the monolingual parser are shown with dashed lines.(The bilingual parser corrects additional errors in the second half of this sentence, which has been removed to improve legibility.)The parser is able to recover the long-distance dependency from the first Chinese word (China) to the last (begun), while skipping over the intervening noun phrase that confused the undertrained monolingual parser.Although, due to the auxiliary verb, “China” and “begun” are siblings in English and not in direct dependency, the QG features still leverage this indirect projection.We start by describing the features we use to augment conditional and generative parsers when scoring pairs of trees (§2).Then we discuss in turn monolingual (§3) and cross-lingual (§4) parser adaptation.Finally, we present experiments on cross-lingual parser projection in conditions when no target language trees are available for training (§5) and when some trees are available (§6).What should our model of source and target trees look like?In our view, traditional approaches based on synchronous grammar are problematic both computationally and linguistically.Full inference takes O(n6) time or worse (depending on the grammar formalism).Yet synchronous models only consider a limited hypothesis space: e.g., parses must be projective, and alignments must decompose according to the recursive parse structure.(For example, two nodes can be aligned only if their respective parents are also aligned.)The synchronous model’s probability mass function is also restricted to decompose in this way, so it makes certain conditional independence assumptions; put another way, it can evaluate only certain properties of the triple (t, a, t0).We instead model (t, a, t0) as an arbitrary graph that includes dependency links among the words of each sentence as well as arbitrary alignment links between the words of the two sentences.This permits non-synchronous and many-to-many alignments.The only hard constraint we impose is that the dependency links within each sentence must constitute a valid monolingual parse—a directed projective spanning tree.1 Given the two sentences w, w0, our probability distribution over possible graphs considers local features of the parses, the alignment, and both jointly.Thus, we learn what local syntactic configurations tend to occur in each language and how they correspond across languages.As a result, we might learn that parses are “mostly synchronous,” but that there are some systematic cross-linguistic 1Non-projective parsing would also be possible. divergences and some instances of sloppy (nonparallel or inexact) translation.Our model is thus a form of quasi-synchronous grammar (QG) (Smith and Eisner, 2006a).In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009).All the models in this paper are conditioned on the source tree t0.Conditionally-trained models of adaptation and projection also condition on the target string w and its alignment a to w0 and thus have the form p(t  |w, w0, t0, a); the unsupervised, generative projection models in §5 have the form p(w, t, a  |w0, t0).The score s of a given tuple of trees, words, and alignment can thus be written as a dot product of weights w with features f and g: � wjgj(t,t0,a,w,w0) + j The features f look only at target words and dependencies.In the conditional models of §3 and §6, these features are those of an edge-factored dependency parser (McDonald et al., 2005).In the generative models of §5, f has the form of a dependency model with valence (Klein and Manning, 2004).All models, for instance, have a feature template that considers the parts of speech of a potential parent-child relation.In order to benefit from the source language, we also need to include bilingual features g. When scoring a candidate target dependency link from word x → y, these features consider the relationship of their corresponding source words x0 and y0.(The correspondences are determined by the alignment a.)For instance, the source tree t0 may contain the link x0 → y0, which would cause a feature for monotonic projection to fire for the x → y edge.If, on the other hand, y0 → x0 E t0, a head-swapping feature fires.If x0 = y0, i.e. x and y align to the same word, the same-word feature fires.Similar features fire when x0 and y0 are in grandparent-grandchild, sibling, c-command, or none-of-the above relationships, or when y aligns to NULL.These alignment classes are called configurations (Smith and Eisner, 2006a, and following).When training is conditioned on the target words (see §3 and §6 below), we conjoin these configuration features with the part of speech and coarse part of speech of one or both of the source and target words, i.e. the feature template has from one to four tags.In conditional training, the exponentiated scores s are normalized by a constant: Z = Et exp[s(t, t', a, w, w')].For the generative model, the locally normalized generative process is explained in §5.3.4.Previous researchers have written fix-up rules to massage the projected links after the fact and learned a parser from the resulting trees (Hwa et al., 2005).Instead, our models learn the necessary transformations that align and transform a source tree into a target tree.Other researchers have tackled the interesting task of learning parsers from unparsed bitext alone (Kuhn, 2004; Snyder et al., 2009); our methods take advantage of investments in high-resource languages such as English.In work most closely related to this paper, Ganchev et al. (2009) constrain the posterior distribution over target-language dependencies to align to source dependencies some “reasonable” proportion of the time (≈ 70%, cf.Table 2 in this paper).This approach performs well but cannot directly learn regular cross-language non-isomorphisms; for instance, some fixup rules for auxiliary verbs need to be introduced.Finally, Huang et al. (2009) use features, somewhat like QG configurations, on the shift-reduce actions in a monolingual, targetlanguage parser.As discussed in §1, the adaptation scenario is a special case of parser projection where the word alignments are one-to-one and observed.To test our handling of QG features, we performed experiments in which training saw the correct parse trees in both source and target domains, and the mapping between them was simple and regular.We also performed experiments where the source trees were replaced by the noisy output of a trained parser, making the mapping more complex and harder to learn.We used the subset of the Penn Treebank from the CoNLL 2007 shared task and converted it to dependency representation while varying two parameters: (1) CoNLL vs. Prague coordination style (Figure 1), and (2) preposition the head vs. the child of its nominal object.We trained an edge-factored dependency parser (McDonald et al., 2005) on “source” domain data that followed one set of dependency conventions.We then trained an edge-factored parser with QG features on a small amount of “target” domain data.The source parser outputs were produced for all target data, both training and test, so that features for the target parser could refer to them.In this task, we know what the gold-standard source language parses are for any given text, since we can produce them from the original Penn Treebank.We can thus measure the contribution of adaptation loss alone, and the combined loss of imperfect source-domain parsing with adaptation (Table 1).When no target domain trees are available, we simply have the performance of the source domain parser on this out-of-domain data.Training a target-domain parser on as few as 10 sentences shows substantial improvements in accuracy.In the “gold” conditions, where the target parser starts with perfect source trees, accuracy approaches 100%; in the realistic “parse” conditions, where the target-domain parser gets noisy source-domain parses, the improvements are quite significant but approach a lower ceiling imposed by the performance of the source parser.2 The adaptation problem in this section is a simple proof of concept of the QG approach; however, more complex and realistic adaptation problems exist.Monolingual adaptation is perhaps most obviously useful when the source parser is a blackbox or rule-based system or is trained on unavailable data.One might still want to use such a parser in some new context, which might require new data or a new annotation standard.We are also interested in scenarios where we want to avoid expensive retraining on large reannotated treebanks.We would like a linguist to be able to annotate a few trees according to a hypothesized theory and then quickly use QG adaptation to get a parser for that theory.One example would be adapting a constituency parser to produce dependency parses.We have concentrated here on adapting between two dependency parse styles, in order to line up with the cross-lingual tasks to which we now turn.As in the adaptation scenario above, many syntactic structures can be transferred from one language to another.In this section, we evaluate the extent of this direct projection on a small handannotated corpus.In §5, we will use a QG generative model to learn dependency parsers from bitext when there are no annotations in the target language.Finally, in §6,we show how QG features can augment a target-language parser trained on a small set of labeled trees.For syntactic annotation projection to work at all, we must hypothesize, or observe, that at least some syntactic structures are preserved in translation.Hwa et al. (2005) have called this intuition the Direct Correspondence Assumption (DCA, with slight notational changes): Given a pair of sentences w and w' that are translations of each other with syntactic structure t and t', if nodes x' and y' of t' are aligned with nodes x and y of t, respectively, and if syntactic relationship R(x', y') holds in t', then R(x, y) holds in t. The validity of this assumption clearly depends on the node-to-node alignment of the two trees.We again work in a dependency framework, where syntactic nodes are simply lexical items.This allows us to use existing work on word alignment.Hwa et al. (2005) tested the DCA under idealized conditions by obtaining hand-corrected dependency parse trees of a few hundred sentences of Spanish-English and Chinese-English bitext.They also used human-produced word alignments.Since their word alignments could be many-tomany, they gave a heuristic Direct Projection Algorithm (DPA) for resolving them into component dependency relations.It should be noted that this process introduced empty words into the projected target language tree and left words that are unaligned to English detached from the tree; as a result, they measured performance in dependency Fscore rather than accuracy.With manual English parses and word alignments, this DPA achieved 36.8% F-score in Spanish and 38.1% in Chinese.With Collins-model English parses and GIZA++ word alignments, F-score was 33.9% for Spanish and 26.3% for Chinese.Compare this to the Spanish attach-left baseline of 31.0% and the Chinese attach-right baselines of 35.9%.These discouragingly low numbers led them to write languagespecific transformation rules to fix up the projected trees.After these rules were applied to the projections of automatic English parses, F-score was 65.7% for English and 52.4% for Chinese.While these F-scores were low, it is useful to look at a subset of the alignment: dependencies projected across one-to-one alignments before the heuristic fix-ups had a much higher precision, if lower recall, than Hwa et al.’s final results.Using Hwa et al.’s data, we calculated that the precision of projection to Spanish and Chinese via these one-to-one links was ≈ 65% (Table 2).There is clearly more information in these direct links than one would think from the F-scores.To exploit this information, however, we need to overcome the problems of (1) learning from partial trees, when not all target words are attached, and (2) learning in the presence of the still considerable noise in the projected one-to-one dependencies—e.g., at least 28% error for Spanish non-punctuation dependencies.What does this noise consist of?Some errors reflect fairly arbitrary annotation conventions in treebanks, e.g. should the auxiliary verb govern the main verb or vice versa.(Examples like this suggest that the projection problem contains the adaptation problem above.)Other errors arise from divergences in the complements required of certain head words.In the German-English translation pair, with co-indexed words aligned, [an [den Libanon1]] denken2 H remember2 Lebanon1 we would prefer that the preposition an attach to denken, even though the preposition’s object Libanon aligns to a direct child of remember.In other words, we would like the grandparentparent-child chain of denken → an → Libanon to align to the parent-child pair of remember → Lebanon.Finally, naturally occurring bitexts contain some number of free or erroneous translations.Machine translation researchers often seek to strike these examples from their training corpora; “free” translations are not usually welcome from an MT system.First, we consider the problem of parser projection when there are zero target-language trees available.As in much other work on unsupervised parsing, we try to learn a generative model that can predict target-language sentences.Our novel contribution is to condition the probabilities of the generative actions on the dependency parse of a source-language translation.Thus, our generative model is a quasi-synchronous grammar, exactly as in (Smith and Eisner, 2006a).3 When training on target sentences w, therefore, we tune the model parameters to maximize not Et p(t, w) as in ordinary EM, but rather Et p(t, w, a  |t', w').We hope that this conditional EM training will drive the model to posit appropriate syntactic relationships in the latent variable t, because—thanks to the structure of the QG model—that is the easiest way for it to exploit the extra information in t', w' to help predict w.4 At test time, t', w' are not made available, so we just use the trained model to find argmaxt p(t  |w), backing off from the conditioning on t', w' and summing over a.Below, we present the specific generative model (§5.1) and some details of training (§5.2).We will then compare three approaches (§5.3): §5.3.2 a straight EM baseline (which does not condition on t', w' at all) §5.3.3 a “hard” projection baseline (which naively projects t', w' to derive direct supervision in the target language) §5.3.4 our conditional EM approach above (which makes t', w' available to the learner for “soft” indirect supervision via QG) Our base models of target-language syntax are generative dependency models that have achieved state-of-the art results in unsupervised dependency structure induction.The simplest version, called Dependency Model with Valence (DMV), has been used in isolation and in combination with other models (Klein and Manning, 2004; Smith and Eisner, 2006b).The DMV generates the right children, and then independently the left children, for each node in the dependency tree.Nodes correspond to words, which are represented by their part-of-speech tags.At each step of generation, the DMV stochastically chooses whether to stop generating, conditioned on the currently generating head; whether it is generating to the right or left; and whether it has yet generated any children on that side.If it chooses to continue, it then 4The contrastive estimation of Smith and Eisner (2005) also used a form of conditional EM, with similar motivation.They suggested that EM grammar induction, which learns to predict w, unfortunately learns mostly to predict lexical topic or other properties of the training sentences that do not strongly require syntactic latent variables.To focus EM on modeling the syntactic relationships, they conditioned the prediction of w on almost complete knowledge of the lexical items.Similarly, we condition on a source translation of w. Furthermore, our QG model structure makes it easy for EM to learn to exploit the (explicitly represented) syntactic properties of that translation when predicting w. stochastically generates the tag of a new child, conditioned on the head.The parameters of the model are thus of the form where head and child are part-of-speech tags, dir E {left, right}, and adj, stop E {true, false}.ROOT is stipulated to generate a single right child.Bilingual configurations that condition on t', w' (§2) are incorporated into the generative process as in Smith and Eisner (2006a).When the model is generating a new child for word x, aligned to x', it first chooses a configuration and then chooses a source word y' in that configuration.The child y is then generated, conditioned on its parent x, most recent sibling a, and its source analogue y'.As in previous work on grammar induction, we learn the DMV from part-of-speech-tagged targetlanguage text.We use expectation maximization (EM) to maximize the likelihood of the data.Since the likelihood function is nonconvex in the unsupervised case, our choice of initial parameters can have a significant effect on the outcome.Although we could also try many random starting points, the initializer in Klein and Manning (2004) performs quite well.The base dependency parser generates the right dependents of a head separately from the left dependents, which allows O(n3) dynamic programming for an n-word target sentence.Since the QG annotates nonterminals of the grammar with single nodes of t', and we consider two nodes of t' when evaluating the above dependency configurations, QG parsing runs in O(n3m2) for an m-word source sentence.If, however, we restrict candidate senses for a target child c to come from links in an IBM Model 4 Viterbi alignment, we achieve O(n3k2), where k is the maximum number of possible words aligned to a given target language word.In practice, k « m, and parsing is not appreciably slower than in the monolingual setting.If all configurations were equiprobable, the source sentence would provide no information to the target.In our QG experiments, therefore, we started with a bias towards direct parent–child links and a very small probability for breakages of locality.The values of other configuration parameters seem, experimentally, less important for insuring accurate learning.Our experiments compare learning on target language text to learning on parallel text.In the latter case, we compare learning from high-precision one-to-one alignments alone, to learning from all alignments using a QG.Our development and test data were drawn from the German TIGER and Spanish Cast3LB treebanks as converted to projective dependencies for the CoNLL 2007 Shared Task (Brants et al., 2002; Civit Torruella and MartiAntonin, 2002).5 Our training data were subsets of the 2006 Statistical Machine Translation Workshop Shared Task, in particular from the German-English and Spanish-English Europarl parallel corpora (Koehn, 2002).The Shared Task provided prebuilt automatic GIZA++ word alignments, which we used to facilitate replicability.Since these word alignments do not contain posterior probabilities or null links, nor do they distinguish which links are in the IBM Model intersection, we treated all links as equally likely when learning the QG.Target language words unaligned to any source language words were the only nodes allowed to align to NULL in QG derivations.We parsed the English side of the bitext with the projective dependency parser described by McDonald et al. (2005) trained on the Penn Treebank §§2–20.Much previous work on unsupervised grammar induction has used gold-standard partof-speech tags (Smith and Eisner, 2006b; Klein and Manning, 2004; Klein and Manning, 2002).While there are no gold-standard tags for the Europarl bitext, we did train a conditional Markov 5We made one change to the annotation conventions in German: in the dependencies provided, words in a noun phrase governed by a preposition were all attached to that preposition.This meant that in the phrase das Kind (“the child”) in, say, subject position, das was the child of Kind; but, in f¨ur das Kind (“for the child”), das was the child of f¨ur.This seems to be a strange choice in converting from the TIGER constituency format, which does in fact annotate NPs inside PPs; we have standardized prepositions to govern only the head of the noun phrase.We did not change any other annotation conventions to make them more like English.In the Spanish treebank, for instance, control verbs are the children of their verbal complements: in quiero decir (“I want to say”=“I mean”), quiero is the child of decir.In German coordinations, the coordinands all attach to the first, but in English, they all attach to the last.These particular divergences in annotation style hurt all of our models equally (since none of them have access to labeled trees).These annotation divergences are one motivation for experiments below that include some target trees. model tagger on a few thousand tagged sentences.This is the only supervised data we used in the target.We created versions of each training corpus with the first thousand, ten thousand, and hundred thousand sentence pairs, each a prefix of the next.Since the target-language-only baseline converged much more slowly, we used a version of the corpora with sentences 15 target words or fewer.Using the target side of the bitext as training data, we initialized our model parameters as described in §5.2 and ran EM.We checked convergence on a development set and measured unlabeled dependency accuracy on held-out test data.We compare performance to simple attach-right and attach left baselines (Table 3).For mostly headfinal German, the “modify next” baseline is better; for mostly head-initial Spanish, “modify previous” wins.Even after several hundred iterations, performance was slightly, but not significantly better than the baseline for German.EM training did not beat the baseline for Spanish.6 The simplest approach to using the high-precision one-to-one word alignments is labeled “hard projection” in the table.We filtered the training corpus to find sentences where enough links were projected to completely determine a target language tree.Of course, we needed to filter more than 1000 sentences of bitext to output 1000 training sentences in this way.We simply perform supervised training with this subset, which is still quite noisy (§4), and performance quickly 6While these results are worse than those obtained previously for this model, the experiments in Klein and Manning (2004) and only used sentences of 10 words or fewer, without punctuation, and with gold-standard tags.Punctuation in particular seems to trip up the initializer: since a sentence-final periods appear in most sentences, EM often decides to make it the head. plateaus.Still, this method substantially improves over the baselines and unsupervised EM.Restricting ourselves to fully projected trees seems a waste of information.We can also simply take all one-to-one projected links, impute expected counts for the remaining dependencies with EM, and update our models.This approach (“hard projection with EM”), however, performed worse than using only the fully projected trees.In fact, only the first iteration of EM with this method made any improvement; afterwards, EM degraded accuracy further from the numbers in Table 3.The quasi-synchronous model used all of the alignments in re-estimating its parameters and performed significantly better than hard projection.Unlike EM on the target language alone, the QG’s performance does not depend on a clever initializer for initial model weights—all parameters of the generative model except for the QG configuration features were initialized to zero.Setting the prior to prefer direct correspondence provides the necessary bias to initialize learning.Error analysis showed that certain types of dependencies eluded the QG’s ability to learn from bitext.The Spanish treebank treats some verbal complements as the heads of main verbs and auxiliary verbs as the children of participles; the QG, following the English, learned the opposite dependency direction.Spanish treebank conventions for punctuation were also a common source of errors.In both German and Spanish, coordinations (a common bugbear for dependency grammars) were often mishandled: both treebanks attach the later coordinands and any conjunctions to the first coordinand; the reverse is true in English.Finally, in both German and Spanish, preposition attachments often led to errors, which is not surprising given the unlexicalized target-language grammars.Rather than trying to adjudicate which dependencies are “mere” annotation conventions, it would be useful to test learned dependency models on some extrinsic task such as relation extraction or machine translation.Finally, we consider the problem of parser projection when some target language trees are available.As in the adaptation case (§3), we train a conditional model (not a generative DMV) of the target tree given the target sentence, using the monolingual and bilingual QG features, including configurations conjoined with tags, outlined above (§2).For these experiments, we used the LDC’s English-Chinese Parallel Treebank (ECTB).Since manual word alignments also exist for a part of this corpus, we were able to measure the loss in accuracy (if any) from the use of an automatic English parser and word aligner.The sourcelanguage English dependency parser was trained on the Wall Street Journal, where it achieved 91% dependency accuracy on development data.However, it was only 80.3% accurate when applied to our task, the English side of the ECTB.7 After parsing the source side of the bitext, we train a parser on the annotated target side, using QG features described above (§2).Both the monolingual target-language parser and the projected parsers are trained to optimize conditional likelihood of the target trees t' with ten iterations of stochastic gradient ascent.In Figure 3, we plot the performance of the target-language parser on held-out bitext.Although projection performance is, not surprisingly, better if we know the true source trees at training and test time, even with the 1-best output of the source parser, QG features help produce a parser as accurate asq one trained on twice the amount of monolingual data.In ablation experiments, we included bilingual features only for directly projected links, with no features for head-swapping, grandparents, etc.When using 1-best English parses, parsers trained only with direct-projection and monolingual features performed worse; when using gold English parses, parsers with directprojection-only features performed better when trained with more Chinese trees.The two related problems of parser adaptation and projection are often approached in different ways.Many adaptation methods operate by simple augmentations of the target feature space, as we have done here (Daume III, 2007).Parser projection, on the other hand, often uses a multi-stage pipeline 7It would be useful to explore whether the techniques of §3 above could be used to improve English accuracy by domain adaptation.In theory a model with QG features trained to perform well on Chinese should not suffer from an inaccurate, but consistent, English parser, but the results in Figure 3 indicate a significant benefit to be had from better English parsing or from joint Chinese-English inference. having twice as much data in the target language.Note that the penalty for using automatic alignments instead of gold alignments is negligible; in fact, using Source text alone is often higher than +Gold alignments.Using gold source trees, however, significantly outperforms using 1-best source trees.(Hwa et al., 2005).The methods presented here move parser projection much closer in efficiency and simplicity to monolingual parsing.We showed that augmenting a target parser with quasi-synchronous features can lead to significant improvements—first in experiments with adapting to different dependency representations in English, and then in cross-language parser projection.As with many domain adaptation problems, it is quite helpful to have some annotated target data, especially when annotation styles vary (Dredze et al., 2007).Our experiments show that unsupervised QG projection improves on parsers trained using only high-precision projected annotations and far outperforms, by more than 35% absolute dependency accuracy, unsupervised EM.When a small number of target-language parse trees is available, projection gives a boost equivalent to doubling the number of target trees.The loss in performance from conditioning only on noisy 1-best source parses points to some natural avenues for improvement.We are exploring methods that incorporate a packed parse forest on the source side and similar representations of uncertainty about alignments.Building on our recent belief propagation work (Smith and Eisner, 2008), we can jointly infer two dependency trees and their alignment, under a joint distribution p(t, a, t'  |w, w') that evaluates the full graph of dependency and alignment edges.
Simple Coreference Resolution with Rich Syntactic and Semantic FeaturesCoreference systems are driven by syntactic, semantic, and discourse constraints.We present a simple approach which completely modularizes these three aspects.In contrast to much current work, which focuses on learning and on the discourse component, our system is deterministic and is driven entirely by syntactic and semantic compatibility as learned from a large, unlabeled corpus.Despite its simplicity and discourse naivete, our system substantially outperforms all unsupervised systems and most supervised ones.Primary contributions include (1) the presentation of a simpleto-reproduce, high-performing baseline and (2) the demonstration that most remaining errors can be attributed to syntactic and semantic factors external to the coreference phenomenon (and perhaps best addressed by non-coreference systems).The resolution of entity reference is influenced by a variety of constraints.Syntactic constraints like the binding theory, the i-within-i filter, and appositive constructions restrict reference by configuration.Semantic constraints like selectional compatibility (e.g. a spokesperson can announce things) and subsumption (e.g.Microsoft is a company) rule out many possible referents.Finally, discourse phenomena such as salience and centering theory are assumed to heavily influence reference preferences.As these varied factors have given rise to a multitude of weak features, recent work has focused on how best to learn to combine them using models over reference structures (Culotta et al., 2007; Denis and Baldridge, 2007; Klenner and Ailloud, 2007).In this work, we break from the standard view.Instead, we consider a vastly more modular system in which coreference is predicted from a deterministic function of a few rich features.In particular, we assume a three-step process.First, a selfcontained syntactic module carefully represents syntactic structures using an augmented parser and extracts syntactic paths from mentions to potential antecedents.Some of these paths can be ruled in or out by deterministic but conservative syntactic constraints.Importantly, the bulk of the work in the syntactic module is in making sure the parses are correctly constructed and used, and this module’s most important training data is a treebank.Second, a self-contained semantic module evaluates the semantic compatibility of headwords and individual names.These decisions are made from compatibility lists extracted from unlabeled data sources such as newswire and web data.Finally, of the antecedents which remain after rich syntactic and semantic filtering, reference is chosen to minimize tree distance.This procedure is trivial where most systems are rich, and so does not need any supervised coreference data.However, it is rich in important ways which we argue are marginalized in recent coreference work.Interestingly, error analysis from our final system shows that its failures are far more often due to syntactic failures (e.g. parsing mistakes) and semantic failures (e.g. missing knowledge) than failure to model discourse phenomena or appropriately weigh conflicting evidence.One contribution of this paper is the exploration of strong modularity, including the result that our system beats all unsupervised systems and approaches the state of the art in supervised ones.Another contribution is the error analysis result that, even with substantial syntactic and semantic richness, the path to greatest improvement appears to be to further improve the syntactic and semantic modules.Finally, we offer our approach as a very strong, yet easy to implement, baseline.We make no claim that learning to reconcile disparate features in a joint model offers no benefit, only that it must not be pursued to the exclusion of rich, nonreference analysis.In coreference resolution, we are given a document which consists of a set of mentions; each mention is a phrase in the document (typically an NP) and we are asked to cluster mentions according to the underlying referent entity.There are three basic mention types: proper (Barack Obama), nominal (president), and pronominal (he).1 For comparison to previous work, we evaluate in the setting where mention boundaries are given at test time; however our system can easily annotate reference on all noun phrase nodes in a parse tree (see Section 3.1.1).In this work we use the following data sets: Development: (see Section 3) We will present evaluations on multiple coreference resolution metrics, as no single one is clearly superior:In this section we develop our system and report developmental results on ACE2004-ROTHDEV (see Section 2.1); we report pairwise F1 figures here, but report on many more evaluation metrics in Section 4.At a high level, our system resembles a pairwise coreference model (Soon et al., 1999; Ng and Cardie, 2002; Bengston and Roth, 2008); for each mention mi, we select either a single-best antecedent amongst the previous mentions m1, ... , mi−1, or the NULL mention to indicate the underlying entity has not yet been evoked.Mentions are linearly ordered according to the position of the mention head with ties being broken by the larger node coming first.While much research (Ng and Cardie, 2002; Culotta et al., 2007; Haghighi and Klein, 2007; Poon and Domingos, 2008; Finkel and Manning, 2008) has explored how to reconcile pairwise decisions to form coherent clusters, we simply take the transitive closure of our pairwise decision (as in Ng and Cardie (2002) and Bengston and Roth (2008)) which can and does cause system errors.In contrast to most recent research, our pairwise decisions are not made with a learned model which outputs a probability or confidence, but instead for each mention mi, we select an antecedent amongst m1, ... , mi_1 or the NULL mention as follows: Initially, there is no syntactic constraint (improved in Section 3.1.3), the antecedent compatibility filter allows proper and nominal mentions to corefer only with mentions that have the same head (improved in Section 3.2), and pronouns have no compatibility constraints (improved in Section 3.1.2).Mention heads are determined by parsing the given mention span with the Stanford parser (Klein and Manning, 2003) and using the Collins head rules (Collins, 1999); Poon and Domingos (2008) showed that using syntactic heads strongly outperformed a simple rightmost headword rule.The mention type is determined by the head POS tag: proper if the head tag is NNP or NNPS, pronoun if the head tag is PRP, PRP$, WP, or WP$, and nominal otherwise.For the selection phase, we order mentions m1, ... , mi_1 according to the position of the head word and select the closest mention that remains after constraint and filtering are applied.This choice reflects the intuition of Grosz et al. (1995) that speakers only use pronominal mentions when there are not intervening compatible mentions.This system yields a rather low 48.9 pairwise F1 (see BASE-FLAT in Table 2).There are many, primarily recall, errors made choosing antecedents for all mention types which we will address by adding syntactic and semantic constraints.In this section, we enrich the syntactic representation and information in our system to improve results.We first focus on fixing the pronoun antecedent choices.A common error arose from the use of mention head distance as a poor proxy for discourse salience.For instance consider the example in Figure 1, the mention America is closest to its in flat mention distance, but syntactically Nintendo ofAmerica holds a more prominent syntactic position relative to the pronoun which, as Hobbs (1977) argues, is key to discourse salience.Mapping Mentions to Parse Nodes: In order to use the syntactic position of mentions to determine anaphoricity, we must associate each mention in the document with a parse tree node.We parse all document sentences with the Stanford parser, and then for each evaluation mention, we find the largest-span NP which has the previously determined mention head as its head.5 Often, this results in a different, typically larger, mention span than annotated in the data.Now that each mention is situated in a parse tree, we utilize the length of the shortest tree path between mentions as our notion of distance.In by agreement constraints (see Section 3.1.2).The pronoun them is closest to the site mention, but has an incompatible number feature with it.The closest (in tree distance, see Section 3.1.1) compatible mention is The Israelis, which is correct particular, this fixes examples such as those in Figure 1 where the true antecedent has many embedded mentions between itself and the pronoun.This change by itself yields 51.7 pairwise F1 (see BASE-TREE in Table 2), which is small overall, but reduces pairwise pronoun antecedent selection error from 51.3% to 42.5%.We now refine our compatibility filtering to incorporate simple agreement constraints between coreferent mentions.Since we currently allow proper and nominal mentions to corefer only with matching head mentions, agreement is only a concern for pronouns.Traditional linguistic theory stipulates that coreferent mentions must agree in number, person, gender, and entity type (e.g. animacy).Here, we implement person, number and entity type agreement.6 A number feature is assigned to each mention deterministically based on the head and its POS tag.For entity type, we use NER labels.Ideally, we would like to have information about the entity type of each referential NP, however this information is not easily obtainable.Instead, we opt to utilize the Stanford NER tagger (Finkel et al., 2005) over the sentences in a document and annotate each NP with the NER label assigned to that mention head.For each mention, when its NP is assigned an NER label we allow it to only be compatible with that NER label.7 For pronouns, we deterministically assign a set of compatible NER values (e.g. personal pronouns can only be a PERpositive and i-within-i constraint.The i-withini constraint disallows coreference between parent and child NPs unless the child is an appositive.Hashed numbers indicate ground truth but are not in the actual trees.SON, but its can be an ORGANIZATION or LOCATION).Since the NER tagger typically does not label non-proper NP heads, we have no NER compatibility information for nominals.We incorporate agreement constraints by filtering the set of possible antecedents to those which have compatible number and NER types with the target mention.This yields 53.4 pairwise F1, and reduces pronoun antecedent errors to 42.5% from 34.4%.An example of the type of error fixed by these agreement constraints is given by Figure 2.Our system has so far focused only on improving pronoun anaphora resolution.However, a plurality of the errors made by our system are amongst nonpronominal mentions.8 We take the approach that in order to align a non-pronominal mention to an antecedent without an identical head, we require evidence that the mentions are compatible.Judging compatibility of mentions generally requires semantic knowledge, to which we return later.However, some syntactic configurations guarantee coreference.The one exploited most in coreference work (Soon et al., 1999; Ng and Cardie, 2002; Luo et al., 2004; Culotta et al., 2007; Poon and Domingos, 2008; Bengston and Roth, 2008) is the appositive construction.Here, we represent apposition as a syntactic feature of an NP indicating that it is coreferent with its parent NP (e.g. it is an exception to the i-within-i constraint that parent and child NPs cannot be coreferent).We deterministically mark a node as NP-APPOS (see Figure 3) when it is the third child in of a parent NP whose expansion begins with (NP , NP), and there is not a conjunction in the expansion (to avoid marking elements in a list as appositive).Role Appositives: During development, we discovered many errors which involved a variant of appositives which we call ‘role appositives’ (see painter in Figure 3), where an NP modifying the head NP describes the role of that entity (typically a person entity).There are several challenges to correctly labeling these role NPs as being appositives.First, the NPs produced by Treebank parsers are flat and do not have the required internal structure (see Figure 3(a)).While fully solving this problem is difficult, we can heuristically fix many instances of the problem by placing an NP around maximum length sequences of NNP tags or NN (and JJ) tags within an NP; note that this will fail for many constructions such as U.S. President Barack Obama, which is analyzed as a flat sequence of proper nouns.Once this internal NP structure has been added, whether the NP immediately to the left of the head NP is an appositive depends on the entity type.For instance, Rabbi Ashi is an apposition but Iranian army is not.Again, a full solution would require its own model, here we mark as appositions any NPs immediately to the left of a head child NP where the head child NP is identified as a person by the NER tagger.9 We incorporate NP appositive annotation as a constraint during filtering.Any mention which corresponds to an appositive node has its set of possible antecedents limited to its parent.Along with the appositive constraint, we implement the i-within-i constraint that any non-appositive NP cannot be be coreferent with its parent; this constraint is then propagated to any node its parent is forced to agree with.The order in which these constraints are applied is important, as illustrated by the example in Figure 4: First the list of possible antecedents for the appositive NP is constrained to only its parent.Now that all appositives have been constrained, we apply the i-withini constraint, which prevents its from having the NP headed by brand in the set of possible antecedents, and by propagation, also removes the NP headed by Gitano.This leaves the NP Wal-Mart as the closest compatible mention.Adding these syntactic constraints to our system yields 55.4 F1, a fairly substantial improvement, but many recall errors remain between mentions with differing heads.Resolving such cases will require external semantic information, which we will automatically acquire (see Section 3.2).Predicate Nominatives: Another syntactic constraint exploited in Poon and Domingos (2008) is the predicate nominative construction, where the object of a copular verb (forms of the verb be) is constrained to corefer with its subject (e.g.Microsoft is a company in Redmond).While much less frequent than appositive configurations (there are only 17 predicate nominatives in our devel9Arguably, we could also consider right modifying NPs (e.g., [Microsoft [Company]1]1) to be role appositive, but we do not do so here. opment set), predicate nominatives are another highly reliable coreference pattern which we will leverage in Section 3.2 to mine semantic knowledge.As with appositives, we annotate object predicate-nominative NPs and constrain coreference as before.This yields a minor improvement to 55.5 F1.While appositives and related syntactic constructions can resolve some cases of non-pronominal reference, most cases require semantic knowledge about the various entities as well as the verbs used in conjunction with those entities to disambiguate references (Kehler et al., 2008).However, given a semantically compatible mention head pair, say AOL and company, one might expect to observe a reliable appositive or predicative-nominative construction involving these mentions somewhere in a large corpus.In fact, the Wikipedia page for AOL10 has a predicate-nominative construction which supports the compatibility of this head pair: AOL LLC (formerly America Online) is an American global Internet services and media company operated by Time Warner.In order to harvest compatible head pairs, we utilize our BLIPP and WIKI data sets (see Section 2), and for each noun (proper or common) and pronoun, we assign a maximal NP mention node for each nominal head as in Section 3.1.1; we then annotate appositive and predicate-nominative NPs as in Section 3.1.3.For any NP which is annotated as an appositive or predicate-nominative, we extract the head pair of that node and its constrained antecedent.The resulting set of compatible head words, while large, covers a little more than half of the examples given in Table 1.The problem is that these highly-reliable syntactic configurations are too sparse and cannot capture all the entity information present.For instance, the first sentence of Wikipedia abstract for Al Gore is: Albert Arnold “Al” Gore, Jr. is an American environmental activist who served as the 45th Vice President of the United States from 1993 to 2001 under President Bill Clinton.The required lexical pattern X who served as Y is a general appositive-like pattern that almost surely indicates coreference.Rather than opt to manually create a set of these coreference patterns as in Hearst (1992), we instead opt to automatically extract these patterns from large corpora as in Snow et al. (2004) and Phillips and Riloff (2007).We take a simple bootstrapping technique: given a set of mention pairs extracted from appositives and predicate-nominative configurations, we extract counts over tree fragments between nodes which have occurred in this set of head pairs (see Figure 5); the tree fragments are formed by annotating the internal nodes in the tree path with the head word and POS along with the subcategorization.We limit the paths extracted in this way in several ways: paths are only allowed to go between adjacent sentences and have a length of at most 10.We then filter the set of paths to those which occur more than a hundred times and with at least 10 distinct seed head word pairs.The vast majority of the extracted fragments are variants of traditional appositives and predicatenominatives with some of the structure of the NPs specified.However there are some tree fragments which correspond to the novel coreference patterns (see Figure 5) of parenthetical alias as well as conjunctions of roles in NPs.We apply our extracted tree fragments to our BLIPP and WIKI data sets and extract a set of compatible word pairs which match these fragments; these words pairs will be used to relax the semantic compatibility filter (see the start of the section); mentions are compatible with prior mentions with the same head or with a semantically compatible head word.This yields 58.5 pairwise F1 (see SEMCOMPAT in Table 2) as well as similar improvements across other metrics.By and large the word pairs extracted in this way are correct (in particular we now have coverage for over two-thirds of the head pair recall errors from Table 1.)There are however wordpairs which introduce errors.In particular citystate constructions (e.g.Los Angeles, California) appears to be an appositive and incorrectly allows our system to have angeles as an antecedent for california.Another common error is that the % symbol is made compatible with a wide variety of common nouns in the financial domain.We present formal experimental results here (see Table 2).We first evaluate our model on the ACE2004-CULOTTA-TEST dataset used in the state-of-the-art systems from Culotta et al. (2007) and Bengston and Roth (2008).Both of these systems were supervised systems discriminatively trained to maximize b3 and used features from many different structured resources including WordNet, as well as domain-specific features (Culotta et al., 2007).Our best b3 result of 79.0 is broadly in the range of these results.We should note that in our work we use neither the gold mention types (we do not model pre-nominals separately) nor do we use the gold NER tags which Bengston and Roth (2008) does.Across metrics, the syntactic constraints and semantic compatibility components contribute most to the overall final result.On the MUC6-TEST dataset, our system outpersion made by the system.Each row is a mention type and the column the predicted mention type antecedent.The majority of errors are made in the NOMINAL category. forms both Poon and Domingos (2008) (an unsupervised Markov Logic Network system which uses explicit constraints) and Finkel and Manning (2008) (a supervised system which uses ILP inference to reconcile the predictions of a pairwise classifier) on all comparable measures.11 Similarly, on the ACE2004-NWIRE dataset, we also outperform the state-of-the-art unsupervised system of Poon and Domingos (2008).Overall, we conclude that our system outperforms state-of-the-art unsupervised systems12 and is in the range of the state-of-the art systems of Culotta et al. (2007) and Bengston and Roth (2008).There are several general trends to the errors made by our system.Table 3 shows the number of pairwise errors made on MUC6-TEST dataset by mention type; note these errors are not equally weighted in the final evaluations because of the transitive closure taken at the end.The most errors are made on nominal mentions with pronouns coming in a distant second.In particular, we most frequently say a nominal is NULL when it has an antecedent; this is typically due to not having the necessary semantic knowledge to link a nominal to a prior expression.In order to get a more thorough view of the cause of pairwise errors, we examined 20 random errors made in aligning each mention type to an antecedent.We categorized the errors as follows: we incorrectly aligned a pronoun to a mention with which it is not semantically compatible (e.g. he aligned to board). mentions with the same head are always compatible.Includes modifier and specificity errors such as allowing Lebanon and Southern Lebanon to corefer.This also includes errors of definiteness in nominals (e.g. the people in the room and Chinese people).Typically, these errors involve a combination of missing syntactic and semantic information.The result of this error analysis is given in Table 4; note that a single error may be attributed to more than one cause.Despite our efforts in Section 3 to add syntactic and semantic information to our system, the largest source of error is still a combination of missing semantic information or annotated syntactic structure rather than the lack of discourse or salience modeling.Our error analysis suggests that in order to improve the state-of-the-art in coreference resolution, future research should consider richer syntactic and semantic information than typically used in current systems.Our approach is not intended as an argument against the more complex, discourse-focused approaches that typify recent work.Instead, we note that rich syntactic and semantic processing vastly reduces the need to rely on discourse effects or evidence reconciliation for reference resolution.Indeed, we suspect that further improving the syntactic and semantic modules in our system may produce greater error reductions than any other route forward.Of course, a system which is rich in all axes will find some advantage over any simplified approach.Nonetheless, our coreference system, despite being relatively simple and having no tunable parameters or complexity beyond the non-reference complexity of its component modules, manages to outperform state-of-the-art unsupervised coreference resolution and be broadly comparable to state-of-the-art supervised systems.
Inducing Probabilistic CCG Grammars from Logical Form with Higher-Order UnificationThis paper addresses the problem of learning to map sentences to logical form, given training data consisting of natural language sentences paired with logical representations of their meaning.Previous approaches have been designed for particular natural languages or specific meaning representations; here we present a more general method.The approach induces a probabilistic CCG grammar that represents the meaning of individual words and defines how these meanings can be combined to analyze complete sentences.We use higher-order unification to define a hypothesis space containing all grammars consistent with the training data, and develop an online learning algorithm that efficiently searches this space while simultaneously estimating the parameters of a log-linear parsing model.Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations.A key aim in natural language processing is to learn a mapping from natural language sentences to formal representations of their meaning.Recent work has addressed this problem by learning semantic parsers given sentences paired with logical meaning representations (Thompson & Mooney, 2002; Kate et al., 2005; Kate & Mooney, 2006; Wong & Mooney, 2006, 2007; Zettlemoyer & Collins, 2005, 2007; Lu et al., 2008).For example, the training data might consist of English sentences paired with lambda-calculus meaning representations: Given pairs like this, the goal is to learn to map new, unseen, sentences to their corresponding meaning.Previous approaches to this problem have been tailored to specific natural languages, specific meaning representations, or both.Here, we develop an approach that can learn to map any natural language to a wide variety of logical representations of linguistic meaning.In addition to data like the above, this approach can also learn from examples such as: Sentence: hangi eyaletin texas ye siniri vardir Meaning: answer(state(borders(tex))) where the sentence is in Turkish and the meaning representation is a variable-free logical expression of the type that has been used in recent work (Kate et al., 2005; Kate & Mooney, 2006; Wong & Mooney, 2006; Lu et al., 2008).The reason for generalizing to multiple languages is obvious.The need to learn over multiple representations arises from the fact that there is no standard representation for logical form for natural language.Instead, existing representations are ad hoc, tailored to the application of interest.For example, the variable-free representation above was designed for building natural language interfaces to databases.Our approach works by inducing a combinatory categorial grammar (CCG) (Steedman, 1996, 2000).A CCG grammar consists of a language-specific lexicon, whose entries pair individual words and phrases with both syntactic and semantic information, and a universal set of combinatory rules that project that lexicon onto the sentences and meanings of the language via syntactic derivations.The learning process starts by postulating, for each sentence in the training data, a single multi-word lexical item pairing that sentence with its complete logical form.These entries are iteratively refined with a restricted higher-order unification procedure (Huet, 1975) that defines all possible ways to subdivide them, consistent with the requirement that each training sentence can still be parsed to yield its labeled meaning.For the data sets we consider, the space of possible grammars is too large to explicitly enumerate.The induced grammar is also typically highly ambiguous, producing a large number of possible analyses for each sentence.Our approach discriminates between analyses using a log-linear CCG parsing model, similar to those used in previous work (Clark & Curran, 2003, 2007), but differing in that the syntactic parses are treated as a hidden variable during training, following the approach of Zettlemoyer & Collins (2005, 2007).We present an algorithm that incrementally learns the parameters of this model while simultaneously exploring the space of possible grammars.The model is used to guide the process of grammar refinement during training as well as providing a metric for selecting the best analysis for each new sentence.We evaluate the approach on benchmark datasets from a natural language interface to a database of US Geography (Zelle & Mooney, 1996).We show that accurate models can be learned for multiple languages with both the variable-free and lambdacalculus meaning representations introduced above.We also compare performance to previous methods (Kate & Mooney, 2006; Wong & Mooney, 2006, 2007; Zettlemoyer & Collins, 2005, 2007; Lu et al., 2008), which are designed with either language- or representation- specific constraints that limit generalization, as discussed in more detail in Section 6.Despite being the only approach that is general enough to run on all of the data sets, our algorithm achieves similar performance to the others, even outperforming them in several cases.The goal of our algorithm is to find a function f : x —* z that maps sentences x to logical expressions z.We learn this function by inducing a probabilistic CCG (PCCG) grammar from a training set {(xZ, zz)|i = 1... n} containing example (sentence, logical-form) pairs such as (“New York borders Vermont”, next to(ny, vt)).The induced grammar consists of two components which the algorithm must learn: tion over the possible parses y, conditioned on the sentence x.We will present the approach in two parts.The lexical induction process (Section 4) uses a restricted form of higher order unification along with the CCG combinatory rules to propose new entries for A.The complete learning algorithm (Section 5) integrates this lexical induction with a parameter estimation scheme that learns 0.Before presenting the details, we first review necessary background.This section provides an introduction to the ways in which we will use lambda calculus and higher-order unification to construct meaning representations.It also reviews the CCG grammar formalism and probabilistic extensions to it, including existing parsing and parameter estimation techniques.We assume that sentence meanings are represented as logical expressions, which we will construct from the meaning of individual words by using the operations defined in the lambda calculus.We use a version of the typed lambda calculus (cf.Carpenter (1997)), in which the basic types include e, for entities; t, for truth values; and i for numbers.There are also function types of the form (e, t) that are assigned to lambda expressions, such as Ax.state(x), which take entities and return truth values.We represent the meaning of words and phrases using lambda-calculus expressions that can contain constants, quantifiers, logical connectors, and lambda abstractions.The advantage of using the lambda calculus lies in its generality.The meanings of individual words and phrases can be arbitrary lambda expressions, while the final meaning for a sentence can take different forms.It can be a full lambdacalculus expression, a variable-free expression such as answer(state(borders(tex))), or any other logical expression that can be built from the primitive meanings via function application and composition.The higher-order unification problem (Huet, 1975) involves finding a substitution for the free variables in a pair of lambda-calculus expressions that, when applied, makes the expressions equal each other.This problem is notoriously complex; in the unrestricted form (Huet, 1973), it is undecidable.In this paper, we will guide the grammar induction process using a restricted version of higherorder unification that is tractable.For a given expression h, we will need to find expressions for f and g such that either h = f(g) or h = Ax.f(g(x)).This limited form of the unification problem will allow us to define the ways to split h into subparts that can be recombined with CCG parsing operations, which we will define in the next section, to reconstruct h. CCG (Steedman, 2000) is a linguistic formalism that tightly couples syntax and semantics, and can be used to model a wide range of language phenomena.For present purposes a CCG grammar includes a lexicon A with entries like the following: where each lexical item w �- X : h has words w, a syntactic category X, and a logical form h expressed as a lambda-calculus expression.For the first example, these are “New York,” NP, and ny.CCG syntactic categories may be atomic (such as S, NP) or complex (such as S\NP/NP).CCG combines categories using a set of combinatory rules.For example, the forward (>) and These rules apply to build syntactic and semantic derivations under the control of the word order information encoded in the slash directions of the lexical entries.For example, given the lexicon above, the sentence New York borders Vermont can be parsed to produce: where each step in the parse is labeled with the combinatory rule (− > or − <) that was used.CCG also includes combinatory rules of forward (> B) and backward (< B) composition: These rules provide for a relaxed notion of constituency which will be useful during learning as we reason about possible refinements of the grammar.We also allow vertical slashes in CCG categories, which act as wild cards.For example, with this extension the forward application combinator (>) could be used to combine the category S/(S�NP) with any of S\NP, S/NP, or SNP.Figure 1 shows two parses where the composition combinators and vertical slashes are used.These parses closely resemble the types of analyses that will be possible under the grammars we learn in the experiments described in Section 8.Given a CCG lexicon A, there will, in general, be many possible parses for each sentence.We select the most likely alternative using a log-linear model, which consists of a feature vector 0 and a parameter vector 0.The joint probability of a logical form z constructed with a parse y, given a sentence x is Section 7 defines the features used in the experiments, which include, for example, lexical features that indicate when specific lexical items in A are used in the parse y.For parsing and parameter estimation, we use standard algorithms (Clark & Curran, 2007), as described below.The parsing, or inference, problem is to find the most likely logical form z given a sentence x, assuming the parameters 0 and lexicon A are known: where the probability of the logical form is found by summing over all parses that produce it: In this approach the distribution over parse trees y is modeled as a hidden variable.The sum over parses in Eq.3 can be calculated efficiently using the inside-outside algorithm with a CKY-style parsing algorithm.To estimate the parameters themselves, we use stochastic gradient updates (LeCun et al., 1998).Given a set of n sentence-meaning pairs {(xi, zi) : i = 1...n}, we update the parameters 0 iteratively, for each example i, by following the local gradient of the conditional log-likelihood objective Oi = log P(zi|xi; 0, A).The local gradient of the individual parameter 0j associated with feature Oj and training instance (xi, zi) is given by: As with Eq.3, all of the expectations in Eq.4 are calculated through the use of the inside-outside algorithm on a pruned parse chart.In the experiments, each chart cell was pruned to the top 200 entries.Before presenting a complete learning algorithm, we first describe how to use higher-order unification to define a procedure for splitting CCG lexical entries.This splitting process is used to expand the lexicon during learning.We seed the lexical induction with a multi-word lexical item xi`S:zi for each training example (xi, zi), consisting of the entire sentence xi and its associated meaning representation zi.For example, one initial lexical item might be: Although these initial, sentential lexical items can parse the training data, they will not generalize well to unseen data.To learn effectively, we will need to split overly specific entries of this type into pairs of new, smaller, entries that generalize better.For example, one possible split of the lexical entry given in (5) would be the pair: New York borders ` S/NP : Ax.next to(ny, x), Vermont ` NP : vt where we broke the original logical expression into two new ones Ax.next to(ny, x) and vt, and paired them with syntactic categories that allow the new lexical entries to be recombined to produce the original analysis.The next three subsections define the set of possible splits for any given lexical item.The process is driven by solving a higher-order unification problem that defines all of the ways of splitting the logical expression into two parts, as described in Section 4.1.Section 4.2 describes how to construct syntactic categories that are consistent with the two new fragments of logical form and which will allow the new lexical items to recombine.Finally, Section 4.3 defines the full set of lexical entry pairs that can be created by splitting a lexical entry.As we will see, this splitting process is overly prolific for any single language and will yield many lexical items that do not generalize well.For example, there is nothing in our original lexical entry above that provides evidence that the split should pair “Vermont” with the constant vt and not Ax.next to(ny, x).Section 5 describes how we estimate the parameters of a probabilistic parsing model and how this parsing model can be used to guide the selection of items to add to the lexicon.The set of possible splits for a logical expression h is defined as the solution to a pair of higherorder unification problems.We find pairs of logical expressions (f, g) such that either f(g) = h or Ax.f(g(x)) = h. Solving these problems creates new expressions f and g that can be recombined according to the CCG combinators, as defined in Section 3.2, to produce h. In the unrestricted case, there can be infinitely many solution pairs (f, g) for a given expression h. For example, when h = tex and f = Ax.tex, the expression g can be anything.Although it would be simple enough to forbid vacuous variables in f and g, the number of solutions would still be exponential in the size of h. For example, when h contains a conjunction, such as h = Ax.city(x) n major(x) n in(x, tex), any subset of the expressions in the conjunction can be assigned to f (or g).To limit the number of possible splits, we enforce the following restrictions on the possible higherorder solutions that will be used during learning: Together, these three restrictions guarantee that the number of splits is, in the worst case, an Ndegree polynomial of the number of constants in h. The constraints were designed to increase the efficiency of the splitting algorithm without impacting performance on the development data.We define the set of possible splits for a category X:h with syntax X and logical form h by enumerating the solution pairs (f, g) to the higher-order unification problems defined above and creating syntactic categories for the resulting expressions.For example, given X :h = S\NP :Ax.in(x, tex), f = AyAx.in(x, y), and g = tex, we would produce the following two pairs of new categories: which were constructed by first choosing the syntactic category for g, in this case NP, and then enumerating the possible directions for the new slash in the category containing f. We consider each of these two steps in more detail below.The new syntactic category for g is determined based on its type, T(g).For example, T(tex) = e and T(Ax.state(x)) = (e, t).Then, the function QT) takes an input type T and returns the syntactic category of T as follows: The basic types e and t are assigned syntactic categories NP and S, and all functional types are assigned categories recursively.For example Q(e, t)) = S|NP and Q(e, (e, t))) = S|NP|NP.This definition of CCG categories is unconventional in that it never assigns atomic categories to functional types.For example, there is no distinct syntactic category N for nouns (which have semantic type he, ti).Instead, the more complex category S|NP is used.Now, we are ready to define the set of all category splits.For a category A = X:h we can define which is a union of sets, each of which includes splits for a single CCG operator.For example, FA(X:h) is the set of category pairs where each pair can be combined with the forward application combinator, described in Section 3.2, to reconstruct X:h. The remaining three sets are defined similarly, and are associated with the backward application and forward and backward composition operators, respectively: where the composition sets FC and BC only accept input categories with the appropriate outermost slash direction, for example FC(X/Y:h).We can now define the lexical splits that will be used during learning.For lexical entry w0:n ` A, with word sequence w0:n = hw0, ... , wni and CCG category A, define the set SL of splits to be: where we enumerate all ways of splitting the words sequence w0:n and aligning the subsequences with categories in SC(A), as defined in the last section.The previous section described how a splitting procedure can be used to break apart overly specific lexical items into smaller ones that may generalize better to unseen data.The space of possible lexical items supported by this splitting procedure is too large to explicitly enumerate.Instead, we learn the parameters of a PCCG, which is used both to guide the splitting process, and also to select the best parse, given a learned lexicon.Figure 2 presents the unification-based learning algorithm, UBL.This algorithm steps through the data incrementally and performs two steps for each training example.First, new lexical items are induced for the training instance by splitting and merging nodes in the best correct parse, given the current parameters.Next, the parameters of the PCCG are updated by making a stochastic gradient update on the marginal likelihood, given the updated lexicon.Inputs and Initialization The algorithm takes as input the training set of n (sentence, logical form) pairs {(xi, zi) : i = 1...n} along with an NP list, ANP, of proper noun lexical items such as Texas ` NP:tex.The lexicon, A, is initialized with a single lexical item xi `S :zi for each of the training pairs along with the contents of the NP list.It is possible to run the algorithm without the initial NP list; we include it to allow direct comparisons with previous approaches, which also included NP lists.Features and initial feature weights are described in Section 7.Step 1: Updating the Lexicon In the lexical update step the algorithm first computes the best correct parse tree y* for the current training example and then uses y* as input to the procedure NEW-LEX, which determines which (if any) new lexical items to add to A. NEW-LEX begins by enumerating all pairs (C, wi:j), for i < j, where C is a category occurring at a node in y* and wi:j are the (two or more) words it spans.For example, in the left parse in Figure 1, there would be four pairs: one with the category C = NP\NP:Ax.border(x) and the phrase wi:j =“ye siniri vardir”, and one for each non-leaf node in the tree.For each pair (C, wi:j), NEW-LEX considers introducing a new lexical item wi:j `C, which allows for the possibility of a parse where the subtree rooted at C is replaced with this new entry.(If C is a leaf node, this item will already exist.)NEW-LEX also considers adding each pair of new lexical items that is obtained by splitting wi:j`C as described in Section 4, thereby considering many different ways of reanalyzing the node.This process creates a set of possible new lexicons, where each lexicon expands A in a different way by adding the items from either a single split or a single merge of a node in y*.For each potential new lexicon A', NEW-LEX computes the probability p(y*|xi, zi; B', A') of the original parse y* under A' and parameters B' that are the same as B but have weights for the new lexical items, as described in Section 7.It also finds the best new parse y' = arg maxy p(y|xi, zi; B', A').1 Finally, NEW-LEX selects the A' with the largest difference in log probability between y' and y*, and returns the new entries in A'.If y* is the best parse for every A', NEW-LEX returns the empty set; the lexicon will not change.Step 2: Parameter Updates For each training example we update the parameters B using the stochastic gradient updates given by Eq.4.Discussion The alternation between refining the lexicon and updating the parameters drives the learning process.The initial model assigns a conditional likelihood of one to each training example (there is a single lexical item for each sentence xi, and it contains the labeled logical form zi).Although the splitting step often decreases the probability of the data, the new entries it produces are less specific and should generalize better.Since we initially assign positive weights to the parameters for new lexical items, the overall approach prefers splitting; trees with many lexical items will initially be much more likely.However, if the learned lexical items are used in too many incorrect parses, the stochastic gradient updates will down weight them to the point where the lexical induction step can merge or re-split nodes in the trees that contain them.This allows the approach to correct the lexicon and, hopefully, improve future performance.Previous work has focused on a variety of different meaning representations.Several approaches have been designed for the variable-free logical representations shown in examples throughout this paper.For example, Kate & Mooney (2006) present a method (KRISP) that extends an existing SVM learning algorithm to recover logical representations.The 1This computation can be performed efficiently by incrementally updating the parse chart used to find y*.Inputs: Training set {(xi, zi) : i = 1... n} where each example is a sentence xi paired with a logical form zi.Set of NP lexical items ANP.Number of iterations T. Learning rate parameter α0 and cooling rate parameter c. Definitions: The function NEW-LEX(y) takes a parse y and returns a set of new lexical items found by splitting and merging categories in y, as described in Section 5.The distributions p(y|x, z; B, A) and p(y, z|x; B, A) are defined by the log-linear model, as described in Section 3.3.Initialization: WASP system (Wong & Mooney, 2006) uses statistical machine translation techniques to learn synchronous context free grammars containing both words and logic.Lu et al. (2008) (Lu08) developed a generative model that builds a single hybrid tree of words, syntax and meaning representation.These algorithms are all language independent but representation specific.Other algorithms have been designed to recover lambda-calculus representations.For example, Wong & Mooney (2007) developed a variant of WASP (A-WASP) specifically designed for this alternate representation.Zettlemoyer & Collins (2005, 2007) developed CCG grammar induction techniques where lexical items are proposed according to a set of hand-engineered lexical templates.Our approach eliminates this need for manual effort.Another line of work has focused on recovering meaning representations that are not based on logic.Examples include an early statistical method for learning to fill slot-value representations (Miller et al., 1996) and a more recent approach for recovering semantic parse trees (Ge & Mooney, 2006).Exploring the extent to which these representations are compatible with the logic-based learning approach we developed is an important area for future work.Finally, there is work on using categorial grammars to solve other, related learning problems.For example, Buszkowski & Penn (1990) describe a unification-based approach for grammar discovery from bracketed natural language sentences and Villavicencio (2002) developed an approach for modeling child language acquisition.Additionally, Bos et al. (2004) consider the challenging problem of constructing broad-coverage semantic representations with CCG, but do not learn the lexicon.Features We use two types of features in our model.First, we include a set of lexical features: For each lexical item L E A, we include a feature OL that fires when L is used.Second, we include semantic features that are functions of the output logical expression z.Each time a predicate p in z takes an argument a with type T (a) in position i it triggers two binary indicator features: O(p,a,i) for the predicate-argument relation; and O(p,T(a),i) for the predicate argument-type relation.Initialization The weights for the semantic features are initialized to zero.The weights for the lexical features are initialized according to coocurrance statistics estimated with the Giza++ (Och & Ney, 2003) implementation of IBM Model 1.We compute translation scores for (word, constant) pairs that cooccur in examples in the training data.The initial weight for each OL is set to ten times the average score over the (word, constant) pairs in L, except for the weights of seed lexical entries in ANP which are set to 10 (equivalent to the highest possible coocurrence score).We used the learning rate α0 = 1.0 and cooling rate c = 10−5 in all training scenarios, and ran the algorithm for T = 20 iterations.These values were selected with cross validation on the Geo880 development set, described below.Data and Evaluation We evaluate our system on the GeoQuery datasets, which contain naturallanguage queries of a geographical database paired with logical representations of each query’s meaning.The full Geo880 dataset contains 880 (Englishsentence, logical-form) pairs, which we split into a development set of 600 pairs and a test set of 280 pairs, following Zettlemoyer & Collins (2005).The Geo250 dataset is a subset of Geo880 containing 250 sentences that have been translated into Turkish, Spanish and Japanese as well as the original English.Due to the small size of this dataset we use 10-fold cross validation for evaluation.We use the same folds as Wong & Mooney (2006, 2007) and Lu et al. (2008), allowing a direct comparison.The GeoQuery data is annotated with both lambda-calculus and variable-free meaning representations, which we have seen examples of throughout the paper.We report results for both representations, using the standard measures of Recall (percentage of test sentences assigned correct logical forms), Precision (percentage of logical forms returned that are correct) and F1 (the harmonic mean of Precision and Recall).Two-Pass Parsing To investigate the trade-off between precision and recall, we report results with a two-pass parsing strategy.When the parser fails to return an analysis for a test sentence due to novel words or usage, we reparse the sentence and allow the parser to skip words, with a fixed cost.Skipping words can potentially increase recall, if the ignored word is an unknown function word that does not contribute semantic content.Tables 1, 2, and 3 present the results for all of the experiments.In aggregate, they demonstrate that our algorithm, UBL, learns accurate models across languages and for both meaning representations.This is a new result; no previous system is as general.We also see the expected tradeoff between precision and recall that comes from the two-pass parsing approach, which is labeled UBL-s. With the ability to skip words, UBL-s achieves the highest recall of all reported systems for all evaluation conditions.However, UBL achieves much higher precision and better overall F1 scores, which are generally comparable to the best performing systems.The comparison to the CCG induction techniques of ZC05 and ZC07 (Table 3) is particularly striking.These approaches used language-specific templates to propose new lexical items and also required as input a set of hand-engineered lexical entries to model phenomena such as quantification and determiners.However, the use of higher-order unification allows UBL to achieve comparable performance while automatically inducing these types of entries.For a more qualitative evaluation, Table 4 shows a selection of lexical items learned with high weights for the lambda-calculus meaning representations.Nouns such as “state” or “estado” are consistently learned across languages with the category S|NP, which stands in for the more conventional N. The algorithm also learns language-specific constructions such as the Japanese case markers “no” and “wa”, which are treated as modifiers that do not add semantic content.Language-specific word order is also encoded, using the slash directions of the CCG categories.For example, “what” and “que” take their arguments to the right in the wh-initial English and Spanish.However, the Turkish wh-word “nelerdir” and the Japanese question marker “nan desu ka” are sentence final, and therefore take their arguments to the left.Learning regularities of this type allows UBL to generalize well to unseen data.There is less variation and complexity in the learned lexical items for the variable-free representation.The fact that the meaning representation is deeply nested influences the form of the induced grammar.For example, recall that the sentence “what states border texas” would be paired with the meaning answer(state(borders(tex))).For this representation, lexical items such as: can be used to construct the desired output.In practice, UBL often learns entries with only a single slash, like those above, varying only in the direction, as required for the language.Even the more complex items, such as those for quantifiers, are consistently simpler than those induced from the lambda-calculus meaning representations.For example, one of the most complex entries learned in the experiments for English is the smallest ` NP\NP/(NP|NP):AfAx.smallest one(f(x)).There are also differences in the aggregate statistics of the learned lexicons.For example, the average length of a learned lexical item for the (lambdacalculus, variable-free) meaning representations is: (1.21,1.08) for Turkish, (1.34,1.19) for English, (1.43,1.25) for Spanish and (1.63,1.42) for Japanese.For both meaning representations the model learns significantly more multiword lexical items for the somewhat analytic Japanese than the agglutinative Turkish.There are also variations in the average number of learned lexical items in the best parses during the final pass of training: 192 for Japanese, 206 for Spanish, 188 for English and 295 for Turkish.As compared to the other languages, the morpologically rich Turkish requires significantly more lexical variation to explain the data.Finally, there are a number of cases where the UBL algorithm could be improved in future work.In cases where there are multiple allowable word orders, the UBL algorithm must learn individual entries for each possibility.For example, the following two categories are often learned with high weight for the Japanese word “chiisai”: and are treated as distinct entries in the lexicon.Similarly, the approach presented here does not model morphology, and must repeatedly learn the correct categories for the Turkish words “nehri,” “nehir,” “nehirler,” and “nehirlerin”, all of which correspond to the logical form Ax.river(x).This paper has presented a method for inducing probabilistic CCGs from sentences paired with logical forms.The approach uses higher-order unification to define the space of possible grammars in a language- and representation-independent manner, paired with an algorithm that learns a probabilistic parsing model.We evaluated the approach on four languages with two meaning representations each, achieving high accuracy across all scenarios.For future work, we are interested in exploring the generality of the approach while extending it to new understanding problems.One potential limitation is in the constraints we introduced to ensure the tractability of the higher-order unification procedure.These restrictions will not allow the approach to induce lexical items that would be used with, among other things, many of the type-raised combinators commonly employed in CCG grammars.We are also interested in developing similar grammar induction techniques for context-dependent understanding problems, such as the one considered by Zettlemoyer & Collins (2009).Such an approach would complement ideas for using high-order unification to model a wider range of language phenomena, such as VP ellipsis (Dalrymple et al., 1991).We thank the reviewers for useful feedback.This work was supported by the EU under IST Cognitive Systems grant IP FP6-2004-IST-4-27657 “Paco-Plus” and ERC Advanced Fellowship 249520 “GRAMPLUS” to Steedman.Kwiatkowski was supported by an EPRSC studentship.Zettlemoyer was supported by a US NSF International Research Fellowship.
Determining Term Subjectivity And Term Orientation For Opinion Miningmining a recent subdiscipline of computational linguistics which is concerned not with the topic a document is about, but with the opinion it expresses.To aid the extraction of opinions from text, recent work has tackled the issue determining the “subjective” terms contained in text, i.e. deciding whether a term that carries opinionated content has a positive or a negative connotation.This is believed to be of key importance for identifying the orientation of documents, i.e. determining whether a document expresses a positive or negative opinion about its subject matter.We contend that the plain determination of the orientation of terms is not a realistic problem, since it starts from the nonrealistic assumption that we already know whether a term is subjective or not; this would imply that a linguistic resource that marks terms as “subjective” or “objective” is available, which is usually not the case.In this paper we confront the task of deciding whether a given term has a positive or a negative connotation, no subjective connotation at this problem thus subsumes the problem of desubjectivity problem of determining orientation.We tackle this problem by testing three different variants of a semi-supervised method previously proposed for orientation detection.Our results show that determining subjectivity is a much harder problem than determining orientation alone.Opinion mining is a recent subdiscipline of computational linguistics which is concerned not with the topic a document is about, but with the opinion it expresses.Opinion-driven content management has several important applications, such as determining critics’ opinions about a given product by classifying online product reviews, or tracking the shifting attitudes of the general public toward a political candidate by mining online forums.Within opinion mining, several subtasks can be identified, all of them having to do with tagging a given document according to expressed opinion: To aid these tasks, recent work (Esuli and Sebastiani, 2005; Hatzivassiloglou and McKeown, 1997; Kamps et al., 2004; Kim and Hovy, 2004; Takamura et al., 2005; Turney and Littman, 2003) has tackled the issue of identifying the orientation of subjective terms contained in text, i.e. determining whether a term that carries opinionated content has a positive or a negative connotation (e.g. deciding that — using Turney and Littman’s (2003) examples — honest and intrepid have a positive connotation while disturbing and superfluous have a negative connotation).This is believed to be of key importance for identifying the orientation of documents, since it is by considering the combined contribution of these terms that one may hope to solve Tasks 1, 2 and 3 above.The conceptually simplest approach to this latter problem is probably Turney’s (2002), who has obtained interesting results on Task 2 by considering the algebraic sum of the orientations of terms as representative of the orientation of the document they belong to; but more sophisticated approaches are also possible (Hatzivassiloglou and Wiebe, 2000; Riloff et al., 2003; Wilson et al., 2004).Implicit in most works dealing with term orientation is the assumption that, for many languages for which one would like to perform opinion mining, there is no available lexical resource where terms are tagged as having either a Positive or a Negative connotation, and that in the absence of such a resource the only available route is to generate such a resource automatically.However, we think this approach lacks realism, since it is also true that, for the very same languages, there is no available lexical resource where terms are tagged as having either a Subjective or an Objective connotation.Thus, the availability of an algorithm that tags Subjective terms as being either Positive or Negative is of little help, since determining if a term is Subjective is itself non-trivial.In this paper we confront the task of determining whether a given term has a Positive connotation (e.g. honest, intrepid), or a Negative connotation (e.g. disturbing, superfluous), or has instead no Subjective connotation at all (e.g. white, triangular); this problem thus subsumes the problem of deciding between Subjective and Objective and the problem of deciding between Positive and Negative.We tackle this problem by testing three different variants of the semi-supervised method for orientation detection proposed in (Esuli and Sebastiani, 2005).Our results show that determining subjectivity and orientation is a much harder problem than determining orientation alone.The rest of the paper is structured as follows.Section 2 reviews related work dealing with term orientation and/or subjectivity detection.Section 3 briefly reviews the semi-supervised method for orientation detection presented in (Esuli and Sebastiani, 2005).Section 4 describes in detail three different variants of it we propose for determining, at the same time, subjectivity and orientation, and describes the general setup of our experiments.In Section 5 we discuss the results we have obtained.Section 6 concludes.Most previous works dealing with the properties of terms within an opinion mining perspective have focused on determining term orientation.Hatzivassiloglou and McKeown (1997) attempt to predict the orientation of subjective adjectives by analysing pairs of adjectives (conjoined by and, or, but, either-or, or neither-nor) extracted from a large unlabelled document set.The underlying intuition is that the act of conjoining adjectives is subject to linguistic constraints on the orientation of the adjectives involved; e.g. and usually conjoins adjectives of equal orientation, while but conjoins adjectives of opposite orientation.The authors generate a graph where terms are nodes connected by “equal-orientation” or “opposite-orientation” edges, depending on the conjunctions extracted from the document set.A clustering algorithm then partitions the graph into a Positive cluster and a Negative cluster, based on a relation of similarity induced by the edges.Turney and Littman (2003) determine term orientation by bootstrapping from two small sets of subjective “seed” terms (with the seed set for Positive containing terms such as good and nice, and the seed set for Negative containing terms such as bad and nasty).Their method is based on computing the pointwise mutual information (PMI) of the target term t with each seed term ti as a measure of their semantic association.Given a target term t, its orientation value O(t) (where positive value means positive orientation, and higher absolute value means stronger orientation) is given by the sum of the weights of its semantic association with the seed positive terms minus the sum of the weights of its semantic association with the seed negative terms.For computing PMI, term frequencies and co-occurrence frequencies are measured by querying a document set by means of the AltaVista search engine1 with a “t” query, a “ti” query, and a “t NEAR ti” query, and using the number of matching documents returned by the search engine as estimates of the probabilities needed for the computation of PMI.Kamps et al. (2004) consider instead the graph defined on adjectives by the WordNet2 synonymy relation, and determine the orientation of a target adjective t contained in the graph by comparing the lengths of (i) the shortest path between t and the seed term good, and (ii) the shortest path between t and the seed term bad: if the former is shorter than the latter, than t is deemed to be Positive, otherwise it is deemed to be Negative.Takamura et al. (2005) determine term orientation (for Japanese) according to a “spin model”, i.e. a physical model of a set of electrons each endowed with one between two possible spin directions, and where electrons propagate their spin direction to neighbouring electrons until the system reaches a stable configuration.The authors equate terms with electrons and term orientation to spin direction.They build a neighbourhood matrix connecting each pair of terms if one appears in the gloss of the other, and iteratively apply the spin model on the matrix until a “minimum energy” configuration is reached.The orientation assigned to a term then corresponds to the spin direction assigned to electrons.The system of Kim and Hovy (2004) tackles orientation detection by attributing, to each term, a positivity score and a negativity score; interestingly, terms may thus be deemed to have both a positive and a negative correlation, maybe with different degrees, and some terms may be deemed to carry a stronger positive (or negative) orientation than others.Their system starts from a set of positive and negative seed terms, and expands the positive (resp. negative) seed set by adding to it the synonyms of positive (resp. negative) seed terms and the antonyms of negative (resp. positive) seed terms.The system classifies then a target term t into either Positive or Negative by means of two alternative learning-free methods based on the probabilities that synonyms of t also appear in the respective expanded seed sets.A problem with this method is that it can classify only terms that share some synonyms with the expanded seed sets.Kim and Hovy also report an evaluation of human inter-coder agreement.We compare this evaluation with our results in Section 5.The approach we have proposed for determining term orientation (Esuli and Sebastiani, 2005) is described in more detail in Section 3, since it will be extensively used in this paper.All these works evaluate the performance of the proposed algorithms by checking them against precompiled sets of Positive and Negative terms, i.e. checking how good the algorithms are at classifying a term known to be subjective into either Positive or Negative.When tested on the same benchmarks, the methods of (Esuli and Sebastiani, 2005; Turney and Littman, 2003) have performed with comparable accuracies (however, the method of (Esuli and Sebastiani, 2005) is much more efficient than the one of (Turney and Littman, 2003)), and have outperformed the method of (Hatzivassiloglou and McKeown, 1997) by a wide margin and the one by (Kamps et al., 2004) by a very wide margin.The methods described in (Hatzivassiloglou and McKeown, 1997) is also limited by the fact that it can only decide the orientation of adjectives, while the method of (Kamps et al., 2004) is further limited in that it can only work on adjectives that are present in WordNet.The methods of (Kim and Hovy, 2004; Takamura et al., 2005) are instead difficult to compare with the other ones since they were not evaluated on publicly available datasets.Riloff et al. (2003) develop a method to determine whether a term has a Subjective or an Objective connotation, based on bootstrapping algorithms.The method identifies patterns for the extraction of subjective nouns from text, bootstrapping from a seed set of 20 terms that the authors judge to be strongly subjective and have found to have high frequency in the text collection from which the subjective nouns must be extracted.The results of this method are not easy to compare with the ones we present in this paper because of the different evaluation methodologies.While we adopt the evaluation methodology used in all of the papers reviewed so far (i.e. checking how good our system is at replicating an existing, independently motivated lexical resource), the authors do not test their method on an independently identified set of labelled terms, but on the set of terms that the algorithm itself extracts.This evaluation methodology only allows to test precision, and not accuracy tout court, since no quantification can be made of false negatives (i.e. the subjective terms that the algorithm should have spotted but has not spotted).In Section 5 this will prevent us from drawing comparisons between this method and our own.Baroni and Vegnaduzzo (2004) apply the PMI method, first used by Turney and Littman (2003) to determine term orientation, to determine term subjectivity.Their method uses a small set S, of 35 adjectives, marked as subjective by human judges, to assign a subjectivity score to each adjective to be classified.Therefore, their method, unlike our own, does not classify terms (i.e. take firm classification decisions), but ranks them according to a subjectivity score, on which they evaluate precision at various level of recall.term orientation by semi-supervised learning The method we use in this paper for determining term subjectivity and term orientation is a variant of the method proposed in (Esuli and Sebastiani, 2005) for determining term orientation alone.This latter method relies on training, in a semisupervised way, a binary classifier that labels terms as either Positive or Negative.A semisupervised method is a learning process whereby only a small subset L C Tr of the training data Tr are human-labelled.In origin the training data in U = Tr − L are instead unlabelled; it is the process itself that labels them, automatically, by using L (with the possible addition of other publicly available resources) as input.The method of (Esuli and Sebastiani, 2005) starts from two small seed (i.e. training) sets Lp and Ln of known Positive and Negative terms, respectively, and expands them into the two final training sets Trp D Lp and Trn D Ln by adding them new sets of terms Up and Un found by navigating the WordNet graph along the synonymy and antonymy relations3.This process is based on the hypothesis that synonymy and antonymy, in addition to defining a relation of meaning, also define a relation of orientation, i.e. that two synonyms typically have the same orientation and two antonyms typically have opposite orientation.The method is iterative, generating two sets Trkp and Trknat each iteration k, where Trkp D Trk−1 p D ... D Tr1 p = Lp and Trkn D Trk−1 n D ... D Tr1 n = Ln.At iteration k, Trkp is obtained by adding to Trk−1 p all synonyms of terms in Trk−1 pand all antonyms of terms in Trk−1 n ; similarly, Trknis obtained by adding to Trk−1 n all synonyms of terms in Trk−1 n and all antonyms of terms in Trk−1 p .If a total of K iterations are performed, then Tr = TrKp U TrKn .The second main feature of the method presented in (Esuli and Sebastiani, 2005) is that terms are given vectorial representations based on their WordNet glosses (i.e. textual definitions).For each term ti in Tr U Te (Te being the test set, i.e. the set of terms to be classified), a textual representation of ti is generated by collating all the glosses of ti as found in WordNet4.Each such representation is converted into vectorial form by standard text indexing techniques (in (Esuli and Sebastiani, 2005) and in the present work, stop words are removed and the remaining words are weighted by cosine-normalized tfidf; no stemming is performed)5.This representation method is based on the assumption that terms with a similar orientation tend to have “similar” glosses: for instance, that the glosses of honest and intrepid will both contain appreciative expressions, while the glosses of disturbing and superfluous will both contain derogative expressions.Note that this method allows to classify any term, independently of its POS, provided there is a gloss for it in the lexical resource.Once the vectorial representations for all terms in TrUT e have been generated, those for the terms in Tr are fed to a supervised learner, which thus generates a binary classifier.This latter, once fed with the vectorial representations of the terms in Te, classifies each of them as either Positive or Negative.In this paper we extend the method of (Esuli and Sebastiani, 2005) to the determination of term subjectivity and term orientation altogether.The benchmark (i.e. test set) we use for our experiments is the General Inquirer (GI) lexicon (Stone et al., 1966).This is a lexicon of terms labelled according to a large set of categories6, each one denoting the presence of a specific trait in the term.The two main categories, and the ones we will be concerned with, are Positive/Negative, which contain 1,915/2,291 terms having a positive/negative orientation (in what follows we will also refer to the category Subjective, which we define as the union of the two categories Positive and Negative).In opinion mining research the GI was first used by Turney and Littman (2003), who reduced the list of terms to 1,614/1,982 entries afit may have more than one sense; dictionaries normally associate one gloss to each sense.5Several combinations of subparts of a WordNet gloss are tested as textual representations of terms in (Esuli and Sebastiani, 2005).Of all those combinations, in the present paper we always use the DGS- combination, since this is the one that has been shown to perform best in (Esuli and Sebastiani, 2005).DGS- corresponds to using the entire gloss and performing negation propagation on its text, i.e. replacing all the terms that occur after a negation in a sentence with negated versions of the term (see (Esuli and Sebastiani, 2005) for details). ter removing 17 terms appearing in both categories (e.g. deal) and reducing all the multiple entries of the same term in a category, caused by multiple senses, to a single entry.Likewise, we take all the 7,582 GI terms that are not labelled as either Positive or Negative, as being (implicitly) labelled as Objective, and reduce them to 5,009 terms after combining multiple entries of the same term, caused by multiple senses, to a single entry.The effectiveness of our classifiers will thus be evaluated in terms of their ability to assign the total 8,605 GI terms to the correct category among Positive, Negative, and Objective7.Similarly to (Esuli and Sebastiani, 2005), our training set is obtained by expanding initial seed sets by means of WordNet lexical relations.The main difference is that our training set is now the union of three sets of training terms Tr = TrKp ∪TrKn ∪TrKo obtained by expanding, through K iterations, three seed sets Tr1p, Tr1n, Tr1o, one for each of the categories Positive, Negative, and Objective, respectively.Concerning categories Positive and Negative, we have used the seed sets, expansion policy, and number of iterations, that have performed best in the experiments of (Esuli and Sebastiani, 2005), i.e. the seed sets Tr1p = {good} and Tr1 n = {bad} expanded by using the union of synonymy and indirect antonymy, restricting the relations only to terms with the same POS of the original terms (i.e. adjectives), for a total of K = 4 iterations.The final expanded sets contain 6,053 Positive terms and 6,874 Negative terms.Concerning the category Objective, the process we have followed is similar, but with a few key differences.These are motivated by the fact that the Objective category coincides with the complement of the union of Positive and Negative; therefore, Objective terms are more varied and diverse in meaning than the terms in the other two categories.To obtain a representative expanded set TrKo , we have chosen the seed set Tr1o = {entity} and we have expanded it by using, along with synonymy and antonymy, the WordNet relation of hyponymy (e.g. vehicle / car), and without imposing the restriction that the two related terms must have the same POS.These choices are strictly related to each other: the term entity is the root term of the largest generalization hierarchy in WordNet, with more than 40,000 terms (Devitt and Vogel, 2004), thus allowing to reach a very large number of terms by using the hyponymy relation8.Moreover, it seems reasonable to assume that terms that refer to entities are likely to have an “objective” nature, and that hyponyms (and also synonyms and antonyms) of an objective term are also objective.Note that, at each iteration k, a given term t is added to Trko only if it does not already belong to either Trp or Trn.We experiment with two different choices for the Tro set, corresponding to the sets generated in K = 3 and K = 4 iterations, respectively; this yields sets Tr3o and Tr4o consisting of 8,353 and 33,870 training terms, respectively.We experiment with three “philosophically” different learning approaches to the problem of distinguishing between Positive, Negative, and Objective terms.Approach I is a two-stage method which consists in learning two binary classifiers: the first classifier places terms into either Subjective or Objective, while the second classifier places terms that have been classified as Subjective by the first classifier into either Positive or Negative.In the training phase, the terms in TrKp ∪ TrKn are used as training examples of category Subjective.Approach II is again based on learning two binary classifiers.Here, one of them must discriminate between terms that belong to the Positive category and ones that belong to its complement (not Positive), while the other must discriminate between terms that belong to the Negative category and ones that belong to its complement (not Negative).Terms that have been classified both into Positive by the former classifier and into (not Negative) by the latter are deemed to be positive, and terms that have been classified both into (not Positive) by the former classifier and into Negative by the latter are deemed to be negative.The terms that have been classified (i) into both (not Positive) and (not Negative), or (ii) into both Positive and Negative, are taken to be Objective.In the training phase of Approach II, the terms in TrKn ∪ TrKo are used as training examples of category (not Positive), and the terms in TrKp ∪ TrKo are used as training examples of category (not Negative).Approach III consists instead in viewing Positive, Negative, and Objective as three categories with equal status, and in learning a ternary classifier that classifies each term into exactly one among the three categories.There are several differences among these three approaches.A first difference, of a conceptual nature, is that only Approaches I and III view Objective as a category, or concept, in its own right, while Approach II views objectivity as a nonexistent entity, i.e. as the “absence of subjectivity” (in fact, in Approach II the training examples of Objective are only used as training examples of the complements of Positive and Negative).A second difference is that Approaches I and II are based on standard binary classification technology, while Approach III requires “multiclass” (i.e.1-of-m) classification.As a consequence, while for the former we use well-known learners for binary classification (the naive Bayesian learner using the multinomial model (McCallum and Nigam, 1998), support vector machines using linear kernels (Joachims, 1998), the Rocchio learner, and its PrTFIDF probabilistic version (Joachims, 1997)), for Approach III we use their multiclass versions9.Before running our learners we make a pass of feature selection, with the intent of retaining only those features that are good at discriminating our categories, while discarding those which are not.Feature selection is implemented by scoring each feature fk (i.e. each term that occurs in the glosses of at least one training term) by means of the mutual information (MI) function, defined as and discarding the x% features fk that minimize it.We will call x% the reduction factor.Note that the set {c1, ... , cm} from Equation 1 is interpreted differently in Approaches I to III, and always consistently with who the categories at stake are.Since the task we aim to solve is manifold, we will evaluate our classifiers according to two evaluation measures: and Objective, i.e. in deciding both term orientation and subjectivity.We present results obtained from running every combination of (i) the three approaches to classification described in Section 4.3, (ii) the four learners mentioned in the same section, (iii) five different reduction factors for feature selection (0%, 50%, 90%, 95%, 99%), and (iv) the two different training sets (Tr3o and Tr4o) for Objective mentioned in Section 4.2.We discuss each of these four dimensions of the problem individually, for each one reporting results averaged across all the experiments we have run (see Table 1).The first and most important observation is that, with respect to a pure term orientation task, accuracy drops significantly.In fact, the best SOaccuracy and the best PNO-accuracy results obtained across the 120 different experiments are .676 and .660, respectively (these were obtained by using Approach II with the PrTFIDF learner and no feature selection, with Tro = Tr3o for the .676 SO-accuracy result and Tro = Tr4o for the .660 PNO-accuracy result); this contrasts sharply with the accuracy obtained in (Esuli and Sebastiani, 2005) on discriminating Positive from Negative (where the best run obtained .830 accuracy), on the same benchmarks and essentially the same algorithms.This suggests that good performance at orientation detection (as e.g. in (Esuli and Sebastiani, 2005; Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003)) may not be a cE{cl,...,cm}, fE{fk,fk} guarantee of good performance at subjectivity detection, quite evidently a harder (and, as we have suggested, more realistic) task.This hypothesis is confirmed by an experiment performed by Kim and Hovy (2004) on testing the agreement of two human coders at tagging words with the Positive, Negative, and Objective labels.The authors define two measures of such agreement: strict agreement, equivalent to our PNO-accuracy, and lenient agreement, which measures the accuracy at telling Negative against the rest.For any experiment, strict agreement values are then going to be, by definition, lower or equal than the corresponding lenient ones.The authors use two sets of 462 adjectives and 502 verbs, respectively, randomly extracted from the basic English word list of the TOEFL test.The intercoder agreement results (see Table 2) show a deterioration in agreement (from lenient to strict) of 16.77% for adjectives and 36.42% for verbs.Following this, we evaluated our best experiment according to these measures, and obtained a “strict” accuracy value of .660 and a “lenient” accuracy value of .821, with a relative deterioration of 24.39%, in line with Kim and Hovy’s observation10.This confirms that determining subjectivity and orientation is a much harder task than determining orientation alone.The second important observation is that there is very little variance in the results: across all 120 experiments, average SO-accuracy and PNOaccuracy results were .635 (with standard deviation Q = .030) and .603 (Q = .036), a mere 6.06% and 8.64% deterioration from the best results reported above.This seems to indicate that the levels of performance obtained may be hard to improve upon, especially if working in a similar framework.Let us analyse the individual dimensions of the problem.Concerning the three approaches to classification described in Section 4.3, Approach II outperforms the other two, but by an extremely narrow margin.As for the choice of learners, on average the best performer is NB, but again by a very small margin wrt the others.On average, the 10We observed this trend in all of our experiments. best reduction factor for feature selection turns out to be 50%, but the performance drop we witness in approaching 99% (a dramatic reduction factor) is extremely graceful.As for the choice of TrKo , we note that Tro and Tr4o elicit comparable levels of performance, with the former performing best at SO-accuracy and the latter performing best at PNO-accuracy.An interesting observation on the learners we have used is that NB, PrTFIDF and SVMs, unlike Rocchio, generate classifiers that depend on P(ci), the prior probabilities of the classes, which are normally estimated as the proportion of training documents that belong to ci.In many classification applications this is reasonable, as we may assume that the training data are sampled from the same distribution from which the test data are sampled, and that these proportions are thus indicative of the proportions that we are going to encounter in the test data.However, in our application this is not the case, since we do not have a “natural” sample of training terms.What we have is one human-labelled training term for each category in {Positive,Negative,Objective}, and as many machine-labelled terms as we deem reasonable to include, in possibly different numbers for the different categories; and we have no indication whatsoever as to what the “natural” proportions among the three might be.This means that the proportions of Positive, Negative, and Objective terms we decide to include in the training set will strongly bias the classification results if the learner is one of NB, PrTFIDF and SVMs.We may notice this by looking at Table 3, which shows the average proportion of test terms classified as Objective by each learner, depending on whether we have chosen Tro to coincide with Tr3o or Tr4o; note that the former (resp. latter) choice means having roughly as many (resp. roughly five times as many) Objective training terms as there are Positive and Negative ones.Table 3 shows that, the more Objective training terms there are, the more test terms NB, PrTFIDF and (in particular) SVMs will classify as Objective; this is not true for Rocchio, which is basically unaffected by the variation in size of Tro.We have presented a method for determining both term subjectivity and term orientation for opinion mining applications.This is a valuable advance with respect to the state of the art, since past work in this area had mostly confined to determining term orientation alone, a task that (as we have arfied as Objective, for each learner and for each choice of the Tro set. gued) has limited practical significance in itself, given the generalized absence of lexical resources that tag terms as being either Subjective or Objective.Our algorithms have tagged by orientation and subjectivity the entire General Inquirer lexicon, a complete general-purpose lexicon that is the de facto standard benchmark for researchers in this field.Our results thus constitute, for this task, the first baseline for other researchers to improve upon.Unfortunately, our results have shown that an algorithm that had shown excellent, stateof-the-art performance in deciding term orientation (Esuli and Sebastiani, 2005), once modified for the purposes of deciding term subjectivity, performs more poorly.This has been shown by testing several variants of the basic algorithm, some of them involving radically different supervised learning policies.The results suggest that deciding term subjectivity is a substantially harder task that deciding term orientation alone.
Automatically Constructing A Lexicon Of Verb Phrase Idiomatic CombinationsWe investigate the lexical and syntactic flexibility of a class of idiomatic expressions.We develop measures that draw on such linguistic properties, and demonstrate that these statistical, corpus-based measures can be successfully used for distinguishing idiomatic combinations from non-idiomatic ones.We also propose a means for automatically determining which syntactic forms a particular idiom can appear in, and hence should be included in its lexical representation.The term idiom has been applied to a fuzzy category with prototypical examples such as by and large, kick the bucket, and let the cat out of the bag.Providing a definitive answer for what idioms are, and determining how they are learned and understood, are still subject to debate (Glucksberg, 1993; Nunberg et al., 1994).Nonetheless, they are often defined as phrases or sentences that involve some degree of lexical, syntactic, and/or semantic idiosyncrasy.Idiomatic expressions, as a part of the vast family of figurative language, are widely used both in colloquial speech and in written language.Moreover, a phrase develops its idiomaticity over time (Cacciari, 1993); consequently, new idioms come into existence on a daily basis (Cowie et al., 1983; Seaton and Macaulay, 2002).Idioms thus pose a serious challenge, both for the creation of widecoverage computational lexicons, and for the development of large-scale, linguistically plausible natural language processing (NLP) systems (Sag et al., 2002).One problem is due to the range of syntactic idiosyncrasy of idiomatic expressions.Some idioms, such as by and large, contain syntactic violations; these are often completely fixed and hence can be listed in a lexicon as “words with spaces” (Sag et al., 2002).However, among those idioms that are syntactically well-formed, some exhibit limited morphosyntactic flexibility, while others may be more syntactically flexible.For example, the idiom shoot the breeze undergoes verbal inflection (shot the breeze), but not internal modification or passivization (?shoot the fun breeze, ?the breeze was shot).In contrast, the idiom spill the beans undergoes verbal inflection, internal modification, and even passivization.Clearly, a words-withspaces approach does not capture the full range of behaviour of such idiomatic expressions.Another barrier to the appropriate handling of idioms in a computational system is their semantic idiosyncrasy.This is a particular issue for those idioms that conform to the grammar rules of the language.Such idiomatic expressions are indistinguishable on the surface from compositional (nonidiomatic) phrases, but a computational system must be capable of distinguishing the two.For example, a machine translation system should translate the idiom shoot the breeze as a single unit of meaning (“to chat”), whereas this is not the case for the literal phrase shoot the bird.In this study, we focus on a particular class of English phrasal idioms, i.e., those that involve the combination of a verb plus a noun in its direct object position.Examples include shoot the breeze, pull strings, and push one’s luck.We refer to these as verb+noun idiomatic combinations (VNICs).The class of VNICs accommodates a large number of idiomatic expressions (Cowie et al., 1983; Nunberg et al., 1994).Moreover, their peculiar behaviour signifies the need for a distinct treatment in a computational lexicon (Fellbaum, 2005).Despite this, VNICs have been granted relatively little attention within the computational linguistics community.We look into two closely related problems confronting the appropriate treatment of VNICs: (i) the problem of determining their degree of flexibility; and (ii) the problem of determining their level of idiomaticity.Section 2 elaborates on the lexicosyntactic flexibility of VNICs, and how this relates to their idiomaticity.In Section 3, we propose two linguistically-motivated statistical measures for quantifying the degree of lexical and syntactic inflexibility (or fixedness) of verb+noun combinations.Section 4 presents an evaluation of the proposed measures.In Section 5, we put forward a technique for determining the syntactic variations that a VNIC can undergo, and that should be included in its lexical representation.Section 6 summarizes our contributions.Although syntactically well-formed, VNICs involve a certain degree of semantic idiosyncrasy.Unlike compositional verb+noun combinations, the meaning of VNICs cannot be solely predicted from the meaning of their parts.There is much evidence in the linguistic literature that the semantic idiosyncrasy of idiomatic combinations is reflected in their lexical and/or syntactic behaviour.A limited number of idioms have one (or more) lexical variants, e.g., blow one’s own trumpet and toot one’s own horn (examples from Cowie et al. 1983).However, most are lexically fixed (nonproductive) to a large extent.Neither shoot the wind nor fling the breeze are typically recognized as variations of the idiom shoot the breeze.Similarly, spill the beans has an idiomatic meaning (“to reveal a secret”), while spill the peas and spread the beans have only literal interpretations.Idiomatic combinations are also syntactically peculiar: most VNICs cannot undergo syntactic variations and at the same time retain their idiomatic interpretations.It is important, however, to note that VNICs differ with respect to the degree of syntactic flexibility they exhibit.Some are syntactically inflexible for the most part, while others are more versatile; as illustrated in 1 and 2: Linguists have explained the lexical and syntactic flexibility of idiomatic combinations in terms of their semantic analyzability (e.g., Glucksberg 1993; Fellbaum 1993; Nunberg et al. 1994).Semantic analyzability is inversely related to idiomaticity.For example, the meaning of shoot the breeze, a highly idiomatic expression, has nothing to do with either shoot or breeze.In contrast, a less idiomatic expression, such as spill the beans, can be analyzed as spill corresponding to “reveal” and beans referring to “secret(s)”.Generally, the constituents of a semantically analyzable idiom can be mapped onto their corresponding referents in the idiomatic interpretation.Hence analyzable (less idiomatic) expressions are often more open to lexical substitution and syntactic variation.We use the observed connection between idiomaticity and (in)flexibility to devise statistical measures for automatically distinguishing idiomatic from literal verb+noun combinations.While VNICs vary in their degree of flexibility (cf.1 and 2 above; see also Moon 1998), on the whole they contrast with compositional phrases, which are more lexically productive and appear in a wider range of syntactic forms.We thus propose to use the degree of lexical and syntactic flexibility of a given verb+noun combination to determine the level of idiomaticity of the expression.It is important to note that semantic analyzability is neither a necessary nor a sufficient condition for an idiomatic combination to be lexically or syntactically flexible.Other factors, such as the communicative intentions and pragmatic constraints, can motivate a speaker to use a variant in place of a canonical form (Glucksberg, 1993).Nevertheless, lexical and syntactic flexibility may well be used as partial indicators of semantic analyzability, and hence idiomaticity.Here we describe our measures for idiomaticity, which quantify the degree of lexical, syntactic, and overall fixedness of a given verb+noun combination, represented as a verb–noun pair.(Note that our measures quantify fixedness, not flexibility.)A VNIC is lexically fixed if the replacement of any of its constituents by a semantically (and syntactically) similar word generally does not result in another VNIC, but in an invalid or a literal expression.One way of measuring lexical fixedness of a given verb+noun combination is thus to examine the idiomaticity of its variants, i.e., expressions generated by replacing one of the constituents by a similar word.This approach has two main challenges: (i) it requires prior knowledge about the idiomaticity of expressions (which is what we are developing our measure to determine); (ii) it needs information on “similarity” among words.Inspired by Lin (1999), we examine the strength of association between the verb and noun constituents of the target combination and its variants, as an indirect cue to their idiomaticity.We use the automatically-built thesaurus of Lin (1998) to find similar words to the noun of the target expression, in order to automatically generate variants.Only the noun constituent is varied, since replacing the verb constituent of a VNIC with a semantically related verb is more likely to yield another VNIC, as in keep/lose one’s cool (Nunberg et al., 1994).Let be the set of the most similar nouns to the noun of the target pair .We calculate the association strength for the target pair, and for each of its variants, , using pointwise mutual information (PMI) (Church et al., 1991): where and is the target noun; is the set of all transitive verbs in the corpus; is the set of all nouns appearing as the direct object of some verb; is the frequency of and occurring as a verb–object pair; is the total frequency of the target verb with any noun in ; is the total frequency of the noun in the direct object position of any verb in .Lin (1999) assumes that a target expression is non-compositional if and only if its value is significantly different from that of any of the variants.Instead, we propose a novel technique that brings together the association strengths ( values) of the target and the variant expressions into a single measure reflecting the degree of lexical fixedness for the target pair.We assume that the target pair is lexically fixed to the extent that its deviates from the average of its variants.Our measure calculates this deviation, normalized using the sample’s standard deviation: Compared to compositional verb+noun combinations, VNICs are expected to appear in more restricted syntactic forms.To quantify the syntactic fixedness of a target verb–noun pair, we thus need to: (i) identify relevant syntactic patterns, i.e., those that help distinguish VNICs from literal verb+noun combinations; (ii) translate the frequency distribution of the target pair in the identified patterns into a measure of syntactic fixedness.Determining a unique set of syntactic patterns appropriate for the recognition of all idiomatic combinations is difficult indeed: exactly which forms an idiomatic combination can occur in is not entirely predictable (Sag et al., 2002).Nonetheless, there are hypotheses about the difference in behaviour of VNICs and literal verb+noun combinations with respect to particular syntactic variations (Nunberg et al., 1994).Linguists note that semantic analyzability is related to the referential status of the noun constituent, which is in turn related to participation in certain morphosyntactic forms.In what follows, we describe three types of variation that are tolerated by literal combinations, but are prohibited by many VNICs.Passivization There is much evidence in the linguistic literature that VNICs often do not undergo passivization.1 Linguists mainly attribute this to the fact that only a referential noun can appear as the surface subject of a passive construction.Determiner Type A strong correlation exists between the flexibility of the determiner preceding the noun in a verb+noun combination and the overall flexibility of the phrase (Fellbaum, 1993).It is however important to note that the nature of the determiner is also affected by other factors, such as the semantic properties of the noun.Pluralization While the verb constituent of a VNIC is morphologically flexible, the morphological flexibility of the noun relates to its referential status.A non-referential noun constituent is expected to mainly appear in just one of the singular or plural forms.The pluralization of the noun is of course also affected by its semantic properties.Merging the three variation types results in a pattern set, , of distinct syntactic patterns, given in Table 1.2 The second step is to devise a statistical measure that quantifies the degree of syntactic fixedness of a verb–noun pair, with respect to the selected set of patterns, .We propose a measure that compares the “syntactic behaviour” of the target pair with that of a “typical” verb–noun pair.Syntactic behaviour of a typical pair is defined as the prior probability distribution over the patterns in .The prior probability of an individual pattern is estimated as: The syntactic behaviour of the target verb–noun pair is defined as the posterior probability distribution over the patterns, given the particular pair.The posterior probability of an individual pattern is estimated as: The degree of syntactic fixedness of the target verb–noun pair is estimated as the divergence of its syntactic behaviour (the posterior distribution 2We collapse some patterns since with a larger pattern set the measure may require larger corpora to perform reliably. over the patterns), from the typical syntactic behaviour (the prior distribution).The divergence of the two probability distributions is calculated using a standard information-theoretic measure, the Kullback Leibler (KL-)divergence: KL-divergence is always non-negative and is zero if and only if the two distributions are exactly the same.Thus, .KL-divergence is argued to be problematic because it is not a symmetric measure.Nonetheless, it has proven useful in many NLP applications (Resnik, 1999; Dagan et al., 1994).Moreover, the asymmetry is not an issue here since we are concerned with the relative distance of several posterior distributions from the same prior.VNICs are hypothesized to be, in most cases, both lexically and syntactically more fixed than literal verb+noun combinations (see Section 2).We thus propose a new measure of idiomaticity to be a measure of the overall fixedness of a given pair.We define as: where weights the relative contribution of the measures in predicting idiomaticity.To evaluate our proposed fixedness measures, we determine their appropriateness as indicators of idiomaticity.We pose a classification task in which idiomatic verb–noun pairs are distinguished from literal ones.We use each measure to assign scores to the experimental pairs (see Section 4.2 below).We then classify the pairs by setting a threshold, here the median score, where all expressions with scores higher than the threshold are labeled as idiomatic and the rest as literal.We assess the overall goodness of a measure by looking at its accuracy (Acc) and the relative reduction in error rate (RER) on the classification task described above.The RER of a measure reflects the improvement in its accuracy relative to another measure (often a baseline).We consider two baselines: (i) a random baseline, , that randomly assigns a label (literal or idiomatic) to each verb–noun pair; (ii) a more informed baseline, , an information-theoretic measure widely used for extracting statistically significant collocations.3 We use the British National Corpus (BNC; “http://www.natcorp.ox.ac.uk/”) to extract verb– noun pairs, along with information on the syntactic patterns they appear in.We automatically parse the corpus using the Collins parser (Collins, 1999), and further process it using TGrep2 (Rohde, 2004).For each instance of a transitive verb, we use heuristics to extract the noun phrase (NP) in either the direct object position (if the sentence is active), or the subject position (if the sentence is passive).We then use NP-head extraction software4 to get the head noun of the extracted NP, its number (singular or plural), and the determiner introducing it.We select our development and test expressions from verb–noun pairs that involve a member of a predefined list of (transitive) “basic” verbs.Basic verbs, in their literal use, refer to states or acts that are central to human experience.They are thus frequent, highly polysemous, and tend to combine with other words to form idiomatic combinations (Nunberg et al., 1994).An initial list of such verbs was selected from several linguistic and psycholinguistic studies on basic vocabulary (e.g., Pauwels 2000; Newman and Rice 2004).We further augmented this initial list with verbs that are semantically related to another verb already in the From the corpus, we extract all verb–noun pairs with minimum frequency of that contain a basic verb.From these, we semi-randomly select an idiomatic and a literal subset.5 A pair is considered idiomatic if it appears in a credible idiom dictionary, such as the Oxford Dictionary of Current Idiomatic English (ODCIE) (Cowie et al., 1983), or the Collins COBUILD Idioms Dictionary (CCID) (Seaton and Macaulay, 2002).Otherwise, the pair is considered literal.We then randomly pull out development and test pairs (half idiomatic and half literal), ensuring both low and high frequency items are included.Sample idioms corresponding to the extracted pairs are: kick the habit, move mountains, lose face, and keep one’s word.Development expressions are used in devising the fixedness measures, as well as in determining the values of the parameters in Eqn.(2) and in Eqn.(4). determines the maximum number of nouns similar to the target noun, to be considered in measuring the lexical fixedness of a given pair.The value of this parameter is determined by performing experiments over the development data, in which ranges from to by steps of ; is set to based on the results.We also experimented with different values of ranging from to by steps of .Based on the development results, the best value for is (giving more weight to the syntactic fixedness measure).Test expressions are saved as unseen data for the final evaluation.We further divide the set of all test expressions, TEST , into two sets corresponding to two frequency bands: TEST contains idiomatic and literal pairs, each with total frequency between and ( ); TEST consists of idiomatic and literal pairs, each with total frequency of or greater ( ).All frequency counts are over the entire BNC.We first examine the performance of the individual fixedness measures, and 5In selecting literal pairs, we choose those that involve a physical act corresponding to the basic semantics of the verb., as well as that of the two baselines, and ; see Table 2.(Results for the overall measure are presented later in this section.)As can be seen, the informed baseline, , shows a large improvement over the random baseline ( error reduction).This shows that one can get relatively good performance by treating verb+noun idiomatic combinations as collocations. performs as well as the informed baseline ( error reduction).This result shows that, as hypothesized, lexical fixedness is a reasonably good predictor of idiomaticity.Nonetheless, the performance signifies a need for improvement.Possibly the most beneficial enhancement would be a change in the way we acquire the similar nouns for a target noun.The best performance (shown in boldface) belongs to , with error reduction over the random baseline, and error reduction over the informed baseline.These results demonstrate that syntactic fixedness is a good indicator of idiomaticity, better than a simple measure of collocation ( ), or a measure of lexical fixedness.These results further suggest that looking into deep linguistic properties of VNICs is both necessary and beneficial for the appropriate treatment of these expressions. is known to perform poorly on low frequency data.To examine the effect of frequency on the measures, we analyze their performance on the two divisions of the test data, corresponding to the two frequency bands, TEST and TEST .Results are given in Table 3, with the best performance shown in boldface.As expected, the performance of drops substantially for low frequency items.Interestingly, although it is a PMI-based measure, performs slightly better when the data is separated based on frequency.The performance of improves quite a bit when it is applied to high frequency items, while it improves only slightly on the low frequency items.These results show that both Fixedness measures perform better on homogeneous data, while retaining comparably good performance on heterogeneous data.These results reflect that our fixedness measures are not as sensitive to frequency as .Hence they can be used with a higher degree of confidence, especially when applied to data that is heterogeneous with regard to frequency.This is important because while some VNICs are very common, others have very low frequency.Table 4 presents the performance of the hybrid measure, , repeating that of and for comparison. outperforms both lexical and syntactic fixedness measures, with a substantial improvement over , and a small, but notable, improvement over .Each of the lexical and syntactic fixedness measures is a good indicator of idiomaticity on its own, with syntactic fixedness being a better predictor.Here we demonstrate that combining them into a single measure of fixedness, while giving more weight to the better measure, results in a more effective predictor of idiomaticity.Our evaluation of the fixedness measures demonstrates their usefulness for the automatic recognition of idiomatic verb–noun pairs.To represent such pairs in a lexicon, however, we must determine their canonical form(s)—Cforms henceforth.For example, the lexical representation of shoot, breeze should include shoot the breeze as a Cform.Since VNICs are syntactically fixed, they are mostly expected to have a single Cform.Nonetheless, there are idioms with two or more acceptable forms.For example, hold fire and hold one’s fire are both listed in CCID as variations of the same idiom.Our approach should thus be capable of predicting all allowable forms for a given idiomatic verb–noun pair.We expect a VNIC to occur in its Cform(s) more frequently than it occurs in any other syntactic patterns.To discover the Cform(s) for a given idiomatic verb–noun pair, we thus examine its frequency of occurrence in each syntactic pattern in .Since it is possible for an idiom to have more than one Cform, we cannot simply take the most dominant pattern as the canonical one.Instead, we calculate a -score for the target pair and each pattern : in which is the mean and the standard deviation over the sample .The statistic indicates how far and in which direction the frequency of occurrence of the pair in pattern deviates from the sample’s mean, expressed in units of the sample’s standard deviation.To decide whether is a canonical pattern for the target pair, we check whether , where is a threshold.For evaluation, we set to , based on the distribution of and through examining the development data.We evaluate the appropriateness of this approach in determining the Cform(s) of idiomatic pairs by verifying its predicted forms against ODCIE and CCID.Specifically, for each of the idiomatic pairs in TEST , we calculate the precision and recall of its predicted Cforms (those whose -scores are above ), compared to the Cforms listed in the two dictionaries.The average precision across the 100 test pairs is 81.7%, and the average recall is 88.0% (with 69 of the pairs having 100% precision and 100% recall).Moreover, we find that for the overwhelming majority of the pairs, , the predicted Cform with the highest -score appears in the dictionary entry of the pair.Thus, our method of detecting Cforms performs quite well.The significance of the role idioms play in language has long been recognized.However, due to their peculiar behaviour, idioms have been mostly overlooked by the NLP community.Recently, there has been growing awareness of the importance of identifying non-compositional multiword expressions (MWEs).Nonetheless, most research on the topic has focused on compound nouns and verb particle constructions.Earlier work on idioms have only touched the surface of the problem, failing to propose explicit mechanisms for appropriately handling them.Here, we provide effective mechanisms for the treatment of a broadly documented and crosslinguistically frequent class of idioms, i.e., VNICs.Earlier research on the lexical encoding of idioms mainly relied on the existence of human annotations, especially for detecting which syntactic variations (e.g., passivization) an idiom can undergo (Villavicencio et al., 2004).We propose techniques for the automatic acquisition and encoding of knowledge about the lexicosyntactic behaviour of idiomatic combinations.We put forward a means for automatically discovering the set of syntactic variations that are tolerated by a VNIC and that should be included in its lexical representation.Moreover, we incorporate such information into statistical measures that effectively predict the idiomaticity level of a given expression.In this regard, our work relates to previous studies on determining the compositionality (inverse of idiomaticity) of MWEs other than idioms.Most previous work on compositionality of MWEs either treat them as collocations (Smadja, 1993), or examine the distributional similarity between the expression and its constituents (McCarthy et al., 2003; Baldwin et al., 2003; Bannard et al., 2003).Lin (1999) and Wermter and Hahn (2005) go one step further and look into a linguistic property of non-compositional compounds—their lexical fixedness—to identify them.Venkatapathy and Joshi (2005) combine aspects of the above-mentioned work, by incorporating lexical fixedness, collocation-based, and distributional similarity measures into a set of features which are used to rank verb+noun combinations according to their compositionality.Our work differs from such studies in that it carefully examines several linguistic properties of VNICs that distinguish them from literal (compositional) combinations.Moreover, we suggest novel techniques for translating such characteristics into measures that predict the idiomaticity level of verb+noun combinations.More specifically, we propose statistical measures that quantify the degree of lexical, syntactic, and overall fixedness of such combinations.We demonstrate that these measures can be successfully applied to the task of automatically distinguishing idiomatic combinations from non-idiomatic ones.We also show that our syntactic and overall fixedness measures substantially outperform a widely used measure of collocation, , even when the latter takes syntactic relations into account.Others have also drawn on the notion of syntactic fixedness for idiom detection, though specific to a highly constrained type of idiom (Widdows and Dorow, 2005).Our syntactic fixedness measure looks into a broader set of patterns associated with a large class of idiomatic expressions.Moreover, our approach is general and can be easily extended to other idiomatic combinations.Each measure we use to identify VNICs captures a different aspect of idiomaticity: reflects the statistical idiosyncrasy of VNICs, while the fixedness measures draw on their lexicosyntactic peculiarities.Our ongoing work focuses on combining these measures to distinguish VNICs from other idiosyncratic verb+noun combinations that are neither purely idiomatic nor completely literal, so that we can identify linguistically plausible classes of verb+noun combinations on this continuum (Fazly and Stevenson, 2005).
Exploiting Shallow Linguistic Information For Relation Extraction From Biomedical LiteratureWe propose an approach for extracting relations between entities from biomedical literature based solely on shallow linguistic information.We use a combination of kernel functions to integrate two different information sources: (i) the whole sentence where the relation appears, and (ii) the local contexts around the interacting entities.We performed experiments on extracting gene and protein interactions from two different data sets.The results show that our approach outperforms most of the previous methods based on syntactic and semantic information.Information Extraction (IE) is the process of finding relevant entities and their relationships within textual documents.Applications of IE range from Semantic Web to Bioinformatics.For example, there is an increasing interest in automatically extracting relevant information from biomedical literature.Recent evaluation campaigns on bio-entity recognition, such as BioCreAtIvE and JNLPBA 2004 shared task, have shown that several systems are able to achieve good performance (even if it is a bit worse than that reported on news articles).However, relation identification is more useful from an applicative perspective but it is still a considerable challenge for automatic tools.In this work, we propose a supervised machine learning approach to relation extraction which is applicable even when (deep) linguistic processing is not available or reliable.In particular, we explore a kernel-based approach based solely on shallow linguistic processing, such as tokenization, sentence splitting, Part-of-Speech (PoS) tagging and lemmatization.Kernel methods (Shawe-Taylor and Cristianini, 2004) show their full potential when an explicit computation of the feature map becomes computationally infeasible, due to the high or even infinite dimension of the feature space.For this reason, kernels have been recently used to develop innovative approaches to relation extraction based on syntactic information, in which the examples preserve their original representations (i.e. parse trees) and are compared by the kernel function (Zelenko et al., 2003; Culotta and Sorensen, 2004; Zhao and Grishman, 2005).Despite the positive results obtained exploiting syntactic information, we claim that there is still room for improvement relying exclusively on shallow linguistic information for two main reasons.First of all, previous comparative evaluations put more stress on the deep linguistic approaches and did not put as much effort on developing effective methods based on shallow linguistic information.A second reason concerns the fact that syntactic parsing is not always robust enough to deal with real-world sentences.This may prevent approaches based on syntactic features from producing any result.Another related issue concerns the fact that parsers are available only for few languages and may not produce reliable results when used on domain specific texts (as is the case of the biomedical literature).For example, most of the participants at the Learning Language in Logic (LLL) challenge on Genic Interaction Extraction (see Section 4.2) were unable to successfully exploit linguistic information provided by parsers.It is still an open issue whether the use of domainspecific treebanks (such as the Genia treebank1) can be successfully exploited to overcome this problem.Therefore it is essential to better investigate the potential of approaches based exclusively on simple linguistic features.In our approach we use a combination of kernel functions to represent two distinct information sources: the global context where entities appear and their local contexts.The whole sentence where the entities appear (global context) is used to discover the presence of a relation between two entities, similarly to what was done by Bunescu and Mooney (2005b).Windows of limited size around the entities (local contexts) provide useful clues to identify the roles of the entities within a relation.The approach has some resemblance with what was proposed by Roth and Yih (2002).The main difference is that we perform the extraction task in a single step via a combined kernel, while they used two separate classifiers to identify entities and relations and their output is later combined with a probabilistic global inference.We evaluated our relation extraction algorithm on two biomedical data sets (i.e. the AImed corpus and the LLL challenge data set; see Section 4).The motivations for using these benchmarks derive from the increasing applicative interest in tools able to extract relations between relevant entities in biomedical texts and, consequently, from the growing availability of annotated data sets.The experiments show clearly that our approach consistently improves previous results.Surprisingly, it outperforms most of the systems based on syntactic or semantic information, even when this information is manually annotated (i.e. the LLL challenge).The problem considered here is that of identifying interactions between genes and proteins from biomedical literature.More specifically, we performed experiments on two slightly different benchmark data sets (see Section 4 for a detailed description).In the former (AImed) gene/protein interactions are annotated without distinguishing the type and roles of the two interacting entities.The latter (LLL challenge) is more realistic (and complex) because it also aims at identifying the roles played by the interacting entities (agent and target).For example, in Figure 1 three entities are mentioned and two of the six ordered pairs of GENIA/topics/Corpus/GTB.html entities actually interact: (sigma(K), cwlH) and (gerE, cwlH).In our approach we cast relation extraction as a classification problem, in which examples are generated from sentences as follows.First of all, we describe the complex case, namely the protein/gene interactions (LLL challenge).For this data set entity recognition is performed using a dictionary of protein and gene names in which the type of the entities is unknown.We generate examples for all the sentences containing at least two entities.Thus the number of examples generated for each sentence is given by the combinations of distinct entities (N) selected two at a time, i.e.NC2.For example, as the sentence shown in Figure 1 contains three entities, the total number of examples generated is 3C2 = 3.In each example we assign the attribute CANDIDATE to each of the candidate interacting entities, while the other entities in the example are assigned the attribute OTHER, meaning that they do not participate in the relation.If a relation holds between the two candidate interacting entities the example is labeled 1 or 2 (according to the roles of the interacting entities, agent and target, i.e. to the direction of the relation); 0 otherwise.Figure 2 shows the examples generated from the sentence in Figure 1.Note that in generating the examples from the sentence in Figure 1 we did not create three negative examples (there are six potential ordered relations between three entities), thereby implicitly under-sampling the data set.This allows us to make the classification task simpler without loosing information.As a matter of fact, generating examples for each ordered pair of entities would produce two subsets of the same size containing similar examples (differing only for the attributes CANDIDATE and OTHER), but with different classification labels.Furthermore, under-sampling allows us to halve the data set size and reduce the data skewness.For the protein-protein interaction task (AImed) we use the correct entities provided by the manual annotation.As said at the beginning of this section, this task is simpler than the LLL challenge because there is no distinction between types (all entities are proteins) and roles (the relation is symmetric).As a consequence, the examples are generated as described above with the following difference: an example is labeled 1 if a relation holds between the two candidate interacting entities; 0 otherwise.The basic idea behind kernel methods is to embed the input data into a suitable feature space F via a mapping function 0 : X → F, and then use a linear algorithm for discovering nonlinear patterns.Instead of using the explicit mapping 0, we can use a kernel function K : X x X → R, that corresponds to the inner product in a feature space which is, in general, different from the input space.Kernel methods allow us to design a modular system, in which the kernel function acts as an interface between the data and the learning algorithm.Thus the kernel function is the only domain specific module of the system, while the learning algorithm is a general purpose component.Potentially any kernel function can work with any kernel-based algorithm.In our approach we use Support Vector Machines (Vapnik, 1998).In order to implement the approach based on shallow linguistic information we employed a linear combination of kernels.Different works (Gliozzo et al., 2005; Zhao and Grishman, 2005; Culotta and Sorensen, 2004) empirically demonstrate the effectiveness of combining kernels in this way, showing that the combined kernel always improves the performance of the individual ones.In addition, this formulation allows us to evaluate the individual contribution of each information source.We designed two families of kernels: Global Context kernels and Local Context kernels, in which each single kernel is explicitly calculated as follows where 0(·) is the embedding vector and II · II is the 2-norm.The kernel is normalized (divided) by the product of the norms of embedding vectors.The normalization factor plays an important role in allowing us to integrate information from heterogeneous feature spaces.Even though the resulting feature space has high dimensionality, an efficient computation of Equation 1 can be carried out explicitly since the input representations defined below are extremely sparse.In (Bunescu and Mooney, 2005b), the authors observed that a relation between two entities is generally expressed using only words that appear simultaneously in one of the following three patterns: Fore-Between: tokens before and between the two candidate interacting entities.For instance: binding of [P1] to [P2], interaction involving [P1] and [P2], association of [P1] by [P2].Between: only tokens between the two candidate interacting entities.For instance: [P1] associates with [P2], [P1] binding to [P2], [P1], inhibitor of [P2].Between-After: tokens between and after the two candidate interacting entities.For instance: [P1] - [P2] association, [P1] and [P2] interact, [P1] has influence on [P2] binding.Our global context kernels operate on the patterns above, where each pattern is represented using a bag-of-words instead of sparse subsequences of words, PoS tags, entity and chunk types, or WordNet synsets as in (Bunescu and Mooney, 2005b).More formally, given a relation example R, we represent a pattern P as a row vector where the function tf(ti, P) records how many times a particular token tz is used in P. Note that, this approach differs from the standard bag-ofwords as punctuation and stop words are included in OP, while the entities (with attribute CANDIDATE and OTHER) are not.To improve the classification performance, we have further extended OP to embed n-grams of (contiguous) tokens (up to n = 3).By substituting OP into Equation 1, we obtain the n-gram kernel Kn, which counts common uni-grams, bi-grams, ... , n-grams that two patterns have in common2.The Global Context kernel KGC(R1, R2) is then defined as where KFB, KB and KBA are n-gram kernels that operate on the Fore-Between, Between and Between-After patterns respectively.The type of the candidate interacting entities can provide useful clues for detecting the agent and target of the relation, as well as the presence of the relation itself.As the type is not known, we use the information provided by the two local contexts of the candidate interacting entities, called left and right local context respectively.As typically done in entity recognition, we represent each local context by using the following basic features: Token The token itself.Lemma The lemma of the token.PoS The PoS tag of the token.Orthographic This feature maps each token into equivalence classes that encode attributes such as capitalization, punctuation, numerals and so on.Formally, given a relation example R, a local context L = t_w, ... , t_1, t0, t+1, ... , t+w is represented as a row vector where fi is a feature function that returns 1 if it is active in the specified position of L, 0 otherwise3.The Local Context kernel KLC(R1, R2) is defined as where Kleft and Kright are defined by substituting the embedding of the left and right local context into Equation 1 respectively.Notice that KLC differs substantially from KGC as it considers the ordering of the tokens and the feature space is enriched with PoS, lemma and orthographic features.Finally, the Shallow Linguistic kernel It follows directly from the explicit construction of the feature space and from closure properties of kernels that KSL is a valid kernel.The two data sets used for the experiments concern the same domain (i.e. gene/protein interactions).However, they present a crucial difference which makes it worthwhile to show the experimental results on both of them.In one case (AImed) interactions are considered symmetric, while in the other (LLL challenge) agents and targets of genic interactions have to be identified.The first data set used in the experiments is the AImed corpus4, previously used for training protein interaction extraction systems in (Bunescu et al., 2005; Bunescu and Mooney, 2005b).It consists of 225 Medline abstracts: 200 are known to describe interactions between human proteins, while the other 25 do not refer to any interaction.There are 4,084 protein references and around 1,000 tagged interactions in this data set.In this data set there is no distinction between genes and proteins and the relations are symmetric.This data set was used in the Learning Language in Logic (LLL) challenge on Genic Interaction extraction5 (Ned´ellec, 2005).The objective of the challenge was to evaluate the performance of systems based on machine learning techniques to identify gene/protein interactions and their roles, agent or target.The data set was collected by querying Medline on Bacillus subtilis transcription and sporulation.It is divided in a training set (80 sentences describing 271 interactions) and a test set (87 sentences describing 106 interactions).Differently from the training set, the test set contains sentences without interactions.The data set is decomposed in two subsets of increasing difficulty.The first subset does not include coreferences, while the second one includes simple cases of coreference, mainly appositions.Both subsets are available with different kinds of annotation: basic and enriched.The former includes word and sentence segmentation.The latter also includes manually checked information, such as lemma and syntactic dependencies.A dictionary of named entities (including typographical variants and synonyms) is associated to the data set.Before describing the results of the experiments, a note concerning the evaluation methodology.There are different ways of evaluating performance in extracting information, as noted in (Lavelli et al., 2004) for the extraction of slot fillers in the Seminar Announcement and the Job Posting data sets.Adapting the proposed classification to relation extraction, the following two cases can be identified: Figure 3 shows a fragment of tagged text drawn from the AImed corpus.It contains three different interactions between pairs of proteins, for a total of seven occurrences of interactions.For example, there are three occurrences of the interaction between IGF-IR and p52Shc (i.e. number 1, 3 and 7).If we adopt the OAOD methodology, all the seven occurrences have to be extracted to achieve the maximum score.On the other hand, if we use the OARD methodology, only one occurrence for each interaction has to be extracted to maximize the score.On the AImed data set both evaluations were performed, while on the LLL challenge only the OAOD evaluation methodology was performed because this is the only one provided by the evaluation server of the challenge.Figure 3: Fragment of the AImed corpus with all proteins and their interactions tagged.The protein names have been highlighted in bold face and their same subscript numbers indicate interaction between the proteins.All the experiments were performed using the SVM package LIBSVM6 customized to embed our own kernel.For the LLL challenge submission, we optimized the regularization parameter C by 10-fold cross validation; while we used its default value for the AImed experiment.In both experiments, we set the cost-factor WZ to be the ratio between the number of negative and positive examples.KSL performance was first evaluated on the AImed data set (Section 4.1).We first give an evaluation of the kernel combination and then we compare our results with the Subsequence Kernel for Relation Extraction (ERK) described in (Bunescu and Mooney, 2005b).All experiments are conducted using 10-fold cross validation on the same data splitting used in (Bunescu et al., 2005; Bunescu and Mooney, 2005b).Table 1 shows the performance of the three kernels defined in Section 3 for protein-protein interactions using the two evaluation methodologies described above.We report in Figure 4 the precision-recall curves of ERK and KSL using OARD evaluation methodology (the evaluation performed by Bunescu and Mooney (2005b)).As in (Bunescu et al., 2005; Bunescu and Mooney, 2005b), the graph points are obtained by varying the threshold on the classifiFinally, Figure 5 shows the learning curve of the combined kernel KSL using the OARD evaluation methodology.The curve reaches a plateau with around 100 Medline abstracts.The system was evaluated on the “basic” version of the LLL challenge data set (Section 4.2).Table 2 shows the results of KSL returned by the scoring service8 for the three subsets of the training set (with and without coreferences, and with their union).Table 3 shows the best results obtained at the official competition performed in April 2005.Comparing the results we see that KSL trained on each subset outperforms the best systems of the LLL challenge9.Notice that the best results at the challenge were obtained by different groups and exploiting the linguistic “enriched” version of the data set.As observed in (Ned´ellec, 2005), the scores obtained using the training set without coreferences and the whole training set are similar.We also report in Table 4 an analysis of the kernel combination.Given that we are interested here in the contribution of each kernel, we evaluated the experiments by 10-fold cross-validation on the whole training set avoiding the submission process.The experimental results show that the combined kernel KSL outperforms the basic kernels KGC and KLC on both data sets.In particular, precision significantly increases at the expense of a lower recall.High precision is particularly advantageous when extracting knowledge from large corpora, because it avoids overloading end users with too many false positives.Although the basic kernels were designed to model complementary aspects of the task (i.e.9After the challenge deadline, Reidel and Klein (2005) achieved a significant improvement, Fl = 68.4% (without coreferences) and Fl = 64.7% (with and without coreferences). presence of the relation and roles of the interacting entities), they perform reasonably well even when considered separately.In particular, KGC achieved good performance on both data sets.This result was not expected on the LLL challenge because this task requires not only to recognize the presence of relationships between entities but also to identify their roles.On the other hand, the outcomes of KLC on the AImed data set show that such kernel helps to identify the presence of relationships as well.At first glance, it may seem strange that KGC outperforms ERK on AImed, as the latter approach exploits a richer representation: sparse sub-sequences of words, PoS tags, entity and chunk types, or WordNet synsets.However, an approach based on n-grams is sufficient to identify the presence of a relationship.This result sounds less surprising, if we recall that both approaches cast the relation extraction problem as a text categorization task.Approaches to text categorization based on rich linguistic information have obtained less accuracy than the traditional bag-of-words approach (e.g.(Koster and Seutter, 2003)).Shallow linguistics information seems to be more effective to model the local context of the entities.Finally, we obtained worse results performing dimensionality reduction either based on generic linguistic assumptions (e.g. by removing words from stop lists or with certain PoS tags) or using statistical methods (e.g. tf.idf weighting schema).This may be explained by the fact that, in tasks like entity recognition and relation extraction, useful clues are also provided by high frequency tokens, such as stop words or punctuation marks, and by the relative positions in which they appear.First of all, the obvious references for our work are the approaches evaluated on AImed and LLL challenge data sets.In (Bunescu and Mooney, 2005b), the authors present a generalized subsequence kernel that works with sparse sequences containing combinations of words and PoS tags.The best results on the LLL challenge were obtained by the group from the University of Edinburgh (Reidel and Klein, 2005), which used Markov Logic, a framework that combines loglinear models and First Order Logic, to create a set of weighted clauses which can classify pairs of gene named entities as genic interactions.These clauses are based on chains of syntactic and semantic relations in the parse or Discourse Representation Structure (DRS) of a sentence, respectively.Other relevant approaches include those that adopt kernel methods to perform relation extraction.Zelenko et al. (2003) describe a relation extraction algorithm that uses a tree kernel defined over a shallow parse tree representation of sentences.The approach is vulnerable to unrecoverable parsing errors.Culotta and Sorensen (2004) describe a slightly generalized version of this kernel based on dependency trees, in which a bag-ofwords kernel is used to compensate for errors in syntactic analysis.A further extension is proposed by Zhao and Grishman (2005).They use composite kernels to integrate information from different syntactic sources (tokenization, sentence parsing, and deep dependency analysis) so that processing errors occurring at one level may be overcome by information from other levels.Bunescu and Mooney (2005a) present an alternative approach which uses information concentrated in the shortest path in the dependency tree between the two entities.As mentioned in Section 1, another relevant approach is presented in (Roth and Yih, 2002).Classifiers that identify entities and relations among them are first learned from local information in the sentence.This information, along with constraints induced among entity types and relations, is used to perform global probabilistic inference that accounts for the mutual dependencies among the entities.All the previous approaches have been evaluated on different data sets so that it is not possible to have a clear idea of which approach is better than the other.The good results obtained using only shallow linguistic features provide a higher baseline against which it is possible to measure improvements obtained using methods based on deep linguistic processing.In the near future, we plan to extend our work in several ways.First, we would like to evaluate the contribution of syntactic information to relation extraction from biomedical literature.With this aim, we will integrate the output of a parser (possibly trained on a domain-specific resource such the Genia Treebank).Second, we plan to test the portability of our model on ACE and MUC data sets.Third, we would like to use a named entity recognizer instead of assuming that entities are already extracted or given by a dictionary.Our long term goal is to populate databases and ontologies by extracting information from large text collections such as Medline.We would like to thank Razvan Bunescu for providing detailed information about the AImed data set and the settings of the experiments.Claudio Giuliano and Lorenza Romano have been supported by the ONTOTEXT project, funded by the Autonomous Province of Trento under the FUP2004 research program.
Bayesian Word Sense InductionSense induction seeks to automatically identify word senses directly from a corpus.A key assumption underlying previous work is that the context surrounding an ambiguous word is indicative of its meaning.Sense induction is thus typically viewed as an unsupervised clustering problem where the aim is to partition a word’s contexts into different classes, each representing a word sense.Our work places sense induction in a Bayesian context by modeling the contexts of the ambiguous word as samples from a multinomial distribution over senses which are in turn characterized as distributions over words.The Bayesian framework provides a principled way to incorporate a wide range of features beyond lexical cooccurrences and to systematically assess their utility on the sense induction task.The proposed approach yields improvements over state-of-the-art systems on a benchmark dataset.Sense induction is the task of discovering automatically all possible senses of an ambiguous word.It is related to, but distinct from, word sense disambiguation (WSD) where the senses are assumed to be known and the aim is to identify the intended meaning of the ambiguous word in context.Although the bulk of previous work has been devoted to the disambiguation problem1, there are good reasons to believe that sense induction may be able to overcome some of the issues associated with WSD.Since most disambiguation methods assign senses according to, and with the aid of, dictionaries or other lexical resources, it is difficult to adapt them to new domains or to languages where such resources are scarce.A related problem concerns the granularity of the sense distinctions which is fixed, and may not be entirely suitable for different applications.In contrast, when sense distinctions are inferred directly from the data, they are more likely to represent the task and domain at hand.There is little risk that an important sense will be left out, or that irrelevant senses will influence the results.Furthermore, recent work in machine translation (Vickrey et al., 2005) and information retrieval (V´eronis, 2004) indicates that induced senses can lead to improved performance in areas where methods based on a fixed sense inventory have previously failed (Carpuat and Wu, 2005; Voorhees, 1993).Sense induction is typically treated as an unsupervised clustering problem.The input to the clustering algorithm are instances of the ambiguous word with their accompanying contexts (represented by co-occurrence vectors) and the output is a grouping of these instances into classes corresponding to the induced senses.In other words, contexts that are grouped together in the same class represent a specific word sense.In this paper we adopt a novel Bayesian approach and formalize the induction problem in a generative model.For each ambiguous word we first draw a distribution over senses, and then generate context words according to this distribution.It is thus assumed that different senses will correspond to distinct lexical distributions.In this framework, sense distinctions arise naturally through the generative process: our model postulates that the observed data (word contexts) are explicitly intended to communicate a latent structure (their meaning).Our work is related to Latent Dirichlet Allocation (LDA, Blei et al. 2003), a probabilistic model of text generation.LDA models each document using a mixture over K topics, which are in turn characterized as distributions over words.The words in the document are generated by repeatedly sampling a topic according to the topic distribution, and selecting a word given the chosen topic.Whereas LDA generates words from global topics corresponding to the whole document, our model generates words from local topics chosen based on a context window around the ambiguous word.Document-level topics resemble general domain labels (e.g., finance, education) and cannot faithfully model more fine-grained meaning distinctions.In our work, therefore, we create an individual model for every (ambiguous) word rather than a global model for an entire document collection.We also show how multiple information sources can be straightforwardly integrated without changing the underlying probabilistic model.For instance, besides lexical information we may want to consider parts of speech or dependencies in our sense induction problem.This is in marked contrast with previous LDA-based models which mostly take only word-based information into account.We evaluate our model on a recently released benchmark dataset (Agirre and Soroa, 2007) and demonstrate improvements over the state-of-the-art.The remainder of this paper is structured as follows.We first present an overview of related work (Section 2) and then describe our Bayesian model in more detail (Sections 3 and 4).Section 5 describes the resources and evaluation methodology used in our experiments.We discuss our results in Section 6, and conclude in Section 7.Sense induction is typically treated as a clustering problem, where instances of a target word are partitioned into classes by considering their co-occurring contexts.Considerable latitude is allowed in selecting and representing the cooccurring contexts.Previous methods have used first or second order co-occurrences (Purandare and Pedersen, 2004; Sch¨utze, 1998), parts of speech (Purandare and Pedersen, 2004), and grammatical relations (Pantel and Lin, 2002; Dorow and Widdows, 2003).The size of the context window also varies, it can be a relatively small, such as two words before and after the target word (Gauch and Futrelle, 1993), the sentence within which the target is found (Bordag, 2006), or even larger, such as the 20 surrounding words on either side of the target (Purandare and Pedersen, 2004).In essence, each instance of a target word is represented as a feature vector which subsequently serves as input to the chosen clustering method.A variety of clustering algorithms have been employed ranging from k-means (Purandare and Pedersen, 2004), to agglomerative clustering (Sch¨utze, 1998), and the Information Bottleneck (Niu et al., 2007).Graph-based methods have also been applied to the sense induction task.In this framework words are represented as nodes in the graph and vertices are drawn between the target and its co-occurrences.Senses are induced by identifying highly dense subgraphs (hubs) in the co-occurrence graph (V´eronis, 2004; Dorow and Widdows, 2003).Although LDA was originally developed as a generative topic model, it has recently gained popularity in the WSD literature.The inferred document-level topics can help determine coarsegrained sense distinctions.Cai et al. (2007) propose to use LDA’s word-topic distributions as features for training a supervised WSD system.In a similar vein, Boyd-Graber and Blei (2007) infer LDA topics from a large corpus, however for unsupervised WSD.Here, LDA topics are integrated with McCarthy et al.’s (2004) algorithm.For each target word, a topic is sampled from the document’s topic distribution, and a word is generated from that topic.Also, a distributional neighbor is selected based on the topic and distributional similarity to the generated word.Then, the word sense is selected based on the word, neighbor, and topic.Boyd-Graber et al. (2007) extend the topic modeling framework to include WordNet senses as a latent variable in the word generation process.In this case the model discovers both the topics of the corpus and the senses assigned to each of its words.Our own model is also inspired by LDA but crucially performs word sense induction, not disambiguation.Unlike the work mentioned above, we do not rely on a pre-existing list of senses, and do not assume a correspondence between our automatically derived sense-clusters and those of any given inventory.2 A key element in these previous attempts at adapting LDA for WSD is the tendency to remain at a high level, document-like, setting.In contrast, we make use of much smaller units of text (a few sentences, rather than a full document), and create an individual model for each (ambiguous) word type.Our induced senses are few in number (typically less than ten).This is in marked contrast to tens, and sometimes hundreds, of topics commonly used in document-modeling tasks.Unlike many conventional clustering methods (e.g., Purandare and Pedersen 2004; Sch¨utze 1998), our model is probabilistic; it specifies a probability distribution over possible values, which makes it easy to integrate and combine with other systems via mixture or product models.Furthermore, the Bayesian framework allows the incorporation of several information sources in a principled manner.Our model can easily handle an arbitrary number of feature classes (e.g., parts of speech, dependencies).This functionality in turn enables us to evaluate which linguistic information matters for the sense induction task.Previous attempts to handle multiple information sources in the LDA framework (e.g., Griffiths et al. 2005; Barnard et al.2003) have been task-specific and limited to only two layers of information.Our model provides this utility in a general framework, and could be applied to other tasks, besides sense induction.The core idea behind sense induction is that contextual information provides important cues regarding a word’s meaning.The idea dates back to (at least) Firth (1957) (“You shall know a word by the company it keeps”), and underlies most WSD and lexicon acquisition work to date.Under this premise, we should expect different senses to be signaled by different lexical distributions.We can place sense induction in a probabilistic setting by modeling the context words around the ambiguous target as samples from a multinomial sense distribution.More formally, we will write P(s) for the distribution over senses s of an ambiguous target in a specific context window and P(w1s) for the probability distribution over context words w given sense s. Each word wi in the context window is generated by first sampling a sense from the sense distribution, then choosing a word from the sense-context distribution.P(si = j) denotes the probability that the jth sense was sampled for the ith word token and P(wi|si = j) the probability of context word wi under sense j.The model thus specifies a distribution over words within a context window: where S is the number of senses.We assume that each target word has C contexts and each context c cate conditional dependencies between variables, whereas plates (the rectangles in the figure) refer to repetitions of sampling steps.The variables in the lower right corner refer to the number of samples. consists of Nc word tokens.We shall write �(j) as a shorthand for P(wi|si = j), the multinomial distribution over words for sense j, and 0(c) as a shorthand for the distribution of senses in context c. Following Blei et al. (2003) we will assume that the mixing proportion over senses 0 is drawn from a Dirichlet prior with parameters a.The role of the hyperparameter a is to create a smoothed sense distribution.We also place a symmetric Dirichlet R on � (Griffiths and Steyvers, 2002).The hyperparmeter R can be interpreted as the prior observation count on the number of times context words are sampled from a sense before any word from the corpus is observed.Our model is represented in graphical notation in Figure 1.The model sketched above only takes word information into account.Methods developed for supervised WSD often use a variety of information sources based not only on words but also on lemmas, parts of speech, collocations and syntactic relationships (Lee and Ng, 2002).The first idea that comes to mind, is to use the same model while treating various features as word-like elements.In other words, we could simply assume that the contexts we wish to model are the union of all our features.Although straightforward, this solution is undesirable.It merges the distributions of distinct feature categories into a single one, and is therefore conceptually incorrect, and can affect the performance of the model.For instance, parts-ofspeech (which have few values, and therefore high probability), would share a distribution with words (which are much sparser).Layers containing more elements (e.g.10 word window) would overwhelm rectangles represent different sources (layers) of information.All layers share the same, instancespecific, sense distribution (0), but each have their own (multinomial) sense-feature distribution (�).Shaded nodes represent observed features f; these can be words, parts of speech, collocations or dependencies. unconditional joint distribution P(s) of the unobserved variables (provided certain criteria are fulfilled).In our model, each element in each layer is a variable, and is assigned a sense label (see Figure 2, where distinct layers correspond to different representations of the context around the target word).From these assignments, we must determine the sense distribution of the instance as a whole.This is the purpose of the Gibbs sampling procedure.Specifically, in order to derive the update function used in the Gibbs sampler, we must provide the conditional probability of the i-th variable being assigned sense si in layer l, given the feature value fi of the context variable and the current sense assignments of all the other variables in the data (s−i): p(si|s−i, f) — p(fi|s, f −i,R) · p(si|s−i,a) (2) The probability of a single sense assignment, si, is proportional to the product of the likelihood (of feature fi, given the rest of the data) and the prior probability of the assignment. smaller ones (e.g.1 word window).Our solution is to treat each information source (or feature type) individually and then combine all of them together in a unified model.Our underlying assumption is that the context window around the target word can have multiple representations, all of which share the same sense distribution.We illustrate this in Figure 2 where each inner rectangle (layer) corresponds to a distinct feature type.We will naively assume independence between multiple layers, even though this is clearly not the case in our task.The idea here is to model each layer as faithfully as possible to the empirical data while at the same time combining information from all layers in estimating the sense distribution of each target instance.Our inference procedure is based on Gibbs sampling (Geman and Geman, 1984).The procedure begins by randomly initializing all unobserved random variables.At each iteration, each random variable si is sampled from the conditional distribution P(si|s−i) where s−i refers to all variables other than si.Eventually, the distribution over samples drawn from this process will converge to the f p(fi |l, S, 0) · p(O |f-i, Rt)dO _ #(fi, si) + Rl For the likelihood term p(fi|s, f −i,R), integrating over all possible values of the multinomial featuresense distribution � gives us the rightmost term in Equation 3, which has an intuitive interpretation.The term #(fi,si) indicates the number of times the feature-value fi was assigned sense si in the rest of the data.Similarly, #(si) indicates the number of times the sense assignment si was observed in the data.Rl is the Dirichlet prior for the featuresense distribution � in the current layer l, and Vl is the size of the vocabulary of that layer, i.e., the number of possible feature values in the layer.Intuitively, the probability of a feature-value given a sense is directly proportional to the number of times we have seen that value and that senseassignment together in the data, taking into account a pseudo-count prior, expressed through R. This can also be viewed as a form of smoothing.A similar approach is taken with regards to the prior probability p(si|s−i,a).In this case, however, all layers must be considered: Here λl is the weight for the contribution of layer l, and αl is the portion of the Dirichlet prior for the sense distribution θ in the current layer.Treating each layer individually, we integrate over the possible values of θ, obtaining a similar count-based term: where #l(si) indicates the number of elements in layer l assigned the sense si, #l indicates the number of elements in layer l, i.e., the size of the layer and S the number of senses.To distribute the pseudo counts represented by α in a reasonable fashion among the layers, we define αl = #l #m · α where #m = ∑l #l, i.e., the total size of the instance.This distributes α according to the relative size of each layer in the instance.Placing these values in Equation 4 we obtain the following: #m+S·α Putting it all together, we arrive at the final update equation for the Gibbs sampling: Note that when dealing with a single layer, Equation 8 collapses to: where #m(si) indicates the number of elements (e.g., words) in the context window assigned to sense si.This is identical to the update equation in the original, word-based LDA model.The sampling algorithm gives direct estimates of s for every context element.However, in view of our task, we are more interested in estimating θ, the sense-context distribution which can be obtained as in Equation 7, but taking into account all sense assignments, without removing assignment i.Our system labels each instance with the single, most probable sense.In this section we discuss our experimental set-up for assessing the performance of the model presented above.We give details on our training procedure, describe our features, and explain how our system output was evaluated.Data In this work, we focus solely on inducing senses for nouns, since they constitute the largest portion of content words.For example, nouns represent 45% of the content words in the British National Corpus.Moreover, for many tasks and applications (e.g., web queries, Jansen et al. 2000) nouns are the most frequent and most important part-of-speech.For evaluation, we used the Semeval-2007 benchmark dataset released as part of the sense induction and discrimination task (Agirre and Soroa, 2007).The dataset contains texts from the Penn Treebank II corpus, a collection of articles from the first half of the 1989 Wall Street Journal (WSJ).It is hand-annotated with OntoNotes senses (Hovy et al., 2006) and has 35 nouns.The average noun ambiguity is 3.9, with a high (almost 80%) skew towards the predominant sense.This is not entirely surprising since OntoNotes senses are less fine-grained than WordNet senses.We used two corpora for training as we wanted to evaluate our model’s performance across different domains.The British National Corpus (BNC) is a 100 million word collection of samples of written and spoken language from a wide range of sources including newspapers, magazines, books (both academic and fiction), letters, and school essays as well as spontaneous conversations.This served as our out-of-domain corpus, and contained approximately 730 thousand instances of the 35 target nouns in the Semeval lexical sample.The second, in-domain, corpus was built from selected portions of the Wall Street Journal.We used all articles (excluding the Penn Treebank II portion used in the Semeval dataset) from the years 1987-89 and 1994 to create a corpus of similar size to the BNC, containing approximately 740 thousand instances of the target words.Additionally, we used the Senseval 2 and 3 lexical sample data (Preiss and Yarowsky, 2001; Mihalcea and Edmonds, 2004) as development sets, for experimenting with the hyper-parameters of our model (see Section 6).Evaluation Methodology Agirre and Soroa (2007) present two evaluation schemes for assessing sense induction methods.Under the first scheme, the system output is compared to the gold standard using standard clustering evaluation metrics (e.g., purity, entropy).Here, no attempt is made to match the induced senses against the labels of the gold standard.Under the second scheme, the gold standard is partitioned into a test and training corpus.The latter is used to derive a mapping of the induced senses to the gold standard labels.The mapping is then used to calculate the system’s F-Score on the test corpus.Unfortunately, the first scheme failed to discriminate among participating systems.The onecluster-per-word baseline outperformed all systems, except one, which was only marginally better.The scheme ignores the actual labeling and due to the dominance of the first sense in the data, encourages a single-sense approach which is further amplified by the use of a coarse-grained sense inventory.For the purposes of this work, therefore, we focused on the second evaluation scheme.Here, most of the participating systems outperformed the most-frequent-sense baseline, and the rest obtained only slightly lower scores.Feature Space Our experiments used a feature set designed to capture both immediate local context, wider context and syntactic context.Specifically, we experimented with six feature categories: ±10-word window (10w), ±5-word window (5w), collocations (1w), word n-grams (ng), part-ofspeech n-grams (pg) and dependency relations (dp).These features have been widely adopted in various WSD algorithms (see Lee and Ng 2002 for a detailed evaluation).In all cases, we use the lemmatized version of the word(s).The Semeval workshop organizers provided a small amount of context for each instance (usually a sentence or two surrounding the sentence containing the target word).This context, as well as the text in the training corpora, was parsed using RASP (Briscoe and Carroll, 2002), to extract part-of-speech tags, lemmas, and dependency information.For instances containing more than one occurrence of the target word, we disambiguate the first occurrence.Instances which were not correctly recognized by the parser (e.g., a target word labeled with the wrong lemma or part-of-speech), were automatically assigned to the largest sensecluster.3Model Selection The framework presented in Section 3 affords great flexibility in modeling the empirical data.This however entails that several parameters must be instantiated.More precisely, our model is conditioned on the Dirichlet hyperparameters α and β and the number of senses S. Additional parameters include the number of iterations for the Gibbs sampler and whether or not the layers are assigned different weights.Our strategy in this paper is to fix α and β and explore the consequences of varying S. The value for the α hyperparameter was set to 0.02.This was optimized in an independent tuning experiment which used the Senseval 2 (Preiss and Yarowsky, 2001) and Senseval 3 (Mihalcea and Edmonds, 2004) datasets.We experimented with α values ranging from 0.005 to 1.The β parameter was set to 0.1 (in all layers).This value is often considered optimal in LDA-related models (Griffiths and Steyvers, 2002).For simplicity, we used uniform weights for the layers.The Gibbs sampler was run for 2,000 iterations.Due to the randomized nature of the inference procedure, all reported results are average scores over ten runs.Our experiments used the same number of senses for all the words, since tuning this number individually for each word would be prohibitive.We experimented with values ranging from three to nine senses.Figure 3 shows the results obtained for different numbers of senses when the model is trained on the WSJ (in-domain) and BNC (out-ofdomain) corpora, respectively.Here, we are using the optimal combination of layers for each system (which we discuss in the following section in detail).For the model trained on WSJ, performance peaks at four senses, which is similar to the average ambiguity in the test data.For the model trained on the BNC, however, the best results are obtained using twice as many senses.Using fewer senses with the BNC-trained system can result in a drop in accuracy of almost 2%.This is due to the shift in domain.As the sense-divisions of the learning domain do not match those of the target domain, finer granularity is required in order to encompass all the relevant distinctions.Table 1 illustrates the senses inferred for the word drug when using the in-domain and out-ofdomain corpora, respectively.The most probable words for each sense are also shown.Firstly, note that the model infers some plausible senses for drug on the WSJ corpus (top half of Table 1).Sense 1 corresponds to the “enforcement” sense of drug, Sense 2 refers to “medication”, Sense 3 to the “drug industry” and Sense 4 to “drugs research”.The inferred senses for drug on the BNC (bottom half of Table 1) are more fine grained.For example, the model finds distinct senses for “medication” (Sense 1 and 7) and “illegal substance” (Senses 2, 4, 6, 7).It also finds a separate sense for “drug dealing” (Sense 5) and “enforcement” (Sense 8).Because the BNC has a broader focus, finer distinctions are needed to cover as many senses as possible that are relevant to the target domain (WSJ).Layer Analysis We next examine which individual feature categories are most informative in our sense induction task.We also investigate whether their combination, through our layered model (see Figure 2), yields performance improvements.We used 4 senses for the system trained on WSJ and 8 for the system trained on the BNC (a was set to 0.02 and b to 0.1) Table 2 (left side) shows the performance of our model when using only one layer.The layer composed of words co-occurring within a ±10-word window (10w), and representing wider, topical, information gives the highest scores on its own.It is followed by the ±5 (5w) and ±1 (1w) word windows, which represent more immediate, local context.Part-of-speech n-grams (pg) and word ngrams (ng), on their own, achieve lower scores, largely due to over-generalization and data sparseness, respectively.The lowest-scoring single layer is the dependency layer (dp), with performance only slightly above the most-frequent-sense baseline (MFS).Dependency information is very informative when present, but extremely sparse.Table 2 (middle) also shows the results obtained when running the layered model with all but one of the layers as input.We can use this information to determine the contribution of each layer by comparing to the combined model with all layers (all).Because we are dealing with multiple layers, there is an element of overlap involved.Therefore, each of the word-window layers, despite relatively high informativeness on its own, does not cause as much damage when it is absent, since the other layers compensate for the topical and local information.The absence of the word n-gram layer, which provides specific local information, does not make a great impact when the 1w and pg layers are present.Finally, we can see that the extremely sparse dependency layer is detrimental to the multi-layer model as a whole, and its removal increases performance.The sparsity of the data in this layer means that there is often little information on which to base a decision.In these cases, the layer contributes a close-to-uniform estimation of the sense distribution, which confuses the combined model.Other layer combinations obtained similar results.Table 2 (right side) shows the most informative two and three layer combinations.Again, dependencies tend to decrease performance.On the other hand, combining features that have similar performance on their own is beneficial.We obtain the best performance overall with a two layered model combining topical (+10w) and local (+5w) contexts.Table 3 replicates the same suite of experiments on the BNC corpus.The general trends are similar.Some interesting differences are apparent, however.The sparser layers, notably word n-grams and dependencies, fare comparatively worse.This is expected, since the more precise, local, information is likely to vary strongly across domains.Even when both domains refer to the same sense of a word, it is likely to be used in a different immediate context, and local contextual information learned in one domain will be less effective in the other.Another observable difference is that the combined model without the dependency layer does slightly better than each of the single layers.The 1w+pg combination improves over its components, which have similar individual performance.Finally, the best performing model on the BNC also combines two layers capturing wider (10w) and more local (5w) contextual information (see Table 3, right side).Comparison to State-of-the-Art Table 4 compares our model against the two best performing sense induction systems that participated in the Semeval-2007 competition.IR2 (Niu et al., 2007) performed sense induction using the Information Bottleneck algorithm, whereas UMND2 (Pedersen, 2007) used k-means to cluster second order co-occurrence vectors associated with the target word.These models and our own model significantly outperform the most-frequent-sense baseline (p < 0.01 using a x2 test).Our best system (10w+5w on WSJ) is significantly better than UMND2 (p < 0.01) and quantitatively better than IR2, although the difference is not statistically significant.This paper presents a novel Bayesian approach to sense induction.We formulated sense induction in a generative framework that describes how the contexts surrounding an ambiguous word might be generated on the basis of latent variables.Our model incorporates features based on lexical information, parts of speech, and dependencies in a principled manner, and outperforms state-of-theart systems.Crucially, the approach is not specific to the sense induction task and can be adapted for other applications where it is desirable to take multiple levels of information into account.For example, in document classification, one could consider an accompanying image and its caption as possible additional layers to the main text.In the future, we hope to explore more rigorous parameter estimation techniques.Goldwater and Griffiths (2007) describe a method for integrating hyperparameter estimation into the Gibbs sampling procedure using a prior over possible values.Such an approach could be adopted in our framework, as well, and extended to include the layer weighting parameters, which have strong potential for improving the model’s performance.In addition, we could allow an infinite number of senses and use an infinite Dirichlet model (Teh et al., 2006) to automatically determine how many senses are optimal.This provides an elegant solution to the model-order problem, and eliminates the need for external cluster-validation methods.Acknowledgments The authors acknowledge the support of EPSRC (grant EP/C538447/1).We are grateful to Sharon Goldwater for her feedback on earlier versions of this work.
A Compression-Based Algorithm For Chinese Word SegmentationChinese is written without using spaces or other word delimiters.Although a text may be thought of as a corresponding sequence of words, there is considerable ambiguity in the placement of boundaries.Interpreting a text as a sequence of words is beneficial for some information retrieval and storage tasks: for example, full-text search, word-based compression, and key phrase extraction.We describe a scheme that infers appropriate positions for word boundaries using an adaptive language model that is standard in text compression.It is trained on a corpus of presegmented text, and when applied to new text, interpolates word boundaries so as to maximize the compression obtained.This simple and general method performs well with respect to specialized schemes for Chinese language segmentation.Chinese is written without using spaces or other word delimiters.Although a text may be thought of as a corresponding sequence of words, there is considerable ambiguity in the placement of boundaries.Interpreting a text as a sequence of words is beneficial for some information retrieval and storage tasks: for example, full-text search, word-based compression, and key phrase extraction.We describe a scheme that infers appropriate positions for word boundaries using an adaptive language model that is standard in text compression.It is trained on a corpus of presegmented text, and when applied to new text, interpolates word boundaries so as to maximize the compression obtained.This simple and general method performs well with respect to specialized schemes for Chinese language segmentation.Languages such as Chinese and Japanese are written without using any spaces or other word delimiters (except for punctuation marks)—indeed, the Western notion of a word boundary is literally alien (Wu 1998).Nevertheless, words are present in these languages, and Chinese words often comprise several characters, typically two, three, or four—five-character words also exist, but they are rare.Many characters can stand alone as words in themselves, while on other occasions the same character is the first or second character of a two-character word, and on still others it participates as a component of a three- or four-character word.This phenomenon causes obvious ambiguities in word segmentation.Readers unfamiliar with Chinese can gain an appreciation of the problem of multiple interpretations from Figure 1, which shows two alternative interpretations of the same Chinese character sequence.The text is a joke that relies on the ambiguity of phrasing.Once upon a time, the story goes, a man set out on a long journey.Before he could return home the rainy season began, and he had to take shelter at a friend's house.But he overstayed his welcome, and one day his friend wrote him a note: the first line in Figure 1.The intended interpretation is shown in the second line, which means &quot;It is raining, the god would like the guest to stay.Although the god wants you to stay, I do not!&quot; On seeing the note, the visitor took the hint and prepared to leave.As a joke he amended the note with the punctuation shown in the third line, which leaves three sentences whose meaning is totally different—&quot;The rainy day, the staying day.Would you like me to stay?Sure!&quot; Example of treating each character in a query as a word.This example relies on ambiguity of phrasing, but the same kind of problem can arise with word segmentation.Figure 2 shows a more prosaic example.For the ordinary sentence of the first line, there are two different interpretations depending on the context of the sentence: &quot;I like New Zealand flowers&quot; and &quot;I like fresh broccoli&quot; respectively.The fact that machine-readable Chinese text is invariably stored in unsegmented form causes difficulty in applications that use the word as the basic unit.For example, search engines index documents by storing a list of the words they contain, and allow the user to retrieve all documents that contain a specified combination of query terms.This presupposes that the documents are segmented into words.Failure to do so, and treating every character as a word in itself, greatly decreases the precision of retrieval since large numbers of extraneous documents are returned that contain characters, but not words, from the query.Figure 3 illustrates what happens when each character in a query is treated as a single-character word.The intended query is &quot;physics&quot; or &quot;physicist.&quot; The first character returns documents about such things as &quot;evidence,&quot; &quot;products,&quot; &quot;body,&quot; &quot;image,&quot; &quot;prices&quot;; while the second returns documents about &quot;theory,&quot; &quot;barber,&quot; and so on.Thus many documents that are completely irrelevant to the query will be returned, causing the precision of information retrieval to decrease greatly.Similar problems occur in word-based compression, speech recognition, and so on.It is true that most search engines allow the user to search for multiword phrases by enclosing them in quotation marks, and this facility could be used to search for multicharacter words in Chinese.This, however, runs the risk of retrieving irrelevant documents in which the same characters occur in sequence but with a different intended segmentation.More importantly, it imposes on the user an artificial requirement to perform manual segmentation on each full-text query.Word segmentation is an important prerequisite for such applications.However, it is a difficult and ill-defined task.According to Sproat et al. (1996) and Wu and Fung (1994), experiments show that only about 75% agreement between native speakers is to be expected on the &quot;correct&quot; segmentation, and the figure reduces as more people become involved.This paper describes a general scheme for segmenting text by inferring the position of word boundaries, thus supplying a necessary preprocessing step for applications like those mentioned above.Unlike other approaches, which involve a dictionary of legal words and are therefore language-specific, it works by using a corpus of alreadysegmented text for training and thus can easily be retargeted for any language for which a suitable corpus of segmented material is available.To infer word boundaries, a general adaptive text compression technique is used that predicts upcoming characters on the basis of their preceding context.Spaces are inserted into positions where their presence enables the text to be compressed more effectively.This approach means that we can capitalize on existing research in text compression to create good models for word segmentation.To build a segmenter for a new language, the only resource required is a corpus of segmented text to train the compression model.The structure of this paper is as follows: The next section reviews previous work on the Chinese segmentation problem.Then we explain the operation of the adaptive text compression technique that will be used to predict word boundaries.Next we show how space insertion can be viewed as a problem of hidden Markov modeling, and how higher-order models, such as the ones used in text compression, can be employed in this way.The following section describes several experiments designed to evaluate the success of the new word segmenter.Finally we discuss the application of language segmentation in digital libraries.Our system for segmenting Chinese text is available on the World Wide Web at http://www.nzdl.org/cgi-bin/congb.It takes GB-encoded input text, which can be cut from a Chinese document and pasted into the input window.'Once the segmenter has been invoked, the result is rewritten into the same window.The problem of segmenting Chinese text has been studied by researchers for many years; see Wu and Tseng (1993) for a detailed survey.Several different algorithms have been proposed, which, generally speaking, can be classified into dictionary-based and statistical-based methods, although other techniques that involve more linguistic information, such as syntactic and semantic knowledge, have been reported in the natural language processing literature.Cheng, Young, and Wong (1999) describe a dictionary-based method.Given a dictionary of frequently used Chinese words, an input string is compared with words in the dictionary to find the one that matches the greatest number of characters of the input.This is called the maximum forward match heuristic.An alternative is to work backwards through the text, resulting in the maximum backward match heuristic.It is easy to find situations where these fail.To use an English example, forward matching fails on the input &quot;the red ... &quot; (it is misinterpreted as &quot;there d &quot;), while backward matching fails on text ending &quot;... his car&quot; (it is misinterpreted as &quot;... hi scar&quot;).Analogous failures occur with Chinese text.Dai, Khoo, and Loh (1999) use statistical methods to perform text segmentation.They concentrate on two-character words, because two characters is the most common word length in Chinese.Several different notions of frequency of characters and bigrams are explored: relative frequency, document frequency, weighted document frequency, and local frequency.They also look at both contextual and positional information.Contextual information is found to be the single most important factor that governs the probability that a bigram forms a word; incorporating the weighted document frequency can improve the model significantly.In contrast, the positional frequency is not found to be helpful in determining words.Ponte and Croft (1996) introduce two models for word segmentation: word-based and bigram models.Both utilize probabilistic automata.In the word-based method, a suffix tree of words in the lexicon is used to initialize the model.Each node is associated with a probability, which is estimated by segmenting training text using the longest match strategy.This makes the segmenter easy to transplant to new languages.The bigram model uses the lexicon to initialize probability estimates for each bigram, and the probability with which each bigram occurs, and uses the Baum-Welch algorithm (Rabiner 1989) to update the probabilities as the training text is processed.Hockenmaier and Brew (1998) present an algorithm, based on Palmer's (1997) experiments, that applies a symbolic machine learning technique—transformation-based error-driven learning (Brill 1995)—to the problem of Chinese word segmentation.Using a set of rule templates and four distinct initial-state annotators, Palmer concludes that the learning technique works well.Hockenmaier and Brew investigate how performance is influenced by different rule templates and corpus size.They use three rule templates: simple bigram rules, trigram rules, and more elaborate rules.Their experiments indicate that training data size has the most significant influence on performance.Good performance can be acquired using simple rules only if the training corpus is large enough.Lee, Ng, and Lu (1999) have recently introduced a new segmentation method for a Chinese spell-checking application.Using a dictionary with single-character word occurrence frequencies, this scheme first divides text into sentences, then into phrases, and finally into words using a small number of word combinations that are conditioned on a heuristic to avoid delay during spell-checking.When compared with forward maximum matching, the new method resolves more than 10% more ambiguities, but enjoys no obvious speed advantage.The way in which Chinese characters are used in names differs greatly from the way they are used in ordinary text, and some researchers, notably Sproat et al. (1996), have established special-purpose recognizers for Chinese names (and translated foreign names), designed to improve the accuracy of automatic segmenters by treating names specially.'Chinese names always take the form family name followed by given name.Whereas family names are limited to a small group of characters, given names can consist of any characters.They normally comprise one or two characters, but three-character names have arisen in recent years to ensure uniqueness when the family name is popular—such as Smith or Jones in English.Sproat et al. (1996) implement special recognizers not only for Chinese names and transliterated foreign names, but for components of morphologically obtained words as well.The approach we present is not specially tailored for name recognition, but because it is fully adaptive it is likely that it would yield good performance on names if lists of names were provided as supplementary training text.This has not yet been tested.Statistical language models are well developed in the field of text compression.Compression methods are usually divided into symbolwise and dictionary schemes (Bell, Cleary, and Witten, 1990).Symbolwise methods, which generally make use of adaptively generated statistics, give excellent compression—in fact, they include the best known methods.Although dictionary methods such as the Ziv-Lempel schemes perform less well, they are used in practical compression utilities like Unix compress and gzip because they are fast.In our work we use the prediction by partial matching (PPM) symbolwise compression scheme (Cleary and Witten 1984), which has become a benchmark in the compression community.It generates &quot;predictions&quot; for each input symbol in turn.Each prediction takes the form of a probability distribution that is provided to an encoder.The encoder is usually an arithmetic coder; the details of coding are of no relevance to this paper.PPM is an n-gram approach that uses finite-context models of characters, where the previous few (say three) characters predict the upcoming one.The conditional probability distribution of characters, conditioned on the preceding few characters, is maintained and updated as each character of input is processed.This distribution, along with the actual value of the preceding few characters, is used to predict each upcoming symbol.Exactly the same distributions are maintained by the decoder, which updates the appropriate distribution as each character is received.This is what we call adaptive modeling: both encoder and decoder maintain the same models—not by communicating the models directly, but by updating them in precisely the same way.Rather than using a fixed context length (three was suggested above), the PPM method chooses a maximum context length and maintains statistics for this and all shorter contexts.The maximum is five in most of the experiments below, and statistics are maintained for models of order 5, 4, 3, 2, 1, and 0.These are not stored separately; they are all kept in a single trie structure.PPM incorporates a simple and highly effective method to combine the predictions of the models of different order—often called the problem of &quot;backoff.&quot; To encode the next symbol, it starts with the maximum-order model (order 5).If that model contains a prediction for the upcoming character, the character is transmitted according to the order 5 distribution.Otherwise, both encoder and decoder escape down to order 4.There are two possible situations.If the order 5 context—that is, the preceding five-character sequence—has not been encountered before, then escape to order 4 is inevitable, and both encoder and decoder can deduce that fact without requiring any communication.If not, that is, if the preceding five characters have been encountered in sequence before but not followed by the upcoming character, then only the encoder knows that an escape is necessary.In this case, therefore, it must signal this fact to the decoder by transmitting an escape event—and space must be reserved for this event in every probability distribution that the encoder and decoder maintain.PPM model after processing the string tobeornottobe; c = count, p = prediction probability.Order 2 c p Order 1 c p Order 0 c p Prediction Prediction Prediction be -, o 1 1/2 b -, e 2 3/4 -4 b 2 3/26 -, esc 1 1/2 -4 esc 1 1/4 -4 e 2 3/26 eo r 1 1/2 e , o 1 1/2 --, n 1 1/26 -, esc 1 1/2 -, esc 1 1/2 -4 o 4 7/26 no t 1 1/2 n -4 o 1 1/2 -, r 1 1/26 -4 esc 1 1/2 , esc 1 1/2 -4 t 3 5/26 ob -, e 2 3/4 o -, b 2 3/8 -4 esc 6 3/13 -, esc 1 1/4 -, r 1 1/8 Order —1 or -- n 1 1/2 -4 t 1 1/8 -4 esc 1 1/2 --). esc 3 3/8 Prediction c p ot ---. t 1 1/2 r —+ n 1 1/2 -4 A 1 1/IA1 -4 esc 1 1/2 -4 esc 1 1/2 rn —p o 1 1/2 t -4 o 2 1/2 -, esc 1 1/2 , t 1 1/6 to -4 b 2 3/4 -, esc 2 1/3 -4 esc 1 1/4 tt -4 o 1 1/2 -, esc 1 1/2 Once any necessary escape event has been transmitted and received, both encoder and decoder agree that the upcoming character will be coded by the order 4 model.Of course, this may not be possible either, and further escapes may take place.Ultimately, the order 0 model may be reached; in this case the character can be transmitted if it is one that has occurred before.Otherwise, there is one further escape (to an order —1 model), and the standard ASCII representation of the character is sent.The only remaining question is how to calculate the probabilities from the counts— a simple matter once we have resolved how much space to allocate for the escape probability.There has been much discussion of this question, and several different methods have been proposed.Our experiments calculate the escape probability in a particular context as where n is the number of times that context has appeared and d is the number of different symbols that have directly followed it (Howard 1993).The probability of a character that has occurred c times in that context is Since there are d such characters, and their counts sum to n, it is easy to confirm that the probabilities in the distribution (including the escape probability) sum to 1.To illustrate the PPM modeling technique, Table 1 shows the model after the string tobeornottobe has been processed.In this illustration the maximum model order is 2 (not 5 as stated above), and each prediction has a count c and a prediction probability p. The probability is determined from the counts associated with the prediction using the formula that we discuss above.IA l is the size of the alphabet, and it is this that determines the probability for each unseen character.The model in Table 1 is used as follows: Suppose the character following tobeornottobe is o.Since the order 2 context is be, and the upcoming symbol has already been seen once in this context, the order 2 model is used for encoding in this case, and the encoding probability is 1/2.Thus the symbol o would be encoded in 1 bit.If the next character, instead of o, were t, this has not been seen in the current order 2 context (which is still be).Consequently an order 2 escape event is coded (probability 1/2, again in the be context), and the context is truncated to e. Checking the order 1 model, the upcoming character t has not been seen in this context, so an order 1 escape event is coded (probability 1/2 in the e context) and the context is truncated to the null context, corresponding to the order 0 model.The character t is finally encoded in this model, with probability 5/26.Thus three encodings occur for this one character, with probabilities 1/2, 1/2, and 5/26 respectively, which together amount to just over 5 bits of information.If the upcoming character had been x instead of t, a final level of escape, this time to order 0, would have occurred (probability 3/13), and the x would be encoded with a probability of 1/256 (assuming that the alphabet has 256 characters) for a total of just over 10 bits.It is clear from Table 1 that, in the context tobeornottobe, if the next character is o it will be encoded by the order 2 model.Hence if an escape occurs down to order 1, the next character cannot be o.This makes it unnecessary to reserve probability space for the occurrence of o in the order 1 (or order 0 or order —1) models.This idea, which is called exclusion, can be exploited to improve compression.A character that occurs at one level is excluded from all lower-order predictions, allowing a greater share of the probability space to be allocated to the other characters in these lower-order models (Bell, Cleary, and Witten 1990).For example, if the character b were to follow tobeornottobe it would be encoded with probabilities (1/2,1/2,3/26), without exclusion, leading to a coding requirement of 5.1 bits.However, if exclusion was exploited, both encoder and decoder will recognize that escape from order 1 to order 0 is inevitable because the order 1 model adds no characters that were not already predicted by the order 2 model.Thus the coding probabilities will be (1/2, 1,3/18) with exclusion, reducing the total code space for b to 3.6 bits.An important special case of the exclusion policy occurs at the lowest-level model: for example, the x at the end of the previous paragraph would finally be encoded with a probability of 1/250 rather than 1/256 because characters that have already occurred can never be predicted in the order —1 context.One slight further improvement to PPM is incorporated in the experiments: deterministic scaling (Teahan 1998).Although it probably has negligible effect on our overall results, we record it here for completeness.Experiments show that in deterministic contexts, for which d =- 1, the probability that the single character that has occurred before reappears is greater than the 1 — 1/(2n) implied by the above estimator.Consequently, in this case the probability is increased in an ad hoc manner to 1 — 1/(6n).Inserting spaces into text can be viewed as a hidden Markov modeling problem.Being entirely adaptive, the method works regardless of what language it is used with.For pedagogical purposes, we will explain it with English text.Between every pair of characters lies a potential space.Figure 4(a) illustrates the model for the fragment tobeornottobe.It contains one node for each letter in the text and one for each possible intercharacter space (represented as dots • in the figure).Any given assignment of word boundaries to this text fragment will correspond to a path through the model from beginning (at the left) to end (at the right).Of all possible paths, we seek the one that gives the best compression according to the PPM text compression method, suitably primed with English text.This path is the correct path, corresponding to the text to be or not to be, shown in bold in Figure 4(b).4.1 Markov Modeling with Context Figure 4 can easily be converted into a Markov model for a given order of PPM.Suppose we use order 1: then we rewrite Figure 4(a) so that the states are bigrams, as shown in Figure 5(a).The interpretation of each state is that it corresponds to the last character of the string that labels the state.The very first state, labeled t, has no prior context—in PPM terms, that character will be transmitted by escaping down to order 0 (or —1).Again, the bold arrows in Figure 5(b) shows the path corresponding to the string with spaces inserted correctly.Growing a tree for order 1 modeling of tobeornottobe.Similar models could be written for higher-order versions of PPM.For example, with an order 3 model, states would be labeled by strings of length four (except for the first few states, where the context would be truncated because they occur at the beginning of the string).And each state would have variants corresponding to all different ways of inserting space into the four-character string.For example, the states corresponding to the sixth character of tobeornottobe would include beor and e•or, as well as •eor, eo.r and .o.r.It is not hard to see that the number of states corresponding to a particular character of the input string increases with model order according to the Fibonnacci series.Figure 5(a) shows two states per symbol for order 1, there are three states per symbol for order 2, five for order 3, eight for order 4, thirteen for order 5, and so on.Given a hidden Markov model like the one in Figure 5(a), where probabilities are supplied for each edge according to an order 1 compression model, the space insertion problem is tantamount to finding the sequence of states through the model, from beginning to end, that maximizes the total probability—or, equivalently, that minimizes the number of bits required to represent the text according to that model.The following Viterbi-style algorithm can be used to solve this problem.Beginning at the initial state, the procedure traces through the model, recording at each state the highest probability of reaching that state from the beginning.Thus the two descendants of the start node, nodes to and t., are assigned the probability of o and conditioned in each case on t being the prior character, respectively.As more arcs are traversed, the associated probabilities are multiplied: thus the node .0 receives the product of the probability of • conditioned on t and of o conditioned on When the node ob is reached, it is assigned the greater of the probabilities associated with the two incoming transitions, and so on throughout the model.This is the standard dynamic programming technique of storing with each state the result of the best way of reaching that state, and using this result to extend the calculation to the next state.To find the optimal state sequence is simply a matter of recording with each state which incoming transition is associated with the greatest probability, and traversing that path in the reverse direction once the final node is reached.These models can be generated dynamically by proceeding to predict each character in turn.Figure 6(a) shows the beginning of the tree that results.First, the initial node t is expanded into its two children, t. and to.Then, these are expanded in turn.The first has one child, •o, because a space cannot be followed by another space.The second has two, o• and ob.Figure 6(b) shows the further expansion of the .o node.However, the two children that are created already exist in the tree, and so the existing versions of these nodes are used instead, as in Figure 6(c).If this procedure is conThe space insertion procedure as implemented. tinued, the graph structure of Figure 5(a) will be created.During creation, probability values can be assigned to the nodes, and back pointers inserted to record the best path to each node.The illustration in Figure 6 is for an order 1 model, but exactly the same procedure applies for higher-order PPM models.Our implementation uses a slight variant of the above procedure for finding the optimal place to insert spaces.At each stage, we consider the possibility of adding either the next character, or the next character followed by a space.This generates the structure shown in Figure 7.Starting with the null string, both t and t• are generated as successor states.From each of these states, either o or o. can be added, and these yield the next states shown.The procedure continues, growing the trellis structure using an incremental strategy similar to that illustrated in Figure 6, but modified to take into account the new growth strategy of adding either the next character or the next character followed by a space.The search strategy we use is a variant of the stack algorithm for sequential decoding (Anderson and Mohan 1984).As new nodes are generated, an ordered list is maintained of the best paths generated so far.Only the best path is extended.The metric used to evaluate a path is the number of bits required for the segmentation sequence it represents, when compressed by the PPM model.It is necessary to delete paths from the list in order to make room for newly generated ones.We remove all paths that were more than m nodes shorter than the best path so far, where m is the order of the PPM model (5 in our experiments).We reasoned that it is extremely unlikely—at least for natural language sequences—that such a path would ever grow to outperform the current best path, because it already lags behind in code length despite the fact that m further letters must be encoded.Before describing experiments to assess the success of the new word segmentation method, we first discuss measures that are used to evaluate the accuracy of automatic segmentation.We then examine the application of the new segmentation method to English text, and show how it achieves results that significantly outperform the state of the art.Next we describe application to a manually segmented corpus of Chinese text; again, excellent results are achieved.In a further experiment where we apply a model generated from the corpus to a new, independent, test file, performance deteriorates considerably—as one might expect.We then apply the method to a different corpus, and investigate how well the model transfers from one corpus to another.We end with a discussion of how the results vary with the order of the compression model used to drive the segmenter.We use three measures to evaluate the accuracy of automatic segmentation: recall, precision, and error rate.All evaluations use hand-segmentation as the gold standard, which the automatic method strives to attain.To define them, we use the terms Number of words occurring in the hand-segmentation Number of words incorrectly identified by the automatic method Number of words correctly identified by the automatic method Recall and precision are standard information retrieval measures used to assess the quality of a retrieval system in terms of how many of the relevant documents are retrieved (recall) and how many of the retrieved documents are relevant (precision): The overall error rate can be defined as error rate = This in principle can give misleading results—an extreme condition is where the automatic method only identifies a single word, leading to a very small error rate of 1/N despite the fact that all words but one are misidentified.However, in all our experiments extreme conditions do not occur because n is always close to N and we find that the error rate is a useful overall indicator of the quality of segmentation.We also used the F-measure to compare our results with others: If the automatic method produces the same number of words as the hand-segmentation, recall and precision both become equal to one minus the error rate.A perfect segmenter will have an error rate of zero and recall and precision of 100%.All these measures can be calculated automatically from a machine-segmented text, along with the hand-segmented gold standard.Both texts are identical except for the points where spaces are inserted: thus we record just the start and end positions of each word in both versions.For example, &quot;A BC AED F&quot; in the machine-segmented version is mapped to (1,1) (2,3) (4,6) (7,7), and &quot;A BC A ED F&quot; in the hand-segmented version becomes (1,1) (2,3) (4,4) (5,6) (7,7).The number of correctly and incorrectly segmented words is counted by comparing these two sets of positions, indicated by matched and mismatched pairs, respectively—three correct and two incorrect, in this example.It may be helpful for non-Chinese readers to briefly illustrate the success of the space insertion method by showing its application to English text.The first part of Table 2 shows the original text, with spaces in the proper places.The second shows the text with spaces removed, used as input to the segmentation procedure.The third shows the output of the PPM-based method described above, while the fourth shows, for comparison, the output of a word-based method for predicting the position of spaces, USeg (Ponte and Croft 1996).For this experiment (first reported by Teahan et al. [19981), PPM was trained on the million-word Brown corpus (Kucera and Francis 1967).USeg was trained on a far larger corpus containing 1 Gb of data from the Tipster collection (Broglio, Callan, and Croft 1994).Both were tested on the same 500 Kb extract from the Wall Street Journal.The recall and precision for PPM were both 99.52%, while the corresponding figures for Useg were 93.56% and 90.03%, respectively.This result is particularly noteworthy because PPM had been trained on only a small fraction of the amount of text needed for the word-based scheme.The same example was used by Ponte and Croft (1996), and the improved performance of the character-based method is evident even in this small example.Although the word Micronite does not occur in the Brown Corpus, it was correctly segmented using PPM.Likewise, inits was correctly split into in and its.PPM makes just two mistakes.First, a space was not inserted into Loews Corp because the single &quot;word&quot; requires only 54.3 bits to encode, whereas Loews Corp requires 55.0 bits.Second, an extra space was added to crocidolite because that reduced the number of bits required from 58.7 to 55.3.Our first series of experiments used part of Guo Jin's Mandarin Chinese PH corpus, containing one million words of newspaper stories from the Xinhua news agency of PR China written between January 1990 and March 1991.It is represented in the standard GB coding scheme.Table 3 shows the distribution of word lengths in the corpus.Single-character words are the most frequent; these and bigrams together constitute almost 94% of words.Nearly half the characters appear as constituents of two-character words.Some published figures for Chinese language statistics indicate that this corpus may overrepresent single-character words and underrepresent bigrams—for example, Liu (1987) gives figures for modern Chinese of 5%, 75%, 14%, and 6% for one-character, two-character, three-character, and longer words, respectively.However, it has been argued that considering the inherent uncertainty in Chinese word segmentation, generalpurpose segmentation algorithms should segment aggressively rather than conservatively (Wu 1998); consequently this corpus seems appropriate for our use.Table 4 shows the results for five 500-word test files from the corpus.We took part of the corpus that was not used for training, divided it into 500-word segments, removed all spaces, and randomly chose five segments as test files.The results show an error rate varying from 1.2% to 6.6%.The resulting F-measures indicate that the new algorithm performs better than the one described in Hockenmaier and Brew (1998), who report an F-measure of 87.9 using trigram rules.This is particularly significant because the two algorithms use training and test data from the same source.The results were also verified by checking them manually.This produces slightly different results, for two reasons.Firstly, human judgment sometimes accepts a segmentation as correct even though it does not correspond exactly with the corpus version.For example, the last word in K. is counted as correct even though in the corpus it is written *.Pt:tt K. Secondly, improper segmentations such as and g occur in the corpus.When the program makes the same mistakes, it counts as correct in automatic checking, but incorrect in manual checking.These two kinds of error virtually canceled each other: when checked manually, file 3, for example, has five fewer errors for the first reason and six more for the second reason, giving error counts of 21 and 20 for automatic and manual checking, respectively.In a second test, models from this corpus were evaluated on completely separate data provided by the Institute of Computational Linguistics of Peking University.This contained 39 sentences (752 characters), some of which are compound sentences.Since no presegmented version was available, all checking was manual.This test is interesting because it includes several sentences that are easily misunderstood, three of which are shown in Figure 8.In the first, which reads &quot;I have learned a lot from it,&quot; the second and third characters combine into from it' and the fourth and fifth characters combine into 'have learned.'However, the third and fourth characters taken together mean 'middle school,' which does not occur in the meaning of the sentence.In the second and third sentences, the first three characters are the same.In the second, &quot;physics is very hard to learn,&quot; the second and third characters should be separated by a space, so that the third character can combine with the following two characters to mean 'to learn.'However, in the third, &quot;physics is one kind of science,&quot; the first three characters make a single word meaning 'physics.'The error rate, recall and precision for this test material are 10.8%, 93.4%, and 89.6%, respectively.Performance is significantly worse than that of Table 4, because of the nature of the test file.Precision is distinctly lower than recall—recall fares better because many relevant words are still retrieved, whereas precision suffers because the automatic segmenter placed too many word boundaries compared with the manual judgment.Two aspects of the training data have a profound influence on the model's accuracy.First, some errors are obviously caused by deficiencies in the training data, such as improperly segmented common words and names.Second, some errors stem from the topics covered by the corpus.It is not surprising that the error rate increases when the training and testing text represent different topic areas—such as training on news text and testing on medical text.The Rocling Standard Segmentation Corpus contains about two million presegmented words, represented in the Big5 coding scheme.We converted it to GB, used one million words for training, and compared the resulting model to that generated from the PH data, also trained on one million words.Both models were tested on 10 randomly chosen 1,000-word segments from each corpus (none of this material was used in training).The results are shown in Table 5, in terms of the mean and standard deviation (sd) of the errors.When the training and testing files come from the same corpus, results are good, with around 42 (for PH) and 45 (for Rocling) errors per thousand words.Not surprisingly, performance deteriorates significantly when the PH model is used to segment the Rocling test files or vice versa.Several differences between the corpora influence performance.Many English words are included in Rocling, whereas in PH only a few letters are used to repof Wtiti- in the PH corpus.Quotation marks also differ: L 1 in Rocling but &quot; &quot; in PH.In addition, as is only to be expected in any large collection of natural language, typographical errors occur in both corpora.The overall result indicates that our algorithm is robust.It performs well so long as the training and testing data come from the same source.5.6 Effect of the Amount of Training Data For the Rocling corpus, we experimented with different amounts of training data.Four models were trained with successively larger amounts of data, 0.5M, 1M, 1.5M, and 2M words, each training file being an extension of the text in the preceding training file.The four models were tested on the 10 randomly-chosen 1,000-word Rocling segments used before.The results for the individual test files, in terms of error rate per thousand words, are shown in Figure 9 and summarized in Table 6.Larger training sets generally give smaller error, which is only to be expected—although the results for some individual test files flatten out and show no further improvement with larger training files, and in some cases more training data actually increases the number of errors.Overall, the error rate is reduced by about 25% for each doubling of the training data.We have experimented with compression models of different orders on the PH corpus.Generally speaking, compression of text improves as model order increases, up to a point determined by the logarithm of the size of the training text.Typically, little compression is gained by going beyond order 5 models.For segmentation, we observe many errors when a model of order 1 is used.For order 3 models, most words are segmented with the same error rate as for order 5 models, though some words are missed when order 2 models are used.Figure 10 shows some cases where the order 3 and order 5 models produce different results.Some order 5 errors are corrected by the order 3 model, though others appear even with the lower-order model.For example, both results in the first row are incorrect: no space should be inserted in this case, and the four characters should stand together.However, the order 3 result is to be preferred to the order 5 result because both two-character words do at least make sense individually, whereas the initial three characters in the order 5 version do not represent a word at all.In the second row, the order 5 result is incorrect because the second component does not represent a word.In the order 3 result, the first word, containing two characters, is a person's name.The second word could also be correct as it stands, though it would be equally correct if a space had been inserted between the two bigrams.On the whole, we find that the order 3 model gives the best results overall, although there is little difference between orders 3, 4, and 5.Word segmentation forms a valuable component of any Chinese digital library system.It improves full-text retrieval in two ways: higher-precision searching (that is, fewer false matches), and the ability to incorporate relevance ranking.This increases the effectiveness of full-text search and helps to provide users with better feedback.For example, one study concludes that the performance of an unsegmented characterbased query is about 10% worse than that of the corresponding segmented query (Broglio, Callan, and Croft 1996).Many emerging digital library technologies also presuppose word segmentation—for example, text summarization, document clustering, and keyphrase extraction all rely on word frequencies.These would not work well on unsegmented text because character frequencies do not generally reflect word frequencies.Once the source text in a digital library exceeds a few megabytes, full-text indexes are needed to process queries in a reasonable time (Witten, Moffat, and Bell 1999).Full-text indexing was developed using languages where word boundaries are notated (principally English), and the techniques that were developed rely on wordbased processing.Although some techniques—for example stemming (Frakes 1992) and casefolding—are not applicable to Chinese information retrieval, many are.Examples include heuristics for relevance ranking, and query expansion using a language thesaurus.Of course, full-text indexes can be built from individual characters rather than words.However, these will suffer from the problem of low precision—searches will return many irrelevant documents, where the same characters are used in contexts different from that of the query.To reduce false matches to a reasonable level, auxiliary indexes (for example, sentence indexes) will have to be created.These will be much larger than regular word-based indexes of paragraphs or documents, and will still not be as accurate.Information retrieval systems often rank the results of each search, giving preference to documents that are more relevant to the query by placing them nearer the beginning of the list.Relevance metrics are based on the observation that infrequent words are more important than common ones and should therefore rate more highly.Word segmentation is essential for this purpose, because the relationship between the frequency of a word and the frequency of the characters that appear within it is often very weak.Without word segmentation, the precision of the result set will be reduced because relevant documents are less likely to be close to the top of the list.L-4 For example, the word 1---1 El (&quot;to go abroad&quot;) is an infrequent word that appears only twenty times in the PH corpus.But its two characters occur frequently: i$ (&quot;to go out&quot;) 13,531 times; and Er (&quot;country&quot;) 45,010 times.In fact III is the second most frequent character in the corpus, appearing in 443 separate words.Character-based ranking would place little weight on these two characters, even though they are 4extremely important if the query is The word -t (&quot;also&quot;) is another frequent character, appearing 4,553 times in the PH corpus.However, in 4,481 of those cases it appears by itself and contributes little to the meaning of the text.If a query contained both of these words, far more weight would be given to than to the individual characters in ± II.Word counts also give feedback on the effectiveness of a query.They help users judge whether their query was too wide or too narrow, and provide information on which of the terms are most appropriate.Word-based processing is essential to a number of emergent new technologies in the digital library field.Statistical approaches are enjoying a resurgence in natural language analysis (Klavans and Resnik 1997): examples include text summarization, document clustering, and keyphrase extraction.All of these statistical approaches are based on words and word frequencies.For instance, keywords and keyphrases for a document can be determined automatically based on features such as the frequency of the phrase in the document relative to its frequency in an independent corpus of like material, and its position of occurrence in the document (Frank et al. 1999).A decomposition of text into its constituent words is an essential prerequisite for the application of such techniques.The problem of word segmentation of Chinese text is important in a variety of contexts, particularly with the burgeoning interest in digital libraries and other systems that store and process text on a massive scale.Existing techniques are either linguistically based, using a dictionary of words, or rely on hand-crafted segmentation rules, or use adaptive models that have been specifically created for the purpose of Chinese word segmentation.We have developed an alternative based on a general-purpose character-level model of text—the kind of models used in the very best text compression schemes.These models are formed adaptively from training text.The advantage of using character-level models is that they do not rely on a dictionary and therefore do not necessarily fail on unusual words.In effect, they can fall back on general properties of language statistics to process novel text.The advantage of basing models on a corpus of training text is that particular characteristics of the text are automatically taken into account in language statistics—as exemplified by the significant differences between the models formed for the PH and Rocling corpora.Encouraging results have been obtained using the new scheme.Our results compare very favorably with the results of Hockenmaier and Brew (1998) on the PH corpus; unfortunately no other researchers have published quantitative results on a standard corpus.Further work is needed to analyze the results of the Rocling corpus in more detail.The next step is to use automatically segmented text to investigate the digital library applications we have described: information retrieval, text summarization, document clustering, and keyphrase extraction.We are grateful to Stuart Inglis, Hong Chen, and John Cleary, who provided advice and assistance.The corrected version of Guo Jin's PH corpus and the Rocling corpus were provided by Julia Hockenmaier and Chris Brew at the University of Edinburgh and the Chinese Knowledge Information Processing Group of Academia Sirtica, respectively.The Institute of Computational Linguistics of Peking University also provided some test material.Bill Teahan acknowledges the generous support of the Department of Information Technology, Lund University, Sweden.Thanks also to the anonymous referees who have helped us to improve the paper significantly.
Hierarchical Phrase-Based Translation  present a statistical machine translation model that uses that contain subphrases.The model is formally a synchronous context-free grammar but is learned from a parallel text without any syntactic annotations.Thus it can be seen as combining fundamental ideas from both syntax-based translation and phrase-based translation.We describe our system’s training and decoding methods in detail, and evaluate it for translation speed and translation accuracy.Using BLEU as a metric of translation accuracy, we find that our system performs significantly better than the Alignment Template System, a state-of-the-art phrasebased system.We present a statistical machine translation model that uses hierarchical phrases—phrases that contain subphrases.The model is formally a synchronous context-free grammar but is learned from a parallel text without any syntactic annotations.Thus it can be seen as combining fundamental ideas from both syntax-based translation and phrase-based translation.We describe our system’s training and decoding methods in detail, and evaluate it for translation speed and translation accuracy.Using BLEU as a metric of translation accuracy, we find that our system performs significantly better than the Alignment Template System, a state-of-the-art phrasebased system.The alignment template translation model (Och and Ney 2004) and related phrase-based models advanced the state of the art in machine translation by expanding the basic unit of translation from words to phrases, that is, substrings of potentially unlimited size (but not necessarily phrases in any syntactic theory).These phrases allow a model to learn local reorderings, translations of multiword expressions, or insertions and deletions that are sensitive to local context.This makes them a simple and powerful mechanism for translation.The basic phrase-based model is an instance of the noisy-channel approach (Brown et al. 1993).Following convention, we call the source language “French” and the target language “English”; the translation of a French sentence f into an English sentence e is modeled as: The phrase-based translation model P( f  |e) “encodes” e into f by the following steps: Other phrase-based models model the joint distribution P(e,f ) (Marcu and Wong 2002) or make P(e) and P(f I e) into features of a log-linear model (Och and Ney 2002).But the basic architecture of phrase segmentation (or generation), phrase reordering, and phrase translation remains the same.Phrase-based models can robustly perform translations that are localized to substrings that are common enough to have been observed in training.But Koehn, Och, and Marcu (2003) find that phrases longer than three words improve performance little for training corpora of up to 20 million words, suggesting that the data may be too sparse to learn longer phrases.Above the phrase level, some models perform no reordering (Zens and Ney 2004; Kumar, Deng, and Byrne 2006), some have a simple distortion model that reorders phrases independently of their content (Koehn, Och, and Marcu 2003; Och and Ney 2004), and some, for example, the Alignment Template System (Och et al. 2004; Thayer et al.2004), hereafter ATS, and the IBM phrase-based system (Tillmann 2004; Tillmann and Zhang 2005), have phrase-reordering models that add some lexical sensitivity.But, as an illustration of the limitations of phrase reordering, consider the following Mandarin example and its English translation: M1)11'1 � Æ JLF01 P �� n �� 0* �Aozhou shi yu Beihan you bangjiao de shaoshu guojia zhiyi Australia is with North Korea have dipl. rels. that few countries one of ..Australia is one of the few countries that have diplomatic relations with North Korea.If we count zhiyi (literally, ‘of-one’) as a single token, then translating this sentence correctly into English requires identifying a sequence of five word groups that need to be reversed.When we run a phrase-based system, ATS, on this sentence (using the experimental setup described herein), we get the following phrases with translations: [Aozhou] [shi]1 [yu Beihan]2 [you] [bangjiao] [de shaoshu guojia zhiyi] [.][Australia] [has] [dipl. rels.][with North Korea]2 [is]1 [one of the few countries] [.] where we have used subscripts to indicate the reordering of phrases.The phrase-based model is able to order “has diplomatic relations with North Korea” correctly (using phrase reordering) and “is one of the few countries” correctly (using a combination of phrase translation and phrase reordering), but does not invert these two groups as it should.We propose a solution to these problems that does not interfere with the strengths of the phrase-based approach, but rather capitalizes on them: Because phrases are good for learning reorderings of words, we can use them to learn reorderings of phrases as well.In order to do this we need hierarchical phrases that can contain other phrases.For example, a hierarchical phrase pair that might help with the above example is (yu 1 you 2 , have 2 with 1 ) (3) where 1 and 2 are placeholders for subphrases (Chiang 2005).This would capture the fact that Chinese prepositional phrases almost always modify verb phrases on the left, whereas English prepositional phrases usually modify verb phrases on the right.Because it generalizes over possible prepositional objects and direct objects, it acts both as a discontinuous phrase pair and as a phrase-reordering rule.Thus it is considerably more powerful than a conventional phrase pair.Similarly, the hierarchical phrase pair ( 1 de 2 , the 2 that 1 ) (4) would capture the fact that Chinese relative clauses modify NPs on the left, whereas English relative clauses modify on the right; and the pair ( 1 zhiyi, one of 1 ) (5) would render the construction zhiyi in English word order.These three rules, along with some conventional phrase pairs, suffice to translate the sentence correctly: [Aozhou] [shi] [[[yu [Beihan]1 you [bangjiao]2] de [shaoshu guojia]3] zhiyi] [Australia] [is] [one of [the [few countries]3 that [have [dipl. rels.]2 with [N. Korea]1]]] The system we describe in this article uses rules like (3), (4), and (5), which we formalize in the next section as rules of a synchronous context-free grammar (CFG).1 Moreover, the system is able to learn them automatically from a parallel text without syntactic annotation.Because our system uses a synchronous CFG, it could be thought of as an example of syntax-based statistical machine translation (MT), joining a line of research (Wu 1997; Alshawi, Bangalore, and Douglas 2000; Yamada and Knight 2001) that has been fruitful but has not previously produced systems that can compete with phrase-based systems in large-scale translation tasks such as the evaluations held by NIST.Our approach differs from early syntax-based statistical translation models in combining the idea of hierarchical structure with key insights from phrase-based MT: Crucially, by incorporating the use of elementary structures with possibly many words, we hope to inherit phrase-based MT’s capacity for memorizing translations from parallel data.Other insights borrowed from the current state of the art include minimum-error-rate training of log-linear models (Och and Ney 2002; Och 2003) and use of an m-gram language model.The conjunction of these various elements presents a considerable challenge for implementation, which we discuss in detail in this article.The result is the first system employing a grammar (to our knowledge) to perform better than phrase-based systems in large-scale evaluations.2Approaches to syntax-based statistical MT have varied in their reliance on syntactic theories, or annotations made according to syntactic theories.At one extreme are those, exemplified by that of Wu (1997), that have no dependence on syntactic theory beyond the idea that natural language is hierarchical.If these methods distinguish between different categories, they typically do not distinguish very many.Our approach, as presented here, falls squarely into this family.By contrast, other approaches, exemplified by that of Yamada and Knight (2001), do make use of parallel data with syntactic annotations, either in the form of phrase-structure trees or dependency trees (Ding and Palmer 2005; Quirk, Menezes, and Cherry 2005).Because syntactically annotated corpora are comparatively small, obtaining parsed parallel text in quantity usually entails running an automatic parser on a parallel corpus to produce noisy annotations.Both of these strands of research have recently begun to explore extraction of larger rules, guided by word alignments.The extraction method we use, which is a straightforward generalization of phrase extraction from word-aligned parallel text, has been independently proposed before in various settings.The method of Block (2000) is the earliest instance we are aware of, though it is restricted to rules with one variable.The same method has also been used by Probst et al. (2002) and Xia and McCord (2004) in conjunction with syntactic annotations to extract rules that are used for reordering prior to translation.Finally, Galley et al. (2004) use the same method to extract a very large grammar from syntactically annotated data.The discontinuous phrases used by Simard et al. (2005) have a similar purpose to synchronous grammar rules; but they have variables that stand for single words rather than subderivations, and they can interleave in non-hierarchical ways.The model is based on a synchronous CFG, elsewhere known as a syntax-directed transduction grammar (Lewis and Stearns 1968).We give here an informal definition and then describe in detail how we build a synchronous CFG for our model.In a synchronous CFG the elementary structures are rewrite rules with aligned pairs of right-hand sides: where X is a nonterminal, γ and α are both strings of terminals and nonterminals, and — is a one-to-one correspondence between nonterminal occurrences in γ and nonterminal occurrences in α.For example, the hierarchical phrase pairs (3), (4), and (5) previously presented could be formalized in a synchronous CFG as: where we have used boxed indices to indicate which nonterminal occurrences are linked by —.The conventional phrase pairs would be formalized as: A synchronous CFG derivation begins with a pair of linked start symbols.At each step, two linked nonterminals are rewritten using the two components of a single rule.When denoting links with boxed indices, we must consistently reindex the newly introduced symbols apart from the symbols already present.For an example using these rules, see Figure 1.The bulk of the grammar consists of automatically extracted rules.The extraction process begins with a word-aligned corpus: a set of triples (f, e, —), where f is a French sentence, e is an English sentence, and — is a (many-to-many) binary relation between positions off and positions of e. The word alignments are obtained by running GIZA++ (Och and Ney 2000) on the corpus in both directions, and forming the union of the two sets of word alignments.We then extract from each word-aligned sentence pair a set of rules that are consistent with the word alignments.This can be thought of in two steps.First, we identify initial phrase pairs using the same criterion as most phrase-based systems (Och and Ney 2004), namely, there must be at least one word inside one phrase aligned to a word inside the other, but no word inside one phrase can be aligned to a word outside the other phrase.For example, suppose our training data contained the fragment Example derivation of a synchronous CFG.Numbers above arrows are rules used at each step. with word alignments as shown in Figure 2a.The initial phrases that would be extracted are shown in Figure 2b.More formally: Definition 1 Given a word-aligned sentence pair ( f, e, —), let fji stand for the substring of f from position i to position j inclusive, and similarly for eji.Then a rule ( fj, ele ) is an initial phrase pair of ( f, e, —) iff: Second, in order to obtain rules from the phrases, we look for phrases that contain other phrases and replace the subphrases with nonterminal symbols.For example, given the initial phrases shown in Figure 2b, we could form the rule where k is an index not used in γ and α, is a rule of (f, e, —).This scheme generates a very large number of rules, which is undesirable not only because it makes training and decoding very slow, but also because it creates spurious ambiguity—a situation where the decoder produces many derivations that are distinct yet have the same model feature vectors and give the same translation.This can result in k-best lists with very few different translations or feature vectors, which is problematic for the minimum-error-rate training algorithm (see Section 4.3).To avoid this, we filter our grammar according to the following constraints, chosen to balance grammar size and performance on our development set: Glue rules.Having extracted rules from the training data, we could let X be the grammar’s start symbol and translate new sentences using only the extracted rules.But for robustness and for continuity with phrase-based translation models, we allow the grammar to divide a French sentence into a sequence of chunks and translate one chunk at a time.We formalize this inside a synchronous CFG using the rules (14) and (15), which we call the glue rules, repeated here: These rules analyze an S (the start symbol) as a sequence of Xs which are translated without reordering.Note that if we restricted our grammar to comprise only the glue rules and conventional phrase pairs (that is, rules without nonterminal symbols on the right-hand side), the model would reduce to a phrase-based model with monotone translation (no phrase reordering).Entity rules.Finally, for each sentence to be translated, we run some specialized translation modules to translate the numbers, dates, numbers, and bylines in the sentence, and insert these translations into the grammar as new rules.3 Such modules are often used by phrase-based systems as well, but here their translations can plug into hierarchical phrases, for example, into the rule allowing it to generalize over numbers of years.Given a French sentence f, a synchronous CFG will have, in general, many derivations that yield f on the French side, and therefore (in general) many possible translations e. We now define a model over derivations D to predict which translations are more likely than others.Following Och and Ney (2002), we depart from the traditional noisy-channel approach and use a more general log-linear model over derivations D: 3 These modules are due to U. Germann and F. J. Och.In a previous paper (Chiang et al. 2005) we reported on translation modules for numbers and names.The present modules are not the same as those, though the mechanism for integrating them is identical.209 Computational Linguistics Volume 33, Number 2 where the φi are features defined on derivations and the λi are feature weights.One of the features is an m-gram language model PLM(e); the remainder of the features we will define as products of functions on the rules used in a derivation: The factors other than the language model factor can be put into a particularly convenient form.A weighted synchronous CFG is a synchronous CFG together with a function w that assigns weights to rules.This function induces a weight function over derivations: It is easy to write dynamic-programming algorithms to find the highest-weight translation or k-best translations with a weighted synchronous CFG.Therefore it is problematic that w(D) does not include the language model, which is extremely important for translation quality.We return to this challenge in Section 5.For our experiments, we use a feature set analogous to the default feature set of Pharaoh (Koehn, Och, and Marcu 2003).The rules extracted from the training bitext have the following features: Finally, for all the rules, there is a word penalty exp(−#T(α)), where #T just counts terminal symbols.This allows the model to learn a general preference for shorter or longer outputs.In order to estimate the parameters of the phrase translation and lexical-weighting features, we need counts for the extracted rules.For each sentence pair in the training data, there is in general more than one derivation of the sentence pair using the rules extracted from it.Because we have observed the sentence pair but have not observed the derivations, we do not know how many times each derivation has been seen, and therefore we do not actually know how many times each rule has been seen.Following Och and others, we use heuristics to hypothesize a distribution of possible rules as though we observed them in the training data, a distribution that does not necessarily maximize the likelihood of the training data.5 Och’s method gives a count of one to each extracted phrase pair occurrence.We likewise give a count of one to each initial phrase pair occurrence, then distribute its weight equally among the rules obtained by subtracting subphrases from it.Treating this distribution as our observed data, we use relative-frequency estimation to obtain P(γ  |α) and P(α  |γ).Finally, the parameters λi of the log-linear model (18) are learned by minimumerror-rate training (Och 2003), which tries to set the parameters so as to maximize the BLEU score (Papineni et al. 2002) of a development set.This gives a weighted synchronous CFG according to (22) that is ready to be used by the decoder.4 This feature uses word alignment information, which is discarded in the final grammar.If a rule occurs in training with more than one possible word alignment, Koehn, Och, and Marcu take the maximum lexical weight; we take a weighted average.5 This approach is similar to that taken by many parsers, such as SPATTER (Magerman 1995) and its successors, which use heuristics to hypothesize an augmented version of the training data, but it is especially reminiscent of the Data Oriented Parsing method (Bod 1992), which hypothesizes a distribution over many possible derivations of each training example from subtrees of varying sizes.In brief, our decoder is a CKY (Cocke-Kasami-Younger) parser with beam search together with a postprocessor for mapping French derivations to English derivations.Given a French sentence f, it finds the English yield of the single best derivation that has French yield f: �eˆ = e arg max P(D) (24) Ds.t.f(D)=f Note that this is not necessarily the highest-probability English string, which would require a more expensive summation over derivations.We now discuss the details of the decoder, focusing attention on efficiently calculating English language-model probabilities for possible translations, which is the primary technical challenge.In the following we present several parsers as deductive proof systems (Shieber, Schabes, and Pereira 1995; Goodman 1999).A parser in this notation defines a space of weighted items, in which some items are designated axioms and some items are designated goals (the items to be proven), and a set of inference rules of the form which means that if all the items Ii (called the antecedents) are provable, with weight wi, then I (called the consequent) is provable, with weight w, provided the side condition φ holds.The parsing process grows a set of provable items: It starts with the axioms, and proceeds by applying inference rules to prove more and more items until a goal is proven.For example, the well-known CKY algorithm for CFGs in Chomsky normal form can be thought of as a deductive proof system whose items can take one of two forms: The axioms would be 6 Treating grammar rules as axioms is not standard practice, but advocated by Goodman (1999).Here, it has the benefit of simplifying the presentation in Section 5.3.4. and the inference rules would be and the goal would be [S, 0, n], where S is the start symbol of the grammar and n is the length of the input string f. Given a synchronous CFG, we could convert its French-side grammar into Chomsky normal form, and then for each sentence, we could find the best parse using CKY.Then it would be a straightforward matter to revert the best parse from Chomsky normal form into the original form and map it into its corresponding English tree, whose yield is the output translation.However, because we have already restricted the number of nonterminal symbols in our rules to two, it is more convenient to use a modified CKY algorithm that operates on our grammar directly, without any conversion to Chomsky normal form.The axioms, inference rules, and goals for the basic decoder are shown in Figure 3.Its time complexity is O(n3), just as CKY’s is.Because this algorithm does not yet incorporate a language model, let us call it the −LM parser.The actual search procedure is given by the pseudocode in Figure 4.It organizes the proved items into an array chart whose cells chart[X, i, j] are sets of items.The cells are ordered such that every item comes after its possible antecedents: smaller spans before larger spans, and X items before S items (because of the unary rule S → �X 1 , X 1 )).Then the parser can proceed by visiting the chart cells in order and trying to prove all the items for each cell.Whenever it proves a new item, it adds the item to the Search procedure for the −LM parser. appropriate chart cell; in order to reconstruct the derivations later, it must also store, with each item, a tuple of back-pointers to the antecedents from which the item was deduced (for axioms, an empty tuple is used).If two items are added to a cell that are equivalent except for their weights or back-pointers, then they are merged (in the MT decoding literature, this is also known as hypothesis recombination), with the merged item taking its weight and back-pointers from the better of the two equivalent items.(However, if we are interested in finding the k-best derivations, the merged item gets the multiset of all the tuples of back-pointers from the equivalent items.These backpointers are used below in Section 5.2.)The algorithm in Figure 4 does not completely search the space of proofs, because it has a constraint that prohibits any X from spanning a substring longer than a fixed limit Λ on the French side, corresponding to the maximum length constraint on initial rules during training.This gives the decoding algorithm an asymptotic time complexity of O(n).In principle Λ should match the initial phrase length limit used in training (as it does in our experiments), but in practice it can be adjusted separately to maximize accuracy or speed.We often want to find not only the best derivation for a French sentence but a list of the k-best derivations.These are used for minimum-error-rate training and for rescoring with a language model (Section 5.3.1).We describe here how to do this using the lazy algorithm of Huang and Chiang (2005).Part of this method will also be reused in our algorithm for fast parsing with a language model (Section 5.3.4).If we conceive of lists as functions from indices to values, we may create a virtual list, a function that computes member values on demand instead of storing all the values statically.The heart of the k-best algorithm is a function MERGEPRODUCTS, which takes a set G of tuples of (virtual) lists with an operator ⊗ and returns a virtual list: Example illustrating MERGEPRODUCTS, where L1 = {1, 2,6, 10} and L2 = {1, 4, 7}.Numbers are negative log-probabilities.It assumes that the input lists are sorted and returns a sorted list.A naive implementation of MERGEPRODUCTS would simply calculate all possible products and sort; however, if we are only interested in the top part of the result, we can implement MERGEPRODUCTS so that the output values are computed lazily and the input lists are accessed only as needed.To do this, we must assume that the multiplication operator ® is monotonic in each of its arguments.By way of motivation, consider the simple case G = {(L1,L2)}.The full set of possible products can be arranged in a two-dimensional grid (see Figure 5a), which we could then sort to obtain MERGEPRODUCTS(G).But because of our assumptions, we know that the first element of MERGEPRODUCTS(G) must be L1[1] ® L2[1].Moreover, we know that the second element must be either L1[1] ® L2[2] or L1[2] ® L2[1].In general (see Figure 5b), if some of the cells have been previously enumerated, the next cell must be one of the cells (shaded gray) adjacent to the previously enumerated ones and we need not consider the others (shaded white).In this way, if we only want to compute the first few elements of MERGEPRODUCTS(G), we can do so by performing a small number of products and discarding the rest of the grid.Figure 6 shows the pseudocode for MERGEPRODUCTS.7 In lines 2–5, a priority queue is initialized with the best element from each L E G, where L ranges over tuples of lists, and 1 stands for a vector whose elements all have the value 1 (the dimensionality of the vector should be evident from the context).The rest of the function creates the virtual list: To enumerate the next element of the list, we first insert the elements adjacent to the previously enumerated element, if any (lines 9–13, where bi stands for the vector whose ith element is 1 and is zero elsewhere), and then enumerate the best element in the priority queue, if any (lines 14–18).We assume standard implementations of 7 This version corrects the behavior of the previously published version in some boundary conditions.Thanks to D. Smith and J.May for pointing those cases out.In the actual implementation, an earlier version is used which has the correct behavior but not for cyclic forests (which the parser never produces).Function for computing the union of products of sorted lists (Huang and Chiang 2005).the priority queue subroutines HEAPIFY, INSERT, and EXTRACTBEST (Cormen et al. 2001).The k-best list generator is then easy to define (Figure 7).First, we generate a parse forest; then we simply apply MERGEPRODUCTS recursively to the whole forest, using memoization to ensure that we generate only one k-best list for each item in the forest.The pseudocode in Figure 7 will find only the weights for the k-best derivations; extending it to output the translations as well is a matter of modifying line 5 to package the English sides of rules together with the weights w, and replacing the real multiplication operator × in line 9 with one that not only multiplies weights but also builds partial translations out of subtranslations.We now turn to the problem of incorporating the language model (LM), describing three methods: first, using the −LM parser to obtain a k-best list of translations and rescoring it with the LM; second, incorporating the LM directly into the grammar in a construction reminiscent of the intersection of a CFG with a finite-state automaton; third, a hybrid method which we call cube pruning.5.3.1 Rescoring.One easy way to incorporate the LM into the model would be to decode first using the −LM parser to produce a k-best list of translations, then to rescore the k-best list using the LM.This method has the potential to be very fast: linear in k. However, because the number of possible translations is exponential in n, we may have to set k extremely high in order to find the true best translation (taking the LM into account) or something acceptably close to it.5.3.2 Intersection.A more principled solution would be to calculate the LM probabilities online.To do this, we view an m-gram LM as a weighted finite state machine M in which each state corresponds to a sequence of (m − 1) English terminal symbols.We can then intersect the English side of our weighted CFG G with this finite-state machine to produce a new weighted CFG that incorporates M. Thus PLM would be part of the rule weights (22) just like the other features.(For notational consistency, however, we write the LM probabilities separately from the rule weights.)In principle this method should admit no search errors, though in practice the blow-up in the effective size of the grammar necessitates pruning of the search space, which can cause search errors.The classic construction for intersecting a (non-synchronous) CFG with a finitestate machine is due to Bar-Hillel, Perles, and Shamir (1964), but we use a slightly different construction proposed by Wu (1996) for inversion transduction grammar and bigram LMs.We present an adaptation of his algorithm to synchronous CFGs with two nonterminals per right-hand side and general m-gram LMs.First, assume that the LM expects a whole sentence to be preceded by (m − 1) start-of-sentence symbols (s) and followed by a single end-of-sentence symbol (/s).The grammar can be made to do this simply by adding a rule and making S’ the new start symbol.First, we define two functions p and q which operate on strings over T U {*}, where T is the English terminal alphabet, and * is a special placeholder symbol that stands for an elided part of an English string.Values of p and q in the “cgisf” example.The function p calculates LM probabilities for all the complete m-grams in a string; the function q elides symbols when all their m-grams have been accounted for.These functions let us correctly calculate the LM score of a sentence piecemeal.For example, let m = 3 and “c g i s f” stand for “colorless green ideas sleep furiously.” Then Table 1 shows some values of p and q.Then we may extend the −LM parser as shown in Figure 8 to use p and q to calculate LM probabilities.We call this parser the +LM parser.The items are of the form [X, i, j; e], signifying that a subtree rooted in X has been recognized spanning from i to j on the French side, and its English translation (possibly with parts elided) is e. The theoretical running time of this algorithm is O(n3|T|4(m−1)), because a deduction can combine up to two starred strings, which each have up to 2(m − 1) terminal symbols.This is far too slow to use in practice, so we must use beam-search to prune the search space down to a reasonable size.5.3.3 Pruning.The chart is organized into cells, each of which contains all the items standing for X spanning fji+1.The rule items are also organized into cells, each of which contains all the rules with the same French side and left-hand side.From here on, let us Inference rules for the +LM parser.Here w[x/X] means the string w with the string x substituted for the symbol X.The function q is defined in the text. consider the item scores as costs, that is, negative log (base-10) probabilities.Then, for each cell, we throw out any item that has a score worse than: In the +LM parser, the score of an item [X, i, j; e] in the chart does not reflect the LM probability of generating the first (m − 1) words of e. Thus two items [X, i, j; e] and [X, i, j; e'] are not directly comparable.To enable more meaningful comparisons, we define a heuristic When comparing items for pruning (and only for pruning), we add this heuristic function to the score of each item.5.3.4 Cube Pruning.Now we can develop a compromise between the rescoring and intersection methods.Consider Figure 9a.To the left of the grid we have four rules with the same French side, and above we have three items with the same category and span, that is, they belong to the same chart cell.Any of the twelve combinations of these rules and items can be used to deduce a new item (whose scores are shown in the grid), and all these new items will go into the same chart cell (partially listed on the right).The intersection method would compute all twelve items and add them to the new chart cell, where most of them will likely be pruned away.In actuality, the grid may be a cube (one dimension for rules and two dimensions for two nonterminals) with up to b3 elements, whereas the target chart cell can hold at most b items (where b is the limit on the size of the cell imposed during pruning).Thus the vast majority of computed items are pruned.But it is possible to compute only a small corner of the cube and preemptively prune the rest of the items without computing them, a method we refer to as cube pruning.The situation pictured in Figure 9a is very similar to k-best list generation.The four rules to the left of the grid can be thought of like a 4-best list for a single −LM rule item (X --� cong X); the three items above the grid, like a 3-best list for the single −LM item [X, 6,8]; and the new items to be deduced, like a k-best list for [X, 5, 8], except that we don’t know what k is in advance.If we could use MERGEPRODUCTS to enumerate the new items best-first, then we could enumerate them until one of them was pruned from the new cell; then the rest of items, which would have a worse score than the pruned item, could be preemptively pruned.MERGEPRODUCTS expects its input lists to be sorted best-first, and the ® operator to be monotonic in each of its arguments.For cube pruning, we sort items (both in the inputs to MERGEPRODUCTS and in the priority queue inside MERGEPRODUCTS) according to their +LM score, including the heuristic function h. The ® operator we use takes one or more antecedent items and forms their consequent item according to Example illustrating hybrid method for incorporating the LM.Numbers are negative the +LM parser.Note that the LM makes this ⊗ only approximately monotonic.This means that the enumeration of new items will not necessarily be best-first.To alleviate this problem, we stop the enumeration not as soon as an item falls outside the beam, but as soon as an item falls outside the beam by a margin of e. This quantity e expresses our guess as to how much the scores of the enumerated items can fluctuate because of the LM.A simpler approach, and probably better in practice, would be simply to set e = 0, that is, to ignore any fluctuation, but increase R and b to compensate.See Figure 9b for an example of cube pruning.The upper-left grid cell is enumerated first, as in the k-best example in Section 5.2, but the choice of the second is different, because of the added LM costs.Then, the third item is enumerated and merged with the first (unlike in the k-best algorithm).Supposing a threshold beam of R = 5 and a margin of e = 0.5, we quit upon considering the next item, because, with a score of 7.7, it falls outside the beam by more than e. The rest of the grid is then discarded.The pseudocode is given in Figure 10.The function INFER+LM is used as the ® operator; it takes a tuple of antecedent +LM items and returns a consequent +LM item according to the inference rules in Figure 8.The procedure REPARSE+LM takes a −LM chart chart as input and produces a +LM chart chart'.The variables u, v stand for items in −LM and u', v', for items in +LM, and the relation v �i v' is defined as follows: For each cell in the input chart, it takes the single item from the cell and constructs the virtual list L of all of its +LM counterparts (lines 9–15).Then, it adds the top items of L to the target cell until the cell is judged to be full (lines 16–20).The implementation of our system, named Hiero, is in Python, a bytecode-interpreted language, and optimized using Psyco, a just-in-time compiler (Rigo 2004), and Pyrex, a Python-like compiled language, with C++ code from the SRI Language Modeling Toolkit (Stolcke 2002).In this section we report on experiments with Mandarin-toEnglish translation.Our evaluation metric is case-insensitive BLEU-4 (Papineni et al. 2002), as defined by NIST, that is, using the shortest (as opposed to closest) reference sentence length for the brevity penalty.We ran the grammar extractor of Section 3.2 on the parallel corpora listed in Table 2 with the exception of the United Nations data, for a total of 28 million words (English side).8 We then filtered this grammar for our development set, which was the 2002 NIST MT evaluation dry-run data, and our test sets, which were the data from the 2003–2005 NIST MT evaluations.Some example rules are shown in Table 3, and the sizes of the filtered grammars are shown in Table 4.We also used the SRI Language Modeling Toolkit to train two trigram language models with modified Kneser–Ney smoothing (Kneser and Ney 1995; Chen and Goodman 1998): one on 2.8 billion words from the English Gigaword corpus, and the other on the English side of the parallel text (28 million words).Table 5 shows the average decoding time on part of the development set for the three LM-incorporation methods described in Section 5.3, on a single processor of a dual 3 GHz Xeon machine.For these experiments, only the Gigaword language model was used.We set b = 30, R = 1 for X cells, b = 15, R = 1 for S cells, and b = 100 for rules except where noted in Table 5.Note that values for R and e are only meaningful relative to the scale of the feature weights; here, the language model weight was 0.06.The feature weights were obtained by minimum-error-rate training using the cube-pruning (e = 0.1) decoder.For the LM rescoring decoder, parsing and k-best list generation used feature weights optimized for the −LM model, but rescoring used the same weights as the other experiments.We tested the rescoring method (k = 103 and 104), the intersection method, and the cube-pruning method (e = 0, 0.1, and 0.2).The LM rescoring decoder (k = 104) is the fastest but has the poorest BLEU score.Identifying and rescoring the k-best derivations is very quick; the execution time is dominated by reconstructing the output strings for the k-best derivations, so it is possible that further optimization could reduce these times.The intersecting decoder has the best score but runs very slowly.Finally, the cubepruning decoder runs almost as fast as the rescoring decoder and translates almost as well as the intersecting decoder.Among these tests, e = 0.1 gives the best results, but in general the optimal setting will depend on the other beam settings and the scale of the feature weights.We compared Hiero against two baselines: the state-of-the-art phrase-based system ATS (Och et al. 2004; Thayer et al.2004), and Hiero itself run as a conventional phrase-based system with monotone translation (no phrase reordering).The ATS baseline was trained on all the parallel data listed in Table 1, for a total of 159 million words (English side).The second language model was also trained on the English side of the whole bitext.Phrases of up to 10 in length on the French side were extracted from the parallel text, and minimum-error-rate training (Och 2003) was performed on the development set for 17 features, the same as used in the NIST 2004 and 2005 evaluations.9 These features are similar to the features used for our system, but also include features for phrase-reordering (which are not applicable to our system), IBM Model 1 in both directions, a missing word penalty, and a feature that controls a fallback lexicon.The other baseline, which we call Hiero Monotone, is the same as Hiero except with the limitation that extracted rules cannot have any nonterminal symbols on their righthand sides.In other words, only conventional phrases can be extracted, of length up to 5.These phrases are combined using the glue rules only, which makes the grammar equivalent to a conventional phrase-based model with monotone translation.Thus this system represents the nearest phrase-based equivalent to our model, to provide a controlled test of the effect of hierarchical phrases.We performed minimum-error-rate training separately on Hiero and Hiero Monotone to maximize their BLEU scores on the development set; the feature weights for Hiero are shown in Table 6.The beam settings used for both decoders were R = 30, b = 30 for X cells, R = 30, b = 15 for S cells, b = 100 for rules, and e = 3.On the test set, we found that Hiero improves over both baselines in all three tests (see Table 7).All improvements are statistically significant (p < 0.01) using the sign test as described by Collins, Koehn, and Kuˇcerov´a (2005).Syntax-based statistical machine translation is a twofold challenge.It is a modeling challenge, in part because of the difficulty of coordinating syntactic structures with potentially messy parallel corpora; it is an implementation challenge, because of the added complexity introduced by hierarchical structures.Here we have addressed the modeling challenge by taking only the fundamental idea from syntax, that language is hierarchically structured, and integrating it conservatively into a phrase-based model typical of the current state of the art.This fusion does no violence to the latter; indeed, we have presented our approach as a logical outgrowth of the phrase-based approach.Moreover, hierarchical structure improves translation accuracy significantly.Feature weights obtained by minimum-error-rate training. language model (large) 1.00 language model (bitext) 1.03 The choice to use hierarchical structures that are more complex than flat structures, as well as rules that contain multiple lexical items instead of one, an m-gram model whose structure cuts across the structure of context-free derivations, and large amounts of training data for meaningful comparison with modern systems—these all threaten to make training a synchronous grammar and translating with it intractable.We have shown how, through training with simple methods inspired by phrase-based models, and translating using a modified CKY with cube pruning, this challenge can be met.Clearly, however, we have only scratched the surface of the modeling challenge.The fact that moving from flat structures to hierarchical structures significantly improves translation quality suggests that more specific ideas from syntax may be valuable as well.There are many possibilities for enriching the simple framework that the present model provides.But the course taken here is one of organic development of an approach known to work well at large-scale tasks, and we plan to stay this course in future work towards more syntactically informed statistical machine translation.I would like to thank Liang Huang, Philipp Koehn, Adam Lopez, Nitin Madnani, Daniel Marcu, Christof Monz, Dragos Munteanu, Philip Resnik, Michael Subotin, Wei Wang, and the anonymous reviewers.This work was partially supported by ONR MURI contract FCPO.810548265, by Department of Defense contract RD-02-5700, and under the GALE program of the Defense Advanced Research Projects Agency, contract HR 0011-06-C-0022.S. D. G.
A Plan-Based Analysis Of Indirect Speech ActWe propose an account of indirect forms of speech acts to request and inform based on the hypothesis that language users can recognize actions being performed by others, infer goals being sought, and cooperate in their achievement.This cooperative behaviour is independently motivated and may or may not be intended by speakers.If the hearer believes it is intended, he or she can recognize the speech act as indirect; otherwise it is interpreted directly.Heuristics are suggested to decide among the interpretations.We propose an account of indirect forms of speech acts to request and inform based on the hypothesis that language users can recognize actions being performed by others, infer goals being sought, and cooperate in their achievement.This cooperative behaviour is independently motivated and may or may not be intended by speakers.If the hearer believes it is intended, he or she can recognize the speech act as indirect; otherwise it is interpreted directly.Heuristics are suggested to decide among the interpretations.Austin [1962] was one of the first to stress the distinction between the action(s) which a speaker performs by uttering a sentence (such as informing, requesting, or convincing) and the truth conditions of propositions contained in the sentence.Actions have effects on the world, and may have preconditions which must obtain for them to be felicitously performed.For actions whose execution involves the use of language (or speech acts), the preconditions may include the speaker holding certain beliefs about the world, and having certain intentions or wants as to how it should change.As well as being important to the study of natural language semantics, speech acts are important to the designer of conversational natural language understanding systems.Such systems should be able to recognize what actions the user is performing.Conversely, if such a system is to acquire information or request assistance from its user, it should know how and when to ask questions and make requests.(See Bruce [1975] for an early attempt.)Cohen and Perrault [1979] (hereafter referred to as CP) argue for the distinction between a competence I This research was supported in part by the National Research Council of Canada under Operating Grant A9285.Thanks to Phil Cohen, Michael McCord, Corot Reason, and John Searle for their comments.We assume the usual responsibility for remaining inaccuracies, misunderstandings, and downright errors. theory of speech acts, which characterizes what utterances an ideal speaker can make in performing what speech acts, and a performance theory which also accounts for how a particular utterance is chosen in given circumstances, or how it is recognized.We are only concerned here with a competence theory.In Perrault, Allen, and Cohen [1978] we suggested that it is useful to consider speech acts in the context of a planning system.A planning system consists of a class of •parameterized procedures called operators, whose execution can modify the world.Each operator is labelled with formulas stating its preconditions and effects.A plan construction algorithm is a procedure which, given a description of some initial state of the world and a goal state to be achieved, constructs a plan, or sequence of operators, to achieve it.It is assumed there, and in all our subsequent work, that language users maintain a model of the world (their beliefs) and a set of goals (their wants).One person S's beliefs may include beliefs about another person A's beliefs and wants, including A's beliefs about S, etc.We do not concern ourselves with obligations, feelings, etc., which clearly can also be affected by speech acts.CP discuss criteria for judging the correctness of the preconditions and effects of the operators corresponding to speech acts, and specifically those of the acts INFORM and REQUEST.However, the conditions on INFORM and REQUEST given in CP are at best necessary and certainly not sufficient.In particuCopyright 1980 by the Association for Computational Linguistics.Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the Journal reference and this copyright notice are included on the first page.To copy otherwise, or to republish, requires a fee and/or specific permission. lar they say nothing about the form of utterances used to perform the speech acts.Several syntactic devices can be used to indicate the speech act being performed: the most obvious are explicit performative verbs such as &quot;I hereby request you to ...&quot;, and mood (indicative for assertions, imperative for requests to do, interrogative for requests to inform).But the mood of an utterance is well known to not completely specify its illocutionary force: 1.a-b can be requests to close the door, 1.c-e can be requests to tell the answer, and 1.f can be an assertion.Furthermore, all these utterances can also be intended literally in some contexts.For example, a parent leaving a child at the train station may ask 1.g expecting a yes/no answer as a confirmation.The object of this paper is to extend the work in CP to account for indirect use of mood, loosely called indirect speech acts.The solution proposed here is based on the following intuitively simple and independently motivated hypotheses: identifying actions being performed by others and goals being sought.An essential part of helpful or cooperative behaviour is the adoption by one agent of a goal of another, followed by an attempt to achieve it.For example, for a store clerk to reply &quot;How many do you want?&quot; to a customer who has asked &quot;Where are the steaks?&quot;, the clerk must have inferred that the customer wants steaks, then he must have decided to get them himself.This might have occurred even if the customer had intended to get the steaks him or herself.Cooperative behaviour must be accounted for independently of speech acts, for it often occurs without the use of language. tends that the hearer recognize not only that B was performed but also that through cooperative behaviour by the hearer, intended by the speaker, the effects of A should be achieved.The speaker must also believe that it is likely that the hearer can recognize this intention.The process by which one agent can infer the plans of another is central to our account of speech acts.Schmidt et al [1978] and Genesereth [1978] present algorithms by which one agent can infer the goals of another, but assuming no interaction between the two.We describe the process in terms of a set of plausible plan inference rules directly related to the rules by which plans can be constructed.Let A and S be two agents and ACT an action.One example of a simple plan inference rule is: &quot;If S believes that A wants to do ACT then it is plausible that S believes that A wants to achieve the effects of ACT.&quot; From simple rules like this can be derived more complex plan inference rules such as: &quot;If S believes that A wants S to recognize A's intention to do ACT, then it is plausible that S believes that A wants S to recognize A's intention to achieve the effects of ACT.&quot; Notice that the complex rule is obtained by introducing &quot;S believes A wants&quot; in the antecedent and consequent of the simple rule, and by interpreting &quot;S recognizes A's intention&quot; as &quot;S comes to believe that A wants&quot;.Throughout the paper we identify &quot;want&quot; and &quot;intend&quot;.We show that rules of the second type can account for S's recognition of many indirect speech acts by A, i.e. those in which S recognizes A's intention that S perform cooperative acts.To distinguish the use of, say, the indicative mood, in an assertion from its use in, say, an indirect request, the speech act operators REQUEST and INFORM of CP are reformulated and two further acts S.REQUEST and S.INFORM are added.These surface level acts are realized literally as indicative and imperative utterances.An S.REQUEST to INFORM is realized as a question.The surface level acts can be recognized immediately as parts of the higher level (or illocutionary level) acts, to which the simple plan construction and inference rules can apply.Alternatively, the complex rules can be applied to the effects of the surface acts, and the intended performance of one of the illocutionary acts inferred later.For example, there are two ways an agent S could be led to tell A the secret after hearing A tell him &quot;Can you tell me the secret?&quot;.Both start with S's recognition that A asked a yes/no question.In the first case, S assumes that A simply wanted to know whether S could tell the secret, then infers that A in fact wants to know the secret and, helpfully, decides to tell it.In the second case S recognizes that A intends S to infer that A wants to know the secret and that A intends S to tell A the secret, and thus that A has requested S to tell the secret.Following a review of the relevant aspects of speech act theory in section 2, section 3 outlines our assumptions about beliefs, goals, actions, plans, and the plan inference process.Section 4 shows how the speech act definitions and the plan inference process can be used to relate literal to indirect meanings for REQUESTs and INFORMs.We show how utterances such as 1.h-1, and even 1.m can be used as requests to pass the salt, and what the origin of the several interpretations of 1.m is.Similarly we show how 1.n can be used to inform while 1.o cannot.Section 5 relates this work to the literature, while section 6 suggests further problems and draws some conclusions.The speech act recognition process described here has been implemented as a computer program and tested by having it simulate an information clerk at a railway station.This domain is real, but sufficiently circumscribed so that interchanges between clerk and patrons are relatively short and are directed towards a limited set of goals.The program accepts as input simple English sentences, parses them using an ATN parser, and produces as output the speech act(s) it recognized and their associated propositional contents.It can handle all the examples discussed here.Details of the implementation can be found in Allen [1979].Prior to Austin [1962], logicians considered the meaning of a sentence to be determined only by its truth value.However, Austin noted that some sentences cannot be classified as true or false; the utterance of one of these sentences constitutes the performance of an action, and hence he named them performatives.To quote Austin: &quot;When I say, before the register or altar, etc., 'I do', I am not reporting on a marriage: I am indulging in it&quot;.Examples like this, and his inability to rigorously distinguish performative sentences from those which purportedly have truth value (which he called constatives) led Austin to the view that all utterances could be described as actions, or speech acts.He classified speech acts into three classes, the locutionary, illocutionary, and perlocutionary acts.A locutionary act is an act of saying something: it is the act of uttering sequences of words drawn from the vocabulary of a given language and conforming to its grammar.An illocutionary act is one performed in making an utterance; &quot;promise&quot;, &quot;warn&quot;, &quot;inform&quot; and &quot;request&quot; are names of illocutionary acts.In general, any verb that can complete the sentence &quot;I hereby <verb> you {that I to] ...&quot; names an illocutionary act.An utterance has illocutionary force F if the speaker intends to perform the illocutionary act F by making that utterance.Verbs that name types of illocutionary acts are called performative verbs.From now on, we take speech acts to mean the illocutionary acts.Perlocutionary acts are performed by making the utterance.For example, S may scare A by warning A, or convince A of something by informing A of it.The success of a perlocutionary act is typically beyond the control of the speaker.For example, S cannot convince A of something against A's will, S can only present A with sufficient evidence so that A will decide to believe it.Perlocutionary acts may or may not be intentional.For instance, S may or may not intend to scare A by warning A. Searle [1969] suggests that illocutionary acts can be defined by providing, for each act, necessary and sufficient conditions for the successful performance of the act.Certain syntactic and semantic devices, such as mood and explicit performative verbs, are used to indicate illocutionary force.One of the conditions included in Searle's account is that the speaker performs an illocutionary act only if he intends that the hearer recognize his intention to perform the act, and thereby recognize the illocutionary force.This is important for it links Austin's work American Journal of Computational Linguistics, Volume 6, Number 3-4, July-December 1980 169 C. Raymond Perrault and James F. Allen A Plan-Based Analysis of Indirect Speech Acts on speech acts with the work of Grice on meaning, and is discussed in the next section.Many philosophers have noted the relationship between communication (or speaker meaning) and the recognition of intention (Grice [1957, 1968], Strawson [1964], Searle [1969], Schiffer [1972].)Grice presents informally his notion of a speaker meaning something as follows: &quot;S meant something by x' is (roughly) equivalent to 'S intended the utterance of x to produce some effect in an audience by means of the recognition of this intention' In other words, in order for S to communicate M by uttering x to A, S must get A to recognize that S intended to communicate M by uttering x.To use and example of Grice's, if I throw a coin out the window expecting a greedy person in my presence to run out and pick it up, I am not necessarily communicating to him that I want him to leave.For me to have successfully communicated, he must at least have recognized that I intended him to leave.The same arguments hold when discussing illocutionary acts.For example, the only way S can request A to do ACT is to get A to recognize S's intention to request A to do ACT.The relation between speech acts and the devices used to indicate them is complicated by the fact that performative verbs are seldom present and the same device can be used to perform many illocutionary acts.The interrogative mood, for example, can be used to request: &quot;Can you pass the salt?&quot; question: &quot;Do you know the time?&quot; inform: &quot;Do you know that Sam got married?&quot; warn: &quot;Did you see the bear behind you?&quot; promise: &quot;Would I miss your party?&quot; As many authors have pointed out, an utterance conveys its indirect illocutionary force by virtue of its literal one (Searle [1975], Morgan [1977], Morgan [1978]).&quot;It's cold here&quot; can function as a request to, say, close the window, in part because it's an assertion that the temperature is low.Most of the literature on the treatment of indirect speech acts within the theory of grammar stems from the work of Gordon and Lakoff [1975] (hereafter GL).They claim that direct and indirect instances of the same speech act have different &quot;meanings&quot;, i.e. different logical forms, and they propose a set of &quot;conversational postulates&quot; by which literal forms &quot;entail&quot; indirect ones.The postulates for requests correspond to conditions that must obtain for a request to be sincere.For A to sincerely request B to do ACT, the following sincerity conditions must hold: (2.3a) Is the salt near you?(2.3b) John asked me to ask you to pass the salt.GL's postulates directly relate the literal form of one speech act to the indirect form of another.Thus they do not predict why certain acts allow certain indirect forms.For example, the postulates do not account for why 2.3c-d can be requests while 2.3e-f cannot.But 2.3e is infelicitous as a (literal) question since there is no context where one can acquire information by querying one's own mental state.Utterance 2.3f is a reasonable question but even if the speaker found out the answer, it would not get him any closer to acquiring the salt (by having the hearer pass it).A theory of indirect speech acts should capture these facts; GL's does not (although they agree it should).Similarly, GL's postulates fail to explain the relation between indirect forms of different speech acts.For example, 2.3g can be an assertion that P and 2.3h cannot, for the same reasons that 2.3i can be a request to do A and 2.3j cannot.The hearer's knowing that P obtains is an intended perlocutionary effect of an informing act, just as the hearer's doing an act A is an intended effect of a request.A speaker can indirectly inform or request by informing the hearer that the speaker desires the perlocutionary effect of that act, and intending that the hearer recognize the speaker's intention that the perlocutionary effect should be achieved.This paper shows that what GL achieve with their postulates can be derived from the five hypotheses given in the Introduction.Our proposal here is a de170 American Journal of Computational Linguistics, Volume 6, Number 3-4, July-December 1980 C. Raymond Perrault and James F. Allen A Plan-Based Analysis of Indirect Speech Acts velopment of Searle [1975].It requires separating the surface form conditions completely from the definitions of the illocutionary acts and introducing an intermediary level, the surface acts.Our theory of indirection will however share with GL some problems brought up by Sadock [1970], Green [1975], and Brown [1980].These are discussed further in section 4.5.Our analysis of indirect REQUESTs and INFORMs relies on the inference by the hearer of some of the goals of the speaker and of some of the actions which the speaker is taking to achieve those goals.Section 3.1 outlines the form of the models of the world which language users are assumed to have, in particular their beliefs about the world (and about other agents), and their goals.In section 3.2 we define actions and how they affect the belief model.The rules for plan construction and inference are considered in sections 3.3 and 3.4.Because of space limitations, this section is very sketchy.More detail, motivation, and problems, are available in Allen [1979] and Allen and Perrault [1980].We assume that every agent S has a set of beliefs about the world, which may include beliefs about other agents' beliefs.Agents can hold false beliefs.As Quine [1956] pointed out, belief creates a context where substitution of coreferential expressions need not preserve truth-value.We add to a first-order language with equality the operator B, and B(A,P) (usually written BA(P)) is to be read &quot;A believes that P&quot;, for any formula P. The B operator is assumed to satisfy the following axiom schemas (inspired by Hintikka [1962]), where P and Q are schema variables ranging over propositions, and A ranges over agents: The rules of inference are Modus Ponens and: If T is a theorem, then BA(T) is a theorem, for every agent A. i.e. every agent believes every valid consequence of the logical axioms.The partial deduction system used in the implementation of Allen [1979] is based on Cohen [1978].The foundations for a more elaborate system can be found in Moore [1979].The word &quot;know&quot; is used in at least three different senses in English.One may know that a proposition P is true, know whether a proposition P is true or know what the referent of a description is.We define &quot;A knows that P&quot;, written KNOW(A,P), as P A BA(P).This is weaker than some definitions of &quot;know&quot; in the philosophical literature, where, among other things, &quot;A knows that P&quot; entails that A believes P for the &quot;right reasons&quot;; i.e. knowledge is true and justified belief (Ayer [1956], but see also Gettier [1963]).If S believes that A knows that P, S is committed to believing that P is true.In other words, if S believes A does not know P, then S must believe that P is true in addition to believing that A does not believe P is true.This problem is analogous to the wide/narrow scope distinction that Russell found in his account of definite descriptions (Russell [1919]).One solution to this problem is to consider KNOW as a &quot;macro&quot; whose expansion is sensitive to negation.Details may be found in Allen [1979].A knows whether a proposition P is true if A KNOWs that P or A KNOWs that —P.Knowing what the referent of a description is requires quantification into belief.One of its arguments is a formula with exactly one free variable.A KNOWREF the departure time of TRAIN1 if TRAIN1 has a unique departure time y, and if A believes that y is TRAIN l's unique departure time.We let W(A,P) (usually written WA(P)) mean &quot;agent A wants P to be true&quot;.P can be either a state or the execution of some action.In the latter case, if ACT is the name of an action, WA(ACT(b)) means &quot;A wants b to do ACT&quot;.The logic of want is even more difficult than that of belief.It is necessary for us to accept the following: American Journal of Computational Linguistics, Volume 6, Number 3-4, July-December 1980 171 The most interesting interactions between the belief and want operators come from the models that agents have of each other's abilities to act and to recognize the actions of others.This will be further discussed in the following section.Actions model ways of changing the world.As with the operators in STRIPS (Fikes and Nilsson [1971]), the actions can be grouped into families represented by action schemas, which can be viewed as parameterized procedure definitions.An action schema consists of a name, a set of parameters with constraints and a set of labelled formulas in the following classes: Effects: Conditions that become true after the execution of the procedure.Body: a set of partially ordered goal states that must be achieved in the course of executing the procedure.In the examples given here, there will never be more than one goal state in a body.Preconditions: Conditions necessary to the successful execution of the procedure.We distinguish for voluntary actions a want precondition: the agent must want to perform the action, i.e. he must want the other preconditions to obtain, and the effects to become true through the achievement of the body.The constraints on the parameters consist of type specifications, and necessary parameter interdependencies.Each action has at least one parameter, namely, the agent or instigator of the action.In the blocks world, for example, the action of putting one block on top of another could be defined as: The preconditions, effects and body provide information to the plan construction and inference processes so that they can reason about the applicability and effect of performing the action in a given context.Finally, the body of the action specifies what steps must be achieved in the course of the execution of the action.Primitive actions have no bodies; their execution is specified by a non-examinable procedure.All agents are assumed to believe that actions achieve their effects and require their preconditions.We need the following axioms: For all agents a and b, and for all actions ACT, if PRE is the precondition of ACT and EFF its effect then: Every predicate and modal operator in these axioms, and throughout the paper, should be indexed by a state or time.The resulting logic would be, accordingly, more complex.The issue is raised again in sect.6.A plan to transform a world W[0] (represented by a formula) into a world W[n] is a sequence of actions Al, ..., An such that the preconditions of Ai are true in W[i-1], and Ai transforms world W[i-1] into W[i].An agent can achieve a goal by constructing and then executing a plan which transforms the current state of the world into one in which the goal obtains.This can be done by finding an operator which, if executed in some world, would achieve the goal.If its preconditions are satisfied in the initial world, the plan is complete.Otherwise, the planning process attempts to achieve the preconditions.This simple view of plan construction as a &quot;backward chaining&quot; process can be refined by assuming different levels of &quot;detail&quot; in the representation of the world and of the operators.This view (as developed in Sacerdoti [1973, 1975], for example) allows plans constructed at one level of detail to be expanded to a lower level through the bodies of their constituent acts.As noted earlier, the agent of an action must believe that its precondition is true to believe that his executing the action will succeed.For agent A to plan that agent S should perform action ACT, A must achieve that S should believe that the precondition of ACT holds, and S's beliefs should not be inconsistent with A's, i.e. it must be true that BA(KNOW(S,P)), where P is the precondition of ACT.We assume that an agent cannot do an action without wanting to do that action.Thus a precondition of every action ACT by an agent A is that WA(ACT(A)).We are concerned with the model that agents have of each other's plan construction and inference process, and consider these two processes as consisting of chains of plausible inferences operating on goals and observed actions.The processes are specified in two parts: first as schemas of rules which conjecture that certain states or actions can be added to a plan being constructed.The plausibility of the plans containing the result of the inferences is then evaluated by rating heuristics.Thus the plan construction and inference rules are not to be interpreted as valid logical rules of inference.There are two inverses to the KNOWIF rule: if A wants to know whether P is true, then A may want P to be true, or A may want P to be false.2 Throughout the rest of the paper agent A will usually denote the constructor/executor of plans, and S (or System) the recognizer of plans (usually constructed by A).PI.W is the special case of the precondition-action rule where the precondition is the want precondition: (PI.W) [Want rule] For all agents S, A, and C and for all actions ACT whose agent is C, it is plausible that The plan inference rules generate formulas which the recognizing agent believes are possible.A separate mechanism is used to evaluate their plausibility.An agent S attempting to infer the plans of another agent A starts with an observed action of A and a (possibly empty) set of goals or expectations which S believes A may be trying to achieve.S attempts to construct a plan involving the action and preferably also including some of the expectations.Plan inference is a search through a space of partial plans each consisting of two parts.One part is constructed using the plan inference rules from the observed action (and called the alternative); the other is constructed using the plan construction rules from an expected goal (and called the expectation).The partial plans are manipulated by a set of tasks which decide what rules are to be applied, what &quot;merges&quot; between alternatives and expectations should be attempted, and when the process terminates.The partial plans and their associated tasks are rated by a set of heuristics, and the most highly rated task is executed first.The rating of a partial plan reflects how likely it is to be part of the &quot;correct&quot; plan, i.e. the plan the speaker is executing.If several incompatible inferences can be made from one point in the alternative, then its rating is divided among them.The heuristics described in this section are based on domain independent relations between actions, their bodies, preconditions, and effects.The need for more domain dependent measures is discussed later.American Journal of Computational Linguistics, Volume 6, Number 3-4, July-December 1980 173 of tions are possible in rules EC.1 - EC.3 and EI.1 EI.3.The heuristics are described here only in terms increasing or decreasing ratings of partial plans.Decrease the rating of a partial plan in which the preconditions of executing actions are currently false.Decrease the rating of a partial plan containing a pending action ACT by an agent A if A is not able to do ACT.3 Decrease the rating of a partial plan in which the effects of a pending act already obtain or are not wanted by the planner.4 Other heuristics depending on how well the utterance fits with the expectations are not immediately relevant to understanding indirect speech acts and will not be discussed here.One further heuristic is added in section 4.3.In general several rating heuristics are applicable to an partial plan.Their effects on the rating of the partial plan are cumulative.A hearer S identifies the illocutionary force of an utterance by recognizing that the speaker A has certain intentions, namely that S should recognize some intention P of A's.This can be represented by a formula of the form BsWA(BsWA(P)).To do the recognition, the simple plan construction and inference rules of sections 3.3 and 3.4 must be extended so that they can operate on these nested formulas.This can be done by assuming that every agent is aware that other agents construct and infer plans in the same way he can.In fact, both the simple inference and construction rules are necessary to derive the extended inference rules.The extended rules are specified by &quot;meta-rules&quot; which show how to construct new PC/PI rules from old ones.The first extended construction rule (EC.1) is: A can achieve that S recognizes that A wants the effect of ACT by achieving that S recognizes that A wants ACT to be done, assuming that S would infer that the effects of ACT are also desired.The same rule applies if we replace &quot;wants the effect of ACT&quot; and &quot;wants ACT to be done&quot; by any pair of Y and X, as given in Figure 1.We assume all these sutistitu(EC.1) If BsWA(X) =i=> BW(Y) is a PI rule, then WA(BsWA(Y)) =c=> WA(BsWA(X)) is a PC rule.Similarly we can generate the corresponding PI rule: (EI.1) If BsWA(X) =i=> BW(Y) is a PI rule, then BsWA(BsWA(X)) =i=> BsWA(BsWA(Y)) is a PI rule.EI.1 allows prefixing BsWA to plan inference rules.Plan construction rules can also be embedded: if A wants S to want to do ACT, then A should be able to achieve this by achieving that S wants the effect of ACT, and by relying on S to plan ACT.In other words: Finally, any agent A can plan for S to recognize A's intention that S plan, and for S to be able to recognize this intention in A.For example, A can plan for S to recognize A's intention that S want to close the door by planning for S to recognize A's intention that S want the door closed.These rules are obtained by using EI.2 as the PI rule which is &quot;extended&quot; by EC.1 and EI.1.Our &quot;toolkit&quot; is now sufficiently full to allow us to consider some speech acts and their recognition.The definitions of the speech acts REQUEST and INFORM used in this paper .are slightly different from the ones in Cohen and Perrault [1979] in that they rely on the existence of speech act bodies to account for indirect forms.Plans including speech acts are now thought of as having two levels, the illocutionary level and the surface level.Acts at the illocutionary level model the intentions motivating an utterance independently of the syntactic forms used to indicate those intentions.Acts at the surface level are realized by utterances having specific illocutionary force indicators.The first illocutionary level act is one by which a speaker informs a hearer that some proposition is true.For A to sincerely inform S that P is true, A must believe A knows that P is true and want to inform S that P (the preconditions), and must intend to get S to know that P is true (the effect), which is done by constructing a plan that will achieve S's recognition of this intention (i.e. that Bs(WA(KNOW(S,P)))).A then must depend on S to bring about the efiect: S must decide to believe what A said.This is made explicit by introducing an admittedly simplistic DECIDE TO BELIEVE act: DECIDE TO BELIEVE(agent, other, P) prec: B(agent,W(other,KNOW(agent,P))) effect: KNOW(agent,P) Thus A can INFORM S of P by achieving BsWA(KNOW(S,P)) followed by DECIDE TO BELIEVE(S,A,P).In many cases, agents reason about INFORM acts to be performed (by others or by themselves) where the information for the propositional content is not known at the time of plan construction.For example, A may plan for S to inform A whether P is true.A cannot plan for S to perform INFORM(S,A,P) since this assumes the truth of P. We get around this difficulty by defining INFORMIF, another view of the INFORM act.INFORMIF(speaker, hearer, P) prec: KNOWIF(speaker,P) A W(speaker,INFORMIF(speaker,hearer,P)) effect: KNOWIF(hearer,P) body: B(hearer,W(speaker,KNOWIF(hearer,P))) Similarly, it must be possible for A to plan for S to tell A the referent of a description, without A knowing the referent.This is the role of the INFORMREF act.INFORMREF(speaker, hearer, D(x)) prec: KNOWREF(speaker,D(x)) A W(speaker,INFORMREF(speaker, hearer,D(x))) effect: KNOWREF(hearer,D(x)) body: B(hearer,W(speaker,KNOWREF( hearer,D(x)))) Request is defined as: REQUEST(speaker, hearer, action) constraint: hearer is agent of action The intention of a request is to get the hearer to want to do the action, and this is accomplished by getting the hearer to believe that the speaker wants the hearer to do the action and then depending on the hearer to decide to do it.To explicitly represent this decision process, a CAUSE TO WANT act defined along the lines of the DECIDE TO BELIEVE act above is necessary.CAUSE TO WANT(agent, other, P) prec: B(other,B(agent,W(agent,P))) effect: W(other,P) As examples of the use of speech acts, &quot;Tell me whether the train is here&quot; and &quot;Is the train here?&quot;, intended literally, are both REQUESTs by A that S INFORMIF the train is here.&quot;When does the train arrive?&quot;, intended literally, is a REQUEST by A that H INFORMREF of the departure time of the train.Finally we define the two surface level acts: S.INFORM produces indicative mood utterances, and S.REQUEST produces imperative utterances, or interrogative utterances, if the requested act is an INFORM.These acts have no preconditions, and serve solely to signal the immediate intention of the speaker, the starting point for all the hearer's inferencing.S.INFORM(speaker, hearer, P) effect: B(hearer,W(speaker,KNOW(hearer,P))) S.REQUEST(speaker, hearer, action) effect: B(hearer,W(speaker,action(hearer))) The effects of S.INFORM match the body of the INFORM act, reflecting the fact that it is a standard way of executing an INFORM.It is important, however, that S.INFORM is only one way of executing an INFORM.The same relationship holds between the S.REQUEST and REQUEST actions.Given the speech act definitions of section 4.1, we say that A performed an illocutionary act IA by uttering x to S if A intends that S should recognize (and be able to recognize) that This definition allows more than one illocutionary act to be performed by a single surface act.In this section we show how the hearer of an utterance can recognize the speaker's intention(s) indicated by a speech act, especially when these intentions are communicated indirectly. prec: W(speaker,action(hearer)) effect: W(hearer,action(hearer)) body: B(hearer,W(speaker,action(hearer))) 5 See Cohen and Perrault [1979] for a discussion of why Searle's preparatory conditions &quot;Speaker believes Hearer can do the action&quot; need not be part of the preconditions on REQUEST.American Journal of Computational Linguistics, Volume 6, Number 3-4, July-December 1980 175 C. Raymond Perrault and James F. Allen A Plan-Based Analysis of Indirect Speech Acts All inferencing by S of A's plans starts from S's recognition that A intended to perform one of the surface acts, and that A in fact wanted to do the act.All inference chains will be shown as starting from a formula of the form BsWA(A do the surface act).The object of the inferencing is to find what illocutionary level act(s) A intended to perform.The action-effect rule applied to the starting formula yields one of the form BsWA(BsWA(P)), i.e.S believes that A wants S to recognize A's intention that P. The inferencing process searches for plausible formulas of the form BsWA(IA(A)) where IA is an illocutionary level act.Example 1 shows a direct request to pass the salt, where the surface request maps directly into the intended request interpretation.6 The actions relevant to the examples given here are: Let us also assume that S presently has the salt, i.e.HAVE(S,SALT) is true, and mutually believed by S and A.The rating heuristics for the complex rules EI.1 to EI.3 are the same as for the PI rules but each heuristic may be applicable several times at different levels.For example, consider the frequently recurring inference chain: It shows the line of inference from the point where S recognizes that A requested S to do ACT (at step (2)) to the point where the effects of the requested action are inferred as part of A's plan.Of interest here is the evaluation of the plausibility of step (3).Two heuristics are applicable.The proposition &quot;Ws(ACT(S))&quot; is 6 To improve readability of inference chains in the examples, we drop the prefix BsWA from all propositions.The formula on line (n) follows from the one on line (n-1) by the rule at the beginning of line (n).Applications of EI.1 will be labelled &quot;rule&quot;/EI.1, where &quot;rule&quot; is a PI rule embedded by EI.1.Similarly, applications of EI.2 and EI.3 will be labelled &quot;rule&quot;/EI.2 and &quot;rule&quot;/EI.3, where &quot;rule&quot; is a PC rule name. evaluated with respect to what S believes A believes.(Remember that BsWA should appear as a prefix to all propositions in inference chains.)If BsBAWs(ACT(S)) is true, the request interpretation is considered unlikely, by the effect-based heuristic.In addition, the preconditions of ACT(S) are considered with respect to what S believes A believes S believes.This step will only be reasonable if S can do the action, by a precondition-based heuristic.To make more explicit the distinction between inferences in BsWA and inferences in BsWABsWA, let us consider two inference chains that demonstrate two interpretations of the utterance &quot;Do you know the secret?&quot;.Lines 1-3 of Example 2 show the chain which leads S to believe that A asked a (literal) yes/no question; lines 1-6 of Example 3 show the interpretation as a request to S to inform A of the secret.Notice that in both interpretations S may be led to believe that A wants to know the secret.In the literal case, S infers A's goal from the literal interpretation, and may tell the secret simply by being helpful (lines 4-9).In the indirect case, S recognizes A's intention that S inform A of the secret (lines 1-6).Telling the secret is then conforming to A's intentions (lines 7-9).There is in fact a third interpretation of this sentence.If A and S both know that A already knows the secret, then the utterance could be intended as &quot;If you don't know the secret, I will tell it to you.&quot; This requires recognizing a conditional action and is beyond our present abilities.Two sets of PI rules are applicable to formulas of the form BsWABsWA(P): the simple rules PI.1 to PI.6 operating &quot;within&quot; the prefix BsWA, and the rules generated by EI.1 and EI.3 which allow the simple rules to apply within the prefix BsWABsWA.To reflect the underlying assumption in our model that intention will always be attributed if possible, the inferences at the most deeply nested level should be preferred.Of course, if the inferences at the nested level lead to unlikely plans, the inferences at the &quot;shallow&quot; levels may be applied.In particular, if there are multiple mutually exclusive inferences at the nested level, then the &quot;shallow&quot; inferences will be preferred.This reflects the fact that the nested inferences model what the speaker intends the hearer to infer.If there are many inferences possible at the nested level, the speaker would not be able to ensure that the hearer would perform the correct (i.e., the intended) one.Example 4 shows the interpretation of &quot;I want you to pass the salt&quot; as a request.Taking the utterance literally, S infers that A wants him to know that A wants him to pass the salt.This yields proposition (2) which leads through the next three inferences to the intention that would be recognized from a request act, i.e. that A wants S to pass the salt (5).Notice that an application of the body-action rule to step (2) yields: INFORM(A, S, WA(PASS(S, A, SALT))), for, in fact, the speaker may be performing both speech acts.The level of inferencing heuristic favours the indirect form.The key step in Example 5 is the application of the know-positive rule from line (3) to line (4).Since, given the context, S assumes that A knows whether S has the salt, the literal interpretation (from (2)) would not produce a reasonable goal for A.This supports the nested know-positive inference, and attributes further intention to the speaker (4).Once this is done, it is easy to infer that A wants S to pass him the salt (5), hence the request interpretation.&quot;Can you pass the salt?&quot; and &quot;Do you want to pass the salt?&quot; are treated similarly, for they inquire about the preconditions on PASS(S, A, SALT).Example 7.&quot;I want the salt.&quot; (= &quot;I want to have the salt.&quot;) Example 7 includes in the step from (3) to (4), an application, through EI.3, of the effect-action rule.A informs S of A's goal of having the salt (2) and then depends on S's planning on that goal to infer the PASS action.Because the action is the &quot;obvious&quot; way of achieving the goal, S believes that A intended him to infer it.Since questions are treated as requests to inform, most of them are handled in a similar manner to the requests above.4.4a-h can all be understood as questions about the departure time of some train.An interesting example of an indirect INFORM is 4.5a for it is very similar to 4.5b-c which both seem to only be requests.The interpretation of 4.5a as an indirect INFORM follows from the fact that inference chains which would make it a REQUEST are all inhibited by the heuristics.In Example 8, the possible body-action inference from (2) to REQUEST(A,S,INFORMIF(S,A,KNOW(S,P))) is downgraded because the embedded inference to (3) is possible.The interesting case is the embedded know-negative inference which is also possible from (3).It implies that BsWA(—KNOW(S,P)), or equivalently But such a goal is highly unlikely.A is attempting to achieve the goal —Bs(P) by having S recognize that A wants P to be true!As a result, no speech act interpretation is possible from this step.For instance, the bodies of the acts INFORM(A, S. P) and INFORM(A, S, —P) are BsWA(P A Bs(P)), and BsWA(—P A Bs(—P)), respectively.Both of these are contradicted by part of 4.5d.Thus the know-negative possibility can be eliminated.This allows the know-positive inference to be recognized as intended, and hence leads to the indirect interpretation as an INFORM(A, S, P).4.5b has only a literal interpretation since both the know-positive and know-negative rules are applicable at the nested level; without a reason to favour either, the literal REQUEST(A,S,INFORMIF(S,A,Bs(P))) is preferred.The interpretations of 4.5c are similar to those of Examples 2 and 3.All the examples of indirect speech acts so far have been explained in terms of rules PI.1-PI.6, and complex inference rules derived from them.In this section, we give one more example relying on somewhat more specific rules.A full investigation of how many such specific rules are necessary to account for common forms of indirect REQUESTs and INFORMs remains to be done.This example shows how a completely non-standard form can be intended indirectly.Suppose that A tells (4.6a) &quot;John asked me to ask you to leave&quot; This has at least three possible interpretations: him to leave.Interpretations c and d can hold even if S decides that A actually does want him to leave.However, in these cases, he would not say that A intended to communicate the intent that he leave, i.e. he would not say the utterance was a REQUEST.Both interpretations rely on axioms ACT.1 and ACT.2 (of section 3.2) which state that if some agent A believes that agent S executed some action ACT, then A may believe that the preconditions of ACT obtained before, and the effects of ACT obtained after, the execution of ACT.They also require a new PC/PI rule: if A wants S to believe some proposition P, then A may get S to believe some proposition Q, as long as A believes that S believes that Q implies P.(PC.!)WA(Bs(P)) =c=> WA(Bs(Q)), if BABs(Q P).(PH) BsWA(Bs(Q)) =i=> BsWA(Bs(P)), if BsBABs(Q P).In Example 9, S recognizes that A asked him to leave.The interpretation depends on S concluding that John performed his REQUEST successfully (through PH and ACT.2), and hence that A wants to request S to leave.It is then an easy step to infer that A wants S to leave, which leads to the request interpretation.Interpretation (c), a simple report of some previous action, follows from (2) by PI.BA.In Example 10, S recognizes that A intended to tell him that John wants him to leave.This depends on the fact that S concludes that John wanted to perform the REQUEST that A reported.Most of the needed inferences call for the use of EI.1 to embed simple inference rules twice.Note that an INFORM act could have been inferred at each of the four previous steps; for example, from (5) the body inference would produce INFORM(A, S, Wj(REQUEST(A, S, LEAVE(S))).But the inferences at the &quot;BsWABsWj&quot; level were so direct that they were continued.The examples of the previous section show how our plan inference rules account for the indirect interpretations of the requests which GL's postulates were designed for, as well as several others.Our approach differs from GL's in that an utterance may carry both a literal and an indirect interpretation, and of course in that its inference rules are language independent.&quot;John asked me to ask you to leave.&quot; (Interpretation d) However, in some ways both solutions are too strong.Consider, for example, the following: (5.a) Can you reach the salt?(5.b) Are you able to reach the salt?(5.c) I hereby ask you to tell me whether you are able to reach the salt.Although 5.a-c are all literally questions about the hearer's ability, only 5.a normally conveys a request.Sadock [1974] suggests that forms such as 5.a differ from 5.b in that the former is an idiom which is directly a request while 5.b is primarily a yes/no question.However, as Brown [1980] points out, this fails to account for responses to 5.a which follow from its literal form.One can answer &quot;Yes&quot; to 5.a and then go on to pass the salt.Brown proposes what she calls &quot;frozen ISA forms&quot; which directly relate surface form and indirect illocutionary force, bypassing the literal force.Frozen forms differ from normal rules mapping illocutionary forces to illocutionary forces in that they point to the relevant normal rule which provides the information necessary to the generation of responses to the surface forms.The speaker of 5.b or 5.c may in fact want the hearer to reach the salt, as does the speaker of 5.a, but he does not want his intention to be recognized by the hearer.Thus it appears that from the hearer's point of view the chain of inferences at the intended level should get turned off, soon after the recognition of the literal act.It seems that in this case (Example 6 of section 4.4) the plausibility of the inferences after step 3 should be strongly decreased.Unfortunately it is not obvious that this can be done without making ,the rating heuristics sensitive to syntax.The indirect interpretation can also be downgraded in the presence of stronger expectations.If a speaker entered a room full of aspiring candidates for employment and said: &quot;I want to know how many people here can write a sort/merge program&quot; and then turning to each individually asked &quot;Can you write a sort/merge?&quot; the question would not be intended as a request to write a program, and would not be recognized as such by a PI algorithm which rated highly an illocutionary act which fits well in an expectation.In several of the earlier examples of questions intended as indirect requests, the literal interpretation is blocked because it leads to acts whose effects were true before the utterance.The literal interpretation of 5.d gets blocked because the reminding gets done as part of the understanding of the literal act.Thus only an indirect interpretation is possible.Sadock [1970] points out that some co-occurrence rules depend on conveyed rather than literal illocutionary force.The morpheme please can occur initially only in sentences which convey a request.These remain problematic for Brown and for us.We have given evidence in this paper for an account of indirect speech acts based on rationality (plan construction), imputing rationality to others (plan inference), surface speech act definitions relating form to &quot;literal&quot; intentions, and illocutionary acts allowing a variety of realizing forms for the same intentions.The reader may object that we are suggesting a complex solution to what appears to be a simple problem.It is important to distinguish here the general explanation of indirect speech acts (which is presented here partly through an algorithm) from the implementation of such an algorithm in a practical natural language understanding system.We claim that the elements necessary for a theoretically satisfying account of indirect speech acts are independently motivated.It is almost certain that a computationally efficient solution to the indirect speech act problem would short-cut many of the inference chains suggested here, although we doubt that all searching can be eliminated in the case of the less standard forms such as 4.6a.The implementation in Brachman et al [1980] does just that.However, the more fundamental account is necessary to evaluate the correctness of the implementations.Many problems remain.Other syntactic forms that have significance with respect to illocutionary force determination should be considered.For example, tag questions such as &quot;John is coming to the party tonight, isn't he?&quot; have not been analysed here (but see Brown [19801).Furthermore, no &quot;why&quot; or &quot;how&quot; questions have been examined.Besides the incorporation of more syntactic information, another critical area that needs work concerns the control of inferencing.To allow the use of specialized inferences, a capability that is obviously required by the general theory, much research needs to be done outlining methods of selecting and restricting such inferences.This paper has concentrated on recognition.Allen [1979] shows how the construction algorithms would have to be modified to allow the generation of surface acts, including indirect forms.McDonald [1980] discusses the planning of low-level syntactic form.According to the definition of INFORM of section 4.1, any utterance that causes S to infer that A has a plan to achieve KNOW(S,P) by achieving BsWA(KNOW(S,P)) is considered by S to be an INFORM.Strawson [1964] argues that one level of recognition of intention is not sufficient for the definition of a speech act.Schiffer [1972] gives a series of counterexamples to show that no finite number of conditions of the form BsWA(BsWA(...(KNOW(S,P))) is sufficient either.The solution he proposes is that the recognition of intention must be mutually believed between the speaker and the hearer.Cohen and Levesque [1980] and Allen [forthcoming] show how the speech act definitions given here can be extended in this direction.We have only considered acts to request and inform because many of their interesting properties can be based on belief and want.At least primitive accounts of the logics of these propositional attitudes are available.Clearly there is room for much work here.Extending the analysis to other speech acts, such as promises, will require a study of other underlying logics such as that of obligation.There also remain many problems with the formalization of actions.We believe this work shows that the concepts of preconditions, effects, and action bodies are fruitful in discussing plan recognition.The operator definitions for speech acts used here are intended to facilitate the statement of the plan construction and inference rules.However, their expressive power is insufficient to handle complex actions involving sequencing, conditionals, disjunctions, iterations, parallelism, discontinuity, and a fortiori requests and promises to do such acts.They are also inadequate, as Moore [1979] points out, to express what the agent of an action knows (and does not know) after the success or failure of an act.Moore's logic of action includes sequencing, conditionals, and iterations, and is being applied to speech acts by Appelt [1980].Much remains to be done to extend it to parallel and discontinuous actions typical of multiple agent situations.These difficulties notwithstanding, we hope that we have helped show that the interaction of logic, philosophy of language, linguistics and artificial intelligence is productive and that the whole will shed light on each of the parts.American Journal of Computational Linguistics, Volume 6, Number 3-4, July-December 1980 181 C. Raymond Perrault and James F. Allen A Plan-Based Analysis of Indirect Speech Acts
Tense As Discourse AnaphorIn this paper, I consider a range of English expressions and show that their context-dependency can be characterized in terms of two properties: 1.They specify entities in an evolving model of the discourse that the listener is constructing; 2.The particular entity specified depends on another entity in that part of the evolving &quot;discourse model&quot; that the listener is currently attending to. expressions have been called show how tensed clauses share these characteristics, usually just attributed to anaphoric noun phrases.This not only allows us to capture in a simple way the but difficult-to-prove intuition that is anaphoric, also contributes to our knowledge of what is needed for understanding narrative text.Philadelphia, PA 19104-6389 In this paper, I consider a range of English expressions and show that their context-dependency can be characterized in terms of two properties: 1.They specify entities in an evolving model of the discourse that the listener is constructing; 2.The particular entity specified depends on another entity in that part of the evolving &quot;discourse model&quot; that the listener is currently attending to.Such expressions have been called anaphors.I show how tensed clauses share these characteristics, usually just attributed to anaphoric noun phrases.This not only allows us to capture in a simple way the oft-stated but difficult-to-prove intuition that tense is anaphoric, but also contributes to our knowledge of what is needed for understanding narrative text.In this paper, I consider a range of English expressions and show that their context-dependency can be characterized in terms of two properties: entity in that part of the evolving &quot;discourse model&quot; that the listener is currently attending to.Two types of expressions have previously been described in these terms: definite pronouns and certain definite noun phrases (NPs).Researchers in computational linguistics and in artificial intelligence have called these expressions anaphors (cf., Woods 1978, Sidner 1983, Bobrow 1977, Hirst 1981, Webber 1983).Linguists, however, have used this term somewhat differently.Many have restricted its use to expressions (usually pronouns) that can be treated analogously to variables in a logical language (Chomsky 1980).A view in linguistics that comes somewhat closer to the Al model can be found in a paper by Sag and Hankamer (1984), who distinguish what they call deep (or modelinterpretive) anaphora from what they call surface anaphora (or ellipsis).Under the former, they include personal pronouns, sentential &quot;it,&quot; and null-complement anaphora, and under the latter, verb phrase (VP) ellipsis, sluicing, gapping, and stripping.The two types are distinguished by whether they make reference to the interpretation of an antecedent—i.e., some object in a model of the world constructed by the interpreter of the sentence of discourse (deep anaphora)—or whether they are interpreted with respect to a previous logical form (surface anaphora).While their deep anaphors include pronouns, Hankamer and Sag do not consider other expressions like NPs in discourse that might also be described in similar model-interpretive terms, nor do they describe in any detail how model interpretation works for the expressions they consider.To avoid confusion then, I will use the term discourse anaphors for expressions that have these two properties.'My main point will be that tensed clauses share these properties as well, and hence should also be considered discourse anaphors.This will capture in a simple way the oft-stated, but difficult-to-prove intuition that tense is anaphoric.To begin with, in Section 2, I characterize the dependency of an anaphoric expression Xb on a discourse entity Ea in terms of an anaphoric function a(Xb,Ea), that itself depends on 1. the ontology of the specified entity Ea and 2. discourse structure and its focusing effect on which Ea entities the listener is attending to.With respect to definite pronouns and NPs, this will essentially be a review of previous research.However, I will argue that some indefinite NPs should also be considered discourse anaphors in just this same way.In Section 3, I will move on to tensed clauses and the notion of tense as anaphor, a notion that goes back to at least Leech in his monograph Meaning and the English Verb (1987).I will review previous attempts to make the notion precise, attempts that require special-purpose machinery to get them to work.Then I will show, in contrast, that the notion can more simply be made precise in terms of a set of similar anaphoric functions that again depend on ontology and discourse structure.Making clear these dependencies contributes to our knowledge of what is needed for understanding narrative text.The notion specify that I am using in my definition of discourse anaphora is based on the notion of a Discourse Model, earlier described in Webber (1983).My basic premise is that in processing a narrative text, a listener is developing a model of at least two things: 1. the entities under discussion, along with their properties and relationships to one another, and 2. the events and situations under discussion, along with their relationships to one another (e.g., consequential relations, simple ordering relations, elaboration relations, etc.).The representation as a whole I call the listener's Discourse Model.2 In this section, I will focus on NPs.(In Section 3, I will turn attention to tensed clauses.)NPs may evoke entities into the listener's Discourse Model corresponding to individuals (Example 1), sets (Example 2), abstract individuals (Example 3), classes (Example 4), etc .3 An NP which evokes a discourse entity also specifies it.One way an NP would be considered anaphoric by the above definition would be if it specified an entity Ea in the model that had already been evoked by some other NP.(In that case, one would say that the two NPs co-specified the same entity.)This basic arrangement is illustrated in Examples 1-3 above and is shown in Figure la.5 Formally, one could say that there is an anaphoric function a, whose value, given the anaphoric noun phrase NPb and the discourse entity Ea, is Ea thatis, a(NPb,Ea) = Ea.This can also be read as NPb specifies Ea by virtue of Ea.Definite pronouns are most often anaphoric in just this way.The other way an NP would be considered a discourse anaphor would be if it used some existing discourse entity Ea to evoke and specify a new discourse entity Eb, as in where NPb—the driver—makes use of the entity associated with the bus mentioned in 5a to specify a new entity—the driver of that bus.Here the anaphoric function is of the form a(NPb,Ea) = Eb.In cooperative discourse, there have to be constraints on the value of a(NPb,Ea), since only NPb is given explicitly.In short, a cooperative speaker must be able to assume that the listener is able to both infer a possible a and single out Ea in his/her evolving Discourse Mode1.6 (This is illustrated in Figure lb.)I will consider each of these two types of constraints in turn.Speakers assume listeners will have no problem with a when a(NPb,Ea) = Ea.Inferring a in other cases follows in large part from the ontology of the entities specified by NPs—i.e., the ontology of our concepts of individuals, sets, mass terms, generics, etc.We view these as having parts (e.g., car: the engine, the wheels), having functional relations (e.g., car: the driver), having roles (e.g., wedding: the bride), etc.These needn't be necessary parts, relations, roles, etc.Our ontology includes possible parts, relations, etc., and these too make it possible for the listener to infer an a such that a(NPb,Ea) = Eb (e.g., room: the chandelier; car: the chauffeur; wedding: the flower girl).Such inferences are discussed at length in the literature, including Clark and Marshall 1981, and Hobbs 1987.7 Before closing this section, there are two more things to say about NPs.First, the above definition of discourse anaphor does not apply to all definite NPs: a definite NP can be used to refer to something unique in the speaker and listener's shared spatio-temporal context (e.g., the telephone—i.e., the one that they both hear ringing) or their shared culture (e.g., the government), to the unique representative of a class (e.g., the duck-billed platypus), to an entire class or set (e.g., the stars), or to a functionally defined entity (e.g., the largest tomato in Scotland).None of these would be considered discourse anaphoric by the above definition.Secondly, though the definition implies that one must consider some indefinite NPs to be discourse anaphors, since they are essentially parasitic on a corresponding anaphoric definite NP, as in the following example: b.The driver stopped the bus when the passengers began to sing &quot;Aida&quot;.The indefinite NP a passenger in (6a) can be paraphrased as some one of the passengers, and thus is parasitic on the anaphoric definite NP the passengers mentioned explicitly in (6b).This does not imply that all indefinite NPs are discourse anaphors.In Mary met a boy with green hair or Fred built an oak desk, the indefinite NPs do not need to be interpreted with respect to another discourse entity and some inferrable relationship with that entity, in order to characterize the discourse entity they specify.In the next section, I will discuss the second kind of constraint on the function a(NPb,Ea) necessary for cooperative use of an anaphor—constraints on identifiable Eas.These involve notions of discourse structure and discourse focus.Before I close, though, I want to point to where I'm going vis-a-vis the anaphoric character of tense and tensed clauses.In contrast with previous accounts of tense as pronoun or tense as loosely context-dependent, I am going to claim that, like an anaphoric definite NP, The ideas presented in this section have been formulated and developed by Barbara Grosz and Candy Sidner, originally independently and later in joint research.It is not a summary of their work:8 it is limited to those of their ideas that are necessary to the concept of anaphor that I am advancing here and the concept of tense as anaphor, in particular.Sidner's thesis (1979, 1983) presents an account of understanding definite pronouns and anaphoric definite NPs that reflects the ease with which people identify the intended specificand of definite pronouns (except in highly ambiguous cases), as well as the intended specificand of anaphoric definite NPs.With respect to noun phrases (but not clauses), Sidner makes the same assumption about evoking, specifying, and co-specifying in a Discourse Model that I have made here.To understand anaphoric expressions, Sidner postulates three mechanisms: The DF corresponds to that entity the listener is most attending to.Pronouns can most easily specify the current DF, slightly less easily a member of the PFL, and with slightly more difficulty, a stacked focus.Specifying an entity pronominally can shift the listener's attention to it, thereby promoting it to be the next DF.Anything else specified in the clause ends up on the PFL, ordered by its original syntactic position.(Sidner introduced a separate &quot;agent focus&quot; to allow two entities to be specified pronominally in the same clause, but it was not a critical feature of her approach.)As for anaphoric definite NPs, they can specify anything previously introduced (whether on the PFL, a stacked focus, or anything else) or anything related in a mutually inferrable way with the current DF or a member of the PFL.In terms of the constraints I mentioned above, it is only those discourse entities that are either the DF or on the PFL that can serve as Ea for an anaphoric definite NP.9 In Sidner (1983) DFs always are stacked for possible resumption later.In Grosz and Sidner (1986) it is an entire focus space (FS) (Grosz 1977) that gets stacked (i.e., the collection of entities L is attending to by virtue of the current discourse segment (DS)) but only when the 9purpose of the current DS is taken to dominate that of the one upcoming.Dominance relations are also specified further according to the type of discourse.In Grosz and Sidner, they are defined for task-related dialogues and arguments.For example, in arguments, one DS purpose (DSP) dominates another if the second provides evidence for a point made in the first.When the dominated DSP is satisfied, its corresponding FS is popped.This stack mechanism models the listener's attentional state.The relations between DSPs constitute the intentional structure of the text.Getting a listener to resume a DS via the stack mechanism is taken to require less effort on a speaker's part than returning to elaborate an argument or subtask description later on.The significance of Sidner (1983) and Grosz and Sidner (1986) for the current enterprise is that: Computational Linguistics, Volume 14, Number 2, June 1988 63 Bonnie Lynn Webber Tense as Discourse Anaphor I reinterpret this in the current framework in terms of the anaphoric function a(NPb,Ea).Within a discourse segment, the entity that is the DF is the most likely Ea.Over the discourse segment, other discourse entities in the segment's focus space may in turn become DF.With a change in discourse segment, however, the DF can change radically to an entity in the focus space associated with the new segment.To hint again at what is to come: in Section 3.2, I will propose a temporal analogue of DF, which I have called temporal focus (TF).In Section 3.3, I will show how gradual movements of the TF are tied in with the ontology of what a tensed clause specifies—i.e., an ontology of events and situations—while more radical movements reflect the effect of discourse structure on TF.Tense may not seem prima facie anaphoric: an isolated sentence like John went to bed or I met a man who looked like a basset hound appears to make sense in a way that a stand-alone He went to bed or The man went to bed does not.On the other hand, if some time or event is established by the context (i.e., either by an event or situation described in the previous discourse or by a temporal adverbial in the current sentence—cf.Passonneau, and Moens and Steedman, this volume), tense will invariably be interpreted with respect to it, as in: In each case, the interpretation of John's going to bed is linked to an explicitly mentioned time or event.This is what underlies all discussion of the anaphoric quality of tense.The assumption that tense is anaphoric (i.e., that its interpretation is linked to some time or event derived from context) goes back many years, although it is not a universally held belief (cf.Comrie 1985).Leech seems to express this view in his Meaning and the English Verb: 63 INDEFINITE TIME Whereas the Present Perfect, in its indefinite past sense, does not name a specific point of time, a definite POINT OF ORIENTATION in the past is normally required for the appropriate use of the Simple Past Tense.The point of orientation may be specified in one of three ways: (a) by an adverbial express of timewhen; (b) by a preceding use of a Past or Perfect Tense; and (c) by implicit definition; i.e., by assumption of a particular time from context.73 The Past Perfect Tense has the meaning of pastin-the-past, or more accurately, 'a time further in the past, seen from the viewpoint of a definite point of time already in the past'.That is, like the Simple Past Tense, the Past Perfect demands an already established past point of reference.(Leech: 47) Leech did not elaborate further on how reference points are used in the interpretation of simple past tense and past perfect tense, or on what has become the main problem in the semantics and pragmatics of tense: reconciling the (usual) forward movement of events in narratives with a belief in the anaphoric (or contextdependent) character of tense.The first explicit reference I have to tense being anaphoric like a definite pronoun is in an article by McCawley (1971:110), who said: However the tense morpheme does not just express the time relationship between the clause it is in and the next higher clause—it also refers to the time of the clause that it is in, and indeed, refers to it in a way that is rather like the way in which personal pronouns refer to what they stand for.McCawley also tried to fit in his view of tense as pronoun with the interpretation of tense in simple narratives.Here he proposed that the event described in one clause serves as the antecedent of the event described in the next, but that it may be related to that event by being either at the same time or &quot;shortly after&quot; it.He did not elaborate on when one relation would be assumed and when the other.Partee (1973) also noted the similarities between tense and definite pronouns.However, she subsequently recognized that taking simple past tense as directly analogous with pronouns was incompatible with the usual forward movement of time in the interpretation in a sequence of sentences denoting events (Partee 1984).Her response was a modification of the claim that tense is anaphoric, saying: I still believe it is reasonable to characterize tense as anaphoric, or more broadly as context-dependent, but I would no longer suggest that this requires them to be viewed as 'referring' to times as pronouns 'refer' to entities, or to treat times as arguments of predicates (256).The particular context-dependent process she proposes for interpreting tensed clauses follows that of Hinrichs 1986, briefly described below.The examples presented above to illustrate the anaphoric quality of tense were all simple past.However, as Leech notes (see above), the past perfect also makes demands on having some reference point already estabBonnie Lynn Webber Tense as Discourse Anaphor lished in the context.Thus it cannot be in terms of the event described in a tensed clause that tense is anaphoric.Instead, several people (Steedman 1982, Hinrichs 1986, Bauerle 1979) have argued that it is that part of tense called by Reichenbach (1947) the point of reference (here abbreviated RT) that is anaphoric.This can be seen by considering the following example: 8. a. John went to the hospital. b.He had twisted his ankle on a patch of ice.It is not the point of the event (here abbreviated ET) of John's twisting his ankle that is interpreted anaphorically with respect to his going to the hospital.Rather, it is the RT of the second clause: its ET is interpreted as prior to that because the clause is in the past perfect (see above).I will now review briefly Hinrichs's proposal as to how tensed clauses are interpreted in context, in order to contrast it with the current proposal.In Hinrichs 1986, Hinrichs makes the simplifying assumption that in a sequence of simple past sentences, the temporal order of events described cannot contradict the order of the sentences.This allows him to focus on the problem of characterizing those circumstances in which the event described by one sentence follows that described by the previous one (Example 9—Hinrichs's Example 15) and when it overlaps it (Example 10— Hinrichs' s Example 21): 9.The elderly gentleman wrote out the check, tore it from the book, and handed it to Costain.10.Mr. Darby slapped his forehead, then collected himself and opened the door again.The brush man was smiling at him hesitantly.Hinrichs bases his account on the Aktionsart of a tensed clause (i.e., its Vendlerian classification as an accomplishment, achievement, activity, or state—including progressives).Assuming an initial reference point in a discourse, the event described by a tensed clause interpreted as an accomplishment or achievement will be included in that reference point and will also introduce a new reference point ordered after the old one.Events associated with the other Aktionsarten include the current reference point in the event time.This means that given a sequence of two clauses interpreted as accomplishments or achievements, their corresponding events will follow one another (cf.Example 9).On the other hand, given a sequence with at least one tensed clause interpreted as an activity or state (including progressive), their corresponding events will be interpreted as overlapping each other (cf.Example 10).Hinrichs relates his reference point to that of Reichenbach.(Thus, the anaphoric character of tense is based on RT and not on the events directly.)However, Hinrichs's notion and Reichenbach's differ with respect to the time of the event described in the tensed clause.While Reichenbach talks about ET and RT being the same for nonprogressive past-tense clauses, in Hinrichs's account the reference point can fall after the event if a nonprogressive past is interpreted as an accomplishment or an achievement.This is necessary to achieve the forward movement of narrative that Hinrichs assumes is always the case (his simplifying assumption) but it is not the same as Reichenbach's RT.It also leads to problems in cases where this simplifying assumption is just wrong—where in a sequence of simple past tenses, there is what appears to be a &quot;backward&quot; movement of time, as in 11. a.For an encore, John played the &quot;Moonlight Sonata&quot;. b.The opening movement he took rather tentatively, but then ... where the second clause should be understood as describing the beginning of the playing event in more detail, not as describing a subsequent event.In the account given below, both forward and backward movement of time fall out of the anaphoric character of tensed clauses, and the dependency of discourse anaphora on discourse structure.&quot; With that background, I will now show how tensed clauses share the two properties I set out in Section 1 (repeated here) and hence are further examples of discourse anaphora: To do this, I need to explain the sense in which tensed clauses specify and the way in which that specification can depend on another element in the current context.Recall that I presume that a listener's developing discourse model represents both the entities being discussed, along with their properties and relations, and the events and situations being discussed, along with their relationships with another.For the rest of this paper, I want to ignore the former and focus on the latter.This I will call event/situation structure, or E/S structure.It represents the listener's best effort at interpreting the speaker's ordering of those events and situations in time and space.One problem in text understanding, then, is that of establishing where in the evolving E/S structure to integrate the event or situation description in the next clause.In this framework, a tensed clause Cb provides two pieces of semantic information: (a) a description of an event or situation, and (b) a particular configuration of ET, RT, and point of speech (abbreviated ST).(Here I may be departing from Reichenbach in treating ET, RT, and ST explicitly as elements of linguistic semantics, quite distinct from entities of type &quot;event&quot; in the events in the model follows (in part) from Cb's particular configuration of ET, RT, and ST.Both the characteristics of Eb (i.e., its ontology) and the configuration of ET, RT, and ST are critical to my account of tense as discourse anaphor.The event ontology I assume follows that of Moens and Steedman (this volume) and of Passonneau (this volume).Both propose that people interpret events as having a tripartite structure (a &quot;nucleus&quot; in Moens and Steedman's terminology) consisting of a preparatory phase (prep), a culmination (cul), and a consequent phase (conseq)—as in Figure 2.This tripartite structure permits a uniform account to be given of aspectual types in English and of how the interpretation of temporal adverbials interacts with the interpretation of tense and aspect.For example, the coercion of clauses from one interpretation to another is defined in terms of which parts of a nucleus they select and how those parts are described.I2 The ET/RT/ST configuration is significant in that, like Steedman 1982, Dowty 1986, Hinrichs 1986, and Partee 1984, I take RT as the basis for anaphora.To indicate this, I single it out as an independent argument to anaphoric functions, here labelled 0.In particular, the following schema holds of a clause Cb linked anaphorically to an event Ea through its RT: The relationship between Eb and Ea then falls out as a consequence of 1. the particular ET/RT/ST configuration of Cb; and 2. the particular function 0 involved.In this case, the relationship between Eb and E„ then depends on the configuration of RTb and ETb.If ETb = RTb, then (minimally) Eb is taken to coincide in some way with Ea.This is shown in Figure 3a.If ETb < RTb (as in the perfect tenses), Eb is taken to precede Ea.This is shown in Figure 3d.Alternatively, 0 may embody part of the tripartite ontology of events mentioned earlier: 13prep links RTb to the preparatory phase of Ea (as shown in Figure 3b)— i.e.: Pprep(Cb, Ea, RTb) = Eb while B conseq links RTb to the consequent phase of Ea (as shown in Figure 3c)—i.e.: (There is a third possibility—that RTb links to the culmination of Ea—but it is not clear to me that it could be distinguished from the simpler 00 function given above, which links RTb to Ea itself.Also, while [3prep and la conseq relations for RTb might theoretically be possible for a perfect, it is not clear to me that these cases could be distinguished from the simpler 0o.In the case of perfects therefore, the relation between Eb and Ea is correspondingly indirect.13 The following example illustrates the case where 0 = and ETb = RTb.12. a. John played the piano. b. Mary played the kazoo.Sentence 12a. evokes a new event entity Ea describable as the event of John playing the piano.Since the tense of (12b) is simple past, ETb = RTb.Given 130(Cb,Ea, RTb) = Eb, then Eb is interpreted as coextensive with Ea.(Whether this is further interpreted as two simultaneous events or a single event of their playing a duet depends on context and, perhaps, world knowledge as well.)This is illustrated in Figure 4.Example 8 (repeated here) illustrates the case /30 where ETb < RTb. be described as having bought some flowers.This is shown in Figure 7.8. a. John went to the hospital. b.He had twisted his ankle on a patch of ice.Clause 8a. evokes an entity Ea describable as John's going to the hospital.Since 8b is past perfect, ETb < RTb.Thus if po(Cb,Ea,RTb) = Eb, the event Eb described by 8b is taken to be prior to Ea.As Moens & Steedman (this volume) point out, the consequences of an event described with a perfect tense are still assumed to hold.Hence the overlap shown in Figure 5: The next example illustrates ,conseq: 13. a. John went into the florist shop. b.He picked out three red roses, two white ones and one pale pink.Clause 13a evokes an entity Ea describable as John's going into a flower shop.Since Clause 13b is simple past, ETb = RTb.Thus given pconseq(Cb,Ea,RTb) = Eb, event Eb is taken as being part of the consequent phase of Ea.That is, John's picking out the roses is taken as happening after his going into the florist shop.This is shown in Figure 6.The next example illustrates the case of pprep: To summarize, I have claimed that: 1. the notion of specification makes sense with respect to tensed clauses; 2. one can describe the anaphoric relation in terms of the RT of a tensed clause Cb, its ET/RT configuration, and an existing event or situation entity Ea—that is, p(Cb,Ea,RTb) = Eb; and 3. there are (at least) three 13 functions—one, po, linking RTb to Ea itself, the other two (Pprep and .Bconseq) embodying parts of a tripartite ontology of events.In the next section, I will discuss constraints on the second argument to p(Cb,Ea,RTb)—that is, constraints on which entities in the evolving E/S structure the specification of a tensed clause can depend on.Recall from Section 2.2 that Sidner introduced the notion of a dynamically changing discourse focus (DF) to capture the intuition that at any point in the discourse, there is one discourse entity that is the prime focus of attention and that is the most likely (although not the only possible) specificand of a definite pronoun.In parallel, I propose a dynamically changing temporal focus (TF), to capture a similar intuition that at any point in the discourse, there is one entity in E/S structure that is most attended to and hence most likely to stand in an anaphoric relation with the RT of the next clause.That is, 13(Cb,TF,RTb) = Eb.If Cb is interpreted as part of the current discourse segment, after its interpretation there are three possibilities: These relationships, which I will call maintenance and local movement of the TF, correspond to Sidner's DF moving gradually among the discourse entities in a discourse segment.(They cover the same phenomena as the micromoves that Nakhimovsky describes in his paper (this volume).)More radical movement of TF correspond to changes in discourse structure.(These Computational Linguistics, Volume 14, Number 2, June 1988 67 Bonnie Lynn Webber Tense as Discourse Anaphor cover similar phenomena to the macromoves described in Nakhimovsky, also this volume.)In cases involving movements into and out of an embedded discourse segment, either 1. the TF will shift to a different entity in E/S structure—either an existing entity or one created in recognition of an embedded narrative; or 2. it will return to the entity previously labeled TF, after completing an embedded narrative.Such movements are described in Section 3.3.2.Other movements, signaled by temporal adverbials and when clauses, are not discussed in this paper.14The following pair of examples illustrate maintenance and local movement of TF within a discourse segment and its link with E/S structure construction.The first I discussed in the previous section to illustrate el ,-conseq• The second is a variation on that example: First consider Example 13.The first clause (13a) evokes an event entity Ea describable as John's going into the florist shop.Since its tense is simple past, Ea is interpreted as prior to ST.Since it begins the discourse, its status is special vis-a-vis both definite NPs and tensed clauses.That is, since no previous TF will have been established yet, the listener takes that entity Ea to serve as TF.'5 This is shown in Figure 8: Partee, and Dowty were out to achieve.Here it falls out simply from the discourse notion of a TF and from the particular anaphoric function Pconseq• 16 Now consider Example 15 (repeated here) whose first clause is the same as Example 13a and hence would be processed in the same way.The tense of the next clause (15b) is past perfect.As I noted above, the only anaphoric function on RTisb and an event entity that makes sense for perfect tenses is 00—that is, Given that perfect tenses imply ET < RT, the event Eb specified by (15b) will be interpreted as being prior to Ea.Moreover, since (15b) is past perfect, the consequent phase of Eb is assumed to still hold with respect to RT,5b.Hence the consequent phase of Eb overlaps Ea.Finally since TF is associated with the event entity at RTb, it remains at Ea.E/S structure at this point resembles Figure 10: Now If Clause 13b is interpreted as being part of the same discourse segment as (13a) it must be the case that 13(Ci3b,TF,RTi3b).Assume the listener takes p to be Pconseq on the basis of world knowledge—that is, 13conseq(C13b,TF,RTi3b).Since the tense of (13b) is simple past, its RT and ET coincide.Thus (13b) specifies a new entity Eb, located within the consequent phase of the TF—that is, Ea—and hence after it.I assume that, following the computation of the anaphoric function, TF becomes associated with the event entity located at RTb.In this case, it is Eb, and TF thereby moves forward (cf.Figure 9).As noted, this is the gradual forward movement of simple narratives that Hinrichs, Now Clause 15c is the same as (13b), and TF is the same as it was at the point of interpreting (13b).Thus not surprisingly, 15c produces the same change in E/S Now structure and in the TF as (13b), resulting in the diagram shown in Figure 11.To illustrate the effect of discourse structure on TF, consider the following variation on Example 15, which had the same structure vis-a-vis sequence of tenses.The first two clauses (a) and (b) are the same as in Example 15 and lead to the same configuration of event entities in E/S structure (as shown in Figure 10).But the most plausible interpretation of (16c) is where the &quot;saying&quot; event is interpreted anaphorically with respect to the &quot;promising&quot; event—that is, where (16b–c) are taken together as (the start of) an embedded discourse, describing an event prior to John's going to the florist's.To handle this, I assume, following Grosz and Sidner 1986, that when the listener recognizes an embedded discourse segment, s/he stores the current TF for possible resumption later.17 However, I also assume the listener recognizes the embedding not when s/he first encounters a perfect-tensed clause Cb, since it needn't signal an embedded discourse, but later, when an immediately following simple past tense clause Cc is most sensibly interpreted with respect to the event entity Ei, that Cb evoked.18 At this point, the listener moves TF from its current position to Eb, caching the previous value for possible resumption later.Following this gross movement, 13(Cc,TF,RTc) will be computed.If is then interpreted as B there will be conseq or Pprepl a second movement of TF.'9 Coming back to Example 16, if Clause 16c is taken as being part of a single discourse segment with (16a–b), she saying something would have to be interpreted with respect to the current TF (Ea)—John's going to the florist.This is implausible under all possible interpretations of 0.20 However, under the assumption that Et, is part of an embedded narrative, the listener can a posteriori shift TF to El, and consider the anaphoric relation with Et, as TF.At this point, the listener can plausibly take p to be B ,--conseq based on world knowledge.Since (16c) is simple past, ETc = RT, the &quot;saying&quot; event Ec is viewed as part of the consequent phase (and hence following) the &quot;promising&quot; event Eb.As in the first case, TF moves to the event located at RT—i.e., to E. This is shown roughly in Figure 12.Notice that this involved two movements of TF—once in response to a perceived embedded segment and a second time, in response to interpreting /3 as B r-conseq• Now consider the following extension to (16): d. So he picked out three red roses, two white ones, and one pale pink.As before, Clauses 17b–c form an embedded narrative, but here the main narrative of John's visit to the florist shop, started at (17a), is continued at (17d).To handle this, I again assume that TF behaves much like Sidner's DF in response to the listener's recognition of the end of an embedded narrative: that is, the cached TF is resumed and processing continues.21 Under this assumption, Clauses 17a–c are interpreted as in the previous example (cf.Figure 12).Recognizing Clause 17d as resuming the embedding segment,22 the previously cached TF (Ea—the going into the florist shop event) is resumed.Again assume that the listener takes the anaphoric function to be Pconseq(Cd,TF,RTd) = Ed on the basis of world knowledge.Since Clause 17d is simple past (ET = RT), the picking out roses event Ed is viewed as part of the consequent phase and hence following the going into the florist shop event.This is shown roughly in Figure 13: Now getting the listener to interpret a text as an embedded narrative requires providing him/her with another event or situation that TF can move to.One way in English is via a perfect-tensed clause, which Computational Linguistics, Volume 14, Number 2, June 1988 69 Bonnie Lynn Webber Tense as Discourse Anaphor explicitly evokes another event, temporally earlier than the one currently in focus.Another way is by lexical indications of an embedded narrative, such as verbs of telling and NPs that themselves denote events or situations (e.g., ones headed by de-verbal nouns).This is illustrated in Example 18.Even though all its clauses are simple past (ET = RT), Clauses 18c–d are most plausibly interpreted as indirect speech describing an event that has occurred prior to the &quot;telling&quot; event.I assume that in response to recognizing this kind of embedded narrative, the listener creates a new node of E/S structure and shifts TF there, caching the previous value of TF for possible resumption later.The temporal location of this new node vis-a-vis the previous TF will depend on information in the tensed clause and on the listener's world knowledge.Notice that, as with embedded narratives cued by the use of a perfect tense, caching the previous TF for resumption later enables the correct interpretation of Clause 18e, which is most plausibly interpreted as following the telling about her sister event.An NP denoting an event or situation (such as one headed by a noun like trip or by a de-verbal noun like installation) can also signal the upcoming possibility of an embedded narrative that will elaborate that event or situation (past, upcoming, or hypothetical) in more detail, as in Example 19.In this case, the original NP and the subsequent clause(s) will be taken as cospecifying the same thing.The question here is how and when TF moves. c. She spent five weeks above the Arctic Circle with two friends. d. The three of them climbed Mt.McKinley.After interpreting Clause 19b, the TF is at the &quot;telling&quot; event.I claim that the NP her trip to Alaska, while evoking a discourse entity, does not affect the TF.If Clause 19c is interpreted as the start of an embedded narrative (as it is here), TF moves to the event entity Ec it evokes (caching the previous value Eb).At this point, using additional reasoning, the listener may recognize an anaphoric relation between Clause 19c and the discourse entity evoked by her trip to Alaska.Support for this, rather than assuming that an event-denoting NP sets up a potential focus, just as I claim a perfect-tensed clause does, comes from the reasoning required to understand the following parallel example, where I would claim TF does not move.I was talking with Mary yesterday.She told me about her trip to Alaska.She had spent five weeks above the Arctic Circle with two friends.The three of them had climbed Mt.McKinley.She said that next year they would go for Aconcagua.The event described in Clause 20c is the same as that described in Clause 19c, and should be interpreted anaphorically with respect to the entity her trip to Alaska in the same way.If this is the case, however, then the anaphoric link does not follow from the movement of TF.Example 20 above illustrates one case of an anaphoric function on an NP and a tensed clause, specifically B(Cb,E„,RTb) where the entity Ea has been evoked by an NP rather than a clause.Another possibility is that a(NPb,Ea) = Eb, where NPb is definite by virtue of an entity evoked by a clause rather than an NP—that is, Eb, is associated with either the preparatory/culmination/consequent structure of Ea, as in 21. a. Mary climbed Mt.McKinley. b.The preparations took her longer than the ascent. or its associated role structure, as in 22. a. John bought a television. b.Although he had intended to buy a 13&quot; b/w set, the salesman convinced him to buy a 25&quot; color, back-projection job. where the salesman fills a particular role in the buying event.Next, notice that ambiguities arise when there is more than one way to plausibly segment the discourse, as in the following example: 23. a. I told Frank about my meeting with Ira. b.We talked about ordering a Butterfly.Here it is plausible to take Clause 23b as the beginning of an embedded narrative, whereby the &quot;talking about&quot; event is interpreted against a new node of E/S structure, situated prior to the &quot;telling Frank&quot; event.(In this case, we is Ira and me.)It is also plausible to take (23b) as continuing the current narrative, whereby the &quot;talking about&quot; event is interpreted with respect to the &quot;telling Frank&quot; event.(In contrast here, we is Frank and me.)Finally, consider things from the point of view of generation.If some event Eb is part of the preparatory phase of some event Ea, and a description of Ea has just been generated using the simple past tense, then Eb could be described using either the simple past, as in Example 24 or past perfect, as in Example 25.In the case of Example 24, the listener/reader recognizes that Eb is part of the preparatory phase of Ea and that Eb therefore precedes Ea.In the case of Example 25, the listener would first recognize that Eb precedes Ea because of the past perfect, but then recognize Eb as part of the preparatory phase of Ea.On the other hand, if Eb simply precedes Ea, but a description of Ea has been generated first, then Eb must be described with a past perfect (Example 26): simple past would not be sufficient (Example 27).26. a. John went to the hospital. b.He had broken his ankle, walking on a patch of ice.27. a. John went to the hospital. b.*He broke his ankle, walking on a patch of ice.In this paper, I have presented a uniform characterization of discourse anaphora in a way that includes definite pronouns, definite NPs, and tensed clauses.In doing so, I have argued that the successful use of discourse anaphors depends on two different things: 1. speakers' and listeners' (mutual) beliefs about the ontology of the things and events being discussed, and 2. speakers' and listeners' (mutual) focus of attention.The former implicates semantics in the explanation of discourse anaphora, the latter, discourse itself.It is important that we as researchers recognize these as two separate systems, as the properties of discourse as an explanatory device are very different from those of semantics.This work was partially supported by ARO grant DAA29-884-9-0027, NSF grant MCS-8219116-CER, and DARPA grant N00014-85-K-0018 to the University of Pennsylvania, by DARPA grant N00014-85-C-0012 to UNISYS Paoli Research Center, and an Alvey grant to the Centre for Speech Technology Research, University of Edinburgh.My thanks to Becky Passonneau, Debby Dahl, Mark Steedman, Ethel Schuster, Candy Sidner, Barbara Grosz, Ellen Bard, Anne Anderson, Tony Sanford, Simon Garrod, and Rich Thomason for their helpful comments on the many earlier versions of this paper.
Structural Ambiguity And Lexical RelationsWe propose that many ambiguous prepositional phrase attachments can be resolved on the basis of the relative strength of association of the preposition with verbal and nominal heads, estimated on the basis of distribution in an automatically parsed corpus.This suggests that a distributional approach can provide an approximate solution to parsing problems that, in the worst case, call for complex reasoning.We propose that many ambiguous prepositional phrase attachments can be resolved on the basis of the relative strength of association of the preposition with verbal and nominal heads, estimated on the basis of distribution in an automatically parsed corpus.This suggests that a distributional approach can provide an approximate solution to parsing problems that, in the worst case, call for complex reasoning.Prepositional phrase attachment is the canonical case of structural ambiguity, as in the timeworn example: Example 1 I saw the man with the telescope.An analysis where the prepositional phrase [pp with the telescope] is part of the object noun phrase has the semantics &quot;the man who had the telescope&quot;; an analysis where the PP has a higher attachment (perhaps as daughter of VP) is associated with a semantics where the seeing is achieved by means of a telescope.The existence of such ambiguity raises problems for language models.It looks like it might require extremely complex computation to determine what attaches to what.Indeed, one recent proposal suggests that resolving attachment ambiguity requires the construction of a discourse model in which the entities referred to in a text are represented and reasoned about (Altmann and Steedman 1988).We take this argument to show that reasoning essentially involving reference in a discourse model is implicated in resolving attachment ambiguities in a certain class of cases.If this phenomenon is typical, there is little hope in the near term for building computational models capable of resolving such ambiguities in unrestricted text.There have been several structure-based proposals about ambiguity resolution in the literature; they are particularly attractive because they are simple and don't demand calculations in the semantic or discourse domains.The two main ones are as follows.For the particular case we are concerned with, attachment of a prepositional phrase in a verb + object context as in Example 1, these two principles—at least given the version of syntax that Frazier assumes—make opposite predictions: Right Association predicts noun attachment, while Minimal Attachment predicts verb attachment.Psycholinguistic work on structure-based strategies is primarily concerned with modeling the time course of parsing and disambiguation, and acknowledges that other information enters into determining a final parse.Still, one can ask what information is relevant to determining a final parse, and it seems that in this domain structurebased disambiguation is not a very good predictor.A recent study of attachment of prepositional phrases in a sample of written responses to a &quot;Wizard of Oz&quot; travel information experiment shows that neither Right Association nor Minimal Attachment accounts for more than 55% of the cases (Whittemore, Ferrara, and Brunner 1990).And experiments by Taraban and McClelland (1988) show that the structural models are not in fact good predictors of people's behavior in resolving ambiguity.Whittemore, Ferrara, and Brunner (1990) found lexical preferences to be the key to resolving attachment ambiguity.Similarly, Taraban and McClelland found that lexical content was key in explaining people's behavior.Various previous proposals for guiding attachment disambiguation by the lexical content of specific words have appeared (e.g.Ford, Bresnan, and Kaplan 1982; Marcus 1980).Unfortunately, it is not clear where the necessary information about lexical preferences is to be found.Jenson and Binot (1987) describe the use of dictionary definitions for disambiguation, but dictionaries are typically rather uneven in their coverage.In the Whittemore, Ferrara, and Brunner study (1990), the judgment of attachment preferences had to be made by hand for the cases that their study covered; no precompiled list of lexical preferences was available.Thus, we are posed with the problem of how we can get a good list of lexical preferences.Our proposal is to use co-occurrence of verbs and nouns with prepositions in a large body of text as an indicator of lexical preference.Thus, for example, the preposition to occurs frequently in the context send NP_, that is, after the object of the verb send.This is evidence of a lexical association of the verb send with to.Similarly, from occurs frequently in the context withdrawal_, and this is evidence of a lexical association of the noun withdrawal with the preposition from.This kind of association is a symmetric notion: it provides no indication of whether the preposition is selecting the verbal or nominal head, or vice versa.We will treat the association as a property of the pair of words.It is a separate issue, which we will not be concerned with in the initial part of this paper, to assign the association to a particular linguistic licensing relation.The suggestion that we want to explore is that the association revealed by textual distribution—whether its source is a complementation relation, a modification relation, or something else—gives us information needed to resolve prepositional attachment in the majority of cases.A 13 million–word sample of Associated Press news stories from 1989 were automatically parsed by the Fidditch parser (Hindle 1983 and in press), using Church's A sample of NP heads, preceding verbs, and following prepositions derived from the parsed corpus. part-of-speech analyzer as a preprocessor (Church 1988), a combination that we will call simply &quot;the parser.&quot; The parser produces a single partial syntactic description of a sentence.Consider Example 2, and its parsed representation in Example 3.The information in the tree representation is partial in the sense that some attachment information is missing: the nodes dominated by &quot;?&quot; have not been integrated into the syntactic representation.Note in particular that many PPs have not been attached.This is a symptom of the fact that the parser does not (in many cases) have the kind of lexical information that we have just claimed is required in resolving PP attachment.Example 2 The radical changes in export and customs regulations evidently are aimed at remedying an extreme shortage of consumer goods in the Soviet Union and assuaging citizens angry over the scarcity of such basic items as soap and windshield wipers.From the syntactic analysis provided by the parser, we extracted a table containing the heads of all noun phrases.For each noun phrase head, we recorded the following preposition if any occurred (ignoring whether or not the parser had attached the preposition to the noun phrase), and the preceding verb if the noun phrase was the object of that verb.The entries in Table 1 are those generated from the text above.Each noun phrase in Example 3 is associated with an entry in the Noun column of the table.Usually this is simply the root of the head of the noun phrase: good is the root of the head of consumer goods.Noun phrases with no head, or where the head is not a common noun, are coded in a special way: DART-PNP represents a noun phrase beginning with a definite article and headed by a proper noun, and VING represents a gerundive noun phrase.PRO-+ represents the empty category which, in the syntactic theory underlying the parser, is assumed to be the object of the passive verb aimed.In cases where a prepositional phrase follows the noun phrase, the head preposition appears in the Prep column; attached and unattached prepositional phrases generate the same kinds of entries.If the noun phrase is an object, the root of the governing verb appears in the Verb column: aim is the root of aimed, the verb governing the empty category [„,, +[.The last column in the table, labeled Syntax, marks with the symbol -V all cases where there is no preceding verb that might license the preposition: the initial subject of Example 2 is such a case.In the 13 million—word sample, 2,661,872 noun phrases were identified.Of these, 467,920 were recognized as the object of a verb, and 753,843 were followed by a preposition.Of the object noun phrases identified, 223,666 were ambiguous verb— noun—preposition triples.The table of verbs, nouns, and prepositions is in several respects an imperfect source of information about lexical associations.First, the parser gives us incorrect analyses in some cases.For instance, in the analysis partially described in Example 4a, the parser incorrectly classified probes as a verb, resulting in a table entry probe lightning in.Similarly, in Example 4b, the infinitival marker to has been misidentified as a preposition. a.[„The space] [v„„, probes] [„detected lightning] [„ in Jupiter's upper atmosphere] and observed auroral emissions like Earth's northern lights in the Jovian polar regions. b.The Bush administration told Congress on Tuesday it wants to [v preserve] [„the right] [„[ to] control entry] to the United States of anyone who was ever a Communist.Second, a preposition in an entry might be structurally related to neither the noun of the entry nor the verb (if there is one), even if the entry is derived from a correct parse.For instance, the phrase headed by the preposition might have a higher locus of attachment: a.The Supreme Court today agreed to consider reinstating the murder conviction of a New York City man who confessed to [VING killing] [,his former girlfriend] [, after] police illegally arrested him at his home.The temporal phrase headed by after modifies confess, but given the procedure described above, Example 5a results in a tuple kill girlfriend after.In the second example, a tuple legalize abortion under is extracted, although the PP headed by under modifies the higher verb shot.Finally, entries of the form verb noun preposition do not tell us whether to induce a lexical association between verb and preposition or between noun and preposition.We will view the first two problems as noise that we do not have the means to eliminate, 1 For present purposes, we can consider a parse correct if it contains no incorrect information in the relevant area.Provided the PPs in Example 5 are unattached, the parses would be correct in this sense.The incorrect information is added by our table construction step, which (given our interpretation of the table) assumes that a preposition following an object NP modifies either the NP or its governing verb. and partially address the third problem in a procedure we will now describe.We want to use the verb-noun-preposition table to derive a table of bigrams counts, where a bigram is a pair consisting of a noun or verb and an associated preposition (or no preposition).To do this we need to try to assign each preposition that occurs either to the noun or to the verb that it occurs with.In some cases it is fairly certain whether the preposition attaches to the noun or the verb; in other cases, this is far less certain.Our approach is to assign the clear cases first, then to use these to decide the unclear cases that can be decided, and finally to divide the data in the remaining unresolved cases between the two hypotheses (verb and noun attachment).The procedure for assigning prepositions is as follows: This procedure gives us bigram counts representing the frequency with which a given noun occurs associated with an immediately following preposition (or no preposition), or a given verb occurs in a transitive use and is associated with a preposition immediately following the object of the verb.We use the following notation: f(w,p) is the frequency count for the pair consisting of the verb or noun w and the preposition p. The unigram frequency count for the word w (either a verb, noun, or preposition) can be viewed as a sum of bigram frequencies, and is written f (w).For instance, if p is a preposition, f (p) = Ew f (w, p).Our object is to develop a procedure to guess whether a preposition is attached to the verb or its object when a verb and its object are followed by a preposition.We assume that in each case of attachment ambiguity, there is a forced choice between two outcomes: the preposition attaches either to the verb or to the noun.'For example, in Example 6, we want to choose between two possibilities: either into is attached to the verb send or it is attached to the noun soldier.Moscow sent more than 100,000 soldiers into Afghanistan ...In particular, we want to choose between two structures: For the verb_attach case, we require not only that the preposition attach to the verb send but also that the noun soldier have no following prepositional phrase attached: since into directly follows the head of the object noun phrase, there is no room for any post-modifier of the noun soldier.We use the notation NULL to emphasize that in order for a preposition licensed by the verb to be in the immediately postnominal position, the noun must have no following complements (or adjuncts).For the case of noun attachment, the verb may or may not have additional prepositional complements following the prepositional phrase associated with the noun.Since we have a forced choice between two outcomes, it is appropriate to use a likelihood ratio to compare the attachment probabilities (cf.Mosteller and Wallace 1964).3 In particular, we look at the log of the ratio of the probability of verb_attach to the probability of noun_attach.We will call this log likelihood ratio the LA (lexical association) score. and Again, the probability of noun attachment does not involve a term indicating that the verb sponsors no (additional) complement; when we observe a prepositional phrase that is in fact attached to the object NP, the verb might or might not have a complement or adjunct following the object phrase.2 Thus we are ignoring the fact that the preposition may in fact be licensed by neither the verb nor the noun, as in Example 5.3 In earlier versions of this paper we used a t-test for deciding attachment and a different procedure for estimating the probabilities.The current procedure has several advantages.Unlike the t-test used previously, it is sensitive to the magnitude of the difference between the two probabilities, not to our confidence in our ability to estimate those probabilities accurately.And our estimation procedure has the property that it defaults (in case of novel words) to the average behavior for nouns or verbs, for instance, reflecting a default preference with of for noun attachment.We can estimate these probabilities from the table of co-occurrence counts as:4 The LA score has several useful properties.The sign indicates which possibility, verb attachment or noun attachment, is more likely; an LA score of zero means they are equally likely.The magnitude of the score indicates how much more probable one outcome is than the other.For example, if the LA score is 2.0, then the probability of verb attachment is four times greater than noun attachment.Depending on the task, we can require a certain threshold of LA score magnitude before making a decision.'As usual, in dealing with counts from corpora we must confront the problem of how to estimate probabilities when counts are small.The maximum likelihood estimate described above is not very good when frequencies are small, and when frequencies are zero, the formula will not work at all.We use a crude adjustment to observed frequencies that has the right general properties, though it is not likely to be a very good estimate when frequencies are small.For our purposes, however—exploring in general the relation of distribution in a corpus to attachment disambiguation—we believe it is sufficient.Other approaches to adjusting small frequencies are discussed in Church et al. (1991) and Gale, Church, Yarowsky (in press).The idea is to use the typical association rates of nouns and verbs to interpolate our probabilities.Where f (N , p) = En f (n,p), f (V , p) = E, f (v, p), f (N) = En f (n) and 4 The nonintegral count for send is a consequence of the data-splitting step Ambiguous Attach 2, and the definition of unigram frequencies as a sum of bigram frequencies.5 An advantage of the likelihood ratio approach is that we can use it in a Bayesian discrimination framework to take into account other factors that might influence our decision about attachment (see Gale, Church, and Yarowsky [in press] for a discussion of this approach).We know of course that other information has a bearing on the attachment decision.For example, we have observed that if the noun phrase object includes a superlative adjective as a premodifier, then noun attachment is certain (for a small sample of 16 cases).We could easily take this into account by setting the prior odds ratio to heavily favor noun attachment: let's suppose that if there is a superlative in the object noun phrase, then noun attachment is say 1000 times more probable than verb attachment; otherwise, they are equally probable.Then following Mosteller and Wallace (1964), we assume that Final attachment odds = log ,(initial odds) + LA.In case there is no superlative in the object, the initial log odds will be zero (verb and noun attachment are equally probable), and the final odds will equal our LA score.If there is a superlative, Final attachment odds = log 2 LA(v, , n, p). and similarly for verbs.When f (n,p) is zero, the estimate used is proportional to this average.If we have seen only one case of a noun and it occurred with a preposition p (that is f (n, p) = 1 and f (n) = 1), then our estimate is nearly cut in half.This is the kind of effect we want, since under these circumstances we are not very confident in 1 as an estimate of P(p I n).When f (n, p) is large, the adjustment factor does not make much difference.In general; this interpolation procedure adjusts small counts in the right direction and has little effect when counts are large.For our current example, this estimation procedure changes the LA score little: The LA score of 5.87 for this example is positive and therefore indicates verb attachment; the magnitude is large enough to suggest a strong preference for verb attachment.This method of calculating the LA score was used both to decide unsure cases in building the bigram tables as described in Ambiguous Attach 1, and to make the attachment decisions in novel ambiguous cases, as discussed in the sections following.To evaluate the performance of the procedure, 1000 test sentences in which the parser identified an ambiguous verb–noun–preposition triple were randomly selected from AP news stories.These sentences were selected from stories included in the 13 million– word sample, but the particular sentences were excluded from the calculation of lexical associations.The two authors first guessed attachments on the verb–noun–preposition triples, making a judgment on the basis of the three headwords alone.The judges were required to make a choice in each instance.This task is in essence the one that we will give the computer—to judge the attachment without any more information than the preposition and the heads of the two possible attachment sites.This initial step provides a rough indication of what we might expect to be achievable based on the information our procedure is using.We also wanted a standard of correctness for the test sentences.We again judged the attachment for the 1000 triples, this time using the full-sentence context, first grading the test sentences separately, and then discussing examples on which there was disagreement.Disambiguating the test sample turned out to be a surprisingly difficult task.While many decisions were straightforward, more than 10% of the sentences seemed problematic to at least one author.There are several kinds of constructions where the attachment decision is not clear theoretically.These include idioms as in Examples 8 and 9, light verb constructions (Example 10), and small clauses (Example 11).Example 8 But over time, misery has given way to mending.Example 9 The meeting will take place in Quantico.Example 10 Bush has said he would not make cuts in Social Security.Example 11 Sides said Francke kept a .38-caliber revolver in his car's glove compartment.In the case of idioms, we made the assignment on the basis of a guess about the syntactic structure of the idiom, though this was sometimes difficult to judge.We chose always to assign light verb constructions to noun attachment, based on the fact that the noun supplies the lexical information about what prepositions are possible, and small clauses to verb attachment, based on the fact that this is a predicative construction lexically licensed by the verb.Another difficulty arose with cases where there seemed to be a systematic semantically based indeterminacy about the attachment.In the situation described by Example 12a, the bar and the described event or events are presumably in the same location, and so there is no semantic reason to decide on one attachment.Example 12b shows a systematic benefactive indeterminacy: if you arrange something for someone, then the thing arranged is also for them.The problem in Example 12c is that signing an agreement usually involves two participants who are also parties to the agreement.Example 13 gives some further examples drawn from another test sample.Example 12 a.... known to frequent the same bars in one neighborhood.In general, we can say that an attachment is semantically indeterminate if situations that verify the meaning associated with one attachment also make the meaning associated with the other attachment true.Even a substantial overlap (as opposed to identity) between the classes of situations verifying the two meanings makes an attachment choice difficult.The problems in determining attachments are heterogeneous.The idiom, light verb, and small clause constructions represent cases where the simple distinction between noun attachment and verb attachment perhaps does not make sense, or is very theory-dependent.It seems to us that the phenomenon of semantically based indeterminacy deserves further exploration.If it is often difficult to decide what licenses a prepositional phrase, we need to develop language models that appropriately capture this.For our present purpose, we decided to make an attachment choice in all cases, in some cases relying on controversial theoretical considerations, or relatively unanalyzed intuitions.In addition to the problematic cases, 120 of the 1000 triples identified automatically as instances of the verb—object—preposition configuration turned out in fact to be other constructions, often as the result of parsing errors.Examples of this kind were given above, in the context of our description of the construction of the verb—noun— preposition table.Some further misidentifications that showed up in the test sample are: identifying the subject of the complement clause of say as its object, as in Example 10, which was identified as (say ministers from), and misparsing two constituents as a single-object noun phrase, as in Example 11, which was identified as (make subject to).First, consider how the simple structural attachment preference schemas perform at predicting the outcome in our test set.Right Association predicts noun attachment and does better, since in our sample there are more noun attachments, but it still has an error rate of 33%.Minimal Attachment, interpreted as entailing verb attachment, has the complementary error rate of 67%.Obviously, neither of these procedures is particularly impressive.Performance on the test sentences for two human judges and the lexical association procedure (LA).LA actual N actual V precision recall N guess 496 89 N .848 .846 V guess 90 205 V .695 .697 neither 0 0 combined .797 .797 Judge 1 actual N actual V precision recall N guess 527 48 N .917 .899 V guess 59 246 V .807 .837 neither 0 0 combined .878 .878 Judge 2 actual N actual V precision recall N guess 482 29 N .943 .823 V guess 104 265 V .718 .901 neither 0 0 combined .849 .849 Now consider the performance of our lexical association (LA) procedure for the 880 standard test sentences.Table 2 shows the performance for the two human judges and for the lexical association attachment procedure.First, we note that the task of judging attachment on the basis of verb, noun, and preposition alone is not easy.The figures in the entry labeled &quot;combined precision&quot; indicate that the human judges had overall error rates of 12-15%.6 The lexical association procedure is somewhat worse than the human judges, with an error rate of 20%, but this is an improvement over the structural strategies.The table also gives results broken down according to N vs. V attachment.The precision figures indicate the proportion of test items assigned to a given category that actually belong to the category.For instance, N precision is the fraction of cases that the procedure identified as N attachments that actually were N attachments.The recall figures indicate the proportion of test items actually belonging to a given category that were assigned to that category: N precision is the fraction of actual N attachments that were identified as N attachments.The LA procedure recognized about 85% of the 586 actual noun attachment examples as noun attachments, and about 70% of the actual verb attachments as verb attachments.If we restrict the lexical association procedure to choose attachment only in cases where the absolute value of the LA score is greater than 2.0 (an arbitrary threshold indicating that the probability of one attachment is four times greater than the other), we get attachment judgments on 621 of the 880 test sentences, with overall precision of about 89%.On these same examples, the judges also showed improvement, as evident in Table 3.7 The fact that an LA score threshold improves precision indicates that the LA score gives information about how confident we can be about an attachment choice.In some applications, this information is useful.For instance, suppose that we wanted to incorporate the PP attachment procedure in a parser such as Fidditch.It might be preferable to achieve increased precision in PP attachment, in return for leaving some PPs unattached.For this purpose, a threshold could be used.Table 4 shows the combined precision and recall levels at various LA thresholds.It is clear that the LA score can be used effectively to trade off precision and recall, with a floor for the forced choice at about 80%.A comparison of Table 3 with Table 2 indicates, however, that the decline in recall is severe for V attachment.And in general, the performance of the LA procedure is worse on V attachment examples than on N attachments, according to both precision and recall criteria.The next section is concerned with a classification of the test examples, which gives insight into why performance on V attachments is worse.Our model takes frequency of co-occurrence as evidence of an underlying relationship but makes no attempt to determine what sort of relationship is involved.It is interesting to see what kinds of relationships are responsible for the associations the model is identifying.To investigate this we categorized the 880 triples according to the nature of the relationship underlying the attachment.In many cases, the decision was difficult.The argument/adjunct distinction showed many gray cases between clear participants in an action and clear adjuncts, such as temporal modifiers.We made rough best guesses to partition the cases into the following categories: argument, adjunct, idiom, small clause, systematic locative indeterminacy, other systematic indeterminacy, and light verb.With this set of categories, 78 of the 880 cases remained so problematic that we assigned them to the category other.Table 5 shows the proportion of items in a given category that were assigned the correct attachment by the lexical association procedure.Even granting the roughness of the categorization, some clear patterns emerge.Our approach is most successful at attaching arguments correctly.Notice that the 378 noun arguments constitute 65% of the total 586 noun attachments, while the 104 verb arguments amount to only 35% of the 294 verb attachments.Furthermore, performance with verb adjuncts is worse than with noun adjuncts.Thus much of the problem with V attachments noted in the previous section appears to be attributable to a problem with adjuncts, particularly verbal ones.Performance on verbal arguments remains worse than performance on nominal ones, however.The remaining cases are all complex in some way, and the performance is poor on these classes, showing clearly the need for a more elaborated model of the syntactic structure that is being identified.The idea that lexical preference is a key factor in resolving structural ambiguity leads us naturally to ask whether existing dictionaries can provide information relevant to disambiguation.The Collins COBUILD English Language Dictionary (Sinclair et al. 1987) is useful for a comparison with the AP sample for several reasons: it was compiled on the basis of a large text corpus, and thus may be less subject to idiosyncrasy than other works, and it provides, in a separate field, a direct indication of prepositions typically associated with many nouns and verbs.From a machine-readable version of the dictionary, we extracted a list of 1,942 nouns associated with a particular preposition, and of 2,291 verbs associated with a particular preposition after an object noun phrase.'These 4,233 pairs are many fewer than the number of associations in the AP sample (see Table 6), even if we ignore the most infrequent pairs.Of the total 76,597 pairs, 20,005 have a frequency greater than 3, and 7,822 have a frequency that is greater than 3 and more than 4 times what one would predict on the basis of the unigram frequencies of the noun or verb and the preposition.'We can use the fixed lexicon of noun—preposition and verb—preposition associations derived from COBUILD to choose attachment in our test set.The COBUILD dictionary has information on 257 of the 880 test verb—noun—preposition triples.In 241 of those cases, there is information only on noun or only on verb association.In these cases, we can use the dictionary to choose the attachment according to the association indicated.In the remaining 16 cases, associations between the preposition and both the noun and the verb are recorded in the dictionary.For these, we select noun attachment, since it is the more probable outcome in general.For the remaining cases, we assume that the dictionary makes no decision.Table 7 gives the results obtained where U is E f (w, p), the total number of token bigrams.It is equivalent tow and p having a w,p mutual information (defined as greater than 2.This threshold of 2, of course, is an arbitrary cutoff. by this attachment procedure.The precision figure is similar to that obtained by the lexical association procedure with a threshold of zero, but the recall is far lower: the dictionary provides insufficient information in most cases.Like the lexicon derived from the COBUILD dictionary, the fixed lexicon of 7,822 corpus-derived associations derived from our bigram table as described above (that is, all bigrams where f (w, p) > 3 and I(w, p) > 2) contains categorical information about associations.Using it for disambiguation in the way the COBUILD dictionary was used gives the results indicated in Table 7.The precision is similar to that which was achieved with the LA procedure with a threshold of 2, although the recall is lower.This suggests that while overall coverage of association pairs is important, the information about the relative strengths of associations contributing to the LA score is also significant.It must be noted that the dictionary information we derived from COBUILD was composed for people to use in printed form.It seems likely that associations were left out because they did not serve this purpose in one way or another.For instance, listing many infrequent or semantically predictable associations might be confusing.Furthermore, our procedure undoubtedly gained advantage from the fact that the test items are drawn from the same body of text as the training corpus.Nevertheless, the results of this comparison suggest that for the purpose of this paper, a partially parsed corpus is a better source of information than a dictionary.This conclusion should not be overstated, however.Table 6 showed that most of the associations in each lexicon are not found in the others.Table 8 is a sample of a verb—preposition association dictionary obtained by merging information from the AP sample and from COBUILD, illustrating both the common ground and the differences between the two lexicons.Each source of information provides intuitively important associations that are missing from the other.In our judgment, the results of the lexical association procedure are good enough to make it useful for some purposes, in particular for inclusion in a parser such as Fidditch.The fact that the LA score provides a measure of confidence increases this usefulness, since in some applications (such as exploratory linguistic analysis of text Verb-(NP)-Preposition associations in the COBUILD dictionary and in the AP sample (with f (v, p) > 3 and I(v,p) > 2.0).AP sample COBUILD approach about as at with corpora) it is advantageous to be able to achieve increased precision in exchange for discarding a proportion of the data.From another perspective, our results are less good than what might be demanded.The performance of the human judges with access just to the verb-noun-preposition triple is a standard of what is possible based on this information, and the lexical association procedure falls somewhat short of this standard.The analysis of underlying relations indicated some particular areas in which the procedure did not do well, and where there is therefore room for improvement.In particular, performance on adjuncts was poor.A number of classes of adjuncts, such as temporal ones, are fairly easy to identify once information about the object of the preposition is taken into account.Beginning with such an identification step (which could be conceived of as adding a feature such as [+temporal] to individual prepositions, or replacing individual token prepositions with an abstract temporal preposition) might yield a lexical association procedure that would do better with adjuncts.But it is also possible that a procedure that evaluates associations with individual nouns and verbs is simply inappropriate for adjuncts.This is an area for further investigation.This experiment was deliberately limited to one kind of attachment ambiguity.However, we expect that the method will be extendable to other instances of PP attachment ambiguity, such as the ambiguity that arises when several prepositional phrases follow a subject NP, and to ambiguities involving other phrases, especially phrases such as infinitives that have syntactic markers analogous to a preposition.We began this paper by alluding to several approaches to PP attachment, specifically work assuming the construction of discourse models, approaches based on structural attachment preferences, and work indicating a dominant role for lexical preference.Our results tend to confirm the importance of lexical preference.However, we can draw no firm conclusions about the other approaches.Since our method yielded incorrect results on roughly 20% of the cases, its coverage is far from complete.This leaves a lot of work to be done, within both psycholinguistic and computational approaches.Furthermore, as we noted above, contemporary psycholinguistic work is concerned with modeling the time course of parsing.Our experiment gives no information about how lexical preference information is exploited at this level of detail, or the importance of such information compared with other factors such as structural preferences at a given temporal stage of the human parsing process.However, the numerical estimates of lexical association we have obtained may be relevant to a psycholinguistic investigation of this issue.We thank Bill Gale, Ken Church, and David Yarowsky for many helpful discussions of this work and are grateful to four reviewers and Christian Rohrer for their comments on an earlier version.
Retrieving Collocations From Text: XtractNatural languages are full of collocations, recurrent combinations of words that co-occur more often than expected by chance and that correspond to arbitrary word usages.Recent work in lexicography indicates that collocations are pervasive in English; apparently, they are common in all types of writing, including both technical and nontechnical genres.Several approaches have been proposed to retrieve various types of collocations from the analysis of large samples of textual data.These techniques automatically produce large numbers of collocations along with statistical figures intended to reflect the relevance of the associations.However, none of these techniques provides functional information along with the collocation.Also, the results produced often contained improper word associations reflecting some spurious aspect of the training corpus that did not stand for true collocations.In this paper, we describe a set of techniques based on statistical methods for retrieving and identifying collocations from large textual corpora.These techniques produce a wide range of collocations and are based on some original filtering methods that allow the production of richer and higher-precision output.These techniques have been implemented and resulted in a tool, techniques are described and some results are presented on a 10 corpus of stock market news reports.A lexicographic evaluation of a retrieval tool has been made, and the estimated precision of 80%.Natural languages are full of collocations, recurrent combinations of words that co-occur more often than expected by chance and that correspond to arbitrary word usages.Recent work in lexicography indicates that collocations are pervasive in English; apparently, they are common in all types of writing, including both technical and nontechnical genres.Several approaches have been proposed to retrieve various types of collocations from the analysis of large samples of textual data.These techniques automatically produce large numbers of collocations along with statistical figures intended to reflect the relevance of the associations.However, none of these techniques provides functional information along with the collocation.Also, the results produced often contained improper word associations reflecting some spurious aspect of the training corpus that did not stand for true collocations.In this paper, we describe a set of techniques based on statistical methods for retrieving and identifying collocations from large textual corpora.These techniques produce a wide range of collocations and are based on some original filtering methods that allow the production of richer and higher-precision output.These techniques have been implemented and resulted in a lexicographic tool, Xtract.The techniques are described and some results are presented on a 10 million—word corpus of stock market news reports.A lexicographic evaluation of Xtract as a collocation retrieval tool has been made, and the estimated precision of Xtract is 80%.Consider the following sentences: voir la porte to see the door die Tar sehen to see the door vedere la porta to see the door ver la puerta to see the door lcapiyi gormek to see the door enfoncer la porte * to push the door through die Tiir aufbrechen * to break the door sfondare la porta * to hit/demolish the door tumbar la puerta * to fall the door kapiyi kirmak * to break the door The above sentences contain expressions that are difficult to handle for nonspecialists.For example, among the eight different expressions referring to the famous Wall Street index, only those used in sentences 1-4 are correct.The expressions used in the starred sentences 5-8 are all incorrect.The rules violated in sentences 5-8 are neither rules of syntax nor of semantics but purely lexical rules.The word combinations used in sentences 5-8 are invalid simply because they do not exist; similarly, the ones used in sentences 1-4 are correct because they exist.Expressions such as these are called collocations.Collocations vary tremendously in the number of words involved, in the syntactic categories of the words, in the syntactic relations between the words, and in how rigidly the individual words are used together.For example, in some cases, the words of a collocation must be adjacent, as in sentences 1-5 above, while in others they can be separated by a varying number of other words.Unfortunately, with few exceptions (e.g., Benson, Benson, and Ilson 1986a) collocations are generally unavailable in compiled form.This creates a problem for persons not familiar with the sublanguagel as well as for several machine applications such as language generation.In this paper we describe a set of techniques for automatically retrieving such collocations from naturally occurring textual corpora.These techniques are based on statistical methods; they have been implemented in a tool, Xtract, which is able to retrieve a wide range of collocations with high performance.Preliminary results obtained with parts of Xtract have been described in the past (e.g., Smadja and McKeown 1990); this paper gives a complete description of the system and the results obtained.Xtract now works in three stages.In the first stage, pairwise lexical relations are retrieved using only statistical information.This stage is comparable to Church and Hanks (1989) in that it evaluates a certain word association between pairs of words.As in Church and Hanks (1989), the words can appear in any order and they can be separated by an arbitrary number of other words.However, the statistics we use provide more information and allow us to have more precision in our output.The output of this first stage is then passed in parallel to the next two stages.In the second stage, multiple-word combinations and complex expressions are identified.This stage produces output comparable to that of Choueka, Klein, and Neuwitz (1983); however the techniques we use are simpler and only produce relevant data.Finally, by combining parsing and statistical techniques the third stage labels and filters collocations retrieved at stage one.The third stage has been evaluated to raise the precision of Xtract from 40% to 80% with a recall of 94%.Section 2 is an introductory section on collocational knowledge, Section 3 describes the type of collocations that are retrieved by Xtract, and Section 4 briefly surveys related efforts and contrasts our work to them.The three stages of Xtract are then introduced in Section 5 and described respectively in Sections 6, 7, and 8.Some results obtained by running Xtract on several corpora are listed and discussed in Section 9.Qualitative and quantitative evaluations of our methods and of our results are discussed in Sections 10 and 11.Finally, several possible applications and tasks for Xtract are discussed in Section 12.There has been a great deal of theoretical and applied work related to collocations that has resulted in different characterizations (e.g., Allerton 1984; Cruse 1986; Menuk 1981).Depending on their interests and points of view, researchers have focused on different aspects of collocations.One of the most comprehensive definition that has been used can be found in the lexicographic work of Benson and his colleagues (Benson 1990).The definition is the following:A collocation is an arbitrary and recurrent word combination (Benson 1990).This definition, however, does not cover some aspects and properties of collocations that have consequences for a number of machine applications.For example, it has been shown that collocations are difficult to translate across languages—this fact obviously has a direct application for machine translation.Many properties of collocations have been identified in the past; however, the tendency was to focus on a restricted type of collocation.In this section, we present four properties of collocations that we have identified and discuss their relevance to computational linguistics.Collocations are difficult to produce for second language learners (Nakhimovsky and Leed 1979).In most cases, the learner cannot simply translate word-for-word what s/he would say in her/his native language.As we can see in Table 1, the word-forword translation of &quot;to open the door&quot; works well in both directions in all five languages.In contrast, translating word-for-word the expression: &quot;to break down/force the door&quot; is a poor strategy in both directions in all five languages.The co-occurrence of &quot;door&quot; and &quot;open&quot; is an open or free combination, whereas the combination &quot;door&quot; and &quot;break down&quot; is a collocation.Learners of English would not produce &quot;to break down a door&quot; whether their first language is French, German, Italian, Spanish, or Turkish, if they were not aware of the construct.Figure 1 illustrates disagreements between British English and American English.Here the problem is even finer than in Table 1 since the disagreement is not across two different languages, but across dialects of English.In each of the sentences given in this figure, there is a different word choice for the American (left side) and the British English (right side).The word choices do not correspond to any syntactic or semantic variation of English but rather to different word usages in both dialects of English.Translating from one language to another requires more than a good knowledge of the syntactic structure and the semantic representation.Because collocations are arbitrary, they must be readily available in both languages for effective machine translation.In addition to nontechnical collocations such as the ones presented before, domainspecific collocations are numerous.Technical jargons are often totally unintelligible for the layman.They contain a large number of technical terms.In addition, familiar words seem to be used differently.In the domain of sailing (Dellenbaugh and Dellenbaugh 1990), for example, some words are unknown to the nonfamiliar reader: rigg, jib, and leeward are totally meaningless to the layman.Some other combinations apparently do not contain any technical words, but these words take on a totally different meaning in the domain.For example, a dry suit is not a suit that is dry but a special type of suit used by sailors to stay dry in difficult weather conditions.Similarly a wet suit is a special kind of suit used for several marine activities.Native speakers are often unaware of the arbitrariness of collocations in nontechnical core English; however, this arbitrariness becomes obvious to the native speaker in specific sublanguages.Some examples of predicative collocations.Linguistically mastering a domain such as the domain of sailing thus requires more than a glossary, it requires knowledge of domain-dependent collocations.The recurrent property simply means that these combinations are not exceptions, but rather that they are very often repeated in a given context.Word combinations such as &quot;to make a decision, to hit a record, to perform an operation&quot; are typical of the language, and collocations such as &quot;to buy short,&quot; &quot;to ease the jib&quot; are characteristic of specific domains.Both types are repeatedly used in specific contexts.By cohesive2 clusters, we mean that the presence of one or several words of the collocations often implies or suggests the rest of the collocation.This is the property mostly used by lexicographers when compiling collocations (Cowie 1981; Benson 1989a).Lexicographers use other people's linguistic judgment for deciding what is and what is not a collocation.They give questionnaires to people such as the one given in Figure 2.This questionnaire contains sentences used by Benson for compiling collocational knowledge for the BBI (Benson 1989b).Each sentence contains an empty slot that can easily be filled in by native speakers.In contrast, second language speakers would not find the missing words automatically but would consider a long list of words having the appropriate semantic and syntactic features such as the ones given in the second column.As a consequence, collocations have particular statistical distributions (e.g., Halliday 1966; Cruse 1986).This means that, for example, the probability that any two adjacent words in a sample will be &quot;red herring&quot; is considerably larger than the probability of &quot;red&quot; times the probability of &quot;herring.&quot; The words cannot be considered as independent variables.We take advantage of this fact to develop a set of statistical techniques for retrieving and identifying collocations from large textual corpora.Collocations come in a large variety of forms.The number of words involved as well as the way they are involved can vary a great deal.Some collocations are very rigid, whereas others are very flexible.For example, a collocation such as the one linking &quot;to make&quot; and &quot;decision&quot; can appear as &quot;to make a decision,&quot; &quot;decisions to be made,&quot; &quot;made an important decision,&quot; etc.In contrast, a collocation such as &quot;The New York Stock Exchange&quot; can only appear under one form; it is a very rigid collocation, a fixed expression.We have identified three types of collocations: rigid noun phrases, predicative relations, and phrasal templates.We discuss the three types in turn, and give some examples of collocations.A predicative relation consists of two words repeatedly used together in a similar syntactic relation.These lexical relations are the most flexible type of collocation.They are hard to identify since they often correspond to interrupted word sequences in the corpus.For example, a noun and a verb will form a predicative relation if they are repeatedly used together with the noun as the object of the verb.&quot;Make-decision&quot; is a good example of a predicative relation.Similarly, an adjective repeatedly modifying a given noun such as &quot;hostile-takeover&quot; also forms a predicative relation.Examples of automatically extracted predicative relations are given in Figure 3.3 This class of collocations is related to Menuk's lexical functions (Mel'aik 1981), and Benson's Ltype relations (Benson, Benson, and Ilson 1986b).Rigid noun phrases involve uninterrupted sequences of words such as &quot;stock market,&quot; &quot;foreign exchange,&quot; &quot;New York Stock Exchange,&quot; &quot;The Dow Jones average of 30 industrials.&quot; They can include nouns and adjectives as well as closed class words, and are similar to the type of collocations retrieved by Choueka (1988) and Amsler (1989).They are the most rigid type of collocation.Examples of rigid noun phrases are:4 &quot;The NYSE's composite index of all its listed common stocks,&quot; &quot;The NASDAQ composite index for the over the counter market,&quot; &quot;leveraged buyout,&quot; &quot;the gross national product,&quot; &quot;White House spokesman Marlin Fitzwater.&quot; In general, rigid noun phrases cannot be broken into smaller fragments without losing their meaning; they are lexical units in and of themselves.Moreover, they often refer to important concepts in a domain, and several rigid noun phrases can be used to express the same concept.In the New York Stock Exchange domain, for example, &quot;The Dow industrials,&quot; &quot;The Dow Jones average of 30 industrial stocks,&quot; &quot;the Dow Jones industrial average,&quot; and &quot;The Dow Jones industrials&quot; represent several ways to express a single concept.As we have seen before, these rigid noun phrases do not seem to follow any simple construction rule, as, for example; the examples given in sentences 6-8 at the beginning of the paper are all incorrect.Phrasal templates consist of idiomatic phrases containing one, several, or no empty slots.They are phrase-long collocations.Figure 4 lists some examples of phrasal templates in the stock market domain.In the figure, the empty slots must be filled in by a number (indicated by *NUMBER* in the figure).More generally, phrasal templates specify the parts of speech of the words that can fill the empty slots.Phrasal templates are quite representative of a given domain and are very often repeated in a rigid way in a given sublanguage.In the domain of weather reports, for example, the sentence &quot;Temperatures indicate previous day's high and overnight low to 8 a.m.&quot; is actually repeated before each weather report.'Unlike rigid noun phrases and predicative relations, phrasal templates are specifically useful for language generation.Because of their slightly idiosyncratic structure, generating them from single words is often a very difficult task for a language generator.As pointed out by Kukich (1983), in general, their usage gives an impression of fluency that could not be equaled with compositional generation alone.There has been a recent surge of research interest in corpus-based computational linguistics methods; that is, the study and elaboration of techniques using large real text as a basis.Such techniques have various applications.Speech recognition (Bahl, Jelinek, and Mercer 1983) and text compression (e.g., Bell, Witten, and Cleary 1989; Guazzo 1980) have been of long-standing interest, and some new applications are currently being investigated, such as machine translation (Brown et al. 1988), spelling correction (Mays, Damerau, and Mercer 1990; Church and Gale 1990), parsing (Debili 1982; Hindle and Rooth 1990).As pointed out by Bell, Witten, and Cleary (1989), these applications fall under two research paradigms: statistical approaches and lexical approaches.In the statistical approach, language is modeled as a stochastic process and the corpus is used to estimate probabilities.In this approach, a collocation is simply considered as a sequence of words (or n-gram) among millions of other possible sequences.In contrast, in the lexical approach, a collocation is an element of a dictionary among a few thousand other lexical items.Collocations in the lexicographic meaning are only dealt with in the lexical approach.Aside from the work we present in this paper, most of the work carried out within the lexical approach has been done in computer-assisted lexicography by Choueka, Klein, and Neuwitz (1983) and Church and his colleagues (Church and Hanks 1989).Both works attempted to automatically acquire true collocations from corpora.Our work builds on Choueka's, and has been developed contemporarily to Church's.Choueka, Klein, and Neuwitz (1983) proposed algorithms to automatically retrieve idiomatic and collocational expressions.A collocation, as defined by Choueka, is a sequence of adjacent words that frequently appear together.In theory the sequences can be of any length, but in actuality, they contain two to six words.In Choueka (1988), experiments performed on an 11 million—word corpus taken from the New York Times archives are reported.Thousands of commonly used expressions such as &quot;fried chicken,&quot; &quot;casual sex,&quot; &quot;chop suey,&quot; &quot;home run,&quot; and &quot;Magic Johnson&quot; were retrieved.Choueka's methodology for handling large corpora can be considered as a first step toward computer-aided lexicography.The work, however, has some limitations.First, by definition, only uninterrupted sequences of words are retrieved; more flexible collocations such as &quot;make-decision,&quot; in which the two words can be separated by an arbitrary number of words, are not dealt with.Second, these techniques simply analyze the collocations according to their observed frequency in the corpus; this makes the results too dependent on the size of the corpus.Finally, at a more general level, although disambiguation was originally considered as a performance task, the collocations retrieved have not been used for any specific computational task.Church and Hanks (1989) describe a different set of techniques to retrieve collocations.A collocation as defined in their work is a pair of correlated words.That is, a collocation is a pair of words that appear together more often than expected.Church et al. (1991) improve over Choueka's work as they retrieve interrupted as well as uninterrupted sequences of words.Also, these collocations have been used by an automatic parser in order to resolve attachment ambiguities (Hindle and Rooth 1990).They use the notion of mutual information as defined in information theory (Shannon 1948; Fano 1961) in a manner similar to what has been used in speech recognition (e.g., Ephraim and Rabiner 1990), or text compression (e.g., Bell, Witten, and Cleary 1989), to evaluate the correlation of common appearances of pairs of words.Their work, however, has some limitations too.First, by definition, it can only retrieve collocations of length two.This limitation is intrinsic to the technique used since mutual information scores are defined for two items.The second limitation is that many collocations identified in Church and Hanks (1989) do not really identify true collocations, but simply pairs of words that frequently appear together such as the pairs &quot;doctor-nurse,&quot; &quot;doctor-bill,&quot; &quot;doctor-honorary,&quot; &quot;doctors-dentists,&quot; &quot;doctors-hospitals,&quot; etc.These co-occurrences are mostly due to semantic reasons.The two words are used in the same context because they are of related meanings; they are not part of a single collocational construct.The work we describe in the rest of this paper is along the same lines of research.It builds on Choueka's work and attempts to remedy the problems identified above.The techniques we describe retrieve the three types of collocations discussed in Section 2, and they have been implemented in a tool, Xtract.Xtract retrieves interrupted as well as uninterrupted sequences of words and deals with collocations of arbitrary length (1 to 30 in actuality).The following four sections describe and discuss the techniques used for Xtract.Xtract consists of a set of tools to locate words in context and make statistical observation to identify collocations.In the upgraded version we describe here, Xtract has been extended and refined.More information is computed and an effort has been made to extract more functional information.Xtract now works in three stages.The three-stage analysis is described in Sections 6, 7, and 8.In the first stage, described in Section 6, Xtract uses straight statistical measures to retrieve from a corpus pairwise lexical relations whose common appearance within a single sentence are correlated.A pair (or bigram) is retrieved if its frequency of occurrence is above a certain threshold and if the words are used in relatively rigid ways.The output of stage one is then passed to both the second and third stage in parallel.In the second stage, described in Section 7, Xtract uses the output bigrams to produce collocations involving more than two words (or n-grams).It analyzes all sentences containing the bigram and the distribution of words and parts of speech for each position around the pair.It retains words (or parts of speech) occupying a position with probability greater than a given threshold.For example, the bigram &quot;average-industrial&quot; produces the ngram &quot;the Dow Jones industrial average,&quot; since the words are always used within rigid noun phrases in the training corpus.In the third stage, described in Section 8, Xtract adds syntactic information to collocations retrieved at the first stage and filters out inappropriate ones.For example, if a bigram involves a noun and a verb, this stage identifies it either as a subject-verb or as a verb-object collocation.If no such consistent relation is observed, then the collocation is rejected.According to Cruse's definition (Cruse 1986), a syntagmatic lexical relation consists of a pair of words whose common appearances within a single phrase structure are correlated.In other words, those two words appear together within a single syntactic construct more often than expected by chance.The first stage of Xtract attempts to identify such pairwise lexical relations and produce statistical information on pairs of words involved together in the corpus.Ideally, in order to identify lexical relations in a corpus one would need to first parse it to verify that the words are used in a single phrase structure.However, in practice, free-style texts contain a great deal of nonstandard features over which automatic parsers would fail.'Fortunately, there is strong lexicographic evidence that most syntagmatic lexical relations relate words separated by at most five other words (Martin, Al, and Van Sterkenburg 1983).In other words, most of the lexical relations involving a word w can be retrieved by examining the neighborhood of w, wherever it occurs, within a span of five (-5 and +5 around w) words.'In the work presented here, we use this simplification and consider that two words co-occur if they are in a single sentence and if there are fewer than five words between them.In this first stage, we thus use only statistical methods to identify relevant pairs of words.These techniques are based on the assumptions that if two words are involved in a collocation then: These two assumptions are used to analyze the word distributions, and we base our filtering techniques on them.In this stage as well as in the two others, we often need part-of-speech information for several purposes.Stochastic part-of-speech taggers such as those in Church (1988) and Garside and Leech (1987) have been shown to reach 95-99% performance on free-style text.We preprocessed the corpus with a stochastic part-of-speech tagger developed at Bell Laboratories by Ken Church (Church 1988).9 In the rest of this section, we describe the algorithm used for the first stage of Xtract in some detail.We assume that the corpus is preprocessed by a part of speech tagger and we note w, a collocate of w if the two words appear in a common sentence within a distance of 5 words.Input: The tagged corpus, a given word w. Output: All the sentences containing w. Description: This actually encompasses the task of identifying sentence boundaries, and the task of selecting sentences containing w. The first task is not simple and is still an open problem.It is not enough to look for a period followed by a blank space as, for example, abbreviations and acronyms such as S.B.F., U.S.A., and A.T.M. often pose a problem.The basic algorithm for isolating sentences is described and implemented by a finite-state recognizer.Our implementation could easily be improved in many ways.For example, it performs poorly on acronyms and often considers them as end of sentences; giving it a list of currently used acronyms such as N.B.A., E.I.K., etc., would significantly improve its performance.Input: Output of Step 1.1, i.e., a set of tagged sentences containing w. Output: A list of words w, with frequency information on how w and w, co-occur.This includes the raw frequency as well as the breakdown into frequencies for each possible position.See Table 2 for example outputs.Description: For each input sentence containing w, we make a note of its collocates and store them along with their position relative to w, their part of speech, and their frequency of appearance.More precisely, for each prospective lexical relation, or for each potential collocate wi, we maintain a data structure containing this information.The data structure is shown in Figure 5.It contains freq„ the frequency of appearance of w, with w so far in the corpus, PP, the part of speech of wi, and pij, (-5 <j < 5, j 0), the frequency of appearance of w, with w such that they are j words apart.The p;s represent the histogram of the frequency of appearances of w and w, in given positions.This histogram will be used in later stages.As an example, if sentence (9) is the current input to step 1.2 and w = takeover, then, the prospective lexical relations identified in sentence (9) are as shown in Table 3.In Table 3, distance is the distance between &quot;takeover&quot; and wi, and PP is the part of speech of w,.The closed class words are not considered at this stage and the other Data structure maintained at stage one by Xtract. words, such as &quot;shareholders,&quot; are rejected because they are more than five words away from &quot;takeover.&quot; For each of the above word pairs, we maintain the associated data structure as indicated in Figure 5.For takeover pill, for example, we would increment freqpin, and the p4 column in the histogram.Table 2 shows the output for the adjective collocates of the word &quot;takeover.&quot; Input: Output of Step 1.2, i.e., a list of words w, with information on how often and how w and wz co-occur.See Table 2 for an example input.Output: Significant word pairs, along with some statistical information describing how strongly the words are connected and how rigidly they are used together.A separate (but similar) statistical analysis is done for each syntactic category of collocates.See Table 4 for an example output.Description: At this step, the statistical distribution of the collocates of w is analyzed, and the interesting word pairs are automatically selected.If part of speech information is available, a separate analysis is made depending on the part of speech of the collocates.This balances the fact that verbs, adjectives, and nouns are simply not equally frequent.For each word w, we first analyze the distribution of the frequencies freq, of its collocates wi, and then compute its average frequency f and standard deviation a around f. We then replace freq, by its associated z-score k,. k, is called the strength of the word pair in Figure 4; it represents the number of standard deviation above the The collocates of &quot;takeover&quot; as retrieved from sentence (9). w w, distance PP takeover pill 4 N takeover make 2 V takeover attempt -1 N takeover expensive -3 J takeover allowing -5 V average of the frequency of the word pair w and w, and is defined as: k frech f a (la) Then, we analyze the distribution of the /Ifs and produce their average pi and variance U, around j3j.In Figure 4 spread represents U, on a scale of 1 to 100.1/, characterizes the shape of the pi, histogram.If U, is small, then the histogram will tend to be flat, which means that w, can be used equivalently in almost any position around w. In contrast, if U, is large, then the histogram will tend to have peaks, which means that w, can only be used in one (or several) specific position around w. LI, is defined by:These analyses are then used to sort out the retrieved data.First, using (1a), collocates with strength smaller than a given threshold 1(0 are eliminated.Then, using (1b), we filter out the collocates having a variance Li, smaller than a given threshold Uo.Finally, we keep the interesting collocates by pulling out the peaks of the 13'1 distributions.These peaks correspond to the js such that the z-score of p' is bigger than a given threshold 1c1.These thresholds have to be determined by the experimenter and are dependent on the use of the retrieved collocations.As described in Smadja (1991), for language generation we found that (lco, k1, Lio) = (1, I, 10) gave good results, but for other tasks different thresholds might be preferable.In general, the lower the threshold the more data are accepted, the higher the recall, and the lower the precision of the results.Section 10 describes an evaluation of the results produced with the above thresholds.More formally, a peak, or lexical relation containing w, at this point is defined as a tuple (wi, distance, strength, spread , j) verifying the following set of inequalities: Some example results are given in Table 4.As shown in Smadja (1991), the whole first stage of Xtract as described above can be performed in 0(S log S) time, in which S is the size of the corpus.The third step of counting frequencies and maintaining the data structure dominates the whole process and as pointed out by Ken Church (personal communication), it can be reduced to a sorting problem.The inequality set (C) is used to filter out irrelevant data, that is pairs of words supposedly not used consistently within a single syntactic structure.This section discusses the importance of each inequality in (C) on the filtering process. strength = freq — f > ko (CO Condition (C1) helps eliminate the collocates that are not frequent enough.This condition specifies that the frequency of appearance of w, in the neighborhood of w must be at least one standard deviation above the average.In most statistical distributions, this thresholding eliminates the vast majority of the lexical relations.For example, for w = &quot;takeover,&quot; among the 3385 possible collocates only 167 were selected, which gives a proportion of 95% rejected.In the case of the standard normal distribution, this would reject some 68% of the cases.This indicates that the actual distribution of the collocates of &quot;takeover&quot; has a large kurtosis.'Among the eliminated collocates were &quot;dormant, dilute, ex., defunct,&quot; which obviously are not typical of a takeover.Although these rejected collocations might be useful for applications such as speech recognition, for example, we do not consider them any further here.We are looking for recurrent combinations and not casual ones.Condition (C2) requires that the histogram of the 10 relative frequencies of appearance of w, within five words of w (or pis) have at least one spike.If the histogram is flat, it will be rejected by this condition.For example, in Figure 5, the histogram associated with w2 would be rejected, whereas the one associated with wl or w, would be accepted.In Table 2, the histogram for &quot;takeover-possible&quot; is clearly accepted (there is a spike for 19_0, whereas the one for &quot;takeover-federal&quot; is rejected.The assumption here is that, if the two words are repeatedly used together within a single syntactic construct, then they will have a marked pattern of co-appearance, i.e., they will not appear in all the possible positions with an equal probability This actually eliminates pairs such as &quot;telephone-television,&quot; &quot;bomb-soldier,&quot; &quot;trouble-problem,&quot; &quot;big-small,&quot; and 10 The kurtosis of the distribution of the collocates probably depends on the word, and there is currently no agreement on the type of distribution that would describe them.&quot;doctor-nurse&quot; where the two words co-occur with no real structural consistency.The two words are often used together because they are associated with the same context rather than for pure structural reasons.Many collocations retrieved in Church and Hanks (1989) were of this type, as they retrieved doctors-dentists, doctors-nurses, doctorbills, doctors-hospitals, nurses-doctor, etc., which are not collocations in the sense defined above.Such collocations are not of interest for our purpose, although they could be useful for disambiguation or other semantic purposes.Condition (C2) filters out exactly this type of collocations. pi, ?pi + (ki x iff,) (c3) Condition (C3) pulls out the interesting relative positions of the two words.Conditions (C2) and (C1) eliminate rows in the output of Step 1.2.(See Figure 2).In contrast, Condition (C3) selects columns from the remaining rows.For each pair of words, one or several positions might be favored and thus result in several /9'1 selected.For example, the pair &quot;expensive-takeover&quot; produced two different peaks, one with only one word in between &quot;expensive&quot; and &quot;takeover,&quot; and the other with two words.Example sentences containing the two words in the two possible positions are: &quot; The provision is aimed at making a hostile takeover prohibitively expensive by enabling Borg Warner's stockholders to buy the ...&quot; &quot;The pill would make a takeover attempt more expensive by allowing the retailer's shareholders to buy more company stock ...&quot; Let us note that this filtering method is an original contribution of our work.Other works such as Church and Hanks (1989) simply focus on an evaluation of the correlation of appearance of a pair of words, which is roughly equivalent to condition (C1).(See next section).However, taking note of their pattern of appearance allows us to filter out more irrelevant collocations with (C2) and (C3).This is a very important point that will allow us to filter out many invalid collocations and also produce more functional information at stages 2 and 3.A graphical interpretation of the filtering method used for Xtract is given in Smadja (1991).7.Xtract Stage Two: From 2-Grams to N-Grams The role of the second stage of Xtract is twofold.It produces collocations involving more than two words, and it filters out some pairwise relations.Stage 2 is related to the work of Choueka (1988), and to some extent to what has been done in speech recognition (e.g., Bahl, Jelinek, and Mercer 1983; Merialdo 1987; Ephraim and Rabiner 1990).In this second stage, Xtract uses the same components used for the first stage but in a different way.It starts with the pairwise lexical relations produced in stage 1 and produces multiple word collocations, such as rigid noun phrases or phrasal templates, from them.To do this, Xtract studies the lexical relations in context, which is exactly what lexicographers do.For each bigram identified at the previous stage, Xtract examines all instances of appearance of the two words and analyzes the distributions of words and parts of speech in the surrounding positions.Input: Output of Stage 1.Similar to Table 4, i.e., a list of bigrams with their statistical information as computed in stage 1.Identical to Stage 1, Step 1.1.Given a pair of words w and wi, and an integer specifying the distance of the two words.11 This step produces all the sentences containing them in the given position.For example, given the bigram takeover-thwart and the distance 2, this step produces sentences like: &quot;Under the recapitalization plan it proposed to thwart the takeover.&quot; Identical to Stage 1, Step 1.2.We compute the frequency of appearance of each of the collocates of w by maintaining a data structure similar to the one given in Figure 5, Input: Output of Step 2.2.Output: N-grams such as in Figure 8.Discussion: Here, the analyses are simpler than for Stage 1.We are only interested in percentage frequencies and we only compute the moment of order 1 of the frequency distributions.Tables produced in Step 2.2 (such as in Figure 5) are used to compute the frequency of appearance of each word in each position around w. For each of the possible relative distances from w, we analyze the distribution of the words and only keep the words occupying the position with a probability greater than a given threshold T.12 If part of speech information is available, the same analysis is also performed with parts of speech instead of actual words.In short, a word w or a part of speech pos is kept in the final n-gram at position i if and only if it satisfies the following inequation: p(e) denotes the probability of event e. Consider the examples given in Figures 6 and 7 that show the concordances (output of step 2.1) for the input pairs: &quot;averageindustrial&quot; and &quot;index-composite.&quot; In Figure 6, the same words are always used from position —4 to position 0.However, at position +1, the words used are always different.&quot;Dow&quot; is used at position —3 in more than 90% of the cases.It is thus part of the produced rigid noun phrases.But &quot;down&quot; is only used a couple of times (out of several hundred) at position +1, 11 The distance is actually optional and can be given in various ways.We can specify the word order, the maximum distance, the exact distance, etc. and will not be part of the produced rigid noun phrases.From those concordances, Xtract produced the five-word rigid noun phrases: &quot;The Dow Jones Industrial Average.&quot; Figure 7 shows that from position —3 to position +7 the words used are always the same.In all the example sentences in which &quot;composite&quot; and &quot;index&quot; are adjacent, the two words are used within a bigger construct of 11 words (also called an 11-gram).However, if we look at position +8 for example, we see that although the words used are different, in all the cases they are verbs.Thus, after the 11-gram we expect to find a verb.In short, Figure 7 helps us produce both the rigid noun phrases &quot;The NYSE's composite index of all its listed common stocks,&quot; as well as the phrasal template &quot;The NYSE's composite index of all its listed common stocks *VERB* *NUMBER* to *NUMBER*.&quot; Figure 8 shows some sample phrasal templates and rigid noun phrases that were produced at this stage.The leftmost column gives the input lexical relations.Some other examples are given in Figure 3.The role of stage 2 is to filter out many lexical relations and replace them by valid ones.It produces both phrasal templates and rigid noun phrases.For example, associations such as &quot;blue-stocks, &quot; &quot;air-controller,&quot; or &quot;advancing-market&quot; were filtered out • Tuesday the Dow Jones industrial average The Dow Jones industrial average ... that sent the Dow Jones industrial average Monday the Dow Jones industrial average The Dow Jones industrial average ... in the Dow Jones industrial average *** rose 26.28 points to 2 304.69.The NYSE s composite index The NYSE s composite index The NYSE s composite index The NYSE s composite index The NYSE s composite index The NYSE s composite index The NYSE s composite index The NYSE s composite index The NYSE s composite index &quot;the NYSE's composite index of all its listed common stocks and respectively replaced by: &quot;blue chip stocks,&quot; &quot;air traffic controllers,&quot; and &quot;the broader market in the NYSE advancing issues.&quot; Thus stage 2 produces n-word collocations from two-word associations.Producing n-word collocations has already been done (e.g., Choueka 1988).&quot; The general method used by Choueka is the following: for each length n, (1 <n <6), produce all the word sequences of length n and sort them by frequency.On a 12 million—word corpus, Choueka retrieved 10 collocations of length six, 115 collocations of length five, 1,024 collocations of length four, 4,777 of length three, and some 15,973 of length two.The threshold imposed was 14.The method we presented in this section has three main advantages when compared to a straight n-gram method like Choueka's. of CPU time and space.In a 10 million—word corpus, with about 60,000 different words, there are about 3.6 x 109 possible bigrams, 2.16 x 1014 trigrams, and 3 x 1033 7-grams.This rapidly gets out of hand.Choueka, for example, had to stop at length six.In contrast, the rigid noun phrases we retrieve are of arbitrary length and are retrieved very easily and in one pass.The method we use starts from bigrams and produces the biggest possible subsuming n-gram.It is based on the fact that if an n-gram is statistically significant, then the included bigrams must also be significant.For example, to identify &quot;The Dow Jones average of 30 industrials,&quot; a traditional n-gram method would compare it to the other 7-grams and determine that it is significant.In contrast, we start from an included significant bigram (for example, &quot;Dow-30&quot;) and we directly retrieve the surrounding n-grams.'8.Xtract Stage Three: Adding Syntax to the Collocations The collocations as produced in the previous stages are already useful for lexicography.For computational use, however, functional information is needed.For example, the collocations should have some syntactic properties.It is not enough to say that &quot;make&quot; goes with &quot;decision&quot;; we need to know that &quot;decision&quot; is used as the direct object of the verb.The advent of robust parsers such as Cass (Abney 1990) and Fidditch (Hindle 1983) has made it possible to process large text corpora with good performance and thus combine statistical techniques with more symbolic analysis.In the past, some similar attempts have been done.Debili (1982) parsed corpora of French texts to identify nonambiguous predicate argument relations.He then used these relations for disambiguation.Hindle and Rooth (1990) later refined this approach by using bigram statistics to enhance the task of prepositional phrase attachment.Church et al. (1989, 1991) have yet another approach; they consider questions such as what does a boat typically do?They are preprocessing a corpus with the Fidditch parser (Hindle 1983) in order to produce a list of verbs that are most likely associated with the subject &quot;boat.&quot; Our goal here is different, as we analyze collocations automatically produced by the first stage of Xtract to either add syntactic information or reject them.For example, if a lexical relation identified at stage 1 involves a noun and a verb, the role of stage 3 is to determine whether it is a subject—verb or a verb—object collocation.If no such consistent relation is observed, then the collocation is rejected.Stage 3 uses a parser but it does not require a complete parse tree.Given a number of sentences, Xtract only needs to know pairwise syntactic (modifier—modified) relations.The parser we used in the experiment reported here is Cass (Abney 1989, 1990), a bottom-up incremental parser.Cass' takes input sentences labeled with part of speech and attempts to identify syntactic structure.One of the subtasks performed by Cass is to identify predicate argument relations, and this is the task we are interested in here.Stage 3 works in the following three steps.All the syntactic labels produced by Cass on sentence (10).Identical to what we did at Stage 2, Step 2.1.Given a pair of words w and w,, a distance of the two words (optional), and a tagged corpus, Xtract produces all the (tagged) sentences containing them in the given position specified by the distance.Input: Output of Step 3.1.A set of tagged sentences each containing both w and w,.Output: For each sentence, a set of syntactic labels such as those shown in Figure 9.Discussion: Cass is called on the concordances.From Cass output, we only retrieve binary syntactic relations (or labels) such as &quot;verb—object&quot; or &quot;verb—subject,&quot; &quot;noun— adjective,&quot; and &quot;noun—noun.&quot; To simplify, we abbreviate them respectively: VO, SV, NJ, NN.For sentence (10) below, for example, the labels produced are shown in Figure 9.10.&quot;Wall Street faced a major test with stock traders returning to action for the first time since last week's epic selloff and investors awaited signs of life from the 5-year-old bull market.&quot; Input: A set of sentences each associated with a set of labels as shown in Figure 9.Output: Collocations with associated syntactic labels as shown in Figure 10.Discussion: For any given sentence containing both w and w,, two cases are possible: either there is a label for the bigram (w, w,), or there is none.For example, for sentence (10), there is a syntactic label for the bigram faced-test, but there is none for the bigram stock-returning.Faced-test enters into a verb object relation, and stock-returning does not enter into any type of relation.If no label is retrieved for the bigram, it means that the parser could not identify a relation between the two words.In this case we introduce a new label: U (for undefined) to label the bigram.At this point, we associate with the sentence the label for the bigram (w, w1).With each of the input sentences, we associate a label for the bigram (w, w1).For example, the label associated with sentence (10) for the bigram faced-test would be VO.A list of labeled sentences for the bigram w = &quot;rose&quot; and w = &quot;prices&quot; is shown in Figure 10.Producing the &quot;prices f] rose,&quot; SV predicative relation at stage 3.Input: A set of sentences containing w and w, each associated with a label as shown in Figure 10.Output: Labeled collocations as shown in Figure 11.Discussion on Step 3.4: At this step, we count the frequencies of each possible label identified for the bigram (w, w,) and perform a statistical analysis of order two for this distribution.We compute the average frequency for the distribution of labels: ft and the standard deviation a-t. We finally apply a filtering method similar to (C2).Let t be a possible label.We keep t if and only if it satisfies inequality (4b) similar to (4a) given before: A collocation is thus accepted if and only if it has a label g satisfying inequality (4b), and g U.Similarly, a collocation is rejected if no label satisfies inequality (4b) or if U satisfies it.Figure 10 shows part of the output of Step 3.3 for w = &quot;rose&quot; and w, = &quot;prices.&quot; As shown in the figure, SV labels are a large majority.Thus, we would label the relation price-rose as an SV relation.An example output of this stage is given in Figure 11.The bigrams labeled U were rejected at this stage.Stage 3 thus produces very useful results.It filters out collocations and rejects more than half of them, thus improving the quality of the results.It also labels the collocations it accepts, thus producing a more functional and usable type of knowledge.For example, if the first two stages of Xtract produce the collocation &quot;make-decision,&quot; the third stage identifies it as a verb—object collocation.If no such relation can be observed, then the collocation is rejected.The produced collocations are not simple word associations but complex syntactic structures.Labeling and filtering are two useful tasks for automatic use of collocations as well as for lexicography.The whole of stage 3 (both as a filter and as a labeler) is an original contribution of our work.Retrieving syntactically labeled collocations is a relatively new concern.Moreover, filtering greatly improves the quality of the results.This is also a possible use of the emerging new parsing technology.Xtract is actually a library of tools implemented using standard C-Unix libraries.The toolkit has several utilities useful for analyzing corpora.Without making any effort to make Xtract efficient in terms of computing resources, the first stage as well as the second stage of Xtract only takes a few minutes to run on a ten-megabyte (pre-tagged) corpus.Xtract is currently being used at Columbia University for various lexical tasks.And it has been tested on many corpora, among them: several ten-megabyte corpora of news stories, a corpus, consisting of some twenty megabytes of New York Times articles, which has already been used by Choueka (1988), the Brown corpus (Francis and Ktfaera 1982), a corpus of the proceedings of the Canadian Parliament, also called the Hansards corpus, which amounts to several hundred megabytes.We are currently working on packaging Xtract to make it available to the research community.The packaged version will be portable, reusable, and faster than the one we used to write this paper.'We evaluate the filtering power of stage 3 in the evaluation section, Section 10.Section 9 presents some results that we obtained with the three stages of Xtract.Results obtained from The Jerusalem Post corpus have already been reported (e.g., Smadja 1991).Figure 12 gives some results for the three-stage process of Xtract on a 10 million—word corpus of stock market reports taken from the Associated Press newswire.The collocations are given in the following format.The first line contains the bigrams with the distance, so that &quot;sales fell —1&quot; says that the two words under consideration are &quot;sales&quot; and &quot;fell,&quot; and that the distance we are considering is —1.The first line is thus the output of stage 1.The second line gives the output of stage 2, i.e., the n-grams.For example, &quot;takeover-thwart&quot; is retrieved as &quot;44 to thwart AT takeover NN &quot;AT stands for article, NN stands for nouns, and 44 is the number of times this collocation has been retrieved in the corpus.The third line gives the retrieved tags for this collocation, so that the syntactic relation between &quot;takeover&quot; and &quot;thwart&quot; is an SV relation.And finally, the last line is an example sentence containing the collocation.Output of the type of Figure 12 is automatically produced.This kind of output is about as far as we have gone automatically.Any further analysis and/or use of the collocations would probably require some manual intervention.Some complete output on the stock market corpus.For the 10 million—word stock market corpus, there were some 60,000 different word forms.Xtract has been able to retrieve some 15,000 collocations in total.We would like to note, however, that Xtract has only been effective at retrieving collocations for words appearing at least several dozen times in the corpus.This means that lowfrequency words were not productive in terms of collocations.Out of the 60,000 words in the corpus, only 8,000 were repeated more than 50 times.This means that for a target Overlap of the manual and automatic evaluations lexicon of size N = 8,000, one should expect at least as many collocations to be added, and Xtract can help retrieve most of them.The third stage of Xtract can thus be considered as a retrieval system that retrieves valid collocations from a set of candidates.This section describes an evaluation experiment of the third stage of Xtract as a retrieval system as well as an evaluation of the overall output of Xtract.Evaluation of retrieval systems is usually done with the help of two parameters: precision and recall (Salton 1989).Precision of a retrieval system is defined as the ratio of retrieved valid elements divided by the total number of retrieved elements (Salton 1989).It measures the quality of the retrieved material.Recall is defined as the ratio of retrieved valid elements divided by the total number of valid elements.It measures the effectiveness of the system.This section presents an evaluation of the retrieval performance of the third stage of Xtract.Deciding whether a given word combination is a valid or invalid collocation is actually a difficult task that is best done by a lexicographer.Jeffery Triggs is a lexicographer working for the Oxford English Dictionary (OED) coordinating the North American Readers program of OED at Bell Communication Research.Jeffery Triggs agreed to go over manually several thousands of collocations.'In order to have an unbiased experiment we had to be able to evaluate the performance of Xtract against a human expert.We had to have the lexicographer and Xtract perform the same task.To do this in an unbiased way we randomly selected a subset of about 4,000 collocations after the first two stages of Xtract.This set of collocations thus contained some good collocations and some bad ones.This data set was then evaluated by the lexicographer and the third stage of Xtract.This allowed 17 I am grateful to Jeffery, whose professionalism and kindness helped me understand some of the difficulty of lexicography.Without him this evaluation would not have been possible. us to evaluate the performances of the third stage of Xtract and the overall quality of the total output of Xtract in a single experiment.The experiment was as follows: We gave the 4,000 collocations to evaluate to the lexicographer, asking him to select the ones that he would consider for a domain-specific dictionary and to cross out the others.The lexicographer came up with three simple tags, YY, Y, and N. Both Y and YY include good collocations, and N includes bad collocations.The difference between YY and Y is that Y collocations are of better quality than YY collocations.YY collocations are often too specific to be included in a dictionary, or some words are missing, etc.After stage 2, about 20% of the collocations are Y, about 20% are YY, and about 60% are N. This told us that the precision of Xtract at stage 2 was only about 40%.Although this would seem like a poor precision, one should compare it with the much lower rates currently in practice in lexicography.For compiling new entries for the OED, for example, the first stage roughly consists of reading numerous documents to identify new or interesting expressions.This task is performed by professional readers.For the OED, the readers for the American program alone produce some 10,000 expressions a month.These lists are then sent off to the dictionary and go through several rounds of careful analysis before actually being submitted to the dictionary.The ratio of proposed candidates to good candidates is usually low.For example, out of the 10,000 expressions proposed each month, fewer than 400 are serious candidates for the OED, which represents a current rate of 4%.Automatically producing lists of candidate expressions could actually be of great help to lexicographers, and even a precision of 40% would be helpful.Such lexicographic tools could, for example, help readers retrieve sublanguage-specific expressions by providing them with lists of candidate collocations.The lexicographer then manually examines the list to remove the irrelevant data.Even low precision is useful for lexicographers, as manual filtering is much faster than manual scanning of the documents (Marcus 1990).Such techniques are not able to replace readers, though, as they are not designed to identify low-frequency expressions, whereas a human reader immediately identifies interesting expressions with as few as one occurrence.The second stage of this experiment was to use Xtract stage 3 to filter out and label the sample set of collocations.As described in Section 8, there are several valid labels (VO, VS, NN, etc.).In this experiment, we grouped them under a single label: T. There is only one nonvalid label: U (for unlabeled).A T collocation is thus accepted by Xtract stage 3, and a U collocation is rejected.The results of the use of stage 3 on the sample set of collocations are similar to the manual evaluation in terms of numbers: about 40% of the collocations were labeled (T) by Xtract stage 3, and about 60% were rejected (U).Figure 13 shows the overlap of the classifications made by Xtract and the lexicographer.In the figure, the first diagram on the left represents the breakdown in T and U of each of the manual categories (Y-YY and N).The diagram on the right represents the breakdown in Y-YY and N of the T and U categories.For example, the first column of the diagram on the left represents the application of Xtract stage 3 on the YY collocations.It shows that 94% of the collocations accepted by the lexicographer were also accepted by Xtract.In other words, this means that the recall of the third stage of Xtract is 94%.The first column of the diagram on the right represents the lexicographic evaluation of the collocations automatically accepted by Xtract.It shows that about 80% of the T collocations were accepted by the lexicographer and that about 20% were rejected.This shows that precision was raised from 40% to 80% with the addition of Xtract stage 3.In summary, these experiments allowed us to evaluate Stage 3 as a retrieval system.The results are: precision = 80% and recall = 94%.Top associations with &quot;price&quot; in NYT, DJ, and AP.In this section, we discuss the extent to which the results are dependent on the corpus used.To illustrate our purpose here, we are using results collected from three different corpora.The first one, DJ, for Dow Jones, is the corpus we used in this paper; it contains (mostly) stock market stories taken from the Associated Press newswire.DJ contains 8-9 million words.The second corpus, NYT, contains articles published in the New York Times during the years 1987 and 1988.The articles are on various subjects.This is the same corpus that was used by Choueka (1988).NYT contains 12 million words.The third corpus, AP, contains stories from the Associated Press newswire on various domains such as weather reports, politics, health, finances, etc.AP is 4 million words.Figure 14 represents the top 10 word associations retrieved by Xtract stage 1 for the three corpora with the word &quot;price.&quot; In this figure, d represents the distance between the two words and w represents the weight associated with the bigram.The weight is a combined index of the statistical distribution as discussed in Section 6, and it evaluates the collocation.There are several differences and similarities among the three columns of the figure in terms of the words retrieved, the order of the words retrieved, and the values of w. We identified two main ways in which the results depend on the corpus.We discuss them in turn.From the different corpora we used, we noticed that our statistical methods were not effective for low-frequency words.More precisely, the statistical methods we use do not seem to be effective on low frequency words (fewer than 100 occurrences).If the word is not frequently used in the corpus or if the corpus is too small, then the distribution of its collocates will not be big enough.For example, from AP, which contains about 1,000 occurrences of the word &quot;rain,&quot; Xtract produced over 170 collocations at stage 1 involving it.In contrast, DJ only contains some 50 occurrences of &quot;rain&quot;' and Xtract could only produce a few collocations with it.Some collocations with &quot;rain&quot; and &quot;hurricane&quot; extracted from AP are listed in Figure 15.Both words are high-frequency words in AP and low-frequency words in DJ.In short, to build a lexicon for a computational linguistics application in a given domain, one should make sure that the important words in the domain are frequent enough in the corpus.For a subdomain of the stock market describing only the fluctuations of several indexes and some of the major events of the day at Wall Street, a corpus of 10 million words appeared to be sufficient.This 10 million—token corpus contains only 5,000 words each repeated more than 100 times.Size and frequency are not the only important criteria.For example, even though &quot;food&quot; is a high-frequency word in DJ, &quot;eat&quot; is not among its collocates, whereas it is among the top ones in the two other corpora.Food is not eaten at Wall Street but rather traded, sold, offered, bought, etc.If the corpus only contains stories in a given domain, most of the collocations retrieved will also be dependent on this domain.We have seen in Section 2 that in addition to jargonistic words, there are a number of more familiar terms that form collocations when used in different domains.A corpus containing stock market stories is obviously not a good choice for retrieving collocations related to weather reports or for retrieving domain independent collocations such as &quot;makedecision.&quot; For a domain-specific application, domain-dependent collocations are of interest, and a domain-specific corpus is exactly what is required.To build a system that generates stock market reports, it is a good choice to use a corpus containing only stock market reports.There is a danger in choosing a too specific corpus however.For example, in Figure 14, we see that the first collocate of &quot;price&quot; in AP is &quot;gouging,&quot; which is not retrieved in either DJ or in NYT.&quot;Price gouging&quot; is not a current practice at Wall Street and this collocation could not be retrieved even on some 20,000 occurrences of the word.An example use of &quot;price gouging&quot; is the following: &quot;The Charleston City Council passed an emergency ordinance barring price gouging later Saturday after learning of an incident in which 5 pound bags of ice were being sold for 10.&quot; More formally, if we compare the columns in Figure 14, we see that the numbers are much higher for DJ than for the other two corpora.This is not due to a size/frequency factor, since &quot;price&quot; occurs about 10,000 times in both NYT and DJ, whereas it only occurs 4,500 times in AP.It rather says that the distribution of collocates around &quot;price&quot; has a much higher variance in DJ than in the other corpora.DJ has much bigger weights because it is focused; the stories are almost all about Wall Street.In contrast, NYT contains a large number of stories with &quot;price,&quot; but they have various origins.&quot;Price&quot; has 4,627 collocates in NYT, whereas it only has 2,830 in DJ.Let us call Ocorpus the variety of a given corpus.One way to measure the variety is to use the information theory measure of entropy for a given language model.Entropy is defined (Shannon 1948) as: where p(w) is the probability of appearance of a given word, w. Entropy measures the predictability of a corpus, in other words, the bigger the entropy of a corpus the less predictable it is.In an ideal language model, the entropy of a corpus should not depend on its size.However, word probabilities are difficult to approximate (see, for example, Bell CD inches of rain.... acid rain....CD inches of rain fell heavy rain .... the Atlantic hurricane season hurricane force winds rain forests to reduce acid rain .... a major hurricane light rain .... the most powerful hurricane to hit the .... an inch of rain .... to save the world s rain forests wind and rain .... a cold rain [19871 for a thorough discussion on probability estimation), and in most cases entropy grows with the size of the corpus.In this section, we use a simple unigram language model trained on the corpus and we approximate the variety of a given corpus by: 0 corpus — E(f(w)/S) log(f (w) /S) w in which f (w) is the frequency of appearance of the word w in the corpus and S is the total number of different word forms in the corpus.In addition, to be fair in our comparison of the three corpora, we have used three (sub)corpora of about one million words for DJ, NYT, and Brown.The 1 million—word Brown corpus (Francis and Ktit'era 1982) contains 43,300 different words, of which only 1091 are repeated more than 100 times.The 0 of the Brown corpus is: °Brown = 10.5.In comparison, the size of DJ is 8,000,000.It contains 59,233 different words of which 5,367 are repeated more than 100 times.DJ 0 ratio is: ODJ = 9.6.And the 0 ratio of NYT which contains stories pertaining to various domains has been estimated at ONyT = 10.4.According to this measure, DJ is much more focused than both the Brown Corpus and NYT because the difference in variety is 1 in the logarithmic scale.This is not a surprise since the subjects it covers are much more restricted, the genre is of only one kind, and the setting is constant.In contrast, the Brown corpus has been designed to be of mixed and rich composition, and NYT is made up of stories and articles related to various subjects and domains.Let us note that several factors might also influence the overall entropy of a given corpus; for example the number of writers, the time span covered by the corpus, etc.In any case, the success of statistical methods such as the ones described in this report also depends on the sublanguage used in the corpus.For a sublanguage-dependent application, the training corpus must be focused, mainly because its vocabulary being restricted, the important words will be more frequent than in a nonrestricted corpus (of equivalent size), and thus the collocations will be easier to retrieve.Other applications might require less focused corpora.For those applications, the problem is even more touchy, as a perfectly balanced corpus is very difficult to compile.A sample of the 1987 DJ text is certainly not a good sample of general English; however, a balanced sample, such as the Brown Corpus, may also be a poor sample.It is doubtful that even a balanced corpus contains enough data on all possible domains, and the very effort of artificially balancing the corpus might also bias the results.Corpus-based techniques are still rarely used in the fields of linguistics, lexicography, and computational linguistics, and the main thrust of the work presented here is to promote its use for any text based application.In this section we discuss several uses of Xtract.Language generation is a novel application for Corpus-Based Computational Linguistics (Boguraev 1989).In Smadja (1991) we show how collocations enhance the task of lexical selection in language generation.Previous language generation works did not use collocations mainly because they did not have the information in compiled form and the lexicon formalisms available did not handle the variability of collocational knowledge.In contrast, we use Xtract to produce the collocations and we use Functional Unification Grammars (FUGs) (Kay 1979) as a representation formalism and a unification engine.We show how the use of FUGs allows us to properly handle the interactions of collocational and various other constraints.We have implemented Cook, a surface sentence generator that uses a flexible lexicon for expressing collocational constraints in the stock market domain.Using Ana (Kukich 1983) as a deep generator, Cook is implemented in FUF (Elhadad 1990), an extended implementation of FUG, and uniformly represents the lexicon and syntax as originally suggested by Halliday (1966).For a more detailed description of Cook the reader is referred to Smadja (1991).According to Benson, Benson, and Ilson (1986a), collocations fall into two major groups: lexical collocations and grammatical collocations.The difference between these two groups lies in the types of words involved.Lexical collocations roughly consist of syntagmatic affinities among open class words such as verbs, nouns, adjectives, and adverbs.In contrast, grammatical collocations generally involve at least one closed class word among particles, prepositions, and auxiliary verbs.Examples of grammatical collocations are: put-up, as in &quot;I can't put up with this anymore,&quot; and fill-out, as in &quot;You have to fill out your 1040 form.&quot;19 Consider the sentences below: 6.* &quot;... a new initiative in the aftermath from the PLO's evacuation from Beirut.&quot; 7.&quot;... a new initiative in the aftershocks from the PLO's evacuation from Beirut.&quot; 8.* &quot;... a new initiative in the aftershocks of the PLO's evacuation from Beirut.&quot; These examples clearly show that the choices of the prepositions are arbitrary.Sentences (1)—(2) and (3)—(4) compare the word associations comparison with/to with association with/to.Although very similar in meaning, the two words select different prepositions.Moreover, the difference of meaning of the two prepositions does not account for the wording choices.Similarly, sentences (5)—(6) and (7)—(8) illustrate the fact that &quot;aftermath&quot; selects the preposition &quot;of&quot; and &quot;aftershock&quot; selects &quot;from.&quot; Grammatical collocations are very similar to lexical collocations in the sense that they also correspond to arbitrary and recurrent word co-occurrences (Benson 1990).In terms of structure, grammatical collocations are much simpler: since many of the grammatical collocations only include one open class word, the separation base-collocator becomes trivial.The open class word is the meaning bearing element, it is the base; and the closed class word is the collocator.For lexicographers, grammatical collocations are somehow simpler than lexical collocations.A large number of dictionaries actually include them.For example, The Random House Dictionary of the English Language (RHDEL) (Flexner 1987) gives: &quot;abreast of, accessible to, accustomed to, careful about, conducive to, conscious of, equal to, expert at, fond of, jealous of,&quot; etc.However, a large number are missing and the information provided is inconsistent and spotty.For example, RHDEL does not include: appreciative of, available to, certain of, clever at, comprehensible to, curious about, difficult for, effective against, faithful to, friendly with, furious at, happy about, hostile to, etc.As demonstrated by Benson, even the most complete learners' dictionaries miss very important grammatical collocations and treat the others inconsistently.'Determiners are lexical elements that are used in conjunction with a noun to bring into correspondence with it a certain sector of reality (Ducrot and Todorov 1979).A noun without determiner has no referent.The role of determiner can be played by several classes of items: articles, (e.g., &quot;a,&quot; &quot;the&quot;), possessives (e.g., &quot;my,&quot; &quot;your&quot;), indefinite adjectives (e.g., &quot;some,&quot; &quot;many,&quot; &quot;few,&quot; &quot;certain&quot;), demonstratives (e.g., &quot;this,&quot; &quot;those&quot;), numbers, etc.Determiner—noun combinations are often based simply on semantic or syntactic criteria.For example in the expression &quot;my left foot,&quot; the determiner &quot;my&quot; is here for semantic reasons.Any other determiner would fail to identify the correct object (my left foot).Classes of nouns such as mass and count are supposed to determine the type of determiners to be used in conjunction with the nouns (Quirk et al. 1972).Mass nouns often refer to objects or ideas that can be divided into smaller parts without losing their meaning.In contrast, count nouns refer to objects that are not dividable.For example, &quot;water&quot; is a mass noun, if you spill half a glass of water you still have Some noun-preposition associations retrieved by Xtract. some water left in your glass.In contrast if you cut a book in two halves and discard one half, you do not have a book any more; &quot;book&quot; is a count noun.Count nouns are often used with numbers and articles, and mass nouns are often used with no articles (or the zero article noted 0) (Quirk et al. 1972).As with other types of word combinations, noun-determiner combinations often lead to collocations.Consider the table given in Table 5.In the table, some noundeterminer combinations are compared.The first four determiners (a, the, 0, some) represent a singular use of the noun, and the last four (many, few, a lot of, a great deal of) represent a plural use.1 and 300 are numbers.0 is the zero article.In the table, a '+' sign means that the combination is frequent and normal; a '-' sign means that the combination is very rare if not forbidden.A '?' sign means that the combination is very low probability and that it would probably require an unusual context.For example, one does not say *&quot;a butter,&quot; one says &quot;some butter,&quot; and the combination butter-many is rather unusual and would only occur in unusual contexts.For example, if one refers to several types of butter, one could say: &quot;Many butters are based on regular butter and an additional spice or flavor, such as rosemary, sage, basil, garlic, etc.&quot; &quot;Book&quot; is a typical count noun in that it can combine with &quot;a&quot; and &quot;many.&quot; &quot;Butter&quot; is a typical mass noun in that it combines with the zero determiner and &quot;a great deal.&quot; However, words such as &quot;police, people, traffic, opinion, weather,&quot; etc. share some characteristics of both mass nouns and count nouns.For example, &quot;weather&quot; is neither a count noun—*&quot;a weather&quot; is incorrect—nor a mass noun—*&quot;a lot of weather&quot; is incorrect (Quirk et al. 1972).However, it shares some characteristics of both types of nouns.Mass noun features include the premodified structures &quot;a lot of good weather,&quot; &quot;some bad weather,&quot; and &quot;what lovely weather.&quot; Count noun features include the plural &quot;go out in all weathers,&quot; &quot;in the worst of weathers.&quot; The problem with such combinations is that, if the word is irregular then the information will probably not be in the dictionary.'Moreover, even if the word is regular, the word itself might not be in the dictionary or the information could simply be difficult to retrieve automatically.Simple tools such as Xtract can hopefully provide such information.Based on a large number of occurrences of the noun, Xtract will be able to make statistical inferences as to the determiners used with it.Such analysis is possible without any modification to Xtract.Actually, only a subpart of Xtract is necessary to retrieve them.We have seen that collocations are difficult to handle for non-native speakers, and that they require special handling for computational applications.In a multilingual environment the problems become even more complex, as each language imposes its own collocational constraints.Consider, for example, the English expressions &quot;House of Parliament&quot; and &quot;House painter.&quot; The natural French translation for &quot;house&quot; is &quot;maison.&quot; However, the two expressions do not use this translation, but respectively &quot;chambre&quot; (&quot;room&quot; in English) and &quot;Witiment&quot; (&quot;building&quot; in English).Translations have to be provided for collocations, and should not be word-based but rather expression-based.Bilingual dictionaries are generally inadequate in dealing with such issues.They generally limit such context-sensitive translations to ambiguous words (e.g., &quot;number&quot; or &quot;rock&quot;) or highly complex words such as &quot;make,&quot; &quot;have,&quot; etc.Moreover, even in these cases, coverage is limited to semantic variants, and lexical collocations are generally omitted.One possible application is the development of compilation techniques for bilingual dictionaries.This would require compiling two monolingual collocational dictionaries and then developing some automatic or assisted translation methods.Those translation methods could be based on the statistical analysis of bilingual corpora currently available.A simple algorithm for translating collocations is given in Smadja (1992).Several other applications such as information retrieval, automatic thesauri compilation, and speech recognition are also discussed in Smadja (1991).21 Note that it might be in some grammar book.For example, Quirk et al. in their extensive grammar book (1972) devote some 100 pages to such noun—determiner combinations.They include a large number of rules and list exceptions to those rules.Corpus analysis is a relatively recent domain of research.With the availability of large samples of textual data and automated tools such as part-of-speech taggers, it has become possible to develop and use automatic techniques for retrieving lexical information from textual corpora.In this paper some original techniques for the automatic extraction of collocations have been presented.The techniques have been implemented in a system, Xtract, and tested on several corpora.Although some other attempts have been made to retrieve collocations from textual corpora, no work has been able to retrieve the full range of the collocations that Xtract retrieves.Thanks to our filtering methods, the collocations produced by Xtract are of better quality.And finally, because of the syntactic labeling, the collocations we produce are richer than the ones produced by other methods.The number and size of available textual corpora is constantly growing.Dictionaries are available in machine-readable form, news agencies provide subscribers with daily reports on various events, publishing companies use computers and provide machine-readable versions of books, magazines, and journals.This amounts to a vast quantity of language data with unused and virtually unlimited, implicit and explicit information about the English language.These textual data can thus be used to retrieve important information that is not available in other forms.The primary goal of the research we presented is to provide a comprehensive lexicographic toolkit to assist in implementing natural language processing, as well as to assist lexicographers in compiling general-purpose dictionaries, as most of the work is still manually performed in this domain.The abundance of text corpora allows a shift toward more empirical studies of language that emphasize the development of automated tools.We think that more research should be conducted in this direction and hope that our work will stimulate research projects along these lines.I would like to thank Steve Abney, Ken Church, Karen Kukich, and Michael Elhadad for making their software tools available to us.Without them, most of the work reported here would not have been possible.Kathy McKeown read earlier versions of this paper and was helpful in both the writing and the research.Finally, the anonymous reviewers for Computational Linguistics made insightful comments on earlier versions of the paper.Part of this work has been done in collaboration with Bell Communication Research, and part of this work has been supported by DARPA grant NO0039-84-C-0165, by NSF grant IRT-84-51438, and by ONR grant NO0014-89-J-1782.
Regular Models Of Phonological Rule SystemsThis paper presents a set of mathematical and computational tools for manipulating and reasoning about regular languages and regular relations and argues that they provide a solid basis for computational phonology.It shows in detail how this framework applies to ordered sets of context-sensitive rewriting rules and also to grammars in Koskenniemi's two-level formalism.This analysis provides a common representation of phonological constraints that supports efficient generation and recognition by a single simple interpreter.This paper presents a set of mathematical and computational tools for manipulating and reasoning about regular languages and regular relations and argues that they provide a solid basis for computational phonology.It shows in detail how this framework applies to ordered sets of context-sensitive rewriting rules and also to grammars in Koskenniemi's two-level formalism.This analysis provides a common representation of phonological constraints that supports efficient generation and recognition by a single simple interpreter.Ordered sets of context-sensitive rewriting rules have traditionally been used to describe the pronunciation changes that occur when sounds appear in different phonological and morphological contexts.Intuitively, these phenomena ought to be cognitively and computationally simpler than the variations and correspondences that appear in natural language syntax and semantics, yet the formal structure of such rules seems to require a complicated interpreter and an extraordinarily large number of processing steps.In this paper, we show that any such rule defines a regular relation on strings if its non-contextual part is not allowed to apply to its own output, and thus it can be modeled by a symmetric finite-state transducer.Furthermore, since regular relations are closed under serial composition, a finite set of rules applying to each other's output in an ordered sequence also defines a regular relation.A single finitestate transducer whose behavior simulates the whole set can therefore be constructed by composing the transducers corresponding to the individual rules.This transducer can be incorporated into efficient computational procedures that are far more economical in both recognition and production than any strategies using ordered rules directly.Since orthographic rules have similar formal properties to phonological rules, our results generalize to problems of word recognition in written text.The mathematical techniques we develop to analyze rewriting rule systems are not limited just to that particular collection of formal devices.They can also be applied to other recently proposed phonological or morphological rule systems.For example, we can show that Koskenniemi's (1983) two-level parallel rule systems also denote regular relations.Section 2 below provides an intuitive grounding for the rest of our discussion by illustrating the correspondence between simple rewriting rules and transducers.Section 3 summarizes the mathematical tools that we use to analyze both rewriting and two-level systems.Section 4 describes the properties of the rewriting rule formalisms we are concerned with, and their mathematical characterization is presented in Sections 5 and 6.A similar characterization of two-level rule systems is provided in Section 7.By way of introduction, we consider some of the computational issues presented by simple morphophonemic rewriting rules such as these: According to these rules an underspecified, abstract nasal phoneme N appearing in the lexical forms iNpractical and iNtractable will be realized as the m in impractical and as the n in intractable.To ensure that these and only these results are obtained, the rules must be treated as obligatory and taken in the order given.As obligatory rules, they must be applied to every substring meeting their conditions.Otherwise, the abstract string iNpractical would be realized as in practical and iNpractical as well as impractical, and the abstract N would not necessarily be removed from iNtractable.Ordering the rules means that the output of the first is taken as the input to the second.This prevents iNpractical from being converted to in practical by Rule 2 without first considering Rule 1.These obligatory rules always produce exactly one result from a given input.This is not the case when they are made to operate in the reverse direction.For example, if Rule 2 is inverted on the string intractable, there will be two results, intractable and iNtractable.This is because intractable is derivable by that rule from both of these strings.Of course, only the segments in iNtractable will eventually match against the lexicon, but in general both the N and n results of this inversion can figure in valid interpretations.Compare the words undecipherable and indecipherable.The n in the prefix un-, unlike the one in in-, does not derive from the abstract N, since it remains unchanged before labials (c.f. unperturbable).Thus the results of inverting this rule must include undecipherable for undecipherable but iNdecipherable for indecipherable so that each of them can match properly against the lexicon.While inverting a rule may sometimes produce alternative outputs, there are also situations in which no output is produced.This happens when an obligatory rule is inverted on a string that it could not have generated.For example, iNput cannot be generated by Rule 1 because the N precedes a labial and therefore would obligatorily be converted to m. There is therefore no output when Rule 1 is inverted on iNput.However, when Rule 2 is inverted on input, it does produce iNput as one of its results.The effect of then inverting Rule 1 is to remove the ambiguity produced by inverting Rule 2, leaving only the unchanged input to be matched against the lexicon.More generally, if recognition is carried out by taking the rules of a grammar in reverse order and inverting each of them in turn, later rules in the new sequence act as filters on ambiguities produced by earlier ones.The existence of a large class of ambiguities that are introduced at one point in the recognition process and eliminated at another has been a major source of difficulty in efficiently reversing the action of linguistically motivated phonological grammars.In a large grammar, the effect of these spurious ambiguities is multiplicative, since the information needed to cut off unproductive paths often does not become available until after they have been pursued for some considerable distance.Indeed, speech understanding systems that use phonological rules do not typically invert them on strings but rather apply them to the lexicon to generate a list of all possible word forms (e.g.Woods et al. 1976; Klatt 1980).Recognition is then accomplished by standard tablelookup procedures, usually augmented with special devices to handle phonological changes that operate across word boundaries.Another approach to solving this computational problem would be to use the reversed cascade of rules during recognition, but to somehow make the filtering information of particular rules available earlier in the process.However, no general and effective techniques have been proposed for doing this.The more radical approach that we explore in this paper is to eliminate the cascade altogether, representing the information in the grammar as a whole in a single more unified device, namely, a finite-state transducer.This device is constructed in two phases.The first is to create for each rule in the grammar a transducer that exactly models its behavior.The second is to compose these individual rule transducers into a single machine that models the grammar as a whole.Johnson (1972) was the first to notice that the noncyclic components of standard phonological formalisms, and particularly the formalism of The Sound Pattern of English (Chomsky and Halle 1968), were equivalent in power to finite-state devices despite a superficial resemblance to general rewriting systems.Phonologists in the SPE tradition, as well as the structuralists that preceded them, had apparently honored an injunction against rules that rewrite their own output but still allowed the output of a rule to serve as context for a reapplication of that same rule.Johnson realized that this was the key to limiting the power of systems of phonological rules.He also realized that basic rewriting/rules were subject to many alternative modes of application offering different expressive possibilities to the linguist.He showed that phonological grammars under most reasonable modes of application remain within the finite-state paradigm.We observed independently the basic connections between rewriting-rule grammars and finite-state transducers in the late 1970s and reported them at the 1981 meeting of the Linguistic Society of America (Kaplan and Kay 1981).The mathematical analysis in terms of regular relations emerged somewhat later.Aspects of that analysis and its extension to two-level systems were presented at conferences by Kaplan (1984, 1985, 1988), in courses at the 1987 and 1991 Linguistics Institutes, and at colloquia at Stanford University, Brown University, the University of Rochester, and the University of Helsinki.Our approach differs from Johnson's in two important ways.First, we abstract away from the many details of both notation and machine description that are crucial to Johnson's method of argumentation.Instead, we rely strongly on closure properties in the underlying algebra of regular relations to establish the major result that phonological rewriting systems denote such sets of string-pairs.We then use the correspondence between regular relations and finite-state transducers to develop a constructive relationship between rewriting rules and transducers.This is accomplished by means of a small set of simple operations, each of which implements a simple mathematical fact about regular languages, regular relations, or both.Second, our more abstract perspective provides a general framework within which to treat other phonological formalisms, existing or yet to be devised.For example, two-level morphology (Koskenniemi 1983), which evolved from our early considerations of rewriting rules, relies for its analysis and implementation on the same algebraic techniques.We are also encouraged by initial successes in adapting these techniques to the autosegmental formalism described by Kay (1987).Supposing for the moment that Rule 2 (N n) is optional, Figure 1 shows the transition diagram of a finite-state transducer that models it.A finite-state transducer has two tapes.A transition can be taken if the two symbols separated by the colon in its label are found at the current position on the corresponding tapes, and the current position advances across those tape symbols.A pair of tapes is accepted if a sequence of transitions can be taken starting at the start-state (conventionally labeled 0) and at the beginning of the tapes and leading to a final state (indicated by double circles) at the end of both tapes.In the machine in Figure 1, there is a transition from state 0 to state 0 that translates every phoneme into itself, reflecting the fact that any phoneme can remain unchanged by the optional rule.These are shown schematically in the diagram.This machine will accept a pair of tapes just in case they stand in a certain relation: they must be identical except for possible replacements of N on the first tape with n on the second.In other words, the second tape must be one that could have resulted from applying the optional rule to the string on the first tape.But the rule is in fact obligatory, and this means that there must be no occurrences of N on the second tape.This condition is imposed by the transducer in Figure 2.In this diagram, the transition label &quot;other&quot; abbreviates the set of labels a:a,b:b, ...z:z, the identity pairs formed from all symbols that belong to the alphabet but are not mentioned explicitly in this particular rule.This diagram shows no transition over the pair N:N and the transducer therefore blocks if it sees N on both tapes.This is another abbreviatory convention that is typically used in implementations to reduce transducer storage requirements, and we use it here to simplify the state diagrams we draw.In formal treatments such as the one we present below, the transition function is total and provides for transitions from every state over every pair of symbols.Any transition we do not show in these diagrams in fact terminates at a single nonfinal state, the &quot;failure&quot; state, which we also do not show.Figure 3 is the more complicated transducer that models the obligatory behavior of Rule 1 (N —> m/ +[labial]).This machine blocks in state 1 if it sees the pair N:m not followed by one of the labials p, b, m. It blocks in state 2 if it encounters the pair N:N followed by a labial on both tapes, thus providing for the situation in which the rule is not applied even though its conditions are satisfied.If it does not block and both tapes are eventually exhausted, it accepts them just in case it is then in one of the final states, 0 or 2, shown as double circles.It rejects the tapes if it ends up in the nonfinal state 1, indicating that the second tape is not a valid translation of the first one.We have described transducers as acceptors of pairs of tapes that stand in a certain relation.But they can also be interpreted asymmetrically, as functions either from more abstract to less abstract strings or the other way around.Either of the tapes can contain an input string, in which case the output will be written on the other.In each transition the machine matches the symbol specified for the input tape and writes the one for the output.When the first tape contains the input, the machine models the generative application of the rule; when the second tape contains the input, it models the inversion of the rule.Thus, compared with the rewriting rules from which they are derived, finite-state transducers have the obvious advantage of formal and computational simplicity.Whereas the exact procedure for inverting rules themselves is not obvious, it is clearly different from the procedure required for generating.The corresponding transducers, on the other hand, have the same straightforward interpretation in both directions.While finite-state transducers are attractive for their formal simplicity, they have a much more important advantage for our purposes.A pair of transducers connected through a common tape models the composition of the relations that those transducers represent.The pair can be regarded as performing a transduction between the outer tapes, and it turns out that a single finite-state transducer can be constructed that performs exactly this transduction without incorporating any analog of the intermediate tape.In short, the relations accepted by finite-state transducers are closed under serial composition.Figure 4 shows the composition of the m-machine in Figure 3 and the n-machine in Figure 2.This transducer models the cascade in which the output of Rule 1 is the input to Rule 2.This machine is constructed so that it encodes all the possible ways in which the m-machine and n-machine could interact through a common tape.The only interesting interactions involve N, and these are summarized in the following table: input m-machine output input n-machine output N labial follows m Tri N nonlabial follows N n An N in the input to the m-machine is converted to m before a labial and this m remains unchanged by the n-machine.The only instances of N that reach the n-machine must therefore be followed by nonlabials and these must be converted to n. Accordingly, after converting N to m, the composed machine is in state 1, which it can leave only by a transition over labials.After converting N to n, it enters state 2, from which there is no labial transition.Otherwise, state 2 is equivalent to the initial state.Figure 5 illustrates the behavior of this machine as a generator applied to the abstract string iNtractable.Starting in state 0, the first transition over the &quot;other&quot; arc produces i on the output tape and returns to state 0.Two different transitions are then possible for the N on the input tape.These carry the machine into states 1 and 2 and output the symbols m and n respectively.The next symbol on the input tape is t. Since this is not a labial, no transition is possible from state 1, and that branch of the process therefore blocks.On the other branch, the t matches the &quot;other&quot; transition back to state 0 and the machine stays in state 0 for the remainder of the string.Since state 0 is a final state, this is a valid derivation of the string intractable.Figure 6 is a similar representation for the generation of impractical.Figures 7 and 8 illustrate this machine operating as a recognizer.As we pointed out earlier, there are two results when the cascade of rules that this machine represents is inverted on the string intractable.As Figure 7 shows, the n can be mapped into n by the n:n transition at state 0 or into N by the transition to state 2.The latter transition is acceptable because the following t is not a labial and thus matches against the &quot;other&quot; transition to state 0.When the following symbol is a labial, as in Figure 8, the process blocks.Notice that the string iNput that would have been written on the intermediate tape before the machines were composed is blocked after the second symbol by constraints coming from the m-machine.Repeated composition reduces the machines corresponding to the rules of a complete phonological grammar to a single transducer that works with only two tapes, one containing the abstract phonological string and the other containing its phonetic realization.General methods for constructing transducers such as these rely on fundamental mathematical notions that we develop in the next section.Formal languages are sets of strings, mathematical objects constructed from a finite alphabet E by the associative operation of concatenation.Formal language theory has classified string sets, the subsets of E*, in various ways and has developed correspondences between languages, grammatical notations for describing their member strings, and automata for recognizing them.A similar conceptual framework can be established for string relations.These are the collections of ordered tuples of strings, the subsets of E* x x E*.We begin by defining an n-way concatenation operation in terms of the familiar concatenation of simple strings.If X = (x1, x2,... xn) and Y = • • • yn) are n-tuples of strings, then the concatenation of X and Y, written X • Y or simply XY, is defined by That is, the n-way concatenation of two string-tuples is the tuple of strings formed by string concatenation of corresponding elements.The length of a string-tuple I XI can be defined in terms of the lengths of its component strings: This has the expected property that IX • Yl = I + I Yl, even if the elements of X or of Y are of different lengths.Just as the empty string c is the identity for simple string concatenation, the n-tuple all of whose elements are € is the identity for n-way concatenation, and the length of such a tuple is zero.With these definitions in hand, it is immediately possible to construct families of string relations that parallel the usual classes of formal languages.Recall, for example, the usual recursive definition of a regular language over an alphabet E (superscript i denotes concatenation repeated i times, according to the usual convention, and we let EE denote E U {€}): Other families of relations can also be defined by analogy to the formal language case.For example, a system of context-free rewriting rules can be used to define a context-free n-relation simply by introducing n-tuples as the terminal symbols of the grammar.The standard context-free derivation procedure will produce tree structures with n-tuple leaves, and the relational yield of such a grammar is taken to be the set of n-way concatenations of these leaves.Our analysis of phonological rule systems does not depend on expressive power beyond the capacity of the regular relations, however, and we therefore confine our attention to the mathematical and computational properties of these more limited systems.The relations we refer to as &quot;regular,&quot; to emphasize the connection to formal language theory, are often known as &quot;rational relations&quot; in the algebraic literature, where they have been extensively studied (e.g.Eilenberg 1974).The descriptive notations and accepting automata for regular languages can also be generalized to the n-dimensional case.An n-way regular expression is simply a regular expression whose terms are n-tuples of alphabetic symbols or €.For ease of writing we separate the elements of an n-tuple by colons.Thus the expression a:b E:C describes the two-relation containing the single pair (a,bc), and a:b:c* q:r:s describes the three-relation {(anq,bnr,cns) I n > 0}.The regular-expression notation provides for concatenation, union, and Kleene-closure of these terms.The accepting automata for regular n-relations are the n-way finite-state transducers.As illustrated by the two-dimensional examples given in Section 2, these are an obvious extension of the standard one-tape finite-state machines.The defining properties of the regular languages, regular expressions, and finitestate machines are the basis for proving the well-known Kleene correspondence theorems showing the equivalence of these three string-set characterizations.These essential properties carry over in the n-way generalizations, and therefore the correspondence theorems also generalize.In particular, simple analogs of the standard inductive proofs show that Every n-way regular expression describes a regular n-relation; Every regular n-relation is described by an n-way regular expression; Every n-tape finite-state transducer accepts a regular n-relation; and Every regular n-relation is accepted by an n-tape finite-state transducer.The strength of our analysis method comes from the equivalence of these different characterizations.While we reason about the regular relations in algebraic and settheoretic terms, we conveniently describe the sets under discussion by means of regular expressions, and we prove essential properties by constructive operations on the corresponding finite-state transducers.In the end, of course, it is the transducers that satisfy our practical, computational goals.A nondeterministic (one-tape) finite-state machine is a quintuple (E,Q,q,F,6), where E is a finite alphabet, Q is a finite set of states, q e Q is the initial state, and F c Q is the set of final states.The transition function 6 is a total function that maps Q x E' to 2, the set of all subsets of Q, and every state s in Q is vacuously a member of 6(s, €).We extend the function 6 to sets of states, so that for any P C Q and a E E', 6(P, a) = Up 6(p, a).We also define the usual extension of 6 to a transition function 6* on E* as follows: for all r in Q, 6*(r,E) = 6(r, e) and for all u c E* and a E E6, 6*(r , ua) = 6(6* (r , u), a).Thus, the machine accepts a string x just in case 6*(q , x) n F is nonempty; that is, if there is a sequence of transitions over x beginning at the initial state and ending at a set of states at least one of which is final.We know, of course, that every regular language is also accepted by a deterministic, &free finite-state machine, but assuming vacuous € transitions at every state reduces the number of special cases that have to be considered in some of the arguments below.A nondeterministic n-way finite-state transducer (fst) is defined by a quintuple similar to that of an fsm except for the transition function 6, a total function that maps Q x E' x x E to 2.Partly to simplify the mathematical presentation and partly because only the binary relations are needed in the analysis of rewriting rules and Koskenniemi's two-level systems, from here on we frame the discussion in terms of binary relations and two-tape transducers.However, the obvious extensions of these properties do hold for the general case, and they may be useful in developing a formal understanding of autosegmental phonological and morphological theories (for an illustration, see Kay 1987).The transition function 6 of a transducer also extends to a function 6* that carries a state and a pair of strings onto a set of states.Transitions in fsts are labeled with pairs of symbols and we continue to write them with a colon separator.Thus, u:v labels a transition over a u on the first tape and a v on the second.A finite-state transducer T defines the regular relation R(T), the set of pairs (x, y) such that 6* (q , x, y) contains a final state.The pair €: e plays the same role as a label of transducer transitions that the singleton c plays in one-tape machines, and the e-removal algorithm for one-tape machines can be generalized to show that every regular relation is accepted by an E: &free transducer.However, it will also be convenient for some arguments below to assume the existence of vacuous E:E transitions.We write xRy if the pair (x, y) belongs to the relation R. The image of a string x under a relation R, which we write x/R, is the set of strings y such that (x, y) is in R. Similarly, R/y is the set of strings that R carries onto y.We extend this notation to sets of strings in the obvious way: X/R = uxex x/R.This relational notation gives us a succinct way of describing the use of a corresponding transducer as either a generator or a recognizer.For example, if R is the regular relation recognized by the transducer in Figure 4, then R/intractable is the set of strings that R maps to intractable, namely {intractable, iNtractable}, as illustrated in Figure 7.Similarly, iNtractableIR is the set of strings {intractable} that R maps from iNtractable (Figure 5).We rely on the equivalence between regular languages and relations and their corresponding finite-state automata, and we frequently do not distinguish between them.When the correspondence between a language L and its equivalent machine must be made explicit, we let M(L) denote a finite-state machine that accepts L. Similarly, we let T(R) denote a transducer that accepts the relation R, as provided by the correspondence theorem.We also rely on several of the closure properties of regular languages (Hoperoft and Ullman 1979): for regular languages Li and L2, L1L2 is the regular language containing all strings xi x2 such that xi E Li and x2 E L2.We use superscripts for repeated concatenation: Ln contains the concatenation of n members of L, and L* contains strings with arbitrary repetitions of strings in L, including zero.The operator Opt is used for optionality, so that Opt(L) is L U {€}.We write L for the complement of L, the regular language containing all strings not in L, namely, E* — L. Finally, Rev(L) denotes the regular language consisting of the reversal of all the strings in L. There are a number of basic connections between regular relations and regular languages.The strings that can occur in the domain and range of a regular relation R (Dom(R) = R/E* and Range(R) = E*/R) are the regular languages accepted by the finite-state machines derived from T(R) by changing all transition labels a:b to a and b respectively, for all a and b in E. Given a regular language L, the identity relation /d(L) that carries every member of L into itself is regular; it is characterized by the fst obtained from an fsm M(L) by changing all transition labels a to a:a.Clearly, for all languages L, L = Dom (Id(L)) = Range(Id(L)).The inverse R-1 of a regular relation R is regular, since it is accepted by a transducer formed from T(R) by changing all labels a:b to b:a.The reversal Rev(R), consisting of pairs containing the reversal of strings in R's pairs, is also regular; its accepting transducer is derived from T(R) by generalizing the standard one-tape fsm construction for regular language reversal.Given a pair of regular languages L1 and L2 whose alphabets can, without loss of generality, be assumed equal, the relation L1 x L2 containing their Cartesian product is regular.To prove this proposition, we let M1 =- (E, Qi, qi , F1, Si) and M2 = (E, Q2, q2, F2, 62) be fsms accepting L1 and L2 respectively and define the fst where for any s1 EQi, S2 E Q2 and a,b E E€ This result holds trivially when x and y are both c by the general definition of 6*.If a and b are in E and u and v are in E*, then, using the definition of 6* and the definition just given for 6 of the Cartesian product machine, we have Thus, 6* ((q , q2), x,y) contains a final state if and only if both 51 (ch, x) and 6(q2,Y) contain final states, so T accepts exactly the strings in L1 x L2.0 Note that L x L is not the same as Id(L), because only the former can map one member of L onto a different one.If L contains the single-character strings a and b, then Id(L) only contains the pairs (a, a) and (b, b) while L x L also contains (a, b) and (b, a).A similar construction is used to prove that regular relations are closed under the composition operator discussed in Section 2.A pair of strings (x, y) belongs to the relation R1 0 R2 if and only if for some intermediate string z, (x, z) E R1 and (z, y) E R2.If T(R1) = (E, Qi, 61) and T(R2) = (E) (227 q2, F2, 82), the composition R1 oR2 is accepted by the composite fst where ((si , s2) , a, b) -=- f(t11t2) for some c E >,t1 E S(Si, a, c) and t2 E 6(S2,c,b)} In essence, the 6 for the composite machine is formed by canceling out the intermediate tape symbols from corresponding transitions in the component machines.By an induction on the number of transitions patterned after the one above, it follows that for any strings x and y, The composite transducer enters a final state just in case both component machines do for some intermediate z.This establishes that the composite transducer does represent the composition of the relations R1 and R2, and that the composition of two regular relations is therefore regular.Composition of regular relations, like composition of relations in general, is associative: (R1 o R2) 0 R3 = R1 0 (R2 0 R3) = R1 0 R2 0 R3.For relations in general we also know that Range(Ri 0R2) = Range(Ri)/ R2.We can use this fact about the range of a composition to prove that the image of a regular language under a regular relation is a regular language.(It is well known that the images under a regular relation of languages in other classes, for example the context-free languages, also remain within those classes (e.g.Harrison 1978), but these other results do not concern us here.)That is, if L is a regular language and R is an arbitrary regular relation, then the languages L/R and R/L are both regular.If L is a regular language, we know there exists a regular relation Id(L) that takes all and only members of L into themselves.Since L = Range(Id(L)) it follows that Id(L)oR is regular and we have already observed that the range of any regular relation is a regular language.By symmetry of argument we know that R/L is also regular.Just like the class of regular languages, the class of regular relations is by definition closed under the operations of union, concatenation, and repeated concatenation.Also, the Pumping Lemma for regular languages immediately generalizes to regular relations, given the definitions of string-tuple length and n-way concatenation and the correspondence to finite-state transducers.The regular relations differ from the regular languages, however, in that they are not closed under intersection and complementation.Suppose that R1 is the relation {(an,bnc*) 1 n > 0} and R2 is the relation { (an ,b*cn) I n > 0}.These relations are regular, since they are defined by the regular expressions a:b* €:c* and e: b* a:c respectively.The intersection R1 n R2 is { (an, bn cn ) I n > 0}.The range of this relation is the context-free language Pic&quot;, which we have seen is not possible if the intersection is regular.The class of regular relations is therefore not closed under intersection, and it immediately follows that it is also not closed under complementation: by De Morgan's law, closure under complementation and union would imply closure under intersection.Nonclosure under complementation further implies that some regular relations are accepted by only nondeterministic transducers.If for every regular relation there is a deterministic acceptor, then the standard technique (Hoperoft and Ullman 1979) of interchanging its final and nonfinal states could be used to produce an fst accepting the complement relation, which would therefore be regular.Closure under intersection and relative difference, however, are crucial for our treatment of two-level rule systems in Section 7.But these properties are required only for the same-length regular relations, and it turns out that this subclass is closed in the necessary ways.The same-length relations contain only string-pairs (x, y) such that the length of x is the same as the length of y.It may seem obvious that the relevant closure properties do hold for this subclass, but for the sake of completeness we sketch the technical details of the constructions by which they can be established.We make use of some auxiliary definitions regarding the path-language of a transducer.A path-string for any finite-state transducer T is a (possibly empty) sequence of symbol-pairs u1: v1 u2: v2 .. un : vn that label the transitions of an accepting path in T. The path-language of T, notated as Paths(T), is simply the set of all path-strings for T. Paths(T) is obviously regular, since it is accepted by the finite-state machine constructed simply by interpreting the transition labels of T as elements of an alphabet of unanalyzable pair-symbols.Also, if P is a finite-state machine that accepts a pair-symbol language, we define the path-relation Rel(P) to be the relation accepted by the fst constructed from P by reinterpreting every one of its pair-symbol labels as the corresponding symbol pair of a transducer label.It is clear for all fsts T that Rel(M(Paths(T))) = R(T), the relation accepted by T. Now suppose that R1 and R2 are regular relations accepted by the transducers Ti and T2, respectively, and note that Paths(TI) n Paths(T2) is in fact a regular language of pair-symbols accepted by some fsm P. Thus Rel(P) exists as a regular relation.Moreover, it is easy to see that Rel(P) C Ri n R2.This is because every string-pair belonging to the path-relation is accepted by a transducer with a path-string that belongs to the path-languages of both T1 and T2.Thus that pair also belongs to both R1 and R2.The opposite containment does not hold of arbitrary regular relations.Suppose a pair (x, y) belongs to both R1 and R2 but that none of its accepting paths in T1 has the same sequence of transition labels as an accepting path in T2.Then there is no path in Paths(TI) n Paths(T2) corresponding to this pair and it is therefore not contained in Rel(P).This situation can arise when the individual transducers have transitions with &containing labels.One transducer may then accept a particular string pair through a sequence of transitions that does not literally match the transition sequence taken by the other on that same pair of strings.For example, the first fst might accept the pair (ab, c) by the transition sequence a: c b:c, while the other accepts that same pair with the sequence a: c b: c. This string-pair belongs to the intersection of the relations, but unless there is some other accepting path common to both machines, it will not belong to Rel(P).Indeed, when we apply this construction to fsts accepting the relations we used to derive the context-free language above, we find that Rel(P) is the empty relation (with no string-pairs at all) instead of the set-theoretic intersection R1 n R2However, if R1 and R2 are accepted by transducers none of whose accepting paths have &containing labels, then a string-pair belonging to both relations will be accepted by identically labeled paths in both transducers.The language Paths(Ti) n Paths(T2) will contain a path-string corresponding to that pair, that pair will belong to Rel(P), and Rel(P) will be exactly R1 n R2.Thus, we complete the proof that the same-length relations are closed under intersection by establishing the following proposition: R is a same-length regular relation if and only if it is accepted by an &free finite-state transducer.The transitions of an &free transducer T set the symbols of the string-pairs it accepts in one-to-one correspondence, so trivially, R(T) is same-length.The proof in the other direction is more tedious.Suppose R is a same-length regular relation accepted by some transducer T which has transitions of the form u:c or €: v (with u and v not c; we know all e: E transitions can be eliminated by the obvious generalization of the one-tape E-removal algorithm).We systematically remove all E-containing transitions in a finite sequence of steps each of which preserves the accepted relation.A path from the start-state to a given nonfinal state will contain some number of u : E transitions and some number of E: v transitions, and those two numbers will not necessarily be identical.However, for all paths to that state the difference between those numbers will be the same, since the discrepancy must be reversed by each path that leads from that state to a final state.Let us define the imbalance characterizing a state to be the difference in the number of u: E and E :v transitions on paths leading to that state.Since an acyclic path cannot produce an imbalance that differs from zero by more than the number of states in the machine, the absolute value of the imbalance is bounded by the machine size.On each iteration our procedure has the effect of removing all states with the maximum imbalance.First, we note that transitions of the form u:v always connect a pair of states with the same imbalance.Such transitions can be eliminated in favor of an equivalent sequence of transitions E: v and U : E through a new state whose imbalance is one less than the imbalance of the original two states.Now suppose that k > 0 is the maximum imbalance for the machine and that all u : v transitions between states of imbalance k have been eliminated.If q is a k-imbalance state, it will be entered only by U: E transitions from k- 1 states and left only by E:v transitions also to k - 1 states.For all transitions u : E from a state p to q and all transitions €: v from q to r, we construct a new transition u:v from p to r. Then we remove state q from the machine along with all transitions entering or leaving it.These manipulations do not change the accepted relation but do reduce by one the number of k-imbalance states.We repeat this procedure for all k states and then move on to the k - 1 states, continuing until no states remain with a positive imbalance.A symmetric procedure is then used to eliminate all the states whose imbalance is negative.In the end, T will have been transformed to an &free transducer that still accepts R. 0 The same-length regular relations are obviously closed under union, concatenation, composition, inverse, and reverse, in addition to intersection, since all of these operations preserve both regularity and string length.An additional path-language argument shows that they are also closed under relative difference.Let T1 and T2 be E-free acceptors for R1 and R2 and construct an fsm P that accepts the regular pairsymbol language Paths(Ti) - Paths(T2).A string-pair belongs to the regular relation Rel(P) if and only if it has an accepting path in Ti but not in T2.Thus Rel(P) is R1 - R2.Being a subset of R1, it is also same-length.Let us summarize the results to this point.If L1, L2, and L are regular languages and R1, R2, and R are regular relations, then we know that the following relations are regular: R1 U R2 R1 • R2 R* R-1 R1 0 R2 Id(L) L1 X L2 Rev(R) We know also that the following languages are regular (x is a string): Furthermore, if R1, R2, and R are in the same-length subclass, then the following also belong to that restricted subclass: Ri U R2 Ri • R2 R* R-1 RI 0 R2 Rev(R) RI n R2 R1 - R2 Id(L) is also same-length for all L. Intersections and relative differences of arbitrary regular relations are not necessarily regular, however.We emphasize that all these set-theoretic, algebraic operations are also constructive and computational in nature: fsms or fsts that accept the languages and relations that these operations specify can be constructed directly from machines that accept their operands.Our rule translation procedures makes use of regular relations and languages created with five special operators.The first operator produces a relation that freely introduces symbols from a designated set S. This relation, Intro(S), is defined by the expression [Id(E) U Re} x ST.If the characters a and b are in E and S is {$}, for example, then Intro(S) contains an infinite set of string pairs including (a, a), (a, $a), (a, a$$$), (ab,$$a$b$$), and so on.Note that Intro(S)-1 removes all elements of S from a string if S is disjoint from E. The second is the Ignore operator.Given a regular language L and a set of symbols S. it produces a regular language notated as Ls and read as &quot;L ignoring S.&quot; The strings of Ls differ from those of L in that occurrences of symbols in S may be freely interspersed.This language is defined by the expression Ls = Range(Id(L) o Intro(S)).It includes only strings that would be in L if some occurrences of symbols in S were ignored.The third and fourth operators enable us to express if-then and if-and-only-if conditions on regular languages.These are the operators If-P-then-S (&quot;if prefix then suffix&quot;) and If-S-then-P (&quot;if suffix then prefix&quot;).Suppose Li and L2 are regular languages and consider the set of strings If-P-then-S(Li, L2) =- {x I for every partition xi x2 of x, if xi E L1, then x2 E 1,2} A string is in this set if each of its prefixes in Li is followed by a suffix in L2.This set is also a regular language: it excludes exactly those strings that have a prefix in Li followed by a suffix not in L2 and can therefore be defined by This operator, the regular-language analog of the logical equivalence between P -- Q and --,(P A --,(2), involves only concatenation and complementation, operations under which regular languages (though not relations) are closed.We can also express the symmetric requirement that a prefix be in Li if its suffix is in L2 by the expression Finally, we can combine these two expressions to impose the requirement that a prefix be in Li if and only if its suffix is in L2: These five special operators, being constructive combinations of more primitive ones, can also serve as components of practical computation.The double complementation in the definitions of these conditional operators, and also in several other expressions to be introduced later, constitutes an idiom for expressing universal quantification.While a regular expression a/37 expresses the proposition that an instance of 0 occurs between some instance of a and some instance of -y, the expression cE/3-y claims that an instance of /3 intervenes between every instance of a and a following instance of -y.Phonological rewriting rules have four parts.Their general form is This says that the string 0 is to be replaced by (rewritten as) the string ,tp whenever it is preceded by A and followed by p. If either A or p is empty, it is omitted and, if both are empty, the rule is reduced to The contexts, or environments, A and p are usually allowed to be regular expressions over a basic alphabet of segments.This makes it easy to write, say, a vowel-harmony rule that replaces a vowel that is not specified for backness as a back or front vowel according as the vowel in the immediately preceding syllable is back or front.This is because the Kleene closure operator can be used to state that any number of consonants can separate the two vowels.The rule might be formulated as follows: where B, is the back counterpart of the vowel V,, and B./ is another (possibly different) back vowel.There is less agreement on the restrictions that should apply to and 0, the portions that we refer to as the center of the rule.They are usually simple strings and some theorists would restrict them to single segments.However, these restrictions are without interesting mathematical consequences and we shall be open to all versions of the theory if we continue to take it that these can also denote arbitrary regular languages.It will be important to provide for multiple applications of a given rule, and indeed, this will turn out to be the major source of difficulty in reexpressing rewriting rules in terms of regular relations and finite-state transducers.We have already remarked that our methods work only if the part of the string that is actually rewritten by a rule is excluded from further rewriting by that same rule.The following optional rule shows that this restriction is necessary to guarantee regularity: ab/a b If this rule is allowed to rewrite material that it introduced on a previous application, it would map the regular language {ab} into the context-free language {atb I I <n}, which we have already seen is beyond the power of regular relations.However, we do not forbid material produced in one application of a rule from serving as context for a subsequent application of that rule, as would routinely be the case for a vowel-harmony rule, for example.It is this restriction on interactions between different applications of a given rule that motivates the notation rather than A0p p The context refers to a part of the string that the current application of the rule does not change but which, since it may have been changed in a previous application, allows for an interaction between successive applications.The important outstanding question concerning interactions between applications of one and the same rule at different positions in a string has to do with the relative order in which they take place.Consider the obligatory rule a b/ab ba as applied to the string abababababa At least three different outcomes are possible, namely: Result (1) is obtained if the first application is at the leftmost eligible position in the string; each successive application applies to the output of any preceding one, and further to the right in the string.We call this the left-to-right strategy.The corresponding right-to-left strategy gives rise to (2).Result (3) comes from identifying all possible rule applications in the original string and carrying them out simultaneously.All three strategies have been advocated by phonologists.We shall assume that each rule is marked individually to show which strategy is to be employed for it.We shall concentrate on these three strategies, but other less obvious ones can also be treated by simple rearrangements of our techniques.Optional rules and most obligatory rules will produce at least one output string, perhaps just a copy of the input if the conditions for application are nowhere satisfied.But certain obligatory rules are anomalous in that they may produce no output at all.The following left-to-right rule is a case in point: € b/b If a string containing the symbol b is input to this rule, another b will be inserted immediately after it, and that one will serve to trigger the rule again.This process will never terminate, and no finite-length output is ever produced.Strange as they may seem, rules like this are useful as filters to eliminate undesired paths of derivation.In contrast to obligatory rules, optional rules typically produce many outputs.For example, if the rule above (a -4 b /ab ba) is marked as optional and left-to-right and is also applied to the string abababababa, the following, in addition to (1), would be among its outputs: The string (4) is similar to (1) except that only the leftmost application of the rule has been carried out.For (5) the application in the middle would not have been possible for the obligatory rule and is possible here only because the necessary context was not destroyed by an application further to the left.Kenstowicz and Kisseberth (1979), who discuss a number of rule application strategies in great detail, cite a case in which one rule seems to be required in the grammars of two languages.However, it must be applied left to right in one, but right to left in the other.In the Australian language Gidabal, the long vowel of certain suffixes becomes short if the vowel of the preceding syllable is long.We find, for example, yaga+ya 'should fix' where we would otherwise expect yaga+ya.(We use + to mark the point at which the suffix begins and a bar over a vowel to show that it is long.)The interesting question concerns what happens when several of these suffixes are added to the same stem.Some examples are: 'is certainly right on the fish' gunam+ba-Fdeiang+be gunam+ba+dang+be 'is certainly right on the stump' The rule that Kenstowicz and Kisseberth propose is essentially the following: This produces the desired result only if applied left to right and only if obligatory.The alternation of long and short vowels results from the fact that each application shortens a long vowel that would otherwise serve as part of the context for a subsequent application.The same rule appears as the rhythmic law in Slovak—all suffix vowels are shortened following a long vowel, as in the following examples: vol+a+me 'we call' chit+a+me 'we read' vol+av+a+me 'we call often' chit+av+a+me 'we read often' This time the rule must be applied either simultaneously or from right to left.It might seem that a transducer mimicking the operation of a right-to-left rule would have to examine its tapes in the opposite order from one that implemented a left-to-right rule, and it is difficult to see how two transducers operating in different directions could then be composed.However, we shall see that directionality in rewriting rules is not mirrored by directionality in the transducers.Instead, directionality determines which of the two tapes the left and right contexts must appear on.In a left-to-right rule, the left context of the rule is to be verified against the portion of the string that results from previous applications of that rule, whereas the right context is to be verified against the portion of the string that has not yet been changed but may eventually be modified by applications further to the right.In a right-to-left rule, the situation is reversed.Consider again the left-to-right rule schema which applies to the string abecido to give (Thatch).The portions of the tapes that support the two applications of the rule are boxed in the diagram on the left below.The diagram on the right shows how it comes about that there are three applications when the rule is taken as moving from right to left.It is often convenient in phonological rules to introduce a special symbol to mark the beginning and end of the string.This allows edge-conditioned string transformations to be encoded in rewriting rules.For example, Kenstowicz and Kisseberth give the following rule to describe the devoicing of final obstruents in German and Russian: [+obstruent] -4 [-voiced] / # We will consider the feature notation exemplified here shortly.For the moment, it can be taken as equivalent to a set of rules whose effect is to replace any segment that is classified as an obstruent by its unvoiced equivalent before the boundary symbol that marks the end of a word.It accounts for the phonological realization of the Russian form xleb 'bread' as xlep.The boundary symbol # is special in the rule formalism in that it can only appear in the context parts of a rule, never in the input or output patterns, and it never matches an element that appears explicitly in the string.Although boundary-context rules require distinctive mathematical treatment, we show below that they also denote only regular string relations.As we have said, we take it that each rule in a grammar will be annotated to show which strategy is to be used in applying it.We also assume that rules are annotated to show whether they are to be taken as obligatory or optional.We have considered only obligatory rules up to now, but optional rules are also commonly used to account for cases of free variation.The mathematical treatment of optional rules will turn out to be a simpler case of what must be done for obligatory rules and, therefore, a natural step in the general development.As well as providing for various strategies for reapplying a single rule, we also consider the possibility of what we call a batch rule.This is a set of rules that the application strategies treat as one entity, the individual rules being otherwise unordered relative to one another.This mode of rule application will turn out to be interesting even if it is not an explicit part of any particular phonological formalism because, as we shall see, it constitutes an essential step in the interpretation of rules that use features to refer to underspecified segments.A good example of this is the vowel-harmony rule referred to earlier, namely Vi -4 B, / BjC* In feature notation, this could be written -consonantal meaning that a segment that is specified as a vowel comes also to be specified as back when the most recent preceding vowel is back; all other features remain unchanged.The grammar will presumably contain another rule that will apply in circumstances when this one does not, namely -consonantal except, of course, that the context can be omitted from whichever of the two is placed second in an ordered list of rules.But this is precisely the question: What is the proper order of this pair of rules?Consider an actual case, namely, vowel harmony in Turkish.Let A represent an abstract vowel with e and a as its front and back realizations, and I another abstract vowel with i and dotless z as its front and back counterparts.The first of these occurs, for example, in the abstract plural suffix lAr, and the second occurs in the possessive suffix Im, meaning 'my.'Both suffixes can be used together, and the harmony is illustrated by the different realizations of the abstract vowels in the forms apartmanlArIm and adreslArIm.These appear as apartmanlarim 'my apartments' and adreslerim 'my addresses.'Using only the simple non-feature notation we started out with, we can describe this variation with the following four rules: The proper surface forms for these words are produced if these rules are ordered as we have given them—inserting front vowels first—and if each of them is applied from left to right.However, applying the rules in this way gives the wrong result when we create the dative possessive form of adres instead of the possessive plural.The dative suffix is spelled simply as the abstract vowel A, and the abstract adresImA should be realized as adresime if harmony is respected.But the rules as given will map adresImA to adresima instead.This is because the earlier rules apply to the final A at a time before the context required for that vowel has been established.Reordering the rules to fix this problem will cause the previous correct analyses to fail.The proper results in all cases come only if we describe Turkish vowel harmony with rules that proceed left to right through the string as a group, applying at each position whichever one matches.This is the mode of application for a set of rules collected together as a batch.The notion of a batch rule apparently has not arisen as a distinctive formal concept in phonological theories.The reason is doubtless that batch rules are unnecessarily prolix and, in particular, they fail to capture generalizations that can almost always be made about the individual rules that make up a batch.Phonologists prefer rules that are based on feature matrices.These rules allow segments to be referred to by specifying which members of a finite set of properties they do or do not have.A feature matrix can specify a segment completely, in which case it is equivalent to the unanalyzable segment names we have been using, or it can leave it underspecified.Feature matrices therefore constitute an abbreviatory convention with the advantage that what is easy to abbreviate will be motivated to just the extent that the features themselves are motivated.An underspecified segment corresponds to a set of fully specified segments, and a rule that contains underspecified segments corresponds to a set of rules that are to be applied in batch mode.Feature matrices based on a well-motivated set of features allow the phonologist to capture significant generalizations and thus effectively to reduce the components of our batch rules to a single rule in most cases.A significant addition that has been made to the basic machinery of feature-based rules consists of variables written with lowercase Greek letters a, 0, 7, etc. and ranging over the values + and -.We can use them, for example, to collapse our vowel-harmony rules into a single one as follows: —consonantal Both occurrences of the variable a must be instantiated to the same value, either + or —, at each application of the rule.What the rule now says is that a vowel takes its backness from the vowel in the preceding syllable; that is, the most recent preceding vowel that is separated from it by zero or more consonants.While the explicit use of variables is an important addition to the notation, it was in fact foreshadowed by a property of the initial feature system, namely that features not explicitly mentioned in the center of a rule were assumed to be carried over from the input to the output.Without this convention, an explicit variable would have been required for each of these.Explicit feature variables do indeed increase the abbreviatory power of the notation, but, as we show below, they can be translated systematically into batch rules over unanalyzable segments.We pay special attention to batch rules, feature matrices, and feature variables because they require some nonobvious extensions to the treatment we provide for ordinary rules with unanalyzable symbols.On the other hand, we have nothing to say about the many other notational devices that phonologists have proposed for collapsing rules.These abbreviatory conventions are either already subsumed by the general regular languages we allow as rule components or can be translated in obvious ways to simply ordered rules or batch rules.We now come to the central problem of proving that an arbitrary rule in our formalism denotes a regular string relation and is thus accepted by an equivalent finite-state transducer.A rule has the general form where 0, '0, A, and p are arbitrary regular expressions.The mode of application of the rule is governed by additional parametric specifications, including for example whether the rule applies from left to right or right to left, and whether it is obligatory or optional.The replacement that such a rule performs is modeled by a relation Replace initially defined as follows: The final asterisk allows for repetitions of the basic 0 x 0 mapping, and Id(E*) allows identical corresponding substrings to come between successive applications of the rule.The 0 x '0 replacement is optional to allow for the possibilities that the rule itself may be optional or that there may be no eligible instances of 0 in the input string.Replace is the set of pairs of strings that are identical except for possible replacements of substrings belonging to 0 by substrings belonging to 0.This set clearly contains all the pairs that satisfy the rule, though perhaps other pairs as well.The problem now is to impose restrictions on this mapping so that it occurs in the proper contexts and in accordance with the parameters specified for the rule.We do this in a series of approximations.As a first step, we might be tempted simply to add the context restrictions as necessary conditions of the 0 x 0 replacement: This relation includes strings where the 0 x 0 replacement occurs only when immediately preceded and followed by identical substrings satisfying A and p, respectively.But this formulation does not allow for the fact, noted above, that the context strings of one application may overlap either the contexts or the center strings of another.For example, consider the following optional rule, which allows an abstract B to be rewritten as b intervocalically: With the definition of Replace just given, the string pair on the left below would be accepted but the pair on the right would not: But the second pair also represents a valid application of the rule, one in which the center vowel is serving as the right context of one application and the left context of the other.The problem is that a given string symbol can simultaneously serve several different roles in the application of a rule, and all possible interactions must be accounted for.As a next approximation, we avoid this confusion by carefully distinguishing and keeping track of these various roles.We first consider how to apply a rule to strings that have been preprocessed so that every instance of the left context A is followed by the auxiliary symbol < and every instance of the right context p is preceded by the symbol >, where < and > are not in E. This means that the replacement operator can be defined solely in terms of these distinct context-marking brackets, without regard to what A and p actually specify and what they might have in common with each other or with 0 and 0.In essence, we assume that the replacement relation for the above rule applies to the upper strings shown below, and that all three string pairs are acceptable because each of the corresponding B-b pairs is bracketed by < and >.>V<B>V<B>V< >V<B>V<B>V< >V<B>V<B>V< >V<b>V<b>V< >V<b>V<B>V< >V<B>V<b>V< To take a somewhat more realistic example, when the rule at the beginning of the paper is applied to the string iNprobable, the preprocessed input string would contain the sequence <i<N<>p<r<o<>b<a<>b<1<e< The left context of the rule is empty, so there is a left-context marker < after every character from the original string.Every labial is an instance of the right context, and accordingly there is a > immediately preceding p's and b's.The rule properly applies to rewrite the N because it is bracketed by < and >.On the other hand, the > is missing and the rule does not apply to the N in the preprocessed version of iNtractable, namely <i<N<t<r<a<c<t<a<>b<1<e< The definition of the Replace operator must be modified in two ways in order to operate on such preprocessed strings.First it must allow the 0 x '0 mapping only between the appropriate context markers.Second, some occurrences of the left and right context strings do not result in rule applications, either because the rule is optional or because the other conditions of the rule are not satisfied.Thus, the relation must disregard the markers corresponding to those occurrences inside the identity substrings between rule applications.Relations with this behavior can be obtained through the use of the ignoring operator defined in Section 3, which is notated by subscripting.Let m (for marker) be {<, >}, the set of both markers.Then our next approximation to the replacement relation is defined as follows: This allows arbitrary strings of matching symbols drawn from EU {<, >} between rule applications and requires < : < and > : > to key off a 0-0 replacement.The subscript m's also indicate that < and > can be ignored in the middle of the replacement, since the appearance of left- or right-context strings is irrelevant in the middle of a given rule application.Figure 9 shows the general form of the state-transition diagram for a transducer that accepts a replacement relation.As before, the start-state is labeled 0 and only transitions are shown from which the final-state is reachable.We must now define relations that guarantee that context-markers do in fact appear on the strings that Replace applies to, and only when sanctioned by instances of A and p. We do this in two stages.First, we use simple relations to construct a Prologue operator that freely introduces the context markers in m: An output string of Prologue is just like the corresponding input except that brackets appear in arbitrary positions.The relation Prologue-I removes all brackets that appear on its input.Second, we define more complex identity relations that pair a string with itself if and only if those markers appear in the appropriate contexts.The P-if-S operator is the key component of these context-identifying predicates.The condition we must impose for the left context is that the left-context bracket < appears if and only if it is immediately preceded by an instance of A.This basic requirement is satisfied by strings in the regular language P-iff-S(E*A, <E*).The situation is slightly more complicated, however, because of two special circumstances.An instance of A may have prefixes that are also A instances.If A is the expression ab*, then a<b< is an acceptable marking but ab< and a<b are not because the two A-instances are not both followed by <.The brackets that necessarily follow such prefixes must not prevent the longer instances from also being identified and marked, and right-context brackets also must not interfere with left-context identification.The ignore operators in the expression P-if-S (E* < A<, <E*<)> allow for these possibilities.This disregards slightly too many brackets, however: since an instance of A< followed by an < is also an instance of A<, it must be followed by another bracket, and so on.The only (finite) strings that belong to this language are those that contain no instances of A at all!To correctly identify and mark left-contexts, the bracket following a A< instance must not be ignored.Thus, the requisite set of strings is the regular language Leftcontext(A, <,>), where the Leftcontext operator is defined as follows: We parameterize this operator for the left-context pattern and the actual brackets so that it can be used in other definitions below.The other complication arises in rules intended to insert or delete material in the string, so that either 0 or 1/) includes the empty string E. Consider the left-to-right rule Iterated applications of this rule can delete an arbitrary sequence of a's, converting strings of the form baaaa... a into simply b.The single b at the beginning serves as left-context for applications of the rule to each of the subsequent a's.This presents a problem for the constructions we have developed so far: The Replace relation requires a distinct < marker for each application of the rule.The < that sanctions the deletion of the leftmost a in the string is therefore not available to delete the next one.However, the Leftcon text operator as defined disallows two left-context brackets in a row.Our solution is to insert an explicit character 0 to represent the deleted material.If Leftcon text ignores this character in A, 0 will always be followed by another left bracket and thus another rule application is possible.The auxiliary symbol 0 is not in E or in the set of context brackets.It will substitute for the empty strings that might appear in the center of rules (in 0 or zp), but it is a genuine symbol in an expanded alphabet which, unlike the normal 6, actually appears as a distinct element in character strings.The Prologue relation is extended to freely introduce 0 as well as the brackets in m: We then construct alternative versions of 0 and lp in which this special symbol replaces the true empty strings.We define { if€çb - U 0 otherwise which contains exactly the same strings as 0 except that the singleton string 0 is included instead of the empty string that otherwise might be in the language.IP° is defined similarly, and then the replacement operator is expressed in terms of these new regular languages: Now we can complete our definition of the left-context identifier: Left-context identifiers.As desired, the regular language denoted by this operator includes strings if and only if every substring belonging to A (ignoring /, r, and 0) is immediately followed by a bracket /.This effect is illustrated by the state-transition diagrams in Figure 10.The machine on the left is a minimal-state acceptor for the empty-context language Leftcontext(c, <, >).It accepts strings that have at least one <, and every 0 or E symbol must be followed by a <.The >-labeled transitions represent the fact that > is being ignored.The machine on the right accepts the language Leftcontext (a, < , >); it requires < to appear after every a or after any 0 that follows an a.This particular machine is nondeterministic so that its organization is easier to understand.An operator for identifying and marking right-context strings can be defined symmetrically: Thus Rightcontext (p, < , >) includes strings if and only if every substring belonging to p (with appropriate ignoring) is immediately preceded by a right-context bracket >.Alternatively, taking advantage of the fact that the reversal of a regular language is also a regular language, we can define Rightcontext in terms of Leftcontext: These context identifiers denote appropriate string-sets even for rules with unspecified contexts, if the vacuous contexts are interpreted as if the empty string had been specified.The empty string indicates that adjacent symbols have no influence on the rule application.If an omitted A is interpreted as 6, for example, every Leftcontext string will have one and only one left-context bracket at its beginning, its end, and between any two E symbols, thus permitting a rule application at every position.We now have components for freely introducing and removing context brackets, for rejecting strings with mislocated brackets, and for representing the rewrite action of a rule between appropriate context markers.The regular relation that models the optional application of a rule is formed by composition of these pieces.The order of composition depends on whether the rule is specified as applying iteratively from left to right or from right to left.As noted in Section 4, the difference is that for left-to-right rules, the left-context expression A can match against the output of a previous (that is, leftward) application of the same rule, but the right-context expression p must match against the as yet unchanged input string.These observations are directly modeled by the order in which the various rule components are combined.For a left-to-right rule, the right context is checked on the input (0) side of the replacement, while the left context is checked on the output (0) side.The regular relation and corresponding transducer for a leftBoth left- and right-context brackets are freely introduced on input strings, strings in which the right-context bracket is mislocated are rejected, and the replacement takes place only between the now-constrained right-context brackets and the still free leftcontext markers.This imposes the restriction on left-context markers that they at least appear before replacements, although they may or may not freely appear elsewhere.The left-context checker ensures that left-context markers do in fact appear only in the proper locations on the output.Finally, all brackets are eliminated, yielding strings in the output language.The context-checking situation is exactly reversed for right-to-left rules: the leftcontext matches against the unchanged input string while the right-context matches against the output.Right-to-left optional application can therefore be modeled simply by interchanging the context-checking relations in the cascade above, to yield The transducer corresponding to this regular relation, somewhat paradoxically, models a right-to-left rule application while moving from left to right across its tapes.Simultaneous optional rule application, in which the sites of all potential string modifications are located before any rewriting takes place, is modeled by a cascade that identifies both left and right contexts on the input side of the replacement: These compositions model the optional application of a rule.Although all potential application sites are located and marked by the context checkers, these compositions do not force a cb-0 replacement to take place for every instance of cb appearing in the proper contexts.To model obligatory rules, we require an additional constraint that rejects string pairs containing sites where the conditions of application are met but the replacement is not carried out.That is, we must restrict the relation so that, disregarding for the moment the effect of overlapping applications, every substring of the form A0p in the first element of a pair corresponds to a Alpp in the second element of that pair.We can refine this restriction by framing it in terms of our context-marking brackets: the Replace relation must not contain a pair with the substring <0> in one element corresponding to something distinct from <lp> in the other.We might try to formulate this requirement by taking the complement of a relation that includes the undesired correspondences, as suggested by the expression This expression might be taken as the starting point for various augmentations that would correctly account for overlapping applications.However, pursuing this line of attack will not permit us to establish the fact that obligatory rules also define regular mappings.First, it involves the complement of a regular relation, and we observed above that the complement of a regular relation (as opposed to the complement of a regular language) is not necessarily regular.Second, even if the resulting relation itself turned out to be regular, the obvious way of entering it into our rule composition is to intersect it with the replacement relation, and we also know that intersection of relations leads to possibly nonregular results.Proving that obligatory rules do indeed define regular mappings requires an even more careful analysis of the roles that context-brackets can play on the various intermediate strings involved in the rule composition.A given left-context bracket can serve in the Replace relation in one of three ways.First, it can be the start of a rule application, provided it appears in front of an appropriate configuration of 0, and right-context brackets.Second, it can be ignored during the identity portions of the strings, the regions between the changes sanctioned by the replacement relation.Third, it can be ignored because it comes in the middle or center of another rule application that started to the left of the bracket in question and extends further to the right.Suppose we encode these three different roles in three distinct left-bracket symbols <, <, and The crucial difference in the case where an obligatory left-to-right rule incorrectly fails to apply is that the left-context preceding the 0° is marked with < instead of <, since it is part of an identity sequence.This situation is undesirable no matter what types of brackets are ignored in the 0° pattern or mark the right-context of this potential application.Whether those brackets are in the center or at the boundary of replacements that are carried out further to the right of the offending situation, the leftward application marked by the < should have taken precedence.The symbols < and > were previously used as auxiliary characters appearing in intermediate strings.With a slight abuse of notation, we now let them act as cover symbols standing for the sets of left and right brackets {<, <, <} and {>, >, >1 respectively, and we let m be the combined set <U>.A substring on the input side of the replacement is then a missed left-to-right application if it matches the simple pattern <0>.Thus, we can force obligatory application of a left-to-right rule by requiring that the strings on the input side of its replacement contain no such substrings, or, to put it in formal terms, that the input strings belong to the regular language Obligatory(, <,>), where Obligatory is defined by the following operator: By symmetry, a missed application of a right-to-left rule matches the pattern <0°.>, and Obligatory(0, <,>) is the appropriate input filter to disallow all such substrings.Note that the obligatory operator involves only regular languages and not relations so that the result is still regular despite the complementation operation.We must now arrange for the different types of brackets to appear on the input to Replace only in the appropriate circumstances.As before, the context identifiers must ensure that none of the brackets can appear unless preceded (or followed) by the appropriate context, and that every occurrence of a context is marked by a bracket freely chosen from the appropriate set of three.The Leftcontext and Rightcontext operators given above will have exactly this effect when they are applied with the new meanings given to <, >, and m. The Replace operator must again be modified, however, because it alone distinguishes the different roles of the context brackets.The following final definition chooses the correct brackets for all parameters of rule application: The behavior of obligatory rules is modeled by inserting the appropriate filter in the sequence of compositions.Left-to-right obligatory rules are modeled by the cascade We remark that even obligatory rules do not necessarily provide a singleton output string.If the language V) contains more than one string, then outputs will be produced for each of these at each application site.Moreover, if 0 contains strings that are suffixes or prefixes (depending on the direction of application) of other strings in 0, then alternatives will be produced for each length of match.A particular formalism may specify how such ambiguities are to be resolved, and these stipulations would be modeled by additional restrictions in our formulation.For example, the requirement that only shortest 0 matches are rewritten could be imposed by ignoring only one of < or > in the mapping part of Replace, depending on the direction of application.There are different formulations for the obligatory application of simultaneous rules, also depending on how competition between overlapping application sites is to be resolved.Intersecting the two obligatory filters, as in the following cascade, models the case where the longest substring matching 0 is preferred over shorter overlapping matches: The operators can be redefined and combined in different ways to model other regimes for overlap resolution.A rule contains the special boundary marker # when the rewriting it describes is conditioned by the beginning or end of the string.The boundary marker only makes sense when it appears in the context parts of the rule; specifically, when it occurs at the left end of a left-context string or the right end of a right-context string.No special treatment for the boundary marker would be required if # appeared as the first and last character of every input and output string and nowhere else.If this were the case, the compositional cascades above would model exactly the intended interpretation wherein the application of the rule is edge-sensitive.Ordinary input and output strings do not have this characteristic, but a simple modification of the Prologue relation can simulate this situation.We defined Prologue above as Intro(m U {O}).We now augment that definition: We have composed an additional relation that introduces the boundary marker at the beginning and end of the already freely bracketed string, and also rejects strings containing the boundary marker somewhere in the middle.The net effect is that strings in the cascade below the Prologue are boundary-marked; bracketed images of the original input strings and the context identifiers can thus properly detect the edges of those strings.The inverse Prologue at the bottom of the cascade removes the boundary marker along with the other auxiliary symbols.It remains to model the application of a set of rules collected together in a single batch.Recall that for each position in the input string each rule in a batch set is considered for application independently.As we have seen several times before, there is a straightforward approach that approximates this behavior.Let {R1, , Rn} be the set of regular relations for rules that are to be applied as a batch and construct the relation [UkRk]*.Because of closure under union, this relation is regular and includes all pairs of strings that are identical except for substrings that differ according to the rewriting specified by at least one of the rules.But also as we have seen several times before, this relation does not completely simulate the batch application of the rules.In particular, it does not allow for overlap between the material that satisfies the application requirements of one rule in the set with the elements that sanction a previous application of another rule.As usual, we account for this new array of overlapping dependencies by introducing a larger set of special marking symbols and carefully managing their occurrences and interactions.A batch rule is a set of subrules {01 1p1 /Al '/A' pn together with a specification of the standard parameters of application (left-to-right, obligatory, etc.).We use superscripts to distinguish the components of the different subrules to avoid (as much as possible) confusion with our other notational conventions.A crucial part of our treatment of an ordinary rule is to introduce special bracket symbols to mark the appearance of its left and right contexts so that its replacements are carried out only in the proper (possibly overlapping) environments.We do the same thing for each of the subrules of a batch, but we use a different set of brackets for each of them.These brackets permit us to code in a single string the context occurrences for all the different subrules with each subrule's contexts distinctively marked.Let <k be the set{ < k <k <k of left-context brackets for the kth subrule I a c /k /).k pk of the batch, let >k be the corresponding set of right-context brackets, and let mk be the set <k U >k.We also redefine the generic cover symbols <, >, and m to stand for the respective collections of all brackets: < = Uk<k, > = Uk>k, m = <U>.Note that with this redefinition of m, the Prologue relation as defined above will now freely introduce all the brackets for all of the subrules.It will also be helpful to notate the set of brackets not containing those for the kth subrule: M—k = m - mk. o Now consider the regular language Leftcontext(A <k >lk , )m k. This contains strings in which all instances of the kth subrule's left-context expression are followed by one of the kth left-context brackets, and those brackets appear only after instances of Ak.The kth right-context brackets are freely distributed, as are all brackets for all the other subrules.Occurrences of all other left-context brackets are restricted in similarly defined regular languages.Putting all these bracket-restrictions together, the language nLeftcontext(Ak , <k, >k)m—k has each subrule's left-context duly marked by one of that subrule's left-context brackets.This leaves all right-context brackets unconstrained; they are restricted to their proper positions by the corresponding right-context language nRightcontext(pk, <k , >k),--k These intersection languages, which are both regular, will take the place of the simple context identifiers when we form the composition cascades to model batch-rule application.These generalized context identifiers are also appropriate for ordinary rules if we regard each of them as a batch containing only one subrule.A replacement operator for batch rules must also be constructed.This must map between input and output strings with context-brackets properly located, ensuring that any of the subrule rewrites are possible at each properly marked position but that the rewrite of the kth subrule occurs only between <k and >&quot;.The complete set where the generic symbol < now stands for { <1 ... <k}, the set of all left-center brackets, and the generic > is assigned a corresponding meaning.We incorporate this relation as the rewrite part of a new definition of the Replace operator, with the generic < and > now representing the sets of all left and right identity brackets: This relation allows for any of the appropriate replacements separated by identity substrings.It is regular because of the union-closure property; this would not be the case, of course, if intersection or complementation had been required for its construction.A model of the left-to-right application optional application of a batch rule is obtained by substituting the new, more complex definitions in the composition cascade for ordinary rules with these application parameters: Optional right-to-left and simultaneous batch rules are modeled by similar substitutions in the corresponding ordinary-rule cascades.Obligatory applications are handled by combining instances of the Obligatory operator constructed independently for each subrule.Obligatory(/i', <k, >k) excludes all strings in which the kth subrule failed to apply, moving from left to right, when its conditions of application were satisfied.The intersection of the obligatory filters for all subrules in the batch ensures that at least one subrule is applied at each position where application is allowed.Thus the behavior of a left-to-right obligatory batch rule is represented by the composition Again, similar substitutions in the cascades for ordinary obligatory rules will model the behavior of right-to-left and simultaneous application.Using only operations that preserve the regularity of string sets and relations, we have modeled the properties of rewriting rules whose components are regular languages over an alphabet of unanalyzable symbols.We have thus established that every such rule denotes a regular relation.We now extend our analysis to rules involving regular expressions with feature matrices and finite feature variables, as in the Turkish vowel harmony rule discussed in Section 4: aback [ —consonantal +syllabic —› [ aback ] / +syllabic [ +consonantal —consonantal We first translate this compact feature notation, well suited for expressing linguistic generalizations, into an equivalent but verbose notation that is mathematically more tractable.The first step is to represent explicitly the convention that features not mentioned in the input or output matrices are left unchanged in the segment that the rule applies to.We expand the input and output matrices with as many variables and features as necessary so that the value of every output feature is completely specified in the rule.The center-expanded version of this example is The input and output feature matrices are now fully specified, and in the contexts the value of any unmentioned feature can be freely chosen.A feature matrix in a regular expression is quite simple to interpret when it does not contain any feature variables.Such a matrix merely abbreviates the union of all segment symbols that share the specified features, and the matrix can be replaced by that set of unanalyzable symbols without changing the meaning of the rule.Thus, the matrix [+consonantal] can be translated to the regular language {p, t, k, b, d... } and treated with standard techniques.Of course, if the features are incompatible, the feature matrix will be replaced by the empty set of segments.A simple translation is also available for feature variables all of whose occurrences are located in just one part of the rule, as in the following fictitious left context: [ ahigh [ +consonantal ]* [ -around ] If a takes on the value +, then the first matrix is instantiated to [+highl and denotes the set of unanalyzable symbols, say {e, 1, that satisfy that description.The last matrix reduces to [-round] and denotes another set of unanalyzable symbols (e.g.{ a, e, }).The whole expression is then equivalent to { e, } {p, t, k, b, d }* {a, e, } On the other hand, if a takes on the value -, then the first matrix is instantiated to [-high] and denotes a different set of symbols, say {a, o... }, and the last one reduces to [+ round].The whole expression on this instantiation of a is equivalent to { a , o, } fp, t, k, b, d... 1* {o, u, On the conventional interpretation, the original expression matches strings that belong to either of these instantiated regular languages.In effect, the variable is used to encode a correlation between choices from different sets of unanalyzable symbols.We can formalize this interpretation in the following way.Suppose 0 is a regular expression over feature matrices containing a single variable a for a feature whose values are drawn from a finite set V. commonly the set { +, -}.Let 0[a -4 v] be the result of substituting v E V for a wherever it occurs in 0, and then replacing each variable-free feature matrix in that result by the set of unanalyzable symbols that satisfy its feature description.Then the interpretation of 0 is given by the formula This translation produces a regular expression that properly models the choice-correlation defined by a in the original expression.Rule expressions containing several locally occurring variables can be handled by an obvious generalization of this substitution scheme.If al ...an are the local variables in 6 whose values come from the finite sets V, the set of n-tuples represents the collection of all possible value instantiations of those variables.If we let 0[i] be the result of carrying out the substitutions indicated for all variables by some i in I, the interpretation of the entire expression is given by the formula Indeed, the input and output expressions will almost always have variables in common, because of the feature variables introduced in the initial center-expansion step.Variables that appear in more than one rule part clearly cannot be eliminated from each part independently, because the correlation between feature instantiations would be lost.A feature-matrix rule is to be interpreted as scanning in the appropriate direction along the input string until a configuration of symbols is encountered that satisfies the application conditions of the rule instantiated to one selection of values for all of its variables.The segments matching the input are then replaced by the output segments determined by that same selection, and scanning resumes until another configuration is located that matches under possibly a different selection of variables values.This behavior is modeled as the batch-mode application of a set of rules each of which corresponds to one variable instantiation of the original rule.Consider a center-expanded rule of the general form 0 —> '0/A p, and let I be the set of possible value instantiations for the feature-variables it contains.Then the collection of instantiated rules is simply The components of the rules in this set are regular languages over unanalyzable segment symbols, all feature matrices and variables having been resolved.Since each instantiated rule is formed by applying the same substitution to each of the original rule components, the cross-component correlation of symbol choices is properly represented.The behavior of the original rule is thus modeled by the relation that corresponds to the batch application of rules in this set, and we have already shown that such a relation is regular.5.7 Summary This completes our examination of individual context-sensitive rewriting rules.We have modeled the input-output behavior of these rules according to a variety of different application parameters.We have expressed the conditions and actions specified by a rule in terms of carefully constructed formal languages and string relations.Our constructions make judicious use of distinguished auxiliary symbols so that crucial informational dependencies can be string-encoded in unambiguous ways.We have also shown how these languages and relations can be combined by set-theoretic operations to produce a single string relation that simulates the rule's overall effect.Since our constructions and operations are all regularity-preserving, we have established the following theorem: For all the application parameters we have considered, every rewriting rule whose components describe regular languages denotes a regular string relation.This theorem has an immediate corollary: The input-output string pairs of every such rewriting rule are accepted by some finitestate transducer.This theoretical result has important practical consequences.The mathematical analysis that establishes the theorem and its corollary is constructive in nature.Not only do we know that an appropriate relation and its corresponding transducer exist, we also know all the operations to perform to construct such a transducer from a particular rule.Thus, given a careful implementation of the calculus of regular languages and regular relations, our analysis provides a general method for compiling complicated rule conditions and actions into very simple computational devices.The individual rules of a grammar are meant to capture independent phonological generalizations.The grammar formalism also specifies how the effects of the different rules are to be combined together to account for any interactions between the generalizations.The simplest method of combination for rewriting rule grammars is for the rules to be arranged in an ordered sequence with the interpretation that the first rule applies to the input lexical string, the second rule applies to the output of the first rule, and so on.As we observed earlier, the typical practice is to place specialized rules with more elaborate context requirements earlier in the sequence so that they will override more general rules appearing later.The combined effect of having one rule operate on the output of another can be modeled by composing the string relations corresponding to each rule.If the string relations for two rules are regular, we know that their composition is also regular.The following result is then established by induction on the number of rules in the grammar: If G = (Ri, ,R,) is a grammar defined as a finite ordered sequence of rewriting rules each of which denotes a regular relation, then the set of input-output string-pairs for the grammar as a whole is the regular relation given by R1 o o R. This theorem also has an immediate corollary: The input-output string pairs of every such rewriting grammar are accepted by a single finite-state transducer.Again, given an implementation of the regular calculus, a grammar transducer can be constructed algorithmically from its rules.We can also show that certain more complex methods of combination also denote regular relations.Suppose a grammar is specified as a finite sequence of rules but with a further specification that rules in some subsequences are to be treated as a block of mutually exclusive alternatives.That is, only one rule in each such subsequence can be applied in any derivation, but the choice of which one varies freely between derivations.The alternative choices among the rules in a block can be modeled as the union of the regular relations they denote individually, and regular relations are closed under this operation.Thus this kind of grammar also reduces to a finite composition of regular relations.In a more intricate arrangement, the grammar might specify a block of alternatives made up of rules that are not adjacent in the ordering sequence.For example, suppose the grammar consists of the sequence (R1, R2, R3, R4, R3), where R2 and R4 constitute a block of exclusive alternatives.This cannot be handled by simple union of the block rules, because that would not incorporate the effect of the intervening rule R3.However, this grammar can be interpreted as abbreviating a choice between two different sequences, (R1, R2, R3, R5) and (R1, R3, R4, R5), and thus denotes the regular relation Ri 0 KR2 0 R3) U (R3 0R4)] 0 R5 The union and composition operators can be interleaved in different ways to show that a wide variety of rule combination regimes are encompassed by the regular relations.There may be grammars specifying even more complex rule interactions, and, depending on the formal details, it may be possible to establish their regularity by other techniques; for example, by carefully managing a set of distinguished auxiliary symbols that code inter-rule constraints.We know, of course, that certain methods for combining regular rules give rise to nonregular mappings.This is true, for example, of unrestricted cyclic application of the rules in a finite ordered sequence.According to a cyclic grammar specification, a given input string is mapped through all the rules in the sequence to produce an output string, and that output string then becomes a new input for a reapplication of all the rules, and the process can be repeated without bound.We can demonstrate that such a grammar is nonregular by considering again the simple optional rule e —> ab/a b We showed before that this rule does not denote a regular relation if it is allowed to rewrite material that was introduced on a previous application.Under those circumstances it would map the regular language {ab} into the context-free language {ab n 1 1 < n}.But we would get exactly the same result from an unrestricted cyclic grammar whose ordered sequence consists only of this single rule.In effect, cyclic reapplication of the rule also permits it to operate arbitrarily, often on its own output.In the worst case, in fact, we know that the computations of an arbitrary Turing machine can be simulated by a rewriting grammar with unrestricted rule reapplication.These results seem to create a dilemma for our regularity analysis.Many phonological formalisms based on ordered sets of rewriting rules provide for cyclic rule applications.The underlying notion is that words have a bracketed structure reflecting their morphological composition.For example, unenforceable has the structure [un[[en[forcellable]].The idea of the cycle is that the ordered sequence of rules is applied to the innermost bracketed portion of a word first.Then the innermost set of brackets is removed and the procedure is repeated.The cycle continues in this way until no brackets remain.The cycle has been a major source of controversy ever since it was first proposed by Chomsky and Halle (1968), and many of the phenomena that motivated it can also be given noncyclic descriptions.Even for cases where a nonrecursive, iterative account has not yet emerged, there may be restrictions on the mode of reapplication that limit the formal power of the grammar without reducing its empirical or explanatory coverage.For example, the bracket erasure convention means that new string material becomes accessible to the rules on each cycle.If, either implicitly or explicitly, there is also a finite bound on the amount of old material to which rules in the new cycle can be sensitive, it may be possible to transform the recursive specification to an equivalent iterative one.This is analogous to the contrast between center-embedding context-free grammars and grammars with only right- or left-linear rules; the latter are known to generate only regular languages.Unfortunately, phonological theories are usually not presented in enough formal detail for us to carry out such a mathematical analysis.The regularity of cyclic phonological formalisms will have to be examined on a case-by-case basis, taking their more precise specifications into account.We have shown that every noncyclical rewriting grammar does denote a regular relation.We now consider the opposite question: Is every regular relation denoted by some noncyclic rewriting grammar?We can answer this question in the affirmative: Theorem Every regular relation is the set of input/output strings of some noncyclic rewriting grammar with boundary-context rules.Let R be an arbitrary regular relation and let T = (E, Q, go, F, 6) be a finite-state transducer that accepts it.Without loss of generality we assume that E and Q are disjoint.We construct a rewriting grammar that simulates the operation of T, deriving a string y from a string x if and only if the pair (x, y) is accepted by T. There will be four rules in the grammar that together implement the provisions that T starts in state go, makes transitions from state to state only as allowed by 6, and accepts a string only if the state it reaches at the end of the string is in F. Let EUQU {#, $} be the alphabet of the grammar, where # is a boundary symbol not in either E or Q and $ is another distinct symbol that will be used in representing the finality of a state.Our rules will introduce states into the string between ordinary tape symbols and remove them to simulate the state-to-state advance of the transducer.The first rule in the grammar sequence is the simple start rule: The effect of this rule is to introduce the start-state as a symbol only at the beginning of the input string, as specified in the rule by the boundary symbol #.The string abc is thus rewritten by this rule to goabc.The following sets of rules are defined to represent the state-to-state transitions and the final states of the transducer: The second rule of the grammar is an obligatory, left-to-right batch rule consisting of all the rules in Transitions U Final.If the transition function carries the transducer from g, to g1 over the pair (u, v), there will be a rule in Transitions that applies to the string .. . g,u ... at the position just after the substring beginning with g, and produces ...g,vgi ... as its output.Because 6 is a total function on Q x E' x E', some subrule will apply at every string position in the left-to-right batch scan.The state-context for the left-most application of this rule is the start-state go, and subrules corresponding to start-state transitions are selected.This introduces a state-symbol that makes available at the next position only subrules corresponding to transitions at one of the start-state's successors.The batch rule eventually writes a state at the very end of the string.If that state is in F, the corresponding Final subrule will apply to insert $ at the end of the string.If the last state is not in F,$ will not be inserted and the state will remain as the last symbol in the string.Thus, after the batch rule has completed its application, an input string x will have been translated to an output string consisting of intermixed symbols from Q and E. We can prove by a simple induction that the string of states obtained by ignoring symbols in E U {#,$} corresponds to a sequence of state-to-state moves that the transducer can make on the pair (x, y), where y comes from ignoring $ and all state-symbols in the output string.Two tasks remain: we must filter the output to eliminate any strings whose derivation does not include a Final subrule application, and we must remove all state-symbols and $ to obtain the ultimate output string.If a Final rule did not apply, then the last element in the batch output string is a state, not the special character $.We must formulate a rule that will &quot;bleed&quot; the derivation, producing no output at all if its input ends in a state-symbol instead of $.We can achieve this with an anomalous obligatory rule whose output would be infinitely long if its input ever satisfies the conditions for its application.The following rule behaves in this way: It has the effect of filtering strings that do not represent transitions to a final state by forcing indefinitely many insertions of $ when no single $ is present.The output of this rule will be all and only the strings that came from a previous application of a Final rule.The last rule of the grammar is a trivial clean-up rule that produces the grammar's final output strings: This completes the proof of the theorem.We have constructed for any regular relation an ordered sequence of four rules (including a batch rule with finitely many subrules) that rewrites a string x to y just in case the pair (x, y) belongs to the relation.We remark that there are alternative but perhaps less intuitive proofs of this theorem framed only in terms of simple nonbatch rules.But this result cannot be established without making use of boundary-context rules.Without such rules we can only simulate a proper subclass of the regular relations, those that permit identity prefixes and suffixes of unbounded length to surround any nonidentity correspondences.It is interesting to note that for much the same reason, Ritchie (1992) also made crucial use of two-level boundary-context rules to prove that the relations denoted by Koskenniemi's (1985) two-level grammars are also coextensive with the regular relations.Moreover, putting Ritchie's result together with ours gives the following: Ordered rewriting grammars with boundaries and two-level constraint grammars with boundaries are equivalent in their expressive power.Although there may be aesthetic or explanatory differences between the two formal systems, empirical coverage by itself cannot be used to choose between them.Inspired in part by our early report of the material presented in this paper (Kaplan and Kay 1981), Koskenniemi (1983) proposed an alternative system for recognizing and producing morphological and phonological word-form variants.Under his proposal, individual generalizations are expressed directly in the state-transition diagrams of finite-state transducers, and their mutual interactions emerge from the fact that every input-output string pair must be accepted simultaneously by all these transducers.Thus, he replaced the serial feeding arrangement of the independent generalizations in a rewriting grammar with a parallel method of combination.In eliminating the intermediate strings that pass from one rewriting rule to another, he also reduced to just two the number of linguistically meaningful levels of representation.In two-level parlance, these are usually referred to as the lexical and surface strings.The lexical-surface string sets of the individual generalizations in Koskenniemi's system are clearly regular, since they are defined outright as finite-state transducers.But it is not immediately obvious that the string relation defined by a whole two-level grammar is regular.Koskenniemi gave an operational specification, not an algebraic one, of how the separate transducers are to interact.A pair of strings is generated by a two-level grammar if the pair is accepted separately by each of the transducers, and furthermore, the label on the transition taken by one fst at a particular string position is identical to the label of the transition that every other fst takes at that string position.In essence, he prescribed a transition function (5 for a whole-grammar transducer that allows transitions between states in cross-product state sets just in case they are permitted by literal-matching transitions in the individual machines.This transition function generalizes to a two-tape transducer the construction of a one-tape finite-state machine for the intersection of two regular languages.We might therefore suspect that the lexical-surface relation for a two-level grammar consisting of transducers Th Tn is the relation n,R(T,).However, what is actually computed under this interpretation is the relation Rel(Paths(T1) n Paths (T2) ...Paths(L)) of the form discussed in Section 3.As we observed, this may be only a proper subset of the relation n,R(T,) when the component relations contain string pairs of unequal length.In this case, the literal-matching transducer may not accept the intersection, a relation that in fact may not even be regular.The individual transducers allowed in two-level specifications do permit the expansion and contraction of strings by virtue of a null symbol 0.If this were treated just like €, we would be able to say very little about the combined relation.However, the effect of Koskenniemi's literal-matching transition function is achieved by treating 0 as an ordinary tape symbol, so that the individual transducers are &free.The intersection of their same-length relations is therefore regular.The length-changing effect of the whole-grammar transducer is then provided by mapping 0 onto e. Thus we embed the same-length intersection n,R(Ti) as a regular inner component of a larger regular relation that characterizes the complete lexical-to-surface mapping: This relation expands its lexical string by freely introducing 0 symbols.These are constrained along with all other symbols by the inner intersection, and then the surface side of the inner relation is contracted by the removal of all O's.The entire outer relation gives an algebraic model of Koskenniemi's operational method for combining individual transducers and for interpreting the null symbol.With this analysis of the two-level system in terms of regularly-closed operations and same-length relations, we have shown that the string relations accepted by parallel two-level automata are in fact regular.We have also shown, by the way, that the two-level system is technically a four-level one, since the inner relation defines two intermediate, 0-containing levels of representation.Still, only the two outer levels are linguistically significant.In typical two-level implementations the Intro relations are implicitly encoded in the interpretation algorithms and do not appear as separate transducers.Koskenniemi (1983) offered an informal grammatical notation to help explicate the intended effect of the individual transducers and make the generalizations encoded in their state-transition diagrams easier to understand and reason about.However, he proposed no method of interpreting or compiling that notation.In later work (e.g.Karttunen, Koskennienni, and Kaplan 1987; Karttunen and Beesley 1992) the mathematical techniques we presented above for the analysis of rewriting systems were used to translate this notation into the equivalent regular relations and corresponding transducers, and thus to create a compiler for a more intuitive and more tractable two-level rule notation.Ritchie (1992) summarizes aspects of this analysis as presented by Kaplan (1988).Ritchie et al. (1992) describe a program that interprets this notation by introducing and manipulating labels assigned to the states of component finitestate machines.Since these labels have no simple set-theoretic significance, such an approach does not illuminate the formal properties of the system and does not make it easy to combine two-level systems with other formal devices.Ignoring some notational details, a grammar of two-level rules (as opposed to fsts) includes a specification of a set of &quot;feasible pairs&quot; of symbols that we denote by 7r.The pairs in 7 contain all the alphabet symbols and 0, but do not contain E except possibly when it is paired with itself in €: €.The relations corresponding to all the individual rules are all subsets of 7r*, and thus are all of the restricted same-length class (since 7 does not contain € paired with an alphabetic symbol).For this class of relations, it makes sense to talk about a correspondence between a symbol in one string in a string-pair and a symbol in the other: in the pair (abc,lmn) for example, we can say that a corresponds to 1, b to m, and c to n, by virtue of the positions they occupy relative to the start of their respective same-length strings.Symbol pairs that correspond in this way must be members of 7r.It also makes sense to talk about corresponding substrings, sequences of string pairs whose symbols correspond to each other in some larger string pair.Corresponding substrings belong to 7r*.The grammar contains a set of rules whose parts are also same-length regularrelation subsets of 7*.There are two basic kinds of rules, context restriction rules and surface coercion rules.A simple context restriction rule is an expression of the formwhere r, A, and p denote subsets of 7r*.Usually T is just a single feasible pair, a singleton element of 7r, but this limitation has no mathematical significance.Either of the contexts A or p can be omitted, in which case it is taken to be €: e. Such a rule is interpreted as denoting a string relation whose members satisfy the following conditions: Every corresponding substring of a string pair that belongs to the relation T must be immediately preceded by a corresponding substring belonging to the leftcontext A and followed by one belonging to the right-context p. In other words, any appearance of T outside the specified contexts is illegal.Under this interpretation, the relation denoted by the rule would include the string pairs (cae, dbf) and (cae,cge), assuming that a:g is in it along with the symbol pairs mentioned in the rule.The first string pair is included because the pair a:b is properly surrounded by c:d and e:f. The second belongs because it contains an instance of a: g instead of a:b and thus imposes no requirements on the surrounding context.The string pair (cae, cbe) is not included, however, because a:b appears in a context not sanctioned by the rule.For strings to satisfy a constraint of this form, they must meet a more complicated set of conditions.Suppose that a corresponding substring belonging to A comes before a corresponding substring belonging to p, and that the lexical side of the paired substring that comes between them belongs to the domain of T. Then that intervening paired substring must itself belong to T. To illustrate, consider the surface coercion version of the context restriction example above: The string pair (cae,dbf) satisfies this constraint because a:b comes between the context substrings c:d and e:f. The pair (cbe,dbf) is also acceptable, because the string intervening between c:d and e:f does not have a on its lexical side.However, the pair (cae,dgf) does not meet the conditions because a:g comes between c:d and e:f. The lexical side of this is the same as the lexical side of the T relation a: b, but the pair a:g itself is not in r. Informally, this rule forces the a to be realized as a surface b when it appears between the specified contexts.Karttunen et al. (1987) introduced a variant of a surface coercion rule called a surface prohibition.This is a rule of the form 7/ A _ p and indicates that a paired substring that comes between instances of A and p and whose lexical side is in the domain of 7- must not itself belong to T. We shall see that the mathematical properties of surface prohibitions follow as immediate corollaries of our surface coercion analysis.The notation also permits compound rules of each type.These are rules in which multiple context pairs are specified.A compound context restriction rule is of the form y Al pl; A2 and is satisfied if each instance of T is surrounded by an instance of some Ak-pk pair.A compound surface coercion rule requires the T surface realization in each of the specified contexts.For convenience, surface coercions and context restrictions can be specified in a single rule, by using <=> as the main connective instead of = or .A bidirectional rule is merely an abbreviation that can be included in a grammar in place of the two subrules formed by replacing the <#. first by = and then .Such rules need no further discussion.To model the conditions imposed by context restriction rules, we recall the If-P-then-S and If-S-then-P operators we defined for regular languages L1 and L2: These operators can be extended to apply to string relations as well, and the results will be regular if the operands are in a regular subclass that is closed under complementation.For notational convenience, we let the overbar in these extensions and in the other expressions below stand for the complement relative to 7r* as opposed to the more usual E* x E*: The conditions for a simple context restriction rule are then modeled by the following relation: Restrict ( r, A, p) = IPS-then-P(7* A, 7-71*) n 'PP-then-S(7*r, pe) The first component ensures that 7- is always preceded by A, and the second guarantees that it is always followed by p. A compound context restriction rule of the form is satisfied if all instances of T are surrounded by substrings meeting the conditions of at least one of the context pairs independently.A candidate model for this disjunctive interpretation is the relation This is incorrect, however, because the scope of the union is too wide.It specifies that there must be some k such that every occurrence of T will be surrounded by Ak-pk, whereas the desired interpretation is that each 7 must be surrounded by Ak-pk, but a different k might be chosen for each occurrence.A better approximation is the relation Because of the outer Kleene * iteration, different instances of the rule can apply at different string-pair positions.But this also has a problem, one that should be familiar from our study of rewriting rules: the iteration causes the different rule instances to match on separate, successive substrings.It does not allow for the possibility that the context substring of one application might overlap with the center and context portions of a preceding one.This problem can be solved with the auxiliary-symbol techniques we developed for rewriting rule overlaps.We introduce left and right context brackets <k and >k for each context pair Ak-pk.These are distinct from all other symbols, and since their identity pairs are now feasible pairs, they are added to 7.These pairs take the place of the actual context relations in the iterative union This eliminates the overlap problem.We then must ensure that these bracket pairs appear only if appropriately followed or preceded by the proper context relation.With m being the set of all bracket pairs and subscripting now indicating that identity pairs of the specified symbols are ignored, we define a two-level left-context operator so that Leftcontext(Ak , <k) enforces the requirement that every <k pair be preceded by an instance of Ak.This is simpler than the rewriting left-context operator because not every instance of A must be marked—only the ones that precede T, and those are picked out independently by the iterative union.That is why this uses a one-way implication instead of a biconditional.As in the rewriting case, the ignoring provides for overlapping instances of A.The right-context operator can be defined symmetrically using If-P-then-S or by reversing the left-context operator: Auxiliary marks are freely introduced on the lexical string.Those marks are appropriately constrained so that matching brackets enclose every occurrence of T, and each bracket marks an occurrence of the associated context relation.The marks are removed at the end.Note that there are only same-length relations in the intermediate expression, and that all brackets introduced at the top are removed at the bottom.Thus the composite relation is regular and also belongs to the same-length subclass, so that the result of intersecting it with the same-length regular relations for other rules will be regular.A surface coercion rule of the form imposes a requirement on the paired substrings that come between all members of the A and p relations.If the lexical side of such a paired substring belongs to the domain of T, then the surface side must be such that the intervening pair belongs to T. To formalize this interpretation, we first describe the set of string pairs that fail to meet the conditions.The complement of this set is then the appropriate relation.The relationT.- = 71-* — T is the set of string pairs in 7r* that are not in T, because either their lexical string is not in the domain of T or 7- associates that lexical string with different surface strings.Id(Dom(T)) o f- is the subset of these whose lexical strings are in the domain of r and whose surface strings must therefore be different than T provides for.The unacceptable string pairs thus belong to the same-length relation 7T*A[Id(Dom(T)) a 7r-lp7r*, and its regular complement in the Coerce operator Coerce(r, A, p) =-- r* A[Id(Dom(r)) o T]p7r* contains all the string pairs that satisfy the rule.For most surface coercions it is also the case that this contains only the pairs that satisfy the rule.But for one special class of coercions, the epenthesis rules, this relation includes more string pairs than we desire.These are rules in which the domain of T includes strings consisting entirely of O's, and the difficulty arises because of the dual nature of two-level O's.They behave formally as actual string symbols in same-length relations, but they are also intended to act as the empty string.In this way they are similar to the c's in the centers of rewriting rules, and they must also be modeled by special techniques.The epenthesis rule 0:b c:c d:d can be used to illustrate the important issues.If this is the only rule in a grammar, then clearly that grammar should allow the string pair (cd, cbd) but disallow the pair (cd, ced), in which e appears instead of b between the surface c and d. It should also disallow the pair (cd, cd), in which c and d are adjacent on both sides and no epenthesis has occurred.This is consistent with the intuition that the 0 in the rule stands for the absence of explicit lexical string material, and that therefore the rule must force a surface b when lexical c and d are adjacent.In our analysis this interpretation of 0 is expressed by having the Intro relation freely introduce O's between any other symbols, mimicking the fact that c can be regarded as freely appearing everywhere.The pair (cd, cbd) is allowed as the composition of pairs (cd, cOd) and (c0d, cbd); the first pair belongs to the Intro relation and the second is sanctioned by the rule.But because O's are introduced freely, the Intro relation includes the identity pair (cd, cd) as well.The Coerce relation as defined above also contains the pair (cd, cd) (= (ccd, cEd)), since €: E is not in [0:0 o 0: M. The grammar as a whole thus allows (cd, cd) as an undesired composition.We can eliminate pairs of this type by formulating a slightly different relation for epenthesis rules such as these.We must still disallow pairs when O's in the domain of 7- are paired with strings not in the range.But we also want to disallow pairs whose lexical strings do not have the appropriate O's to trigger the grammar's epenthesis coercions.This can be accomplished by a modified version of the Coerce relation that also excludes realizations of the empty string by something not in T. We replace the Dom (7-) expression in the definition above with the relation Dom (T U {E: E}).The twolevel literature is silent about whether or not an epenthesis rule should also reject strings with certain other insertion patterns.On one view, the rule only restricts the insertion of singleton strings and thus pairs such as (cd, cbbd) and (cd, ceed) would be included in the relation.This view is modeled by using the Dom(r U {E: c}) expression.On another view, the rule requires that lexically adjacent c and d must be separated by exactly one b on the surface, so that (cd, cbbd) and (cd, ceed) would be excluded in addition to (cd, ced) and (cd, cd).We can model this second interpretation by using 0* instead of Dom(r U { E:E}).The relation then restricts the surface realization of any number of introduced O's.It is not clear which of these interpretations leads to a more convenient formalism, but each of them can be modeled with regular devices.Karttunen and Beesley (1992, p. 22) discuss a somewhat different peculiarity that shows up in the analysis of epenthesis rules where one context is omitted (or equivalently, one context includes the pair E: c).The rule requires that a b corresponding to nothing in the lexical string must appear in the surface string after every c:c pair.If we use either the Dom (T u {E : ED or 0* expressions in defining the coercion relation for this rule, the effect is not what we intend.The resulting relation does not allow strings in which a E: c follows c :c, because c is included in the restrictive domain expression.But c:E follows and precedes every symbol pair, of course, so the result is a relation that simply prohibits all occurrences of c:c. If, however, we revert to using the domain expression without the { cc} union, we fall back into the difficulty we saw with two-context epenthesis rules: the resulting relation properly ensures that nothing other than b can be inserted after c:c, but it leaves open the possibility of c:c followed by no insertion at all.A third formulation is necessary to model the intended interpretation of onecontext epenthesis rules.This is given by the relation 7r*AT-7r* if only the left-context is specified, or 7r*T-p7r* if only p appears.These exclude all strings where an instance of the relevant context is followed by paired substrings not in r, either because the appropriate number of lexical O's were not (freely) introduced or because those O's correspond to unacceptable surface material.These two prescriptions can be brought together into the single formula 7r*AT-p7r* for all one-context rules, since whichever context is missing is treated as the identity pair €: €.We can bring out the similarity between this formula and the original Coerce relation by observing that this one is equivalent to 71-*A[Id(Dom(71-*)) o T-1p7r* because Id(Dom(e)) o 7T- and T- are the same relation.We now give a general statement of the Coerce relation that models surface coercions whether they are epenthetic or non-epenthetic: and neither A nor p contains €: E; X = 7r* if T has only epenthetic pairs and one of A or p does contain €:€.This definition assumes that T is homogeneous in that either all its string-pairs are epenthetic or none of them are, but we must do further analysis to guarantee that this is the case.In the formalism we are considering, T is permitted to be an arbitrary samelength relation, not just the single unit-length pair that two-level systems typically provide for.If T contains more than one string-pair, the single rule is interpreted as imposing the constraints that would be imposed by a conjunction of rules formed by substituting for T each of its member string-pairs in turn.Without further specification and even if 7- contains infinitely many pairs, this is the interpretation modeled by the Coerce relation, provided that T is homogeneous.To deal with heterogeneous T relations, we separate the epenthetic and nonepenthetic pairs into two distinct and homogeneous subrelations.We partition an arbitrary T into the subrelations r° and T° defined as We then recast a rule of the form T 4= A p as the conjunction of the two rules These rules taken together represent the desired interpretation of the original, and each of them is properly modeled by exactly one variant of the Coerce relation.We have now dealt with the major complexities that surface coercion rules present.The compound forms of these rules are quite easy to model.A rule of the form is interpreted as coercing to the surface side of 7 if any of the context conditions are met.Auxiliary symbols are not needed to model this interpretation, since there is no iteration to introduce overlap difficulties.The relation for this rule is given simply by the intersection of the individual relations: nCoerce(r, )k We conclude our discussion of two-level rules with a brief mention of surface prohibitions.Recall that a prohibition ruleindicates that a paired substring must not belong to r if it comes between instances of A and p and its lexical side is in the domain of T. We can construct a standard surface coercion rule that has exactly this interpretation by using the complement of 7- restricted to T'S domain: [Id(Dom(T)) 0 A p As desired, the left side is the relation that maps each string in the domain of T to all strings other than those to which T maps it.Surface prohibitions are thus reduced to ordinary surface coercions.The relation for a grammar of rules is formed just as for a grammar of parallel automata.The intersection of the relations for all the individual rules is constructed as a same-length inner relation.This is then composed with the 0 introduction and removal relations to form the outer lexical-to-surface map.Rule-based two-level grammars thus denote regular relations, just as the original transducer-based grammars do.Some grammars may make use of boundary-context rules, in which case a special symbol # can appear in contexts to mark the beginning and end of the strings.These can be modeled with exactly the same technique we outlined for rewriting rules: we compose the additional relation [e:#/d (Em*0 # Em*o) €:#] at the beginning of the four-level cascade and compose its inverse at the end.As we mentioned before, the two-level grammars with boundary-context rules are the ones that Ritchie (1992) showed were complete for the regular relations.In reasoning about these systems, it is important to keep clearly in mind the distinction between the outer and inner relations.Ritchie (1992), for example, also proved that the &quot;languages&quot; generated by two-level grammars with regular contexts are closed under intersection, but this result does not hold if a grammar's language is taken to be its outer relation.Suppose that G1 has the set {a :b, 0 :c} as its feasible pairs and the vacuous a:b = as its only rule, and that G2 has the pairs {a :c, 0:b} and rule a:c = .The domain of both outer (0-free) relations is a*.A string a&quot; is mapped by G1 into strings containing n b's with c's freely intermixed and by G2 into strings containing n c's with b's freely intermixed.The range of the intersection of the outer relations for G1 and G2 thus contains strings with the same number of b's and c's but occurring in any order.This set is not regular, since intersecting it with the regular language b*c* produces the context-free language bnc&quot;.The intersection of the two outer relations is therefore also not regular and so cannot be the outer relation of any regular two-level grammar.We have shown how our regular analysis techniques can be applied to two-level systems as well as rewriting grammars, and that grammars in both frameworks denote only regular relations.These results open up many new ways of partitioning the account of linguistic phenomena in order to achieve descriptions that are intuitively more satisfying but without introducing new formal power or computational machinery.Karttunen, Kaplan, and Zaenen (1992), for example, argued that certain French morphological patterns can be better described as the composition of two separate two-level grammars rather than as a single one.As another option, an entire two-level grammar can be embedded in place of a single rule in an ordered rewriting system.As long as care is taken to avoid inappropriate complementations and intersections, all such arrangements will denote regular relations and can be implemented by a uniform finite-state transducer mechanism.Our aim in this paper has been to provide the core of a mathematical framework for phonology.We used systems of rewriting rules, particularly as formulated in SPE, to give concreteness to our work and to the paper.However, we continually sought solutions in terms of algebraic abstractions of sufficiently high level to free them from any necessary attachment to that or any other specific theory.If our approach proves useful, it will only be because it is broad enough to encompass new theories and new variations on old ones.If we have chosen our abstractions well, our techniques will extend smoothly and incrementally to new formal systems.Our discussion of two-level rule systems illustrates how we expect such extensions to unfold.These techniques may even extend to phonological systems that make use of matched pairs of brackets.Clearly, context-free mechanisms are sufficient to enforce dependencies between corresponding brackets, but further research may show that accurate phonological description does not exploit the power needed to maintain the balance between particular pairs, and thus that only regular devices are required for the analysis and interpretation of such systems.An important goal for us was to establish a solid basis for computation in the domain of phonological and orthographic systems.With that in mind, we developed a well-engineered computer implementation of the calculus of regular languages and relations, and this has made possible the construction of practical language processing systems.The common data structures that our programs manipulate are clearly states, transitions, labels, and label pairs—the building blocks of finite automata and transducers.But many of our initial mistakes and failures arose from attempting also to think in terms of these objects.The automata required to implement even the simplest examples are large and involve considerable subtlety for their construction.To view them from the perspective of states and transitions is much like predicting weather patterns by studying the movements of atoms and molecules or inverting a matrix with a Turing machine.The only hope of success in this domain lies in developing an appropriate set of high-level algebraic operators for reasoning about languages and relations and for justifying a corresponding set of operators and automata for computation.From a practical point of view, the result of the work reported here has been a set of powerful and sometimes quite complex tools for compiling phonological grammars in a variety of formalisms into a single representation, namely a finite-state transducer.This representation has a number of remarkable advantages: (1) The program required to interpret this representation is simple almost to the point of triviality, no matter how intricate the original grammars might have been.(2) That same program can be used to generate surface or textual forms from underlying lexical representations or to analyze text into a lexical string; the only difference is in which of the two symbols on a transition is regarded as the input and which the output.(3) The interpreter is constant even under radical changes in the theory and the formalism that informed the compiler.(4) The compiler consists almost entirely of an implementation of the basic calculus.Given the operators and data types that this makes available, only a very few lines of code make up the compiler for a particular theory.Reflecting on the way the relation for a rewriting rule is constructed from simpler relations, and on how these are composed to create a single relation for a complete grammar, we come naturally to a consideration of how that relation should comport with the other parts of a larger language-processing system.We can show, for example, that the result of combining together a list of items that have exceptional phonological behavior with a grammar-derived relation for general patterns is still a regular relation with an associated transducer.If E is a relation for a finite list of exceptional input-output pairs and P is the general phonological relation, then the combination is given by E u [Id(Dom(E)) p] This relation is regular because E is regular (as is any finite list of pairs); it suppresses the general mapping provided by P for the exceptional items, allowing outputs for them to come from E only.As another example, the finite list of formatives in a lexicon L can be combined with a regular phonology (perhaps with exceptions already folded in) by means of the composition Id(L) o P. This relation enshrines not only the phonological regularities of the language but its lexical inventory as well, and its corresponding transducer would perform phonological recognition and lexical lookup in a single sequence of transitions.This is the sort of arrangement that Karttunen et al. (1992) discuss.Finally, we know that many language classes are closed under finitestate transductions or composition with regular relations—the images of context-free languages, for example, are context-free.It might therefore prove advantageous to seek ways of composing phonology and syntax to produce a new system with the same formal properties as syntax alone.We are particularly indebted to Danny Bobrow for helpful discussions in the early stages of the research on rewriting systems.Our understanding and analysis of two-level systems is based on very productive discussions with Lauri Karttunen and Kimmo Koskenniemi.We would like to thank John Maxwell, Mary Dalrymple, Andy Daniels, Chris Manning, and especially Kenneth Beesley for detailed comments on earlier versions of this paper.Finally, we are also indebted to the anonymous referees for identifying a number of technical and rhetorical weaknesses.We, of course, are responsible for any remaining errors.
Preemptive Information Extraction Using Unrestricted Relation Discoverysurface text patterns for a question answering system. of the 40th Annual Meeting of the AsEvery day, a large number of news articles are created and reported, many of which are unique.But certain types of events, such as hurricanes or murders, are reported again and again throughout a year.The goal of Information Extraction, or IE, is to retrieve a certain type of news event from past articles and present the events as a table whose columns are filled with a name of a person or company, according to its role in the event.However, existing IE techniques require a lot of human labor.First, you have to specify the type of information you want and collect articles that include this information.Then, you have to analyze the articles and manually craft a set of patterns to capture these events.Most existing IE research focuses on reducing this burden by helping people create such patterns.But each time you want to extract a different kind of information, you need to repeat the whole process: specify articles and adjust its patterns, either manually or semiautomatically.There is a bit of a dangerous pitfall here.First, it is hard to estimate how good the system can be after months of work.Furthermore, you might not know if the task is even doable in the first place.Knowing what kind of information is easily obtained in advance would help reduce this risk.An IE task can be defined as finding a relation among several entities involved in a certain type of event.For example, in the MUC-6 management succession scenario, one seeks a relation between COMPANY, PERSON and POST involved with hiring/firing events.For each row of an extracted table, you can always read it as “COMPANY hired (or fired) PERSON for POST.” The relation between these entities is retained throughout the table.There are many existing works on obtaining extraction patterns for pre-defined relations (Riloff, 1996; Yangarber et al., 2000; Agichtein and Gravano, 2000; Sudo et al., 2003).Unrestricted Relation Discovery is a technique to automatically discover such relations that repeatedly appear in a corpus and present them as a table, with absolutely no human intervention.Unlike most existing IE research, a user does not specify the type of articles or information wanted.Instead, a system tries to find all the kinds of relations that are reported multiple times and can be reported in tabular form.This technique will open up the possibility of trying new IE scenarios.Furthermore, the system itself can be used as an IE system, since an obtained relation is already presented as a table.If this system works to a certain extent, tuning an IE system becomes a search problem: all the tables are already built “preemptively.” A user only needs to search for a relevant table.We implemented a preliminary system for this technique and obtained reasonably good performance.Table 1 is a sample relation that was extracted as a table by our system.The columns of the table show article dates, names of hurricanes and the places they affected respectively.The headers of the table and its keywords were also extracted automatically.In Unrestricted Relation Discovery, the discovery process (i.e. creating new tables) can be formulated as a clustering task.The key idea is to cluster a set of articles that contain entities bearing a similar relation to each other in such a way that we can construct a table where the entities that play the same role are placed in the same column.Suppose that there are two articles A and B, and both report hurricane-related news.Article A contains two entities “Katrina” and “New Orleans”, and article B contains “Longwang” and “Taiwan”.These entities are recognized by a Named Entity (NE) tagger.We want to discover a relation among them.First, we introduce a notion called “basic pattern” to form a relation.A basic pattern is a part of the text that is syntactically connected to an entity.Some examples are “X is hit” or “Y’s residents”.Figure 1 shows several basic patterns connected to the entities “Katrina” and “New Orleans” in article A.Similarly, we obtain the basic patterns for article B.Now, in Figure 2, both entities “Katrina” and “Longwang” have the basic pattern “headed” in common.In this case, we connect these two entities to each other.Furthermore, there is also a common basic pattern “was-hit” shared by “New Orleans” and “Taiwan”.Now, we found two sets of entities that can be placed in correspondence at the same time.What does this mean?We can infer that both entity sets (“Katrina”-“New Orleans” and “Longwang”-“Taiwan”) represent a certain relation that has something in common: a hurricane name and the place it affected.By finding multiple parallel correspondences between two articles, we can estimate the similarity of their relations.Generally, in a clustering task, one groups items by finding similar pairs.After finding a pair of articles that have a similar relation, we can bring them into the same cluster.In this case, we cluster articles by using their basic patterns as features.However, each basic pattern is still connected to its entity so that we can extract the name from it.We can consider a basic pattern to represent something like the “role” of its entity.In this example, the entities that had “headed” as a basic pattern are hurricanes, and the entities that had “was-hit” as a basic pattern are the places it affected.By using basic patterns, we can align the entities into the corresponding column that represents a certain role in the relation.From this example, we create a two-by-two table, where each column represents the roles of the entities, and each row represents a different article, as shown in the bottom of Figure 2.We can extend this table by finding another article in the same manner.In this way, we gradually extend a table while retaining a relation among its columns.In this example, the obtained table is just what an IE system (whose task is to find a hurricane name and the affected place) would create.However, these articles might also include other things, which could represent different relations.For example, the governments might call for help or some casualties might have been reported.To obtain such relations, we need to choose different entities from the articles.Several existing works have tried to extract a certain type of relation by manually choosing different pairs of entities (Brin, 1998; Ravichandran and Hovy, 2002).Hasegawa et al. (2004) tried to extract multiple relations by choosing entity types.We assume that we can find such relations by trying all possible combinations from a set of entities we have chosen in advance; some combinations might represent a hurricane and government relation, and others might represent a place and its casualties.To ensure that an article can have several different relations, we let each article belong to several different clusters.In a real-world situation, only using basic patterns sometimes gives undesired results.For example, “(President) Bush flew to Texas” and “(Hurricane) Katrina flew to New Orleans” both have a basic pattern “flew to” in common, so “Bush” and “Katrina” would be put into the same column.But we want to separate them in different tables.To alleviate this problem, we put an additional restriction on clustering.We use a bag-of-words approach to discriminate two articles: if the word-based similarity between two articles is too small, we do not bring them together into the same cluster (i.e. table).We exclude names from the similarity calculation at this stage because we want to link articles about the same type of event, not the same instance.In addition, we use the frequency of each basic pattern to compute the similarity of relations, since basic patterns like “say” or “have” appear in almost every article and it is dangerous to rely on such expressions.In the above explanation, we have assumed that we can obtain enough basic patterns from an article.However, the actual number of basic patterns that one can find from a single article is usually not enough, because the number of sentences is rather small in comparison to the variation of expressions.So having two articles that have multiple basic patterns in common is very unlikely.We extend the number of articles for obtaining basic patterns by using a cluster of comparable articles that report the same event instead of a single article.We call this cluster of articles a “basic cluster.” Using basic clusters instead of single articles also helps to increase the redundancy of data.We can give more confidence to repeated basic patterns.Note that the notion of “basic cluster” is different from the clusters used for creating tables explained above.In the following sections, a cluster for creating a table is called a “metacluster,” because this is a cluster of basic clusters.A basic cluster consists of a set of articles that report the same event which happens at a certain time, and a metacluster consists of a set of events that contain the same relation over a certain period.We try to increase the number of articles in a basic cluster by looking at multiple news sources simultaneously.We use a clustering algorithm that uses a vector-space-model to obtain basic clusters.Then we apply cross-document coreference resolution to connect entities of different articles within a basic cluster.This way, we can increase the number of basic patterns connected to each entity.Also, it allows us to give a weight to entities.We calculate their weights using the number of occurrences within a cluster and their position within an article.These entities are used to obtain basic patterns later.We also use a parser and tree normalizer to generate basic patterns.The format of basic patterns is crucial to performance.We think a basic pattern should be somewhat specific, since each pattern should capture an entity with some relevant context.But at the same time a basic pattern should be general enough to reduce data sparseness.We choose a predicate-argument structure as a natural solution for this problem.Compared to traditional constituent trees, a predicate-argument structure is a higher-level representation of sentences that has gained wide acceptance from the natural language community recently.In this paper we used a logical feature structure called GLARF proposed by Meyers et al. (2001a).A GLARF converter takes a syntactic tree as an input and augments it with several features.Figure 3 shows a sample GLARF structure obtained from the sentence “Katrina hit Louisiana’s coast.” We used GLARF for two reasons: first, unlike traditional constituent parsers, GLARF has an ability to regularize several linguistic phenomena such as participial constructions and coordination.This allows us to handle this syntactic variety in a uniform way.Second, an output structure can be easily converted into a directed graph that represents the relationship between each word, without losing significant information from the original sentence.Compared to an ordinary constituent tree, it is easier to extract syntactic relationships.In the next section, we discuss how we used this structure to generate basic patterns.The overall process to generate basic patterns and discover relations from unannotated news articles is shown in Figure 4.Theoretically this could be a straight pipeline, but due to the nature of the implementation we process some stages separately and combine them in the later stage.In the following subsection, we explain each component.First of all, we need a lot of news articles from multiple news sources.We created a simple web crawler that extract the main texts from web pages.We observed that the crawler can correctly take the main texts from about 90% of the pages from each news site.We ran the crawler every day on several news sites.Then we applied a simple clustering algorithm to the obtained articles in order to find a set of articles that talk about exactly the same news and form a basic cluster.We eliminate stop words and stem all the other words, then compute the similarity between two articles by using a bag-of-words approach.In news articles, a sentence that appears in the beginning of an article is usually more important than the others.So we preserved the word order to take into account the location of each sentence.First we computed a word vector from each article: where Vw(A) is a vector element of word w in article A, IDF(w) is the inverse document frequency of word w, and POS(w, A) is a list of w’s positions in the article. avgwords is the average number of words for all articles.Then we calculated the cosine value of each pair of vectors: We computed the similarity of all possible pairs of articles from the same day, and selected the pairs whose similarity exceeded a certain threshold (0.65 in this experiment) to form a basic cluster.After getting a set of basic clusters, we pass them to an existing statistical parser (Charniak, 2000) and rule-based tree normalizer to obtain a GLARF structure for each sentence in every article.The current implementation of a GLARF converter gives about 75% F-score using parser output.For the details of GLARF representation and its conversion, see Meyers et al. (2001b).In parallel with parsing and GLARFing, we also apply NE tagging and coreference resolution for each article in a basic cluster.We used an HMM-based NE tagger whose performance is about 85% in Fscore.This NE tagger produces ACE-type Named Entities 1: PERSON, ORGANIZATION, GPE, LOCATION and FACILITY 2.After applying singledocument coreference resolution for each article, we connect the entities among different articles in the same basic cluster to obtain cross-document coreference entities with simple string matching.After getting a GLARF structure for each sentence and a set of documents whose entities are tagged and connected to each other, we merge the two outputs and create a big network of GLARF structures whose nodes are interconnected across different sentences/articles.Now we can generate basic patterns for each entity.First, we compute the weight for each cross-document entity E in a certain basic cluster as follows: where e ∈ E is an entity within one article and mentions(e) and firstsent(e) are the number of mentions of entity e in a document and the position of the sentence where entity e first appeared, respectively.C is a constant value which was 0.5 in this experiment.To reduce combinatorial complexity, we took only the five most highly weighted entities from each basic cluster to generate basic patterns.We observed these five entities can cover major relations that are reported in a basic cluster.Next, we obtain basic patterns from the GLARF structures.We used only the first ten sentences in each article for getting basic patterns, as most important facts are usually written in the first few sentences of a news article.Figure 5 shows all the basic patterns obtained from the sentence “Katrina hit Louisiana’s coast.” The shaded nodes “Katrina” and “Louisiana” are entities from which each basic pattern originates.We take a path of GLARF nodes from each entity node until it reaches any predicative node: noun, verb, or adjective in this case.Since the nodes “hit” and “coast” can be predicates in this example, we obtain three unique paths “Louisiana+T-POS:coast (Louisiana’s coast)”, “Katrina+SBJ:hit (Katrina hit something)”, and “Katrina+SBJ:hit-OBJ:coast (Katrina hit some coast)”.To increase the specificity of patterns, we generate extra basic patterns by adding a node that is immediately connected to a predicative node.(From this example, we generate two basic patterns: “hit” and “hit-coast” from the “Katrina” node.)Notice that in a GLARF structure, the type of each argument such as subject or object is preserved in an edge even if we extract a single path of a graph.Now, we replace both entities “Katrina” and “Louisiana” with variables based on their NE tags and obtain parameterized patterns: “GPE+T-POS:coast (Louisiana’s coast)”, “PER+SBJ:hit (Katrina hit something)”, and “PER+SBJ:hit-OBJ:coast (Katrina hit some coast)”.After taking all the basic patterns from every basic cluster, we compute the Inverse Cluster Frequency (ICF) of each unique basic pattern.ICF is similar to the Inverse Document Frequency (IDF) of words, which is used to calculate the weight of each basic pattern for metaclustering.Finally, we can perform metaclustering to obtain tables.We compute the similarity between each basic cluster pair, as seen in Figure 6.XA and XB are the set of cross-document entities from basic clusters cA and cB, respectively.We examine all possible mappings of relations (parallel mappings of multiple entities) from both basic clusters, and find all the mappings M whose similarity score exceeds a certain threshold. wordsim(cA, cB) is the bag-of-words similarity of two clusters.As a weighting function we used ICF: We then sort the similarities of all possible pairs of basic clusters, and try to build a metacluster by taking the most strongly connected pair first.Note that in this process we may assign one basic cluster to several different metaclusters.When a link is found between two basic clusters that were already assigned to a metacluster, we try to put them into all the existing metaclusters it belongs to.However, we allow a basic cluster to be added only if it can fill all the columns in that table.In other words, the first two basic clusters (i.e. an initial two-row table) determines its columns and therefore define the relation of that table.We used twelve newspapers published mainly in the U.S. We collected their articles over two months (from Sep. 21, 2005 - Nov. 27, 2005).We obtained 643,767 basic patterns and 7,990 unique types.Then we applied metaclustering to these basic clusters and obtained 302 metaclusters (tables).We then removed duplicated rows and took only the tables that had 3 or more rows.Finally we had 101 tables.The total number the of articles and clusters we used are shown in Table 2.We evaluated the obtained tables as follows.For each row in a table, we added a summary of the source articles that were used to extract the relation.Then for each table, an evaluator looks into every row and its source article, and tries to come up with a sentence that explains the relation among its columns.The description should be as specific as possible.If at least half of the rows can fit the explanation, the table is considered “consistent.” For each consistent table, the evaluator wrote down the sentence using variable names ($1, $2, ...) to refer to its columns.Finally, we counted the number of consistent tables.We also counted how many rows in each table can fit the explanation.We evaluated 48 randomly chosen tables.Among these tables, we found that 36 tables were consistent.We also counted the total number of rows that fit each description, shown in Table 3.Table 4 shows the descriptions of the selected tables.The largest consistent table was about hurricanes (Table 5).Although we cannot exactly measure the recall of each table, we tried to estimate the recall by comparing this hurricane table to a manually created one (Table 6).We found 6 out of 9 hurricanes 3.It is worth noting that most of these hurricane names were automatically disambiguated although our NE tagger didn’t distinguish a hurricane name from a person ber of fitted/total rows. name.The second largest table (about nominations of officials) is shown in Table 7.We reviewed 10 incorrect rows from various tables and found 4 of them were due to coreference errors and one error was due to a parse error.The other 4 errors were due to multiple basic patterns distant from each other that happened to refer to a different event reported in the same cluster.The causes of the one remaining error was obscure.Most inconsistent tables were a mixture of multiple relations and some of their rows still looked consistent.We have a couple of open questions.First, the overall recall of our system might be lower than existing IE systems, as we are relying on a cluster of comparable articles rather than a single document to discover an event.We might be able to improve this in the future by adjusting the basic clustering algorithm or weighting schema of basic patterns.Secondly, some combinations of basic patterns looked inherently vague.For example, we used the two basic patterns “pitched” and “’s-series” in the following sentence (the patterns are underlined): Ervin Santana pitched 5 1-3 gutsy innings in his postseason debut for the Angels, Adam Kennedy hit a goahead triple that sent Yankees outfielders crashing to the ground, and Los Angeles beat New York 5-3 Monday night in the decisive Game 5 of their AL playoff series.It is not clear whether this set of patterns can yield any meaningful relation.We are not sure how much this sort of table can affect overall IE performance.In this paper we proposed Preemptive Information Extraction as a new direction of IE research.As its key technique, we presented Unrestricted Relation Discovery that tries to find parallel correspondences between multiple entities in a document, and perform clustering using basic patterns as features.To increase the number of basic patterns, we used a cluster of comparable articles instead of a single document.We presented the implementation of our preliminary system and its outputs.We obtained dozens of usable tables.Sep. and Nov. (from Wikipedia).Rows with a star (*) were actually extracted.The number of the source articles that contained a mention of the hurricane is shown in the right column.This research was supported by the National Science Foundation under Grant IIS-00325657.This paper does not necessarily reflect the position of the U.S. Government.We would like to thank Prof. Ralph Grishman who provided useful suggestions and discussions.
Extracting Parallel Sentences from Comparable Corpora using Document Level AlignmentThe quality of a statistical machine translation (SMT) system is heavily dependent upon the amount of parallel sentences used in training.In recent years, there have been several approaches developed for obtaining parallel sentences from non-parallel, or comparable data, such as news articles published within the same time period (Munteanu and Marcu, 2005), or web pages with a similar structure (Resnik and Smith, 2003).One resource not yet thoroughly explored is Wikipedia, an online encyclopedia containing linked articles in many languages.We advance the state of the art in parallel sentence extraction by modeling the document level alignment, motivated by the observation that parallel sentence pairs are often found in close proximity.We also include features which make use of the additional annotation given by Wikipedia, and features using an automatically induced lexicon model.Results for both accuracy in sentence extraction and downstream improvement in an SMT system are presented.For any statistical machine translation system, the size of the parallel corpus used for training is a major factor in its performance.For some language pairs, such as Chinese-English and Arabic-English, large amounts of parallel data are readily available, but for most language pairs this is not the case.The domain of the parallel corpus also strongly influences the quality of translations produced.Many parallel corpora are taken from the news domain, or from parliamentary proceedings.Translation quality suffers when a system is not trained on any data from the domain it is tested on.While parallel corpora may be scarce, comparable, or semi-parallel corpora are readily available in several domains and language pairs.These corpora consist of a set of documents in two languages containing similar information.(See Section 2.1 for a more detailed description of the types of nonparallel corpora.)In most previous work on extraction of parallel sentences from comparable corpora, some coarse document-level similarity is used to determine which document pairs contain parallel sentences.For identifying similar web pages, Resnik and Smith (2003) compare the HTML structure.Munteanu and Marcu (2005) use publication date and vector-based similarity (after projecting words through a bilingual dictionary) to identify similar news articles.Once promising document pairs are identified, the next step is to extract parallel sentences.Usually, some seed parallel data is assumed to be available.This data is used to train a word alignment model, such as IBM Model 1 (Brown et al., 1993) or HMM-based word alignment (Vogel et al., 1996).Statistics from this word alignment model are used to train a classifier which identifies bilingual sentence pairs as parallel or not parallel.This classifier is applied to all sentence pairs in documents which were found to be similar.Typically, some pruning is done to reduce the number of sentence pairs that need to be classified.While these methods have been applied to news corpora and web pages, very little attention has been given to Wikipedia as a source of parallel sentences.This is surprising, given that Wikipedia contains annotated article alignments, and much work has been done on extracting bilingual lexicons on this dataset.Adafre and de Rijke (2006) extracted similar sentences from Wikipedia article pairs, but only evaluated precision on a small number of extracted sentences.In this paper, we more thoroughly investigate Wikipedia’s viability as a comparable corpus, and describe novel methods for parallel sentence extraction.Section 2 describes the multilingual resources available in Wikipedia.Section 3 gives further background on previous methods for parallel sentence extraction on comparable corpora, and describes our approach, which finds a global sentence alignment between two documents.In Section 4, we compare our approach with previous methods on datasets derived from Wikipedia for three language pairs (Spanish-English, German-English, and Bulgarian-English), and show improvements in downstream SMT performance by adding the parallel data we extracted.2 Wikipedia as a Comparable Corpus Wikipedia (Wikipedia, 2004) is an online collaborative encyclopedia available in a wide variety of languages.While the English Wikipedia is the largest, with over 3 million articles, there are 24 language editions with at least 100,000 articles.Articles on the same topic in different languages are also connected via “interwiki” links, which are annotated by users.This is an extremely valuable resource when extracting parallel sentences, as the document alignment is already provided.Table 1 shows how many of these “interwiki” links are present between the English Wikipedia and the 16 largest non-English Wikipedias.Wikipedia’s markup contains other useful indicators for parallel sentence extraction.The many hyperlinks found in articles have previously been used as a valuable source of information.(Adafre and de Rijke, 2006) use matching hyperlinks to identify similar sentences.Two links match if the articles they refer to are connected by an “interwiki” link.Also, images in Wikipedia are often stored in a central source across different languages; this allows identification of captions which may be parallel (see Figure 1).Finally, there are other minor forms of markup which may be useful for finding similar content across languages, such as lists and section headings.In Section 3.3, we will explain how features are derived from this markup.Fung and Cheung (2004) give a more fine-grained description of the types of non-parallel corpora, which we will briefly summarize.A noisy parallel corpus has documents which contain many parallel sentences in roughly the same order.Comparable corpora contain topic aligned documents which are not translations of each other.The corpora Fung and Cheung (2004) examine are quasi-comparable: they contain bilingual documents which are not necessarily on the same topic.Wikipedia is a special case, since the aligned article pairs may range from being almost completely parallel (e.g., the Spanish and English entries for “Antiparticle”) to containing almost no parallel sentences (the Spanish and English entries for “John Calvin”), despite being topic-aligned.It is best characterized as a mix of noisy parallel and comparable article pairs.Some Wikipedia authors will translate articles from another language; others write the content themselves.Furthermore, even articles created through translations may later diverge due to independent edits in either language.In this section, we will focus on methods for extracting parallel sentences from aligned, comparable documents.The related problem of automatic document alignment in news and web corpora has been explored by a number of researchers, including Resnik and Smith (2003), Munteanu and Marcu (2005), Tillmann and Xu (2009), and Tillmann (2009).Since our corpus already contains document alignments, we sidestep this problem, and will not discuss further details of this issue.That said, we believe that our methods will be effective in corpora without document alignments when combined with one of the aforementioned algorithms.Much of the previous work involves building a binary classifier for sentence pairs to determine whether or not they are parallel (Munteanu and Marcu, 2005; Tillmann, 2009).The training data usually comes from a standard parallel corpus.There is a substantial class imbalance (O(n) positive examples, and O(n2) negative examples), and various heuristics are used to mitigate this problem.Munteanu and Marcu (2005) filter out negative examples with high length difference or low word overlap (based on a bilingual dictionary).We propose an alternative approach: we learn a ranking model, which, for each sentence in the source document, selects either a sentence in the target document that it is parallel to, or “null”.This formulation of the problem avoids the class imbalance issue of the binary classifier.In both the binary classifier approach and the ranking approach, we use a Maximum Entropy classifier, following Munteanu and Marcu (2005).In Wikipedia article pairs, it is common for parallel sentences to occur in clusters.A global sentence alignment model is able to capture this phenomenon.For both parallel and comparable corpora, global sentence alignments have been used, though the alignments were monotonic (Gale and Church, 1991; Moore, 2002; Zhao and Vogel, 2002).Our model is a first order linear chain Conditional Random Field (CRF) (Lafferty et al., 2001).The set of source and target sentences are observed.For each source sentence, we have a hidden variable indicating the corresponding target sentence to which it is aligned (or null).The model is similar to the discriminative CRF-based word alignment model of (Blunsom and Cohn, 2006).Our features can be grouped into four categories.Features derived from word alignments We use a feature set inspired by (Munteanu and Marcu, 2005), who defined features primarily based on IBM Model 1 alignments (Brown et al., 1993).We also use HMM word alignments (Vogel et al., 1996) in both directions (source to target and target to source), and extract the following features based on these four alignments:1 We also define two more features which are independent of word alignment models.One is a sentence length feature taken from (Moore, 2002), which models the length ratio between the source and target sentences with a Poisson distribution.The other feature is the difference in relative document position of the two sentences, capturing the idea that the aligned articles have a similar topic progression.The above features are all defined on sentence pairs, and are included in the binary classifier and ranking model.In the sequence model, we use additional distortion features, which only look at the difference between the position of the previous and current aligned sentences.One set of features bins these distances; another looks at the absolute difference between the expected position (one after the previous aligned sentence) and the actual position.Features derived from Wikipedia markup Three features are derived from Wikipedia’s markup.The first is the number of matching links in the sentence pair.The links are weighted by their inverse frequency in the document, so a link that appears often does not contribute much to this feature’s value.The image feature fires whenever two sentences are captions of the same image, and the list feature fires when two sentences are both items in a list.These last two indicator features fire with a negative value when the feature matches on one sentence and not the other.None of the above features fire on a null alignment, in either the ranker or CRF.There is also a bias feature for these two models, which fires on all non-null alignments.Word-level induced lexicon features A common problem with approaches for parallel sentence classification, which rely heavily on alignment models trained from unrelated corpora, is low recall due to unknown words in the candidate sentence-pairs.One approach that begins to address this problem is the use of self-training, as in (Munteanu and Marcu, 2005).However, a selftrained sentence pair extraction system is only able to acquire new lexical items that occur in parallel sentences.Within Wikipedia, many linked article pairs do not contain any parallel sentences, yet contain many words and phrases that are good translations of each other.In this paper we explore an alternative approach to lexicon acquisition for use in parallel sentence extraction.We build a lexicon model using an approach similar to ones developed for unsupervised lexicon induction from monolingual or comparable corpora (Rapp, 1999; Koehn and Knight, 2002; Haghighi et al., 2008).We briefly describe the lexicon model and its use in sentence-extraction.The lexicon model is based on a probabilistic model P(wt|ws, T, 5) where wt is a word in the target language, ws is a word in the source language, and T and 5 are linked articles in the target and source languages, respectively.We train this model similarly to the sentenceextraction ranking model, with the difference that we are aligning word pairs and not sentence pairs.The model is trained from a small set of annotated Wikipedia article pairs, where for some words in the source language we have marked one or more words as corresponding to the source word (in the context of the article pair), or have indicated that the source word does not have a corresponding translation in the target article.The word-level annotated articles are disjoint from the sentence-aligned articles described in Section 4.The following features are used in the lexicon model: Translation probability.This is the translation probability p(wtlws) from the HMM word alignment model trained on the seed parallel data.We also use the probability in the other direction, as well as the log-probabilities in the two directions.Position difference.This is the absolute value of the difference in relative position of words ws and wt in the articles 5 and T. Orthographic similarity.This is a function of the edit distance between source and target words.The edit distance between words written in different alphabets is computed by first performing a deterministic phonetic translation of the words to a common alphabet.The translation is inexact and this is a promising area for improvement.A similar source of information has been used to create seed lexicons in (Koehn and Knight, 2002) and as part of the feature space in (Haghighi et al., 2008).Context translation probability.This feature looks at all words occurring next to word ws in the article S and next to wt in the article T in a local context window (we used one word to the left and one word to the right), and computes several scoring functions measuring the translation correspondence between the contexts (using the IBM Model 1 trained from seed parallel data).This feature is similar to distributional similarity measures used in previous work, with the difference that it is limited to contexts of words within a linked article pair.Distributional similarity.This feature corresponds more closely to context similarity measures used in previous work on lexicon induction.For each source headword ws, we collect a distribution over context positions o ∈ {−2, −1, +1, +2} and context words vs in those positions based on a count of times a context word occurred at that offset from a headword: P(o, vs|ws) ∝ weight(o) · Qws, o, vs).Adjacent positions −1 and +1 have a weight of 2; other positions have a weight of 1.Likewise we gather a distribution over target words and contexts for each target headword P(o, vt|wt).Using an IBM Model 1 word translation table P(vt|vs) estimated on the seed parallel corpus, we estimate a cross-lingual context distribution as fine the similarity of a words ws and wt as one minus the Jensen-Shannon divergence of the distributions over positions and target words.2 Given this small set of feature functions, we train the weights of a log-linear ranking model for P(wt|ws, T, S), based on the word-level annotated Wikipedia article pairs.After a model is trained, we generate a new translation table Pl,(t|s) which is defined as Pl,,(t|s) ∝ EtET,sCS P(t|s,T,S).The summation is over occurrences of the source and target word in linked Wikipedia articles.This new translation table is used to define another HMM word-alignment model (together with distortion probabilities trained from parallel data) for use in the sentence extraction models.Two copies of each feature using the HMM word alignment model are generated: one using the seed data HMM 2We restrict our attention to words with ten or more occurrences, since rare words have poorly estimated distributions.Also we discard the contribution from any context position and word pair that relates to more than 1,000 distinct source or target words, since it explodes the computational overhead and has little impact on the final similarity score. model, and another using this new HMM model.The training data for Bulgarian consisted of two partially annotated Wikipedia article pairs.For German and Spanish we used the feature weights of the model trained on Bulgarian, because we did not have word-level annotated Wikipedia articles.We annotated twenty Wikipedia article pairs for three language pairs: Spanish-English, BulgarianEnglish, and German-English.Each sentence in the source language was annotated with possible parallel sentences in the target language (the target language was English in all experiments).The pairs were annotated with a quality level: 1 if the sentences contained some parallel fragments, 2 if the sentences were mostly parallel with some missing words, and 3 if the sentences appeared to be direct translations.In all experiments, sentence pairs with quality 2 or 3 were taken as positive examples.The resulting datasets are available at http://research.microsoft.com/enus/people/chrisq/wikidownload.aspx.For our seed parallel data, we used the Europarl corpus (Koehn, 2005) for Spanish and German and the JRC-Aquis corpus for Bulgarian, plus the article titles for parallel Wikipedia documents, and translations available from Wiktionary entries.3 Using 5-fold cross-validation on the 20 document pairs for each language condition, we compared the binary classifier, ranker, and CRF models for parallel sentence extraction.To tune for precision/recall, we used minimum Bayes risk decoding.We define the loss L(T, µ) of picking target sentence T when the correct target sentence is µ as 0 if r = µ, A if 'r = NULL and µ =6 NULL, and 1 otherwise.By modifying the null loss A, the precision/recall trade-off can be adjusted.For the CRF model, we used posterior decoding to make the minimum risk decision rule tractable.As a summary measure of the performance of the models at different levels of recall we use average precision as defined in (Ido et al., 2006).We also report recall at precision of 90 and 80 percent.Table 2 compares the different models in all three language pairs.In our next set of experiments, we looked at the effects of the Wikipedia specific features.Since the ranker and CRF are asymmetric models, we also experimented with running the models in both directions and combining their outputs by intersection.These results are shown in Table 3.Identifying the agreement between two asymmetric models is a commonly exploited trick elsewhere in machine translation.It is mostly effective here as well, improving all cases except for the Bulgarian-English CRF where the regression is slight.More successful are the Wikipedia features, which provide an auxiliary signal of potential parallelism.The gains from adding the lexicon-based features can be dramatic as in the case of Bulgarian (the CRF model average precision increased by nearly 9 points).The lower gains on Spanish and German may be due in part to the lack of language-specific training data.These results are very promising and motivate further exploration.We also note that this is perhaps the first successful practical application of an automatically induced word translation lexicon.We also present results in the context of a full machine translation system to evaluate the potential utility of this data.A standard phrasal SMT system (Koehn et al., 2003) serves as our testbed, using a conventional set of models: phrasal models of source given target and target given source; lexical weighting models in both directions, language model, word count, phrase count, distortion penalty, and a lexicalized reordering model.Given that the extracted Wikipedia data takes the standard form of parallel sentences, it would be easy to exploit this same data in a number of systems.For each language pair we explored two training conditions.The “Medium” data condition used easily downloadable corpora: Europarl for GermanEnglish and Spanish-English, and JRC/Acquis for Bulgarian-English.Additionally we included titles of all linked Wikipedia articles as parallel sentences in the medium data condition.The “Large” data condition includes all the medium data, and also includes using a broad range of available sources such as data scraped from the web (Resnik and Smith, 2003), data from the United Nations, phrase books, software documentation, and more.In each condition, we explored the impact of including additional parallel sentences automatically extracted from Wikipedia in the system training data.For German-English and Spanish-English, we extracted data with the null loss adjusted to achieve an estimated precision of 95 percent, and for English-Bulgarian a precision of 90 percent.Table 4 summarizes the characteristics of these data sets.We were pleasantly surprised at the amount of parallel sentences extracted from such a varied comparable corpus.Apparently the average Wikipedia article contains at least a handful of parallel sentences, suggesting this is a very fertile ground for training MT systems.The extracted Wikipedia data is likely to make the greatest impact on broad domain test sets – indeed, initial experimentation showed little BLEU gain on in-domain test sets such as Europarl, where out-of-domain training data is unlikely to provide appropriate phrasal translations.Therefore, we experimented with two broad domain test sets.First, Bing Translator provided a sample of translation requests along with translations in GermanEnglish and Spanish-English, which acted our standard development and test set.Unfortunately no such tagged set was available in Bulgarian-English, so we held out a portion of the large system’s training data to use for development and test.In each language pair, the test set was split into a development portion (“Dev A”) used for minimum error rate training (Och, 2003) and a test set (“Test A”) used for final evaluation.Second, we created new test sets in each of the three language pairs by sampling parallel sentences from held out Wikipedia articles.To ensure that this test data was clean, we manually filtered the sentence pairs that were not truly parallel and edited them as necessary to improve adequacy.We called this “Wikitest”.This test set is available at http://research.microsoft.com/enus/people/chrisq/wikidownload.aspx.Characteristics of these test sets are summarized in Table 5.We evaluated the resulting systems using BLEU4 (Papineni et al., 2002); the results are presented in Table 6.First we note that the extracted Wikipedia data are very helpful in medium data conditions, significantly improving translation performance in all conditions.Furthermore we found that the extracted Wikipedia sentences substantially improved translation quality on held-out Wikipedia articles.In every case, training on medium data plus Wikipedia extracts led to equal or better translation quality than the large system alone.Furthermore, adding the Wikipedia data to the large data condition still made substantial improvements.Our first substantial contribution is to demonstrate that Wikipedia is a useful resource for mining parallel data.The sheer volume of extracted parallel sentences within Wikipedia is a somewhat surprising result in the light of Wikipedia’s construction.We are also releasing several valuable resources to the community to facilitate further research: manually aligned document pairs, and an edited test set.Hopefully this will encourage research into Wikipedia as a resource for machine translation.Secondly, we improve on prior pairwise models by introducing a ranking approach for sentence pair extraction.This ranking approach sidesteps the problematic class imbalance issue, resulting in improved average precision while retaining simplicity and clarity in the models.Also by modeling the sentence alignment of the articles globally, we were able to show a substantial improvement in task accuracy.Furthermore a small sample of annotated articles is sufficient to train these global level features, and the learned classifiers appear very portable across languages.It is difficult to say whether such improvement will carry over to other comparable corpora with less document structure and meta-data.We plan to address this question in future work.Finally, initial investigations have shown that substantial gains can be achieved by using an induced word-level lexicon in combination with sentence extraction.This helps address modeling word pairs that are out-of-vocabulary with respect to the seed parallel lexicon, while avoiding some of the issues in bootstrapping.
Cross-lingual Word Clusters for Direct Transfer of Linguistic StructureIt has been established that incorporating word cluster features derived from large unlabeled corpora can significantly improve prediction of linguistic structure.While previous work has focused primarily on English, we extend these results to other languages along two dimensions.First, we show that these results hold true for a number of languages across families.Second, and more interestingly, we provide an algorithm for inducing cross-lingual clusters and we show that features derived from these clusters significantly improve the accuracy of cross-lingual structure prediction.Specifically, we show that by augmenting direct-transfer systems with cross-lingual cluster features, the relative error of delexicalized dependency parsers, trained on English treebanks and transferred to foreign languages, can be reduced by up to 13%.When applying the same method to direct transfer of named-entity recognizers, we observe relative improvements of up to 26%.The ability to predict the linguistic structure of sentences or documents is central to the field of natural language processing (NLP).Structures such as named-entity tag sequences (Bikel et al., 1999) or sentiment relations (Pang and Lee, 2008) are inherently useful in data mining, information retrieval and other user-facing technologies.More fundamental structures such as part-of-speech tag sequences (Ratnaparkhi, 1996) or syntactic parse trees (Collins, 1997; K¨ubler et al., 2009), on the other hand, comprise the core linguistic analysis for many important downstream tasks such as machine translation (Chiang, * The majority of this work was performed while the author was an intern at Google, New York, NY.2005; Collins et al., 2005).Currently, supervised data-driven methods dominate the literature on linguistic structure prediction (Smith, 2011).Regrettably, the majority of studies on these methods have focused on evaluations specific to English, since it is the language with the most annotated resources.Notable exceptions include the CoNLL shared tasks (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003; Buchholz and Marsi, 2006; Nivre et al., 2007) and subsequent studies on this data, as well as a number of focused studies on one or two specific languages, as discussed by Bender (2011).While annotated resources for parsing and several other tasks are available in a number of languages, we cannot expect to have access to labeled resources for all tasks in all languages.This fact has given rise to a large body of research on unsupervised (Klein and Manning, 2004), semi-supervised (Koo et al., 2008) and transfer (Hwa et al., 2005) systems for prediction of linguistic structure.These methods all attempt to benefit from the plethora of unlabeled monolingual and/or cross-lingual data that has become available in the digital age.Unsupervised methods are appealing in that they are often inherently language independent.This is borne out by the many recent studies on unsupervised parsing that include evaluations covering a number of languages (Cohen and Smith, 2009; Gillenwater et al., 2010; Naseem et al., 2010; Spitkovsky et al., 2011).However, the performance for most languages is still well below that of supervised systems and recent work has established that the performance is also below simple methods of linguistic transfer (McDonald et al., 2011).In this study we focus on semi-supervised and linguistic-transfer methods for multilingual structure prediction.In particular, we pursue two lines of research around the use of word cluster features in discriminative models for structure prediction: guages for dependency parsing and 4 languages for named-entity recognition (NER).This is the first study with such a broad view on this subject, in terms of language diversity.2.Cross-lingual word cluster features for transferring linguistic structure from English to other languages.We develop an algorithm that generates cross-lingual word clusters; that is clusters of words that are consistent across languages.This is achieved by means of a probabilistic model over large amounts of monolingual data in two languages, coupled with parallel data through which cross-lingual word-cluster constraints are enforced.We show that by augmenting the delexicalized direct transfer system of McDonald et al. (2011) with cross-lingual cluster features, we are able to reduce its error by up to 13% relative.Further, we show that by applying the same method to direct-transfer NER, we achieve a relative error reduction of 26%.In line with much previous work on word clusters for tasks such as dependency parsing and NER, for which local syntactic and semantic constraints are of importance, we induce word clusters by means of a probabilistic class-based language model (Brown et al., 1992; Clark, 2003).However, rather than the more commonly used model of Brown et al. (1992), we use the predictive class bigram model introduced by Uszkoreit and Brants (2008).The two models are very similar, but whereas the former takes classto-class transitions into account, the latter directly models word-to-class transitions.By ignoring classto-class transitions, an approximate maximum likelihood clustering can be found efficiently with the distributed exchange algorithm (Uszkoreit and Brants, 2008).This is a useful property, as we later develop an algorithm for inducing cross-lingual word clusters that calls this monolingual algorithm as a subroutine.More formally, let C : V H 1, ... , K be a (hard) clustering function that maps each word type from the vocabulary, V, to one of K cluster identities.With the model of Uszkoreit and Brants (2008), the likelihood of a sequence of word tokens, w = (wi)mi=1, with wi E V U {S}, where S is a designated start-ofsegment symbol, factors as Compare this to the model of Brown et al. (1992): By incorporating cross-lingual cluster features in a m linguistic transfer system, we are for the first time L'(w; C) = P(wi|C(wi))P(C(wi)|C(wi−1)) . combining SSL and cross-lingual transfer. i=1Word cluster features have been shown to be useful in various tasks in natural language processing, including syntactic dependency parsing (Koo et al., 2008; Haffari et al., 2011; Tratz and Hovy, 2011), syntactic chunking (Turian et al., 2010), and NER (Freitag, 2004; Miller et al., 2004; Turian et al., 2010; Faruqui and Pad´o, 2010).Intuitively, the reason for the effectiveness of cluster features lie in their ability to aggregate local distributional information from large unlabeled corpora, which aid in conquering data sparsity in supervised training regimes as well as in mitigating cross-domain generalization issues.While the use of class-to-class transitions can lead to more compact models, which is often useful for conquering data sparsity, when clustering large data sets we can get reliable statistics directly on the wordto-class transitions (Uszkoreit and Brants, 2008).In addition to the clustering model that we make use of in this study, a number of additional word clustering and embedding variants have been proposed.For example, Turian et al. (2010) assessed the effectiveness of the word embedding techniques of Collobert and Weston (2008) and Mnih and Hinton (2007) along with the word clustering technique of Brown et al.(1992) for syntactic chunking and NER.Recently, Dhillon et al. (2011) proposed a word embedding method based on canonical correlation analysis that provides state-of-the art results for wordbased SSL for English NER.As an alternative to clustering words, Lin and Wu (2009) proposed a phrase clustering approach that obtained the state-of-the-art result for English NER.Before moving on to the multilingual setting, we conduct a set of monolingual experiments where we evaluate the use of the monolingual word clusters just described as features for dependency parsing and NER.In the parsing experiments, we study the following thirteen languages:1 Danish (DA), German (DE), Greek (EL), English (EN), Spanish (ES), French (FR), Italian (IT), Korean (KO), Dutch (NL), Portugese (PT), Russian (RU), Swedish (SV) and Chinese (ZH) – representing the Chinese, Germanic, Hellenic, Romance, Slavic, Altaic and Korean genera.In the NER experiments, we study three Germanic languages: German (DE), English (EN) and Dutch (NL); and one Romance language: Spanish (ES).Details of the labeled and unlabeled data sets used are given in Appendix A.For all experiments we fixed the number of clusters to 256 as this performed well on held-out data.Furthermore, we only clustered the 1 million most frequent word types in each language for both efficiency and sparsity reasons.For languages in which our unlabeled data did not have at least 1 million types, we considered all types.All of the parsing experiments reported in this study are based on the transition-based dependency parsing paradigm (Nivre, 2008).For all languages and settings, we use an arc-eager decoding strategy, with a beam of eight hypotheses, and perform ten epochs of the averaged structured perceptron algorithm (Zhang and Clark, 2008).We extend the state-of-the-art feature model recently introduced by Zhang and Nivre (2011) by adding an additional word cluster based feature template for each word based template.Additionally, we add templates where one or more partof-speech feature is replaced with the corresponding cluster feature.The resulting set of additional feature templates are shown in Table 1.The expanded feature model includes all of the feature templates defined by Zhang and Nivre (2011), which we also use as the baseline model, whereas Table 1 only shows our new templates due to space limitations.For all NER experiments, we use a sequential firstorder conditional random field (CRF) with a unit variance Normal prior, trained with L-BFGS until c-convergence (c = 0.0001, typically obtained after less than 400 iterations).The feature model used for the NER tagger is shown in Table 2.These are similar to the features used by Turian et al. (2010), with the main difference that we do not use any long range features and that we add templates that conjoin adjacent clusters and adjacent tags as well as templates that conjoin label transitions with tags, clusters and capitalization features.The results of the parsing experiments, measured with labeled accuracy score (LAS) on all sentence lengths, excluding punctuation, are shown in Table 3.The baselines are all comparable to the state-of-theart.On average, the addition of word cluster features yields a 6% relative reduction in error and upwards of 15% (for RU).All languages improve except FR, which sees neither an increase nor a decrease in LAS.We observe an average absolute increase in LAS of approximately 1%, which is inline with previous observations (Koo et al., 2008).It is perhaps not surprising that RU sees a large gain as it is a highly inflected language, making observations of lexical features far more sparse.Some languages, e.g., FR, NL, and ZH see much smaller gains.One likely culprit is a divergence between the tokenization schemes used in the treebank and in our unlabeled data, which for Indo-European languages is closely related to the Penn Treebank tokenization.For example, the NL treebank contains many multi-word tokens that are typically broken apart by our automatic tokenizer.The NER results, in terms of F1 measure, are listed in Table 4.Introducing word cluster features for NER reduces relative errors on the test set by 21% (39% on the development set) on average.Broken down per language, reductions on the test set vary from substantial for NL (30%) and EN (26%), down to more modest for DE (17%) and ES (12%).The addition of cluster features most markedly improve recognition of the PER category, with an average error reduction on the test set of 44%, while the reductions for ORG (19%), LOC (17%) and MISC (10%) are more modest, but still significant.Although our results are below the best reported results for EN and DE (Lin and Wu, 2009; Faruqui and Pad´o, 2010), the relative improvements of adding word clusters are inline with previous results on NER for EN (Miller et al., 2004; Turian et al., 2010), who report error reductions of approximately 25% from adding word cluster features.Slightly higher reductions where achieved for DE by Faruqui and Pad´o (2010), who report a reduction of 22%.Note that we did not tune hyper-parameters of the supervised learning methods and of the clustering method, such as the number of clusters (Turian et al., 2010; Faruqui and Pad´o, 2010), and that we did not apply any heuristic for data cleaning such as that used by Turian et al. (2010).All results of the previous section rely on the availability of large quantities of language specific annotations for each task.Cross-lingual transfer methods and unsupervised methods have recently been shown to hold promise as a way to at least partially sidestep the demand for labeled data.Unsupervised methods attempt to infer linguistic structure without using any annotated data (Klein and Manning, 2004) or possibly by using a set of linguistically motivated rules (Naseem et al., 2010) or a linguistically informed model structure (Berg-Kirkpatrick and Klein, 2010).The aim of transfer methods is instead to use knowledge induced from labeled resources in one or more source languages to construct systems for target languages in which no or few such resources are available (Hwa et al., 2005).Currently, the performance of even the most simple direct transfer systems far exceeds that of unsupervised systems (Cohen et al., 2011; McDonald et al., 2011; Søgaard, 2011).Our starting point is the delexicalized direct transfer method proposed by McDonald et al. (2011) based on work by Zeman and Resnik (2008).This method was shown to outperform a number of state-of-the-art unsupervised and transfer-based baselines.The method is simple; for a given training set, the learner ignores all lexical identities and only observes features over other characteristics, e.g., part-of-speech tags, orthographic features, direction of syntactic attachment, etc.The learner builds a model from an annotated source language data set, after which the model is used to directly make target language predictions.There are three basic assumptions that drive this approach.First, that high-level tasks, such as syntactic parsing, can be learned reliably using coarse-grained statistics, such as part-of-speech tags, in place of fine-grained statistics such as lexical word identities.Second, that the parameters of features over coarsegrained statistics are in some sense language independent, e.g., a feature that indicates that adjectives modify their closest noun is useful in all languages.Third, that these coarse-grained statistics are robustly available across languages.The approach proposed by McDonald et al. (2011) relies on these three assumptions.Specifically, by replacing fine-grained language specific part-of-speech tags with universal part-of-speech tags, generated with the method described by Das and Petrov (2011), a universal parser is achieved that can be applied to any language for which universal part-of-speech tags are available.Below, we extend this approach to universal parsing by adding cross-lingual word cluster features.A cross-lingual word clustering is a clustering of words in two languages, in which the clusters correspond to some meaningful cross-lingual property.For example, prepositions from both languages should be in the same cluster, proper names from both languages in another cluster and so on.By adding features defined over these clusters, we can, to some degree, re-lexicalize the delexicalized models, while maintaining the “universality” of the features.This approach is outlined in Figure 1.Assuming that we have an algorithm for generating cross-lingual word clusters (see Section 4.2), we can augment the delexicalized parsing algorithm to use these word cluster features at training and testing time.In order to further motivate the proposed approach, consider the accuracy of the supervised English parser.A parser with lexical, part-of-speech and cluster features achieves 90.7% LAS (see Table 3).If we remove all lexical and cluster features, the same parser achieves 83.1%.However, if we add back just the cluster features, the accuracy jumps back up to 89.5%, which is only 1.2% below the full system.Thus, if we can accurately learn cross-lingual clusters, there is hope of regaining some of the accuracy lost due to the delexicalization process.Our first method for inducing cross-lingual clusters has two stages.First, it clusters a source language (S) as in the monolingual case, and then projects these clusters to a target language (T), using word alignments.Given two aligned word sequences set of scored alignments from the source language to the target language, where (wTj , wSaj, sj,aj) ∈ AT |S is an alignment from the ajth source word to the jth target word, with score sj,aj ≥ δ.2 We use the shorthand j ∈ AT |S to denote those target words wTj that are aligned to some source word wSaj.Provided a clustering CS, we assign the target word t ∈ VT to the cluster with which it is most often aligned: where [·� is the indicator function.We refer to the cross-lingual clusters induced in this way as PROJECTED CLUSTERS.This simple projection approach has two potential drawbacks.First, it only provides a clustering of those target language words that occur in the word 2In our case, the alignment score corresponds to the conditional alignment probability p(wTj |wSa� ).All E-alignments are ignored and we use S = 0.95 throughout. aligned data, which is typically smaller than our monolingual data sets.Second, the mapped clustering may not necessarily correspond to an acceptable target language clustering in terms of monolingual likelihood.In order to tackle these issues, we propose the following more complex model.First, to find clusterings that are good according to both the source and target language, and to make use of more unlabeled data, we model word sequences in each language by the monolingual language model with likelihood function defined by equation (1).Denote these likelihood functions respectively by LS(wS; CS) and LT (wT; CT), where we have overloaded notation so that the word sequences denoted by wS and wT include much more plentiful non-aligned data when taken as an argument of the monolingual likelihood functions.Second, we couple the clusterings defined by these individual models, by introducing additional factors based on word alignments, as proposed by Och (1999): and the symmetric LS|T(wS; AS|T, CS, CT).Note that the simple projection defined by equation (2) correspond to a hard assignment variant of this probabilistic formulation when the source clustering is fixed.Combining all four factors results in the joint monolingual and cross-lingual objective function The intuition of this approach is that the clusterings CS and CT are forced to jointly explain the source and target data, treating the word alignments as a form of soft constraints.We approximately optimize (3) with the alternating procedure in Algorithm 1, in which we iteratively maximize LS and LT , keeping the other factors fixed.In this way we can generate cross-lingual clusterings using all the monolingual data while forcing the clusterings to obey the word alignment constraints.We refer to the clusters induced with this method as X-LINGUAL CLUSTERS.In practice we found that each unconstrained monolingual run of the exchange algorithm (lines † Optimized via the exchange algorithm keeping the cluster of projected words fixed and only clustering additional words not in the projection.1 and 3) moves the clustering too far from those that obey the word alignment constraints, which causes the procedure to fail to converge.However, we found that fixing the clustering of the words that are assigned clusters in the projection stages (lines 2 and 4) and only clustering the remaining words works well in practice.Furthermore, we found that iterating the procedure has little effect on performance and set N = 1 for all subsequent experiments.In our first set of experiments on using cross-lingual cluster features, we evaluate direct transfer of our EN parser, trained on Stanford style dependencies (De Marneffe et al., 2006), to the the ten non-EN Indo-European languages listed in Section 3.We exclude KO and ZH as initial experiments proved direct transfer a poor technique when transferring parsers between such diverse languages.We study the impact of using cross-lingual cluster features by comparing the strong delexicalized baseline model of McDonald et al. (2011), which only has features derived from universal part-of-speech tags, projected from English with the method of Das and Petrov (2011), to the same model when adding features derived from cross-lingual clusters.In both cases the feature models are the same as those used in Section 3.1, except that they are delexicalized by removing all lexical word-identity features.We evaluate both the PROJECTED CLUSTERS and the X-LINGUAL CLUSTERS.For these experiments we train the perceptron for only five epochs in order to prevent over-fitting, which is an acute problem due to the divergence between the training and testing data sets in this setting.Furthermore, in accordance to standard practices we only evaluate unlabeled attachment score (UAS) due to the fact that each treebank uses a different – possibly non-overlapping – label set.In our second set of experiments, we evaluate direct transfer of a NER system trained on EN to DE, ES and NL.We use the same feature models as in the monolingual case, with the exception that we use universal part-of-speech tags for all languages and we remove the capitalization feature when transferring from EN to DE.Capitalization is both a prevalent and highly predictive feature of named-entities in EN, while in DE, capitalization is even more prevalent, but has very low predictive power.Interestingly, while delexicalization has shown to be important for direct transfer of dependency-parsers (McDonald et al., 2011), we noticed in preliminary experiments that it substantially degrades performance for NER.We hypothesize that this is because word features are predictive of common proper names and that these are often translated directly across languages, at least in the case of newswire text.As for the transfer parser, when training the source NER model, we regularize the model more heavily by setting Q = 0.1.Appendix A contains the details of the training, testing, unlabeled and parallel/aligned data sets.Table 5 lists the results of the transfer experiments for dependency parsing.The baseline results are comparable to those in McDonald et al. (2011) and thus also significantly outperform the results of recent unsupervised approaches (Berg-Kirkpatrick and Klein, 2010; Naseem et al., 2010).Importantly, crosslingual cluster features are helpful across the board and give a relative error reduction ranging from 3% for DA to 13% for PT, with an average reduction of 6%, in terms of unlabeled attachment score (UAS).This shows the utility of cross-lingual cluster features for syntactic transfer.However, X-LINGUAL CLUSTERS provides roughly the same performance as PROJECTED CLUSTERS suggesting that even simple methods of cross-lingual clustering are sufficient for direct transfer dependency parsing.We would like to stress that these results are likely to be under-estimating the parsers’ actual ability to predict Stanford-style dependencies in the target languages.This is because the target language annotations that we use for evaluation differ from the Stanford dependency annotation.Some of these differences are warranted in that certain target language phenomena are better captured by the native annotation.However, differences such as choice of lexical versus functional head are more arbitrary.To highlight this point we run two additional experiments.First, we had linguists, who were also fluent speakers of German, re-annotate the DE test set so that unlabeled arcs are consistent with Stanfordstyle dependencies.Using this data, NO CLUSTERS obtains 60.0% UAS, PROJECTED CLUSTERS 63.6% and X-LINGUAL CLUSTERS 64.4%.When compared to the scores on the original data set (48.9%, 50.3% and 50.7%, respectively) we can see that not only is the baseline system doing much better, but that the improvements from cross-lingual clustering are much more pronounced.Next, we investigated the accuracy of subject and object dependencies, as these are often annotated in similar ways across treebanks, typically modifying the main verb of the sentence.The bottom half of Table 5 gives the scores when restricted to such dependencies in the gold data.We measure the percentage of modifiers in subject and object dependencies that modify the correct word.Indeed, here we see the difference in performance become clearer, with the cross-lingual cluster model reducing errors by 14% relative to the non-cross-lingual model and upwards of 22% relative for IT.We now turn to the results of the transfer experiments for NER, listed in Table 6.While the performance of the transfer systems is very poor when no word clusters are used, adding cross-lingual word clusters give substantial improvements across all languages.The simple PROJECTED CLUSTERS work well, but the X-LINGUAL CLUSTERS provide even larger improvements.On average the latter reduce errors on the test set by 22% in terms of FI and up to 26% for ES.We also measure how well the direct transfer NER systems are able to detect entity boundaries (ignoring the entity categories).Here, on average, the best clusters provide a 24% relative error reduction on the test set (75.8 vs. 68.1 FI).To our knowledge there are no comparable results on transfer learning of NER systems.Based on the results of this first attempt at this scenario, we believe that transfer learning by multilingual word clusters could be developed into a practical way to construct NER systems for resource poor languages.In the first part of this study, we showed that word clusters induced from a simple class-based language model can be used to significantly improve on stateof-the-art supervised dependency parsing and NER for a wide range of languages and even across language families.Although the improvements vary between languages, the addition of word cluster features never has a negative impact on performance.This result has important practical consequences as it allows practitioners to simply plug in word cluster features into their current feature models.Given previous work on word clusters for various linguistic structure prediction tasks, these results are not too surprising.However, to our knowledge this is the first study to apply the same type of word cluster features across languages and tasks.In the second part, we provided two simple methods for inducing cross-lingual word clusters.The first method works by projecting word clusters, induced from monolingual data, from a source language to a target language directly via word alignments.The second method, on the other hand, makes use of monolingual data in both the source and the target language, together with word alignments that act as constraints on the joint clustering.We then showed that by using these cross-lingual word clusters, we can significantly improve on direct transfer of discriminative models for both parsing and NER.As in the monolingual case, both types of cross-lingual word cluster features yield improvements across the board, with the more complex method providing a significantly larger improvement for NER.Although the performance of transfer systems is still substantially below that of supervised systems, this research provides one step towards bridging this gap.Further, we believe that it opens up an avenue for future work on multilingual clustering methods, cross-lingual feature projection and domain adaptation for direct transfer of linguistic structure.We thank John DeNero for help with creating the word alignments; Reut Tsarfaty and Joakim Nivre for rewarding discussions on evaluation; Slav Petrov and Kuzman Ganchev for discussions on cross-lingual clustering; and the anonymous reviewers, along with Joakim Nivre, for valuable comments that helped improve the paper.The first author is grateful for the financial support of the Swedish National Graduate School of Language Technology (GSLT).In the parsing experiments, we use the following data sets.For DA, DE, EL, ES, IT, NL, PT and SV, we use the predefined training and evaluation data sets from the CoNLL 2006/2007 data sets (Buchholz and Marsi, 2006; Nivre et al., 2007).For EN we use sections 02-21, 22, and 23 of the Penn WSJ Treebank (Marcus et al., 1993) for training, development and evaluation.For FR we used the French Treebank (Abeill´e and Barrier, 2004) with splits defined in Candito et al. (2010).For KO we use the Sejong Korean Treebank (Han et al., 2002) randomly splitting the data into 80% training, 10% development and 10% evaluation.For RU we use the SynTagRus Treebank (Boguslavsky et al., 2000; Apresjan et al., 2006) randomly splitting the data into 80% training, 10% development and 10% evaluation.For ZH we use the Penn Chinese Treebank v6 (Xue et al., 2005) using the proposed data splits from the documentation.Both EN and ZH were converted to dependencies using v1.6.8 of the Stanford Converter (De Marneffe et al., 2006).FR was converted using the procedure defined in Candito et al. (2010).RU and KO are native dependency treebanks.For the CoNLL data sets we use the part-of-speech tags provided with the data.For all other data sets, we train a part-of-speech tagger on the training data in order to tag the development and evaluation data.For the NER experiments we use the training, development and evaluation data sets from the CoNLL 2002/2003 shared tasks (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) for all four languages (DE, EN, ES and NL).The data set for each language consists of newswire text annotated with four entity categories: Location (LOC), Miscellaneous (MISC), Organization (ORG) and Person (PER).We use the part-of-speech tags supplied with the data, except for ES where we instead use universal part-of-speech tags (Petrov et al., 2011).Unlabeled data for training the monolingual cluster models was extracted from one year of newswire articles from multiple sources from a news aggregation website.This consists of 0.8 billion (DA) to 121.6 billion (EN) tokens per language.All word alignments for the cross-lingual clusterings were produced with the dual decomposition aligner described by DeNero and Macherey (2011) using 10.5 million (DA) to 12.1 million (FR) sentences of aligned web data.
Improved Statistical Alignment ModelsIn this paper, we present and compare various single-word based alignment models for statistical machine translation.We discuss the five IBM alignment models, the Hidden- Markov alignment model, smoothing techniques and various modifications.We present different methods to combine alignments.As evaluation criterion we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment.We show that models with a first-order dependence and a fertility model lead to significantly better results than the simple models IBM-1 or IBM-2, which are not able to go beyond zero-order dependencies.In statistical machine translation we set up a statistical translation model Pr(fillef) which describes the relationship between a source language (SL) string f and a target language (TL) string ef.In (statistical) alignment models Pr(fil , aflef), a 'hidden' alignment a is introduced which describes a mapping from source word fi to a target word Ca,.We discuss here the IBM translation models IBM-1 to IBM-5 (Brown et al., 1993b) and the Hidden-Markov alignment model (Vogel et al., 1996; Och and Ney, 2000).The different alignment models we present provide different decompositions of Pr(fil ,41e{).An alignment â for which holds = arg max Pr(fil for a specific model is called Viterbi alignment of this model.So far, no well established evaluation criterion exists in the literature for these alignment models.For various reasons (nonunique reference translation, over-fitting and statistically deficient models) it seems hard to use training/test perplexity as in language modeling.Using translation quality is problematic, as translation quality is not well defined and as there are additional influences such as language model or decoder properties.We propose in this paper to measure the quality of an alignment model using the quality of the Viterbi alignment compared to a manually produced alignment.This allows an automatic evaluation, once a reference alignment has been produced.In addition, it results in a very precise and reliable evaluation criterion that is well suited to assess various design decisions in modeling and training of statistical alignment models.In this paper we use the models IBM-1 to IBM-5 from (Brown et al., 1993b) and the Hidden-Markov alignment model (HMM) from (Vogel et al., 1996; Och and Ney, 2000).All these models provide different decompositions of the probability Pr(fil ,41e{).The alignment a may contain alignments ai = 0 with the 'empty' word co to account for French words that are not aligned to any English word.All models include lexicon parameters p(f le) and additional parameters describing the probability of an alignment.We now sketch the structure of the six models: cient as they waste probability mass on non-strings.IBM-5 is a reformulation of IBM-4 with a suitably refined alignment model in order to avoid deficiency.So the main differences of these models lie in the alignment model (which may be zeroorder or first-order), in the existence of an explicit fertility model and whether the model is deficient or not.For HMM, IBM-4 and IBM-5 it is straightforward to extend the alignment parameters to include a dependence on the word classes of the words around the alignment position.In the HMM alignment model we allow for a dependence from the class E = C(ea,_,).Correspondingly, we can include similar dependencies on French and English word classes in IBM-4 and IBM-5 (Brown et al., 1993b).The classification of the words into a given number of classes (here: 50) is performed automatically by another statistical learning procedure (Kneser and Ney, 1991).The training of all alignment models is done by the EM-algorithm using a parallel training corpus (f(8), e(s)), s = 1, ...,S .In the Estep the counts for one sentence pair (f, e) are calculated.For the lexicon parameters the counts are: Correspondingly, the alignment and fertility probabilities can be estimated.The models IBM-1, IBM-2 and HMM have a particularly simple mathematical form so that the EM algorithm can be performed exactly, i.e. in the E-step it is possible to efficiently consider all alignments.For the HMM we do this using the Baum-Welch algorithm (Baum, 1972).Since there is no efficient way in the fertility models IBM-3 to 5 to avoid the explicit summation over all alignments in the EMalgorithm, the counts are collected only over a subset of promising alignments.For IBM3, IBM-4 and IBM-5 we perform the count collection only over a small number of good alignments.In order to keep the training fast we can take into account only a small fraction of all alignments.We will compare three different possibilities of using subsets of different size:This work has been partially supported as part of the Verbmobil project (contract number 01 IV 701 T4) by the German Federal Ministry of Education, Science, Research and Technology and as part of the EuTrans project by the by the European Community (ESPRIT project number 30268).
A Comparative Study On Reordering Constraints In Statistical Machine TranslationIn statistical machine translation, the generation of a translation hypothesis is computationally expensive.If arbitrary wordreorderings are permitted, the search problem is NP-hard.On the other hand, if we restrict the possible word-reorderings in an appropriate way, we obtain a polynomial-time search algorithm.In this paper, we compare two different reordering constraints, namely the ITG constraints and the IBM constraints.This comparison includes a theoretical discussion on the permitted number of reorderings for each of these constraints.We show a connection between the ITG constraints and the since 1870 known We evaluate these constraints on two tasks: the Verbmobil task and the Canadian Hansards task.The evaluation consists of two parts: First, we check how many of the Viterbi alignments of the training corpus satisfy each of these constraints.Second, we restrict the search to each of these constraints and compare the resulting translation hypotheses.The experiments will show that the baseline ITG constraints are not sufficient on the Canadian Hansards task.Therefore, we present an extension to the ITG constraints.These extended ITG constraints increase the alignment coverage from about 87% to 96%.In statistical machine translation, we are given a source language (‘French’) sentence fJ1 = f1 ... fj ... fJ, which is to be translated into a target language (‘English’) sentence eI1 = e1 ... ei ... eI.Among all possible target language sentences, we will choose the sentence with the highest probability: The decomposition into two knowledge sources in Eq.2 is the so-called source-channel approach to statistical machine translation (Brown et al., 1990).It allows an independent modeling of target language model Pr(eI1) and translation model Pr(fJ1 |eI1).The target language model describes the well-formedness of the target language sentence.The translation model links the source language sentence to the target language sentence.It can be further decomposed into alignment and lexicon model.The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language.We have to maximize over all possible target language sentences.In this paper, we will focus on the alignment problem, i.e. the mapping between source sentence positions and target sentence positions.As the word order in source and target language may differ, the search algorithm has to allow certain word-reorderings.If arbitrary word-reorderings are allowed, the search problem is NP-hard (Knight, 1999).Therefore, we have to restrict the possible reorderings in some way to make the search problem feasible.Here, we will discuss two such constraints in detail.The first constraints are based on inversion transduction grammars (ITG) (Wu, 1995; Wu, 1997).In the following, we will call these the ITG constraints.The second constraints are the IBM constraints (Berger et al., 1996).In the next section, we will describe these constraints from a theoretical point of view.Then, we will describe the resulting search algorithm and its extension for word graph generation.Afterwards, we will analyze the Viterbi alignments produced during the training of the alignment models.Then, we will compare the translation results when restricting the search to either of these constraints.In this section, we will discuss the reordering constraints from a theoretical point of view.We will answer the question of how many word-reorderings are permitted for the ITG constraints as well as for the IBM constraints.Since we are only interested in the number of possible reorderings, the specific word identities are of no importance here.Furthermore, we assume a one-to-one correspondence between source and target words.Thus, we are interested in the number of word-reorderings, i.e. permutations, that satisfy the chosen constraints.First, we will consider the ITG constraints.Afterwards, we will describe the IBM constraints.Let us now consider the ITG constraints.Here, we interpret the input sentence as a sequence of blocks.In the beginning, each position is a block of its own.Then, the permutation process can be seen as follows: we select two consecutive blocks and merge them to a single block by choosing between two options: either keep them in monotone order or invert the order.This idea is illustrated in Fig.1.The white boxes represent the two blocks to be merged.Now, we investigate, how many permutations are obtainable with this method.A permutation derived by the above method can be represented as a binary tree where the inner nodes are colored either black or white.At black nodes the resulting sequences of the children are inverted.At white nodes they are kept in monotone order.This representation is equivalent to the parse trees of the simple grammar in (Wu, 1997).We observe that a given permutation may be constructed in several ways by the above method.For instance, let us consider the identity permutation of 1, 2,..., n. Any binary tree with n nodes and all inner nodes colored white (monotone order) is a possible representation of this permutation.To obtain a unique representation, we pose an additional constraint on the binary trees: if the right son of a node is an inner node, it has to be colored with the opposite color.With this constraint, each of these binary trees is unique and equivalent to a parse tree of the ’canonical-form’ grammar in (Wu, 1997).In (Shapiro and Stephens, 1991), it is shown that the number of such binary trees with n nodes is the (n − 1)th large Schr¨oder number Sn−1.The (small) Schr¨oder numbers have been first described in (Schr¨oder, 1870) as the number of bracketings of a given sequence (Schr¨oder’s second problem).The large Schr¨oder numbers are just twice the Schr¨oder numbers.Schr¨oder remarked that the ratio between two consecutive Schr¨oder numbers approaches 3 + 2.\/2 = 5.8284.... A second-order recurrence for the large Schr¨oder numbers is: with n > 2 and S0 = 1, S1 = 2.The Schr¨oder numbers have many combinatorical interpretations.Here, we will mention only two of them.The first one is another way of viewing at the ITG constraints.The number of permutations of the sequence 1, 2, ..., n, which avoid the subsequences (3, 1, 4, 2) and (2, 4, 1, 3), is the large Schr¨oder number Sn−1.More details on forbidden subsequences can be found in (West, 1995).The interesting point is that a search with the ITG constraints cannot generate a word-reordering that contains one of these two subsequences.In (Wu, 1997), these forbidden subsequences are called ’inside-out’ transpositions.Another interpretation of the Schr¨oder numbers is given in (Knuth, 1973): The number of permutations that can be sorted with an output-restricted doubleended queue (deque) is exactly the large Schr¨oder number.Additionally, Knuth presents an approximation for the large Schr¨oder numbers: where c is set to 2 �(3√2 − 4)/π.This approximation function confirms the result of Schr¨oder, and we obtain Sn ∈ o((3 + √8)n), i.e. the Schr¨oder numbers grow like (3 + √8)n ≈ 5.83n.In this section, we will describe the IBM constraints (Berger et al., 1996).Here, we mark each position in the source sentence either as covered or uncovered.In the beginning, all source positions are uncovered.Now, the target sentence is produced from bottom to top.A target position must be aligned to one of the first k uncovered source positions.The IBM constraints are illustrated in Fig.2.For most of the target positions there are k permitted source positions.Only towards the end of the sentence this is reduced to the number of remaining uncovered source positions.Let n denote the length of the input sequence and let rn denote the permitted number of permutations with the IBM constraints.Then, we obtain: Typically, k is set to 4.In this case, we obtain an asymptotic upper and lower bound of 4n, i.e. rn ∈ o(4n).In Tab.1, the ratio of the number of permitted reorderings for the discussed constraints is listed as a function of the sentence length.We see that for longer sentences the ITG constraints allow for more reorderings than the IBM constraints.For sentences of length 10 words, there are about twice as many reorderings for the ITG constraints than for the IBM constraints.This ratio steadily increases.For longer sentences, the ITG constraints allow for much more flexibility than the IBM constraints.Now, let us get back to more practical aspects.Reordering constraints are more or less useless, if they do not allow the maximization of Eq.2 to be performed in an efficient way.Therefore, in this section, we will describe different aspects of the search algorithm for the ITG constraints.First, we will present the dynamic programming equations and the resulting complexity.Then, we will describe pruning techniques to accelerate the search.Finally, we will extend the basic algorithm for the generation of word graphs.The ITG constraints allow for a polynomial-time search algorithm.It is based on the following dynamic programming recursion equations.During the search a table Qjl,jr,eb,et is constructed.Here, Qjl,jr,eb,et denotes the probability of the best hypothesis translating the source words from position jl (left) to position jr (right) which begins with the target language word eb (bottom) and ends with the word et (top).This is illustrated in Fig.3.Here, we initialize this table with monotone translations of IBM Model 4.Therefore, Q0jl jr eb et denotes the probability of the best monotone hypothesis of IBM Model 4.Alternatively, we could use any other single-word based lexicon as well as phrasebased models for this initialization.Our choice is the IBM Model4 to make the results as comparable as possible to the search with the IBM constraints.We introduce a new parameter pm (m=ˆ monotone), which denotes the probability of a monotone combination of two partial hypotheses.We formulated this equation for a bigram language model, but of course, the same method can also be applied for a trigram language model.The resulting algorithm is similar to the CYK-parsing algorithm.It has a worst-case complexity of O(J3 ' E4).Here, J is the length of the source sentence and E is the vocabulary size of the target language.Although the described search algorithm has a polynomial-time complexity, even with a bigram language model the search space is very large.A full search is possible but time consuming.The situation gets even worse when a trigram language model is used.Therefore, pruning techniques are obligatory to reduce the translation time.Pruning is applied to hypotheses that translate the same subsequence fjr jl of the source sentence.We use pruning in the following two ways.The first pruning technique is histogram pruning: we restrict the number of translation hypotheses per sequence fjr jl .For each sequence fjr jl , we keep only a fixed number of translation hypotheses.The second pruning technique is threshold pruning: the idea is to remove all hypotheses that have a low probability relative to the best hypothesis.Therefore, we introduce a threshold pruning parameter q, with 0 < q < 1.Let Q3�l,jr denote the maximum probability of all translation hypotheses for fjr Applying these pruning techniques the computational costs can be reduced significantly with almost no loss in translation quality.The generation of word graphs for a bottom-top search with the IBM constraints is described in (Ueffing et al., 2002).These methods cannot be applied to the CYK-style search for the ITG constraints.Here, the idea for the generation of word graphs is the following: assuming we already have word graphs for the source sequences fkjl and fjr in monotone or inverted order.Now, we describe this idea in a more formal way.A word graph is a directed acyclic graph (dag) with one start and one end node.The edges are annotated with target language words or phrases.We also allow 2-transitions.These are edges annotated with the empty word.Additionally, edges may be annotated with probabilities of the language or translation model.Each path from start node to end node represents one translation hypothesis.The probability of this hypothesis is calculated by multiplying the probabilities along the path.During the search, we have to combine two word graphs in either monotone or inverted order.This is done in the following way: we are given two word graphs w1 and w2 with start and end nodes (s1, g1) and (s2,g2), respectively.First, we add an 2-transition (g1, s2) from the end node of the first graph w1 to the start node of the second graph w2 and annotate this edge with the probability of a monotone concatenation pm.Second, we create a copy of each of the original word graphs w1 and w2.Then, we add an 2-transition (g2, s1) from the end node of the copied second graph to the start node of the copied first graph.This edge is annotated with the probability of a inverted concatenation 1 — pm.Now, we have obtained two word graphs: one for a monotone and one for a inverted concatenation.The final word graphs is constructed by merging the two start nodes and the two end nodes, respectively.Let W(jl, jr) denote the word graph for the source sequence fjr jl .This graph is constructed from the word graphs of all subsequences of (jl, jr).Therefore, we assume, these word graphs have already been produced.For all source positions k with jl < k < jr, we combine the word graphs W (jl, k) and W (k + 1, jr) as described above.Finally, we merge all start nodes of these graphs as well as all end nodes.Now, we have obtained the word graph W(jl, jr) for the source sequence fjr jl .As initialization, we use the word graphs of the monotone IBM4 search.In this section, we will extend the ITG constraints described in Sec.2.1.This extension will go beyond basic reordering constraints.We already mentioned that the use of consecutive phrases within the ITG approach is straightforward.The only thing we have to change is the initialization of the Q-table.Now, we will extend this idea to phrases that are non-consecutive in the source language.For this purpose, we adopt the view of the ITG constraints as a bilingual grammar as, e.g., in (Wu, 1997).For the baseline ITG constraints, the resulting grammar is: A— [AA]  |(AA)  |f/e  |f/2  |2/e Here, [AA] denotes a monotone concatenation and (AA) denotes an inverted concatenation.Let us now consider the case of a source phrase consisting of two parts f1 and f2.Let e denote the corresponding target phrase.We add the productions A — [e/f1 A 2/f2]  |(e/f1 A 2/f2) to the grammar.The probabilities of these productions are, dependent on the translation direction, p(e|f1, f2) or p(f1, f2|e), respectively.Obviously, these productions are not in the normal form of an ITG, but with the method described in (Wu, 1997), they can be normalized.In the following sections we will present results on two tasks.Therefore, in this section we will show the corpus statistics for each of these tasks.The first task we will present results on is the Verbmobil task (Wahlster, 2000).The domain of this corpus is appointment scheduling, travel planning, and hotel reservation.It consists of transcriptions of spontaneous speech.Table 2 shows the corpus statistics of this corpus.The training corpus (Train) was used to train the IBM model parameters.The remaining free parameters, i.e. pm and the model scaling factors (Och and Ney, 2002), were adjusted on the development corpus (Dev).The resulting system was evaluated on the test corpus (Test).Table 2: Statistics of training and test corpus for the Verbmobil task (PP=perplexity, SL=sentence length).Additionally, we carried out experiments on the Canadian Hansards task.This task contains the proceedings of the Canadian parliament, which are kept by law in both French and English.About 3 million parallel sentences of this bilingual data have been made available by the Linguistic Data Consortium (LDC).Here, we use a subset of the data containing only sentences with a maximum length of 30 words.Table 3 shows the training and test corpus statistics.In this section, we will investigate for each of the constraints the coverage of the training corpus alignment.For this purpose, we compute the Viterbi alignment of IBM Model 5 with GIZA++ (Och and Ney, 2000).This alignment is produced without any restrictions on word-reorderings.Then, we check for every sentence if the alignment satisfies each of the constraints.The ratio of the number of satisfied alignments and the total number of sentences is referred to as coverage.Tab.4 shows the results for the Verbmobil task and for the Canadian Hansards task.It contains the results for both translation directions German-English (S—*T) and English-German (T—*S) for the Verbmobil task and French-English (S—*T) and English-French (T—*S) for the Canadian Hansards task, respectively.For the Verbmobil task, the baseline ITG constraints and the IBM constraints result in a similar coverage.It is about 91% for the German-English translation direction and about 88% for the EnglishGerman translation direction.A significantly higher coverage of about 96% is obtained with the extended ITG constraints.Thus with the extended ITG constraints, the coverage increases by about 8% absolute.For the Canadian Hansards task, the baseline ITG constraints yield a worse coverage than the IBM constraints.Especially for the English-French translation direction, the ITG coverage of 73.6% is very low.Again, the extended ITG constraints obtained the best results.Here, the coverage increases from about 87% for the IBM constraints to about 96% for the extended ITG constraints.In our experiments, we use the following error criteria: The WER is computed as the minimum number of substitution, insertion and deletion operations that have to be performed to convert the generated sentence into the target sentence.For each test sentence, not only a single reference translation is used, as for the WER, but a whole set of reference translations.For each translation hypothesis, the WER to the most similar sentence is calculated (Nießen et al., 2000).This score measures the precision of unigrams, bigrams, trigrams and fourgrams with respect to a whole set of reference translations with a penalty for too short sentences (Papineni et al., 2001).BLEU measures accuracy, i.e. large BLEU scores are better.For a more detailed analysis, subjective judgments by test persons are necessary.Each translated sentence was judged by a human examiner according to an error scale from 0.0 to 1.0 (Nießen et al., 2000).In this section, we will present the translation results for both the IBM constraints and the baseline ITG constraints.We used a single-word based search with IBM Model 4.The initialization for the ITG constraints was done with monotone IBM Model 4 translations.So, the only difference between the two systems are the reordering constraints.In Tab.5 the results for the Verbmobil task are shown.We see that the results on this task are similar.The search with the ITG constraints yields slightly lower error rates.Some translation examples of the Verbmobil task are shown in Tab.6.We have to keep in mind, that the Verbmobil task consists of transcriptions of spontaneous speech.Therefore, the source sentences as well as the reference translations may have an unorthodox grammatical structure.In the first example, the German verb-group (“w¨urde vorschlagen”) is split into two parts.The search with the ITG constraints is able to produce a correct translation.With the IBM constraints, it is not possible to translate this verb-group correctly, because the distance between the two parts is too large (more than four words).As we see in the second example, in German the verb of a subordinate clause is placed at the end (“¨ubernachten”).The IBM search is not able to perform the necessary long-range reordering, as it is done with the ITG search.The ITG constraints were introduced in (Wu, 1995).The applications were, for instance, the segmentation of Chinese character sequences into Chinese “words” and the bracketing of the source sentence into sub-sentential chunks.In (Wu, 1996) the baseline ITG constraints were used for statistical machine translation.The resulting algorithm is similar to the one presented in Sect.3.1, but here, we use monotone translation hypotheses of the full IBM Model 4 as initialization, whereas in (Wu, 1996) a single-word based lexicon model is used.In (Vilar, 1998) a model similar to Wu’s method was considered.We have described the ITG constraints in detail and compared them to the IBM constraints.We draw the following conclusions: especially for long sentences the ITG constraints allow for higher flexibility in word-reordering than the IBM constraints.Regarding the Viterbi alignment in training, the baseline ITG constraints yield a similar coverage as the IBM constraints on the Verbmobil task.On the Canadian Hansards task the baseline ITG constraints were not sufficient.With the extended ITG constraints the coverage improves significantly on both tasks.On the Canadian Hansards task the coverage increases from about 87% to about 96%.We have presented a polynomial-time search algorithm for statistical machine translation based on the ITG constraints and its extension for the generation of word graphs.We have shown the translation results for the Verbmobil task.On this task, the translation quality of the search with the baseline ITG constraints is already competitive with the results for the IBM constraints.Therefore, we expect the search with the extended ITG constraints to outperform the search with the IBM constraints.Future work will include the automatic extraction of the bilingual grammar as well as the use of this grammar for the translation process.
Dependency Tree Kernels For Relation ExtractionWe extend previous work on tree kernels to estimate the similarity between the dependency trees of sentences.Using this kernel within a Support Vector Machine, we detect and classify relations between entities in the Automatic Content Extraction (ACE) corpus of news articles.We examine the utility of different features such as Wordnet hypernyms, parts of speech, and entity types, and find that the dependency tree kernel achieves a 20% F1 improvement over a “bag-of-words” kernel.The ability to detect complex patterns in data is limited by the complexity of the data’s representation.In the case of text, a more structured data source (e.g. a relational database) allows richer queries than does an unstructured data source (e.g. a collection of news articles).For example, current web search engines would not perform well on the query, “list all California-based CEOs who have social ties with a United States Senator.” Only a structured representation of the data can effectively provide such a list.The goal of Information Extraction (IE) is to discover relevant segments of information in a data stream that will be useful for structuring the data.In the case of text, this usually amounts to finding mentions of interesting entities and the relations that join them, transforming a large corpus of unstructured text into a relational database with entries such as those in Table 1.IE is commonly viewed as a three stage process: first, an entity tagger detects all mentions of interest; second, coreference resolution resolves disparate mentions of the same entity; third, a relation extractor finds relations between these entities.Entity tagging has been thoroughly addressed by many statistical machine learning techniques, obtaining greater than 90% F1 on many datasets (Tjong Kim Sang and De Meulder, 2003).Coreference resolution is an active area of research not investigated here (Pasula et al., 2002; McCallum and Wellner, 2003).We describe a relation extraction technique based on kernel methods.Kernel methods are nonparametric density estimation techniques that compute a kernel function between data instances, where a kernel function can be thought of as a similarity measure.Given a set of labeled instances, kernel methods determine the label of a novel instance by comparing it to the labeled training instances using this kernel function.Nearest neighbor classification and support-vector machines (SVMs) are two popular examples of kernel methods (Fukunaga, 1990; Cortes and Vapnik, 1995).An advantage of kernel methods is that they can search a feature space much larger than could be represented by a feature extraction-based approach.This is possible because the kernel function can explore an implicit feature space when calculating the similarity between two instances, as described in the Section 3.Working in such a large feature space can lead to over-fitting in many machine learning algorithms.To address this problem, we apply SVMs to the task of relation extraction.SVMs find a boundary between instances of different classes such that the distance between the boundary and the nearest instances is maximized.This characteristic, in addition to empirical validation, indicates that SVMs are particularly robust to over-fitting.Here we are interested in detecting and classifying instances of relations, where a relation is some meaningful connection between two entities (Table 2).We represent each relation instance as an augmented dependency tree.A dependency tree represents the grammatical dependencies in a sentence; we augment this tree with features for each node (e.g. part of speech) We choose this representation because we hypothesize that instances containing similar relations will share similar substructures in their dependency trees.The task of the kernel function is to find these similarities.We define a tree kernel over dependency trees and incorporate this kernel within an SVM to extract relations from newswire documents.The tree kernel approach consistently outperforms the bag-ofwords kernel, suggesting that this highly-structured representation of sentences is more informative for detecting and distinguishing relations.Kernel methods (Vapnik, 1998; Cristianini and Shawe-Taylor, 2000) have become increasingly popular because of their ability to map arbitrary objects to a Euclidian feature space.Haussler (1999) describes a framework for calculating kernels over discrete structures such as strings and trees.String kernels for text classification are explored in Lodhi et al. (2000), and tree kernel variants are described in (Zelenko et al., 2003; Collins and Duffy, 2002; Cumby and Roth, 2003).Our algorithm is similar to that described by Zelenko et al. (2003).Our contributions are a richer sentence representation, a more general framework to allow feature weighting, as well as the use of composite kernels to reduce kernel sparsity.Brin (1998) and Agichtein and Gravano (2000) apply pattern matching and wrapper techniques for relation extraction, but these approaches do not scale well to fastly evolving corpora.Miller et al. (2000) propose an integrated statistical parsing technique that augments parse trees with semantic labels denoting entity and relation types.Whereas Miller et al. (2000) use a generative model to produce parse information as well as relation information, we hypothesize that a technique discriminatively trained to classify relations will achieve better performance.Also, Roth and Yih (2002) learn a Bayesian network to tag entities and their relations simultaneously.We experiment with a more challenging set of relation types and a larger corpus.In traditional machine learning, we are provided a set of training instances S = {x1 ... xN}, where each instance xi is represented by some ddimensional feature vector.Much time is spent on the task of feature engineering – searching for the optimal feature set either manually by consulting domain experts or automatically through feature induction and selection (Scott and Matwin, 1999).For example, in entity detection the original instance representation is generally a word vector corresponding to a sentence.Feature extraction and induction may result in features such as part-ofspeech, word n-grams, character n-grams, capitalization, and conjunctions of these features.In the case of more structured objects, such as parse trees, features may include some description of the object’s structure, such as “has an NP-VP subtree.” Kernel methods can be particularly effective at reducing the feature engineering burden for structured objects.By calculating the similarity between two objects, kernel methods can employ dynamic programming solutions to efficiently enumerate over substructures that would be too costly to explicitly include as features.Formally, a kernel function K is a mapping tion over the instance x.The kernel function must be symmetric [K(x, y) = K(y, x)] and positivesemidefinite.By positive-semidefinite, we require that the if x1, ... , xn E X, then the n x n matrix G defined by Gij = K(xi, xj) is positive semidefinite.It has been shown that any function that takes the dot product of feature vectors is a kernel function (Haussler, 1999).A simple kernel function takes the dot product of the vector representation of instances being compared.For example, in document classification, each document can be represented by a binary vector, where each element corresponds to the presence or absence of a particular word in that document.Here, Oi(x) = 1 if word i occurs in document x.Thus, the kernel function K(x, y) returns the number of words in common between x and y.We refer to this kernel as the “bag-of-words” kernel, since it ignores word order.When instances are more structured, as in the case of dependency trees, more complex kernels become necessary.Haussler (1999) describes convolution kernels, which find the similarity between two structures by summing the similarity of their substructures.As an example, consider a kernel over strings.To determine the similarity between two strings, string kernels (Lodhi et al., 2000) count the number of common subsequences in the two strings, and weight these matches by their length.Thus, Oi(x) is the number of times string x contains the subsequence referenced by i.These matches can be found efficiently through a dynamic program, allowing string kernels to examine long-range features that would be computationally infeasible in a feature-based method.Given a training set S = {xs ... xN}, kernel methods compute the Gram matrix G such that Gij = K(xi,xj).Given G, the classifier finds a hyperplane which separates instances of different classes.To classify an unseen instance x, the classifier first projects x into the feature space defined by the kernel function.Classification then consists of determining on which side of the separating hyperplane x lies.A support vector machine (SVM) is a type of classifier that formulates the task of finding the separating hyperplane as the solution to a quadratic programming problem (Cristianini and Shawe-Taylor, 2000).Support vector machines attempt to find a hyperplane that not only separates the classes but also maximizes the margin between them.The hope is that this will lead to better generalization performance on unseen instances.Our task is to detect and classify relations between entities in text.We assume that entity tagging has been performed; so to generate potential relation instances, we iterate over all pairs of entities occurring in the same sentence.For each entity pair, we create an augmented dependency tree (described below) representing this instance.Given a labeled training set of potential relations, we define a tree kernel over dependency trees which we then use in an SVM to classify test instances.A dependency tree is a representation that denotes grammatical relations between words in a sentence (Figure 1).A set of rules maps a parse tree to a dependency tree.For example, subjects are dependent on their verbs and adjectives are dependent the dependency tree. on the nouns they modify.Note that for the purposes of this paper, we do not consider the link labels (e.g.“object”, “subject”); instead we use only the dependency structure.To generate the parse tree of each sentence, we use MXPOST, a maximum entropy statistical parser1; we then convert this parse tree to a dependency tree.Note that the left-to-right ordering of the sentence is maintained in the dependency tree only among siblings (i.e. the dependency tree does not specify an order to traverse the tree to recover the original sentence).For each pair of entities in a sentence, we find the smallest common subtree in the dependency tree that includes both entities.We choose to use this subtree instead of the entire tree to reduce noise and emphasize the local characteristics of relations.We then augment each node of the tree with a feature vector (Table 3).The relation-argument feature specifies whether an entity is the first or second argument in a relation.This is required to learn asymmetric relations (e.g.X OWNS Y).Formally, a relation instance is a dependency tree T with nodes It0 ... tn}.The features of node ti are given by 0(ti) = Iv1 ... vd}.We refer to the jth child of node ti as ti[j], and we denote the set of all children of node ti as ti[c].We reference a subset j of children of ti by ti[j] C_ ti[c].Finally, we refer to the parent of node ti as ti.p.From the example in Figure 1, t0[1] = t2, t0[I0,1}] = It1, t2}, and t1.p = t0.We now define a kernel function for dependency trees.The tree kernel is a function K(T1, T2) that returns a normalized, symmetric similarity score in the range (0, 1) for two trees T1 and T2.We define a slightly more general version of the kernel described by Zelenko et al. (2003).We first define two functions over the features of tree nodes: a matching function m(ti, tj) E I0, 1} and a similarity function s(ti, tj) E (0, oc].Let the feature vector 0(ti) = Iv1 ... vd} consist of two possibly overlapping subsets 0m(ti) C_ 0(ti) and 0s(ti) C_ 0(ti).We use 0m(ti) in the matching function and 0s(ti) in the similarity function.We define and where C(vq, vr) is some compatibility function between two feature values.For example, in the simplest case where s(ti, tj) returns the number of feature values in common between feature vectors 0s(ti) and 0s(tj).We can think of the distinction between functions m(ti, tj) and s(ti, tj) as a way to discretize the similarity between two nodes.If 0m(ti) =� 0m(tj), then we declare the two nodes completely dissimilar.However, if 0m(ti) = 0m(tj), then we proceed to compute the similarity s(ti, tj).Thus, restricting nodes by m(ti, tj) is a way to prune the search space of matching subtrees, as shown below.For two dependency trees T1, T2, with root nodes r1 and r2, we define the tree kernel K(T1, T2) as where Kc is a kernel function over children.Let a and b be sequences of indices such that a is a sequence a1 < a2 < ... < an, and likewise for b.Let d(a) = an − a1 + 1 and l(a) be the length of a.Then we have Kc(ti[c], tj[c]) = The constant 0 < A < 1 is a decay factor that penalizes matching subsequences that are spread out within the child sequences.See Zelenko et al. (2003) for a proof that K is kernel function.Intuitively, whenever we find a pair of matching nodes, we search for all matching subsequences of the children of each node.A matching subsequence of children is a sequence of children a and b such that m(ai, bi) = 1 (bi < n).For each matching pair of nodes (ai, bi) in a matching subsequence, we accumulate the result of the similarity function s(ai, bj) and then recursively search for matching subsequences of their children ai[c], bj[c].We implement two types of tree kernels.A contiguous kernel only matches children subsequences that are uninterrupted by non-matching nodes.Therefore, d(a) = l(a).A sparse tree kernel, by contrast, allows non-matching nodes within matching subsequences.Figure 2 shows two relation instances, where each node contains the original text plus the features used for the matching function, 0m(ti) = Igeneralpos, entity-type, relation-argument}.(“NA” denotes the feature is not present for this node.)The contiguous kernel matches the following substructures: It0[0], u0[0]}, It0[2], u0[1]}, It3[0], u2[0]}.Because the sparse kernel allows non-contiguous matching sequences, it matches an additional substructure It0[0, *, 2], u0[0, *,1]}, where (*) indicates an arbitrary number of non-matching nodes.Zelenko et al. (2003) have shown the contiguous kernel to be computable in O(mn) and the sparse kernel in O(mn3), where m and n are the number of children in trees T1 and T2 respectively.We extract relations from the Automatic Content Extraction (ACE) corpus provided by the National Institute for Standards and Technology (NIST).The data consists of about 800 annotated text documents gathered from various newspapers and broadcasts.Five entities have been annotated (PERSON, ORGANIZATION, GEO-POLITICAL ENTITY, LOCATION, FACILITY), along with 24 types of relations (Table 2).As noted from the distribution of relationship types in the training data (Figure 3), data imbalance and sparsity are potential problems.In addition to the contiguous and sparse tree kernels, we also implement a bag-of-words kernel, which treats the tree as a vector of features over nodes, disregarding any structural information.We also create composite kernels by combining the sparse and contiguous kernels with the bagof-words kernel.Joachims et al. (2001) have shown that given two kernels K1, K2, the composite kernel K12(xi, xj) = K1(xi, xj)+K2(xi, xj) is also a kernel.We find that this composite kernel improves performance when the Gram matrix G is sparse (i.e. our instances are far apart in the kernel space).The features used to represent each node are shown in Table 3.After initial experimentation, the set of features we use in the matching function is φm(ti) = {general-pos, entity-type, relationargument}, and the similarity function examines the remaining features.In our experiments we tested the following five kernels: We also experimented with the function C(vq, vr), the compatibility function between two feature values.For example, we can increase the importance of two nodes having the same Wordnet hypernym2.If vq, vr are hypernym features, then we can define When > 1, we increase the similarity of nodes that share a hypernym.We tested a number of weighting schemes, but did not obtain a set of weights that produced consistent significant improvements.See Section 8 for altern α ate approaches to setting C. Table 4 shows the results of each kernel within an SVM.(We augment the LibSVM3 implementation to include our dependency tree kernel.)Note that, although training was done over all 24 relation subtypes, we evaluate only over the 5 high-level relation types.Thus, classifying a RESIDENCE relation as a LOCATED relation is deemed correct4.Note also that K0 is not included in Table 4 because of burdensome computational time.Table 4 shows that precision is adequate, but recall is low.This is a result of the aforementioned class imbalance – very few of the training examples are relations, so the classifier is less likely to identify a testing instances as a relation.Because we treat every pair of mentions in a sentence as a possible relation, our training set contains fewer than 15% positive relation instances.To remedy this, we retrain each SVMs for a binary classification task.Here, we detect, but do not classify, relations.This allows us to combine all positive relation instances into one class, which provides us more training samples to estimate the class boundary.We then threshold our output to achieve an optimal operating point.As seen in Table 5, this method of relation detection outperforms that of the multi-class classifier.We then use these binary classifiers in a cascading scheme as follows: First, we use the binary SVM to detect possible relations.Then, we use the SVM trained only on positive relation instances to classify each predicted relation.These results are shown in Table 6.The first result of interest is that the sparse tree kernel, K0, does not perform as well as the contiguous tree kernel, K1.Suspecting that noise was introduced by the non-matching nodes allowed in the sparse tree kernel, we performed the experiment with different values for the decay factor A = {.9,.5,.1}, but obtained no improvement.The second result of interest is that all tree kernels outperform the bag-of-words kernel, K2, most noticeably in recall performance, implying that the and C denote the kernel used for relation detection and classification, respectively. structural information the tree kernel provides is extremely useful for relation detection.Note that the average results reported here are representative of the performance per relation, except for the NEAR relation, which had slightly lower results overall due to its infrequency in training.We have shown that using a dependency tree kernel for relation extraction provides a vast improvement over a bag-of-words kernel.While the dependency tree kernel appears to perform well at the task of classifying relations, recall is still relatively low.Detecting relations is a difficult task for a kernel method because the set of all non-relation instances is extremely heterogeneous, and is therefore difficult to characterize with a similarity metric.An improved system might use a different method to detect candidate relations and then use this kernel method to classify the relations.The most immediate extension is to automatically learn the feature compatibility function C(vq, vr).A first approach might use tf-idf to weight each feature.Another approach might be to calculate the information gain for each feature and use that as its weight.A more complex system might learn a weight for each pair of features; however this seems computationally infeasible for large numbers of features.One could also perform latent semantic indexing to collapse feature values into similar “categories” — for example, the words “football” and “baseball” might fall into the same category.Here, C(vq, vr) might return α1 if vq = vr, and α2 if vq and vr are in the same category, where α1 > α2 > 0.Any method that provides a “soft” match between feature values will sharpen the granularity of the kernel and enhance its modeling power.Further investigation is also needed to understand why the sparse kernel performs worse than the contiguous kernel.These results contradict those given in Zelenko et al. (2003), where the sparse kernel achieves 2-3% better F1 performance than the contiguous kernel.It is worthwhile to characterize relation types that are better captured by the sparse kernel, and to determine when using the sparse kernel is worth the increased computational burden.
Identifying Agreement And Disagreement In Conversational Speech: Use Of Bayesian Networks To Model Pragmatic DependenciesWe describe a statistical approach for modeling agreements and disagreements in conversational interaction.Our approach first identifies adjacency pairs using maximum entropy ranking based on a set of lexical, durational, and structural features that look both forward and backward in the discourse.We then classify utterances as agreement or disagreement using these adjacency pairs and features that represent various pragmatic influences of previous agreement or disagreement on the current utterance.Our approach achieves 86.9% accuracy, a 4.9% increase over previous work.One of the main features of meetings is the occurrence of agreement and disagreement among participants.Often meetings include long stretches of controversial discussion before some consensus decision is reached.Our ultimate goal is automated summarization of multi-participant meetings and we hypothesize that the ability to automatically identify agreement and disagreement between participants will help us in the summarization task.For example, a summary might resemble minutes of meetings with major decisions reached (consensus) along with highlighted points of the pros and cons for each decision.In this paper, we present a method to automatically classify utterances as agreement, disagreement, or neither.Previous work in automatic identification of agreement/disagreement (Hillard et al., 2003) demonstrates that this is a feasible task when various textual, durational, and acoustic features are available.We build on their approach and show that we can get an improvement in accuracy when contextual information is taken into account.Our approach first identifies adjacency pairs using maximum entropy ranking based on a set of lexical, durational and structural features that look both forward and backward in the discourse.This allows us to acquire, and subsequently process, knowledge about who speaks to whom.We hypothesize that pragmatic features that center around previous agreement between speakers in the dialog will influence the determination of agreement/disagreement.For example, if a speaker disagrees with another person once in the conversation, is he more likely to disagree with him again?We model context using Bayesian networks that allows capturing of these pragmatic dependencies.Our accuracy for classifying agreements and disagreements is 86.9%, which is a 4.9% improvement over (Hillard et al., 2003).In the following sections, we begin by describing the annotated corpus that we used for our experiments.We then turn to our work on identifying adjacency pairs.In the section on identification of agreement/disagreement, we describe the contextual features that we model and the implementation of the classifier.We close with a discussion of future work.The ICSI Meeting corpus (Janin et al., 2003) is a collection of 75 meetings collected at the International Computer Science Institute (ICSI), one among the growing number of corpora of humanto-human multi-party conversations.These are naturally occurring, regular weekly meetings of various ICSI research teams.Meetings in general run just under an hour each; they have an average of 6.5 participants.These meetings have been labeled with adjacency pairs (AP), which provide information about speaker interaction.They reflect the structure of conversations as paired utterances such as questionanswer and offer-acceptance, and their labeling is used in our work to determine who are the addressees in agreements and disagreements.The annotation of the corpus with adjacency pairs is described in (Shriberg et al., 2004; Dhillon et al., 2004).Seven of those meetings were segmented into spurts, defined as periods of speech that have no pauses greater than .5 second, and each spurt was labeled with one of the four categories: agreement, disagreement, backchannel, and other.1 We used spurt segmentation as our unit of analysis instead of sentence segmentation, because our ultimate goal is to build a system that can be fully automated, and in that respect, spurt segmentation is easy to obtain.Backchannels (e.g.“uhhuh” and “okay”) were treated as a separate category, since they are generally used by listeners to indicate they are following along, while not necessarily indicating agreement.The proportion of classes is the following: 11.9% are agreements, 6.8% are disagreements, 23.2% are backchannels, and 58.1% are others.Inter-labeler reliability estimated on 500 spurts with 2 labelers was considered quite acceptable, since the kappa coefficient was .63 (Cohen, 1960).Adjacency pairs (AP) are considered fundamental units of conversational organization (Schegloff and Sacks, 1973).Their identification is central to our problem, since we need to know the identity of addressees in agreements and disagreements, and adjacency pairs provide a means of acquiring this knowledge.An adjacency pair is said to consist of two parts (later referred to as A and B) that are ordered, adjacent, and produced by different speakers.The first part makes the second one immediately relevant, as a question does with an answer, or an offer does with an acceptance.Extensive work in conversational analysis uses a less restrictive definition of adjacency pair that does not impose any actual adjacency requirement; this requirement is problematic in many respects (Levinson, 1983).Even when APs are not directly adjacent, the same constraints between pairs and mechanisms for selecting the next speaker remain in place (e.g. the case of embedded question and answer pairs).This relaxation on a strict adjacency requirement is particularly important in interactions of multiple speakers since other speakers have more opportunities to insert utterances between the two elements of the AP construction (e.g. interrupted, abandoned or ignored utterances; backchannels; APs with multiple second elements, e.g. a question followed by answers of multiple speakers).2 Information provided by adjacency pairs can be used to identify the target of an agreeing or disagreeing utterance.We define the problem of AP identification as follows: given the second element (B) of an adjacency pair, determine who is the speaker of the first element (A).A quite effective baseline algorithm is to select as speaker of utterance A the most recent speaker before the occurrence of utterance B.This strategy selects the right speaker in 79.8% of the cases in the 50 meetings that were annotated with adjacency pairs.The next subsection describes the machine learning framework used to significantly outperform this already quite effective baseline algorithm.We view the problem as an instance of statistical ranking, a general machine learning paradigm used for example in statistical parsing (Collins, 2000) and question answering (Ravichandran et al., 2003).3 The problem is to select, given a set of possible candidates (in our case, potential A speakers), the one candidate that maximizes a given conditional probability distribution.We use maximum entropy modeling (Berger et al., 1996) to directly model the conditional probability , where each in The only role of the denominator is to ensure that is a proper probability distribution.It is defined as: To find the most probable speaker of part A, we use the following decision rule: Note that we have also attempted to model the problem as a binary classification problem where is an observation associated with the corresponding speaker.is represented here by only one variable for notational ease, but it possibly represents several lexical, durational, structural, and acoustic observations.Given feature functions and model parameters , the probability of the maximum entropy model is defined as: each speaker is either classified as speaker A or not, but we abandoned that approach, since it gives much worse performance.This finding is consistent with previous work (Ravichandran et al., 2003) that compares maximum entropy classification and re-ranking on a question answering task.We will now describe the features used to train the maximum entropy model mentioned previously.To rank all speakers (aside from the B speaker) and to determine how likely each one is to be the A speaker of the adjacency pair involving speaker B, we use four categories of features: structural, durational, lexical, and dialog act (DA) information.For the remainder of this section, we will interchangeably use A to designate either the potential A speaker or the most recent utterance4 of that speaker, assuming the distinction is generally unambiguous.We use B to designate either the B speaker or the current spurt for which we need to identify a corresponding A part.The feature sets are listed in Table 1.Structural features encode some helpful information regarding ordering and overlap of spurts.Note that with only the first feature listed in the table, the maximum entropy ranker matches exactly the performance of the baseline algorithm (79.8% accuracy).Regarding lexical features, we used a countbased feature selection algorithm to remove many first-word and last-word features that occur infrequently and that are typically uninformative for the task at hand.Remaining features essentially contained function words, in particular sentence-initial indicators of questions (“where”, “when”, and so on).Note that all features in Table 1 are “backwardlooking”, in the sense that they result from an analysis of context preceding B.For many of them, we built equivalent “forward-looking” features that pertain to the closest utterance of the potential speaker A that follows part B.The motivation for extracting these features is that speaker A is generally expected to react if he or she is addressed, and thus, to take the floor soon after B is produced.We used the labeled adjacency pairs of 50 meetings and selected 80% of the pairs for training.To train the maximum entropy ranking model, we used the generalized iterative scaling algorithm (Darroch and Ratcliff, 1972) as implemented in YASMET.5 Table 2 summarizes the accuracy of our statistical ranker on the test data with different feature sets: the performance is 89.39% when using all feature sets, and reaches 90.2% after applying Gaussian smoothing and using incremental feature selection as described in (Berger et al., 1996) and implemented in the yasmetFS package.6 Note that restricting ourselves to only backward looking features decreases the performance significantly, as we can see in Table 2.We also wanted to determine if information about dialog acts (DA) helps the ranking task.If we hypothesize that only a limited set of paired DAs (e.g. offer-accept, question-answer, and apologydownplay) can be realized as adjacency pairs, then knowing the DA category of the B part and of all potential A parts should help in finding the most meaningful dialog act tag among all potential A parts; for example, the question-accept pair is admittedly more likely to correspond to an AP than e.g. backchannel-accept.We used the DA annotation that we also had available, and used the DA tag sequence of part A and B as a feature.7 When we add the DA feature set, the accuracy reaches 91.34%, which is only slightly better than our 90.20% accuracy, which indicates that lexical, durational, and structural features capture most of the informativeness provided by DAs.This improved accuracy with DA information should of course not be considered as the actual accuracy of our system, since DA information is difficult to acquire automatically (Stolcke et al., 2000).This section focusses on the use of contextual information, in particular the influence of previous agreements and disagreements and detected adjacency pairs, to improve the classification of agreements and disagreements.We first define the classification problem, then describe non-contextual features, provide some empirical evidence justifying our choice of contextual features, and finally evaluate the classifier.We need to first introduce some notational conventions and define the classification problem with the agreement/disagreement tagset.In our classification problem, each spurt among the spurts of a meeting must be assigned a tag AGREEDISAGREEBACKCHANNELOTHER.To specify the speaker of the spurt (e.g. speaker B), the notation will sometimes be augmented to incorporate speaker information, as with , and to designate the addressee of B (e.g. listener A), we will use the notation .For example, AGREE simply means that B agrees with A in the spurt of index .This notation makes it obvious that we do not necessarily assume that agreements and disagreements are reflexive 7The annotation of DA is particularly fine-grained with a choice of many optional tags that can be associated with each DA.To deal with this problem, we used various scaled-down versions of the original tagset. relations.We define: as the tag of the most recent spurt before that is produced by Y and addresses X.This definition will help our multi-party analyses of agreement and disagreement behaviors.Many of the local features described in this subsection are similar in spirit to the ones used in the previous work of (Hillard et al., 2003).We did not use acoustic features, since the main purpose of the current work is to explore the use of contextual information.Table 3 lists the features that were found most helpful at identifying agreements and disagreements.Regarding lexical features, we selected a list of lexical items we believed are instrumental in the expression of agreements and disagreements: agreement markers, e.g.“yes” and “right”, as listed in (Cohen, 2002), general cue phrases, e.g.“but” and “alright” (Hirschberg and Litman, 1994), and adjectives with positive or negative polarity (Hatzivassiloglou and McKeown, 1997).We incorporated a set of durational features that were described in the literature as good predictors of agreements: utterance length distinguishes agreement from disagreement, the latter tending to be longer since the speaker elaborates more on the reasons and circumstances of her disagreement than for an agreement (Cohen, 2002).Duration is also a good predictor of backchannels, since they tend to be quite short.Finally, a fair amount of silence and filled pauses is sometimes an indicator of disagreement, since it is a dispreferred response in most social contexts and can be associated with hesitation (Pomerantz, 1984).We first performed several empirical analyses in order to determine to what extent contextual information helps in discriminating between agreement and disagreement.By integrating the interpretation of the pragmatic function of an utterance into a wider context, we aim to detect cases of mismatch between a correct pragmatic interpretation and the surface form of the utterance, e.g. the case of weak or “empty” agreement, which has some properties of downright agreement (lexical items of positive polarity), but which is commonly considered to be a disagreement (Pomerantz, 1984).While the actual classification problem incorporates four classes, the BACKCHANNEL class is igTable 3.Local features for agreement and disagreement classification nored here to make the empirical study easier to interpret.We assume in that study that accurate AP labeling is available, but for the purpose of building and testing a classifier, we use only automatically extracted adjacency pair information.We tested the validity of four pragmatic assumptions: dency: a tag is influenced by , the most recent tag of the same speaker addressing the same listener; for example, it might be reasonable to assume that if speaker B disagrees with A, B is likely to disagree with A in his or her next speech addressing A. for which exists, then a tag is influenced by and ; an example of such an influence is a case where speaker first agrees with , then speaker disagrees with , from which one could possibly conclude that is actually in disagreement with .Table 4 presents the results of our empirical evaluation of the first three assumptions.For comparison, the distribution of classes is the following: 18.8% are agreements, 10.6% disagreements, and 70.6% other.The dependencies empirically evaluated in the two last columns are non-local; they create dependencies between spurts separated by an arbitrarily long time span.Such long range dependencies are often undesirable, since the influence of one spurt on the other is often weak or too difficult to capture with our model.Hence, we made a Markov assumption by limiting context to an arbitrarily chosen value .In this analysis subsection and for all classification results presented thereafter, we used a value of .The table yields some interesting results, showing quite significant variations in class distribution when it is conditioned on various types of contextual information.We can see for example, that the proportion of agreements and disagreements (respectively 18.8% and 10.6%) changes to 13.9% and 20.9% respectively when we restrict the counts to spurts that are preceded by a DISAGREE.Similarly, that distribution changes to 21.3% and 7.3% when the previous tag is an AGREE.The variable is even more noticeable between probabilities and Structural features: is the previous/next spurt of the same speaker? is the previous/next spurt involving the same B speaker?Durational features: duration of the spurt seconds of overlap with any other speaker seconds of silence during the spurt speech rate in the spurt Lexical features: number of words in the spurt number of content words in the spurt perplexity of the spurt with respect to four language models, one for each class first and last word of the spurt number of instances of adjectives with positive polarity (Hatzivassiloglou and McKeown, 1997) idem, with adjectives of negative polarity number of instances in the spurt of each cue phrase and agreement/disagreement token listed in (Hirschberg and Litman, 1994; Cohen, 2002) .In 26.1% of the cases where a given speaker B disagrees with A, he or she will continue to disagree in the next exchange involving the same speaker and the same listener.Similarly with the same probability distribution, a tendency to agree is confirmed in 25% of the cases.The results in the last column are quite different from the two preceding ones.While agreements in response to agreements ( AGREEAGREE ) are slightly less probable than agreements without conditioning on any previous tag ( AGREE ), the probability of an agreement produced in response to a disagreement is quite high (with 23.4%), even higher than the proportion of agreements in the entire data (18.8%).This last result would arguably be quite different with more quarrelsome meeting participants.Table 5 represents results concerning the fourth pragmatic assumption.While none of the results characterize any strong conditioning of by and , we can nevertheless notice some interesting phenomena.For example, there is a tendency for agreements to be transitive, i.e. if X agrees with A and B agrees with X within a limited segment of speech, then agreement between B and A is confirmed in 22.5% of the cases, while the probability of the agreement class is only 18.8%.The only slightly surprising result appears in the last column of the table, from which we cannot conclude that disagreement with a disagreement is equivalent to agreement.This might be explained by the fact that these sequences of agreement and disagreement do not necessarily concern the same propositional content.The probability distributions presented here are admittedly dependent on the meeting genre and particularly speaker personalities.Nonetheless, we believe this model can as well be used to capture salient interactional patterns specific to meetings with different social dynamics.We will next discuss our choice of a statistical model to classify sequence data that can deal with non-local label dependencies, such as the ones tested in our empirical study.Extensive research has targeted the problem of labeling sequence information to solve a variety of problems in natural language processing.Hidden Markov models (HMM) are widely used and considerably well understood models for sequence labeling.Their drawback is that, as most generative models, they are generally computed to maximize the joint likelihood of the training data.In order to define a probability distribution over the sequences of observation and labels, it is necessary to enumerate all possible sequences of observations.Such enumeration is generally prohibitive when the model incorporates many interacting features and long-range dependencies (the reader can find a discussion of the problem in (McCallum et al., 2000)).Conditional models address these concerns.Conditional Markov models (CMM) (Ratnaparkhi, 1996; Klein and Manning, 2002) have been successfully used in sequence labeling tasks incorporating rich feature sets.In a left-to-right CMM as shown in Figure 1(a), the probability of a sequence of L tags is decomposed as: is the vector of observations and each is the index of a spurt.The probability distribution associated with each state of the Markov chain only depends on the preceding tag and the local observation .However, in order to incorporate more than one label dependency and, in particular, to take into account the four pragmatic and , there is then a direct dependency between and , and the probability model becomes .This is a simplifying example; in practice, each label is dependent on a fixed number of other labels. contextual dependencies discussed in the previous subsection, we must augment the structure of our model to obtain a more general one.Such a model is shown in Figure 1(b), a Bayesian network model that is well-understood and that has precisely defined semantics.To this Bayesian network representation, we apply maximum entropy modeling to define a probability distribution at each node ( ) dependent on the observation variable and the five contextual tags used in the four pragmatic dependencies.8 For notational simplicity, the contextual tags representing these pragmatic dependencies are represented here as a vector ( ,, and so on).Given feature functions Again, the only role of the denominator is to ensure that sums to 1, and need not be computed when searching for the most probable tags.Note that in our case, the structure of the Bayesian network is known and need not be inferred, since AP identification is performed before the actual agreement and disagreement classification.Since tag sequences are known during training, the inference of a model for sequence labels is no more difficult than inferring a model in a non-sequential case.We compute the most probable sequence by performing a left-to-right decoding using a beam search.The algorithm is exactly the same as the one described in (Ratnaparkhi, 1996) to find the most probable part-of-speech sequence.We used a large beam of size =100, which is not computationally prohibitive, since the tagset contains only four elements.Note however that this algorithm can lead to search errors.An alternative would be to use a variant of the Viterbi algorithm, which was successfully used in (McCallum et al., 2000) to decode the most probable sequence in a CMM.We had 8135 spurts available for training and testing, and performed two sets of experiments to evaluate the performance of our system.The tools used to perform the training are the same as those described in section 3.4.In the first set of experiments, we reproduced the experimental setting of (Hillard et al., 2003), a three-way classification (BACKCHANNEL and OTHER are merged) using hand-labeled data of a single meeting as a test set and the remaining data as training material; for this experiment, we used the same training set as (Hillard et al., 2003).Performance is reported in Table 6.In the second set of experiments, we aimed at reducing the expected variance of our experimental results and performed N-fold cross-validation in a four-way classification task, at each step retaining the hand-labeled data of a meeting for testing and the rest of the data for training.Table 7 summarizes the performance of our classifier with the different feature sets in this classification task, distinguishing the case where the four label-dependency pragmatic features are available during decoding from the case where they are not.First, the analysis of our results shows that with our three local feature sets only, we obtain substantially better results than (Hillard et al., 2003).This might be due to some additional features the latter work didn’t exploit (e.g. structural features and adjective polarity), and to the fact that the learning algorithm used in our experiments might be more accurate than decision trees in the given task.Second, the table corroborates the findings of (Hillard et al., 2003) that lexical information make the most helpful local features.Finally, we observe that by incorporating label-dependency features representing pragmatic influences, we further improve the performance (about 1% in Table 7).This seems to indicate that modeling label dependencies in our classification problem is useful.We have shown how identification of adjacency pairs can help in designing features representing pragmatic dependencies between agreement and disagreement labels.These features are shown to be informative and to help the classification task, yielding a substantial improvement (1.3% to reach a 86.9% accuracy in three-way classification).We also believe that the present work may be useful in other computational pragmatic research focusing on multi-party dialogs, such as dialog act (DA) classification.Most previous work in that area is limited to interaction between two speakers (e.g.Switchboard, (Stolcke et al., 2000)).When more than two speakers are involved, the question of who is the addressee of an utterance is crucial, since it generally determines what DAs are relevant after the addressee’s last utterance.So, knowledge about adjacency pairs is likely to help DA classification.In future work, we plan to extend our inference process to treat speaker ranking (i.e.AP identification) and agreement/disagreement classification as a single, joint inference problem.Contextual information about agreements and disagreements can also provide useful cues regarding who is the addressee of a given utterance.We also plan to incorporate acoustic features to increase the robustness of our procedure in the case where only speech recognition output is available.We are grateful to Mari Ostendorf and Dustin Hillard for providing us with their agreement and disagreement labeled data.This material is based on research supported by the National Science Foundation under Grant No.IIS-012196.Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.
Seeing Stars: Exploiting Class Relationships For Sentiment Categorization With Respect To Rating Scalesaddress the wherein rather than simply decide whether a review is “thumbs up” or “thumbs down”, as in previous sentiment analysis work, one must determine an author’s evaluation with respect to a multi-point scale (e.g., one to five “stars”).This task represents an interesting twist on standard multi-class text categorization because there are several different degrees of similarity between class labels; for example, “three stars” is intuitively closer to “four stars” than to “one star”.We first evaluate human performance at task.Then, we apply a based on a labeling formulation of the problem, that alters a given-ary classifier’s output in an explicit attempt to ensure that similar items receive similar labels.We show that the meta-algorithm can provide significant improvements over both multi-class and regression versions of SVMs when we employ a novel similarity measure appropriate to the problem.There has recently been a dramatic surge of interest in sentiment analysis, as more and more people become aware of the scientific challenges posed and the scope of new applications enabled by the processing of subjective language.(The papers collected by Qu, Shanahan, and Wiebe (2004) form a representative sample of research in the area.)Most prior work on the specific problem of categorizing expressly opinionated text has focused on the binary distinction of positive vs. negative (Turney, 2002; Pang, Lee, and Vaithyanathan, 2002; Dave, Lawrence, and Pennock, 2003; Yu and Hatzivassiloglou, 2003).But it is often helpful to have more information than this binary distinction provides, especially if one is ranking items by recommendation or comparing several reviewers’ opinions: example applications include collaborative filtering and deciding which conference submissions to accept.Therefore, in this paper we consider generalizing to finer-grained scales: rather than just determine whether a review is “thumbs up” or not, we attempt to infer the author’s implied numerical rating, such as “three stars” or “four stars”.Note that this differs from identifying opinion strength (Wilson, Wiebe, and Hwa, 2004): rants and raves have the same strength but represent opposite evaluations, and referee forms often allow one to indicate that one is very confident (high strength) that a conference submission is mediocre (middling rating).Also, our task differs from ranking not only because one can be given a single item to classify (as opposed to a set of items to be ordered relative to one another), but because there are settings in which classification is harder than ranking, and vice versa.One can apply standard-ary classifiers or regression to this rating-inference problem; independent work by Koppel and Schler (2005) considers such methods.But an alternative approach that explicitly incorporates information about item similarities together with label similarity information (for instance, “one star” is closer to “two stars” than to “four stars”) is to think of the task as one of metric labeling (Kleinberg and Tardos, 2002), where label relations are encoded via a distance metric.This observation yields a meta-algorithm, applicable to both semi-supervised (via graph-theoretic techniques) and supervised settings, that alters a given -ary classifier’s output so that similar items tend to be assigned similar labels.In what follows, we first demonstrate that humans can discern relatively small differences in (hidden) evaluation scores, indicating that rating inference is indeed a meaningful task.We then present three types of algorithms — one-vs-all, regression, and metric labeling — that can be distinguished by how explicitly they attempt to leverage similarity between items and between labels.Next, we consider what item similarity measure to apply, proposing one based on the positive-sentence percentage.Incorporating this new measure within the metriclabeling framework is shown to often provide significant improvements over the other algorithms.We hope that some of the insights derived here might apply to other scales for text classifcation that have been considered, such as clause-level opinion strength (Wilson, Wiebe, and Hwa, 2004); affect types like disgust (Subasic and Huettner, 2001; Liu, Lieberman, and Selker, 2003); reading level (Collins-Thompson and Callan, 2004); and urgency or criticality (Horvitz, Jacobs, and Hovel, 1999).We first ran a small pilot study on human subjects in order to establish a rough idea of what a reasonable classification granularity is: if even people cannot accurately infer labels with respect to a five-star scheme with half stars, say, then we cannot expect a learning algorithm to do so.Indeed, some potential obstacles to accurate rating inference include lack of calibration (e.g., what an understated author intends as high praise may seem lukewarm), author inconsistency at assigning fine-grained ratings, and For data, we first collected Internet movie reviews in English from four authors, removing explicit rating indicators from each document’s text automatically.Now, while the obvious experiment would be to ask subjects to guess the rating that a review represents, doing so would force us to specify a fixed rating-scale granularity in advance.Instead, we examined people’s ability to discern relative differences, because by varying the rating differences represented by the test instances, we can evaluate multiple granularities in a single experiment.Specifically, at intervals over a number of weeks, we authors (a non-native and a native speaker of English) examined pairs of reviews, attemping to determine whether the first review in each pair was (1) more positive than, (2) less positive than, or (3) as positive as the second.The texts in any particular review pair were taken from the same author to factor out the effects of cross-author divergence.As Table 1 shows, both subjects performed perfectly when the rating separation was at least 3 “notches” in the original scale (we define a notch as a half star in a four- or five-star scheme and 10 points in a 100-point scheme).Interestingly, although human performance drops as rating difference decreases, even at a one-notch separation, both subjects handily outperformed the random-choice baseline of 33%.However, there was large variation in accuracy between subjects.2 Because of this variation, we defined two different classification regimes.From the evidence above, a three-class task (categories 0, 1, and 2 — essentially “negative”, “middling”, and “positive”, respectively) seems like one that most people would do quite well at (but we should not assume 100% human accuracy: according to our one-notch results, people may misclassify borderline cases like 2.5 stars).Our study also suggests that people could do at least fairly well at distinguishing full stars in a zero- to four-star scheme.However, when we began to construct five-category datasets for each of our four authors (see below), we found that in each case, either the most negative or the most positive class (but not both) contained only about 5% of the documents.To make the classes more balanced, we folded these minority classes into the adjacent class, thus arriving at a four-class problem (categories 0-3, increasing in positivity).Note that the four-class problem seems to offer more possibilities for leveraging class relationship information than the three-class setting, since it involves more class pairs.Also, even the two-category version of the rating-inference problem for movie reviews has proven quite challenging for many automated classification techniques (Pang, Lee, and Vaithyanathan, 2002; Turney, 2002).We applied the above two labeling schemes to a scale dataset3 containing four corpora of movie reviews.All reviews were automatically preprocessed to remove both explicit rating indicators and objective sentences; the motivation for the latter step is that it has previously aided positive vs. negative classification (Pang and Lee, 2004).All of the 1770, 902, 1307, or 1027 documents in a given corpus were written by the same author.This decision facilitates interpretation of the results, since it factors out the effects of different choices of methods for calibrating authors’ scales.4 We point out that but since our goal is to recover a reviewer’s “true” recommendation, reader-author agreement is more relevant.While another factor might be degree of English fluency, in an informal experiment (six subjects viewing the same three pairs), native English speakers made the only two errors. it is possible to gather author-specific information in some practical applications: for instance, systems that use selected authors (e.g., the Rotten Tomatoes movie-review website — where, we note, not all authors provide explicit ratings) could require that someone submit rating-labeled samples of newlyadmitted authors’ work.Moreover, our results at least partially generalize to mixed-author situations (see Section 5.2).Recall that the problem we are considering is multicategory classification in which the labels can be naturally mapped to a metric space (e.g., points on a line); for simplicity, we assume the distance metric throughout.In this section, we present three approaches to this problem in order of increasingly explicit use of pairwise similarity information between items and between labels.In order to make comparisons between these methods meaningful, we base all three of them on Support Vector Machines (SVMs) as implemented in Joachims’ The standard SVM formulation applies only to binary classification.One-vs-all (OVA) (Rifkin and Klautau, 2004) is a common extension to the-ary case.Training consists of building, for each label, an SVM binary classifier distinguishing labelfrom “not-”.We consider the final output to be a label preference function , defined as the signed distance of (test) item to the side of the vs. not-decision plane.Clearly, OVA makes no explicit use of pairwise label or item relationships.However, it can perform well if each class exhibits sufficiently distinct language; see Section 4 for more discussion.Alternatively, we can take a regression perspective by assuming that the labels come from a discretization of a continuous function mapping from the or negative.Even though Eric Lurio uses a 5 star system, his grading is very relaxed.So, 2 stars can be positive.” Thus, calibration may sometimes require strong familiarity with the authors involved, as anyone who has ever needed to reconcile conflicting referee reports probably knows.feature space to a metric space.If we choose from a family of sufficiently “gradual” functions, then similar items necessarily receive similar labels.In particular, we consider linear,-insensitive SVM regression (Vapnik, 1995; Smola and Sch¨olkopf, 1998); the idea is to find the hyperplane that best fits the training data, but where training points whose labels are within distanceof the hyperplane incur no loss.Then, for (test) instance, the label preference function is the negative of the distance betweenand the value predicted for by the fitted hyperplane function.Wilson, Wiebe, and Hwa (2004) used SVM regression to classify clause-level strength of opinion, reporting that it provided lower accuracy than other methods.However, independently of our work, Koppel and Schler (2005) found that applying linear regression to classify documents (in a different corpus than ours) with respect to a three-point rating scale provided greater accuracy than OVA SVMs and other algorithms.Regression implicitly encodes the “similar items, similar labels” heuristic, in that one can restrict consideration to “gradual” functions.But we can also think of our task as a metric labeling problem (Kleinberg and Tardos, 2002), a special case of the maximum a posteriori estimation problem for Markov random fields, to explicitly encode our desideratum.Suppose we have an initial label preference function , perhaps computed via one of the two methods described above.Also, let be a distance metric on labels, and let denote the nearest neighbors of item according to some item-similarity function .Then, it is quite natural to pose our problem as finding a mapping of instances to labels (respecting the original labels of the training instances) that minimizes learning6 (Atkeson, Moore, and Schaal, 1997).)In a sense, we are using explicit item and label similarity information to increasingly penalize the initial classifier as it assigns more divergent labels to similar items.In this paper, we only report supervised-learning experiments in which the nearest neighbors for any given test item were drawn from the training set alone.In such a setting, the labeling decisions for different test items are independent, so that solving the requisite optimization problem is simple.Aside: transduction The above formulation also allows for transductive semi-supervised learning as well, in that we could allow nearest neighbors to come from both the training and test sets.We intend to address this case in future work, since there are important settings in which one has a small number of labeled reviews and a large number of unlabeled reviews, in which case considering similarities between unlabeled texts could prove quite helpful.In full generality, the corresponding multi-label optimization problem is intractable, but for many families of functions (e.g., convex) there exist practical exact or approximation algorithms based on techniques for finding minimum s-t cuts in graphs (Ishikawa and Geiger, 1998; Boykov, Veksler, and Zabih, 1999; Ishikawa, 2003).Interestingly, previous sentiment analysis research found that a minimum-cut formulation for the binary subjective/objective distinction yielded good results (Pang and Lee, 2004).Of course, there are many other related semi-supervised learning algorithms that we would like to try as well; see Zhu (2005) for a survey.We need to specify an item similarity function to use the metric-labeling formulation described in Section 3.3.We could, as is commonly done, employ a term-overlap-based measure such as the cosine between term-frequency-based document vectors (henceforth “TO(cos)”).However, Table 2 respond to different versions of nearest-neighbor learning, e.g., majority-vote, weighted average of labels, or weighted median of labels. test where is monotonically increasing (we chose unless otherwise specified) and is a trade-off and/or scaling parameter.(The inner summation is familiar from work in locally-weighted shows that in aggregate, the vocabularies of distant classes overlap to a degree surprisingly similar to that of the vocabularies of nearby classes.Thus, item similarity as measured by TO(cos) may not correlate well with similarity of the item’s true labels.We can potentially develop a more useful similarity metric by asking ourselves what, intuitively, accounts for the label relationships that we seek to exploit.A simple hypothesis is that ratings can be determined by the positive-sentence percentage (PSP) of a text, i.e., the number of positive sentences divided by the number of subjective sentences.(Termbased versions of this premise have motivated much sentiment-analysis work for over a decade (Das and Chen, 2001; Tong, 2001; Turney, 2002).)But counterexamples are easy to construct: reviews can contain off-topic opinions, or recount many positive aspects before describing a fatal flaw.We therefore tested the hypothesis as follows.To avoid the need to hand-label sentences as positive or negative, we first created a sentence polarity dataset7 consisting of 10,662 movie-review “snippets” (a striking extract usually one sentence long) downloaded from www.rottentomatoes.com; each snippet was labeled with its source review’s label (positive or negative) as provided by Rotten Tomatoes.Then, we trained a Naive Bayes classifier on this data set and applied it to our scale dataset to identify the positive sentences (recall that objective sentences were already removed).Figure 1 shows that all four authors tend to exhibit a higher PSP when they write a more positive review, and we expect that most typical reviewers would follow suit.Hence, PSP appears to be a promising basis for computing document similarity for our rating-inference task.In particular, we defined to be the two-dimensional vector , and then set the itemsimilarity function required by the metric-labeling optimization function (Section 3.3) to But before proceeding, we note that it is possible that similarity information might yield no extra benefit at all.For instance, we don’t need it if we can reliably identify each class just from some set of distinguishing terms.If we define such terms as frequent ones ( ) that appear in a single class 50% or more of the time, then we do find many instances; some examples for one author are: “meaningless”, “disgusting” (class 0); “pleasant”, “uneven” (class 1); and “oscar”, “gem” (class 2) for the three-class case, and, in the four-class case, “flat”, “tedious” (class 1) versus “straightforward”, “likeable” (class 2).Some unexpected distinguishing terms for this author are “lion” for class 2 (threeclass case), and for class 2 in the four-class case, “jennifer”, for a wide variety of Jennifers.This section compares the accuracies of the approaches outlined in Section 3 on the four corpora comprising our scale dataset.(Results usingerror were qualitatively similar.)Throughout, when we refer to something as “significant”, we mean statistically so with respect to the paired-test, .The results that follow are based on ’s default parameter settings for SVM regression and OVA.Preliminary analysis of the effect of varying the regression parameterin the four-class case revealed that the default value was often optimal.The notation “AB” denotes metric labeling where method A provides the initial label preference function and B serves as similarity measure.To train, we first select the meta-parameters and by running 9-fold cross-validation within the training set.Fixing and to those values yielding the best performance, we then re-train A (but with SVM parameters fixed, as described above) on the whole training set.At test time, the nearest neighbors of each item are also taken from the full training set.Figure 2 summarizes our average 10-fold crossvalidation accuracy results.We first observe from the plots that all the algorithms described in Section 3 always definitively outperform the simple baseline of predicting the majority class, although the improvements are smaller in the four-class case.Incidentally, the data was distributed in such a way that the absolute performance of the baseline itself does not change much between the three- and four-class case (which implies that the three-class datasets were relatively more balanced); and Author c’s datasets seem noticeably easier than the others.We now examine the effect of implicitly using label and item similarity.In the four-class case, regression performed better than OVA (significantly so for two authors, as shown in the righthand table); but for the three-category task, OVA significantly outperforms regression for all four authors.One might initially interprete this “flip” as showing that in the four-class scenario, item and label similarities provide a richer source of information relative to class-specific characteristics, especially since for the non-majority classes there is less data available; whereas in the three-class setting the categories are better modeled as quite distinct entities.However, the three-class results for metric labeling on top of OVA and regression (shown in Figure 2 by black versions of the corresponding icons) show that employing explicit similarities always improves results, often to a significant degree, and yields the best overall accuracies.Thus, we can in fact effectively exploit similarities in the three-class case.Additionally, in both the three- and four- class scenarios, metric labeling often brings the performance of the weaker base method up to that of the stronger one (as indicated by the “disappearance” of upward triangles in corresponding table rows), and never hurts performance significantly.In the four-class case, metric labeling and regression seem roughly equivalent.One possible interpretation is that the relevant structure of the problem is already captured by linear regression (and perhaps a different kernel for regression would have improved its three-class performance).However, according to additional experiments we ran in the four-class situation, the test-set-optimal parameter settings for metric labeling would have produced significant improvements, indicating there may be greater potential for our framework.At any rate, we view the fact that metric labeling performed quite well for both rating scales as a definitely positive result.Q: Metric labeling looks like it’s just combining SVMs with nearest neighbors, and classifier combination often improves performance.Couldn’t we get the same kind of results by combining SVMs with any other reasonable method?A: No.For example, if we take the strongest base SVM method for initial label preferences, but replace PSP with the term-overlap-based cosine (TO(cos)), performance often drops significantly.This result, which is in accordance with Section 4’s data, suggests that choosing an item similarity function that correlates well with label similarity is important.(ovaPSP ovaTO(cos) [3c]; regPSPregTO(cos) [4c]) Q: Could you explain that notation, please?A: Triangles point toward the significantly better algorithm for some dataset.For instance, “M N [3c]” means, “In the 3-class task, method M is significantly better than N for two author datasets and significantly worse for one dataset (so the algorithms were statistically indistinguishable on the remaining dataset)”.When the algorithms being compared are statistically indistinguishable on Average ten-fold cross-validation accuracies.Open icons: SVMs in either one-versus-all (square) or regression (circle) mode; dark versions: metric labeling using the corresponding SVM together with the positive-sentence percentage (PSP).The-axes of the two plots are aligned.Significant differences, three-class data Significant differences, four-class data all four datasets (the “no triangles” case), we indicate this with an equals sign (“=”).Q: Thanks.Doesn’t Figure 1 show that the positive-sentence percentage would be a good classifier even in isolation, so metric labeling isn’t necessary?A: No.Predicting class labels directly from the PSP value via trained thresholds isn’t as effective (ovaPSP threshold PSP [3c]; regPSP threshold PSP [4c]).Alternatively, we could use only the PSP component of metric labeling by setting the label preference function to the constant function 0, but even with test-set-optimal parameter settings, doing so underperforms the trained metric labeling algorithm with access to an iniregPSP 0 [4c]).Q: What about using PSP as one of the features for input to a standard classifier?A: Our focus is on investigating the utility of similarity information.In our particular rating-inference setting, it so happens that the basis for our pairwise similarity measure can be incorporated as an item-specific feature, but we view this as a tangential issue.That being said, preliminary experiments show that metric labeling can be better, barely (for test-set-optimal parameter settings for both algorithms: significantly better results for one author, four-class case; statistically indistinguishable otherwise), although one needs to determine an appropriate weight for the PSP feature to get good performance.Q: You defined the “metric transformation” function as the identity function , imposing greater loss as the distance between labels assigned to two similar items increases.Can you do just as well if you penalize all non-equal label assignments by the same amount, or does the distance between labels really matter?A: You’re asking for a comparison to the Potts model, which sets to the function if , otherwise.In the one setting in which there is a significant difference between the two, the Potts model does worse (ovaPSPovaPSP [3c]).Also, employing the Potts model generally leads to fewer significant improvements over a chosen base method (compare Figure 2’s tables with: regPSPreg [3c]; ovaPSP ova [3c]; ovaPSP ova [4c]; but note that regPSPreg [4c]).We note that optimizing the Potts model in the multi-label case is NPhard, whereas the optimal metric labeling with the identity metric-transformation function can be efficiently obtained (see Section 3.3).Q: Your datasets had many labeled reviews and only one author each.Is your work relevant to settings with many authors but very little data for each?A: As discussed in Section 2, it can be quite difficult to properly calibrate different authors’ scales, since the same number of “stars” even within what is ostensibly the same rating system can mean different things for different authors.But since you ask: we temporarily turned a blind eye to this serious issue, creating a collection of 5394 reviews by 496 authors with at most 80 reviews per author, where we pretended that our rating conversions mapped correctly into a universal rating scheme.Preliminary results on this dataset were actually comparable to the results reported above, although since we are not confident in the class labels themselves, more work is needed to derive a clear analysis of this setting.(Abusing notation, since we’re already playing fast and loose: [3c]: baseline 52.4%, reg 61.4%, regPSP 61.5%, ova (65.4%)ovaPSP (66.3%); [4c]: baseline 38.8%, reg (51.9%) regPSP (52.7%), ova (53.8%)ovaPSP (54.6%)) In future work, it would be interesting to determine author-independent characteristics that can be used on (or suitably adapted to) data for specific authors.Q: How about trying — A: —Yes, there are many alternatives.A few that we tested are described in the Appendix, and we propose some others in the next section.We should mention that we have not yet experimented with all-vs.-all (AVA), another standard binary-tomulti-category classifier conversion method, because we wished to focus on the effect of omitting pairwise information.In independent work on 3-category rating inference for a different corpus, Koppel and Schler (2005) found that regression outperformed AVA, and Rifkin and Klautau (2004) argue that in principle OVA should do just as well as AVA.But we plan to try it out.In this paper, we addressed the rating-inference problem, showing the utility of employing label similarity and (appropriate choice of) item similarity — either implicitly, through regression, or explicitly and often more effectively, through metric labeling.In the future, we would like to apply our methods to other scale-based classification problems, and explore alternative methods.Clearly, varying the kernel in SVM regression might yield better results.Another choice is ordinal regression (McCullagh, 1980; Herbrich, Graepel, and Obermayer, 2000), which only considers the ordering on labels, rather than any explicit distances between them; this approach could work well if a good metric on labels is lacking.Also, one could use mixture models (e.g., combine “positive” and “negative” language models) to capture class relationships (McCallum, 1999; Schapire and Singer, 2000; Takamura, Matsumoto, and Yamada, 2004).We are also interested in framing multi-class but non-scale-based categorization problems as metric labeling tasks.For example, positive vs. negative vs. neutral sentiment distinctions are sometimes considered in which neutral means either objective (Engstr¨om, 2004) or a conflation of objective with a rating of mediocre (Das and Chen, 2001).(Koppel and Schler (2005) in independent work also discuss various types of neutrality.)In either case, we could apply a metric in which positive and negative are closer to objective (or objective+mediocre) than to each other.As another example, hierarchical label relationships can be easily encoded in a label metric.Finally, as mentioned in Section 3.3, we would like to address the transductive setting, in which one has a small amount of labeled data and uses relationships between unlabeled items, since it is particularly well-suited to the metric-labeling approach and may be quite important in practice.Acknowledgments We thank Paul Bennett, Dave Blei, Claire Cardie, Shimon Edelman, Thorsten Joachims, Jon Kleinberg, Oren Kurland, John Lafferty, Guy Lebanon, Pradeep Ravikumar, Jerry Zhu, and the anonymous reviewers for many very useful comments and discussion.We learned of Moshe Koppel and Jonathan Schler’s work while preparing the cameraready version of this paper; we thank them for so quickly answering our request for a pre-print.Our descriptions of their work are based on that pre-print; we apologize in advance for any inaccuracies in our descriptions that result from changes between their pre-print and their final version.We also thank CMU for its hospitality during the year.This paper is based upon work supported in part by the National Science Foundation (NSF) under grant no.IIS-0329064 and CCR-0122581; SRI International under subcontract no.03-000211 on their project funded by the Department of the Interior’s National Business Center; and by an Alfred P. Sloan Research Fellowship.Any opinions, findings, and conclusions or recommendations expressed are those of the authors and do not necessarily reflect the views or official policies, either expressed or implied, of any sponsoring institutions, the U.S. government, or any other entity.In our setting, we can also incorporate class relations by directly altering the output of a binary classifier, as follows.We first train a standard SVM, treating ratings greater than 0.5 as positive labels and others as negative labels.If we then consider the resulting classifier to output a positivity-preference function , we can then learn a series of thresholds to convert this value into the desired label set, under the assumption that the bigger is, the more positive the review.9 This algorithm always outperforms the majority-class baseline, but not to the degree that the best of SVM OVA and SVM regression does.Koppel and Schler (2005) independently found in a three-class study that thresholding a positive/negative classifier trained only on clearly positive or clearly negative examples did not yield large improvements.
Extracting Semantic Orientations Of Words Using Spin ModelWe propose a method for extracting semantic orientations of words: desirable or undesirable.Regarding semantic orientations as spins of electrons, we use the mean field approximation to compute the approximate probability function of the system instead of the intractable actual probability function.We also propose a criterion for parameter selection on the basis of magnetization.Given only a small number of seed words, the proposed method extracts semantic orientations with high accuracy in the experiments on English lexicon.The result is comparable to the best value ever reported.Identification of emotions (including opinions and attitudes) in text is an important task which has a variety of possible applications.For example, we can efficiently collect opinions on a new product from the internet, if opinions in bulletin boards are automatically identified.We will also be able to grasp people’s attitudes in questionnaire, without actually reading all the responds.An important resource in realizing such identification tasks is a list of words with semantic orientation: positive or negative (desirable or undesirable).Frequent appearance of positive words in a document implies that the writer of the document would have a positive attitude on the topic.The goal of this paper is to propose a method for automatically creating such a word list from glosses (i.e., definition or explanation sentences ) in a dictionary, as well as from a thesaurus and a corpus.For this purpose, we use spin model, which is a model for a set of electrons with spins.Just as each electron has a direction of spin (up or down), each word has a semantic orientation (positive or negative).We therefore regard words as a set of electrons and apply the mean field approximation to compute the average orientation of each word.We also propose a criterion for parameter selection on the basis of magnetization, a notion in statistical physics.Magnetization indicates the global tendency of polarization.We empirically show that the proposed method works well even with a small number of seed words.Turney and Littman (2003) proposed two algorithms for extraction of semantic orientations of words.To calculate the association strength of a word with positive (negative) seed words, they used the number of hits returned by a search engine, with a query consisting of the word and one of seed words (e.g., “word NEAR good”, “word NEAR bad”).They regarded the difference of two association strengths as a measure of semantic orientation.They also proposed to use Latent Semantic Analysis to compute the association strength with seed words.An empirical evaluation was conducted on 3596 words extracted from General Inquirer (Stone et al., 1966).Hatzivassiloglou and McKeown (1997) focused on conjunctive expressions such as “simple and well-received” and “simplistic but well-received”, where the former pair of words tend to have the same semantic orientation, and the latter tend to have the opposite orientation.They first classify each conjunctive expression into the same-orientation class or the different-orientation class.They then use the classified expressions to cluster words into the positive class and the negative class.The experiments were conducted with the dataset that they created on their own.Evaluation was limited to adjectives.Kobayashi et al. (2001) proposed a method for extracting semantic orientations of words with bootstrapping.The semantic orientation of a word is determined on the basis of its gloss, if any of their 52 hand-crafted rules is applicable to the sentence.Rules are applied iteratively in the bootstrapping framework.Although Kobayashi et al.’s work provided an accurate investigation on this task and inspired our work, it has drawbacks: low recall and language dependency.They reported that the semantic orientations of only 113 words are extracted with precision 84.1% (the low recall is due partly to their large set of seed words (1187 words)).The handcrafted rules are only for Japanese.Kamps et al. (2004) constructed a network by connecting each pair of synonymous words provided by WordNet (Fellbaum, 1998), and then used the shortest paths to two seed words “good” and “bad” to obtain the semantic orientation of a word.Limitations of their method are that a synonymy dictionary is required, that antonym relations cannot be incorporated into the model.Their evaluation is restricted to adjectives.The method proposed by Hu and Liu (2004) is quite similar to the shortest-path method.Hu and Liu’s method iteratively determines the semantic orientations of the words neighboring any of the seed words and enlarges the seed word set in a bootstrapping manner.Subjective words are often semantically oriented.Wiebe (2000) used a learning method to collect subjective adjectives from corpora.Riloff et al. (2003) focused on the collection of subjective nouns.We later compare our method with Turney and Littman’s method and Kamps et al.’s method.The other pieces of research work mentioned above are related to ours, but their objectives are different from ours.We give a brief introduction to the spin model and the mean field approximation, which are wellstudied subjects both in the statistical mechanics and the machine learning communities (Geman and Geman, 1984; Inoue and Carlucci, 2001; Mackay, 2003).A spin system is an array of N electrons, each of which has a spin with one of two values “+1 (up)” or “−1 (down)”.Two electrons next to each other energetically tend to have the same spin.This model is called the Ising spin model, or simply the spin model (Chandler, 1987).The energy function of a spin system can be represented as where xi and xj (∈ x) are spins of electrons i and j, matrix W = {wij} represents weights between two electrons.In a spin system, the variable vector x follows the Boltzmann distribution: where Z(W) = Ex exp(−QE(x, W)) is the normalization factor, which is called the partition function and Q is a constant called the inversetemperature.As this distribution function suggests, a configuration with a higher energy value has a smaller probability.Although we have a distribution function, computing various probability values is computationally difficult.The bottleneck is the evaluation of Z(W), since there are 2N configurations of spins in this system.We therefore approximate P(x|W) with a simple function Q(x; 0).The set of parameters 0 for Q, is determined such that Q(x; 0) becomes as similar to P(x|W) as possible.As a measure for the distance between P and Q, the variational free energy F is often used, which is defined as the difference between the mean energy with respect to Q and the entropy of Q : The parameters θ that minimizes the variational free energy will be chosen.It has been shown that minimizing F is equivalent to minimizing the KullbackLeibler divergence between P and Q (Mackay, 2003).We next assume that the function Q(x; θ) has the factorial form : Using the mean-field method developed in statistical mechanics, we determine the semantic orientations on the network in a global manner.The global optimization enables the incorporation of possibly noisy resources such as glosses and corpora, while existing simple methods such as the shortest-path method and the bootstrapping method cannot work in the presence of such noisy evidences.Those methods depend on less-noisy data such as a thesaurus.Simple substitution and transformation leads us to the following variational free energy: With the usual method of Lagrange multipliers, we obtain the mean field equation : xi = Pxi xi exp µβxi Pj wijxj)We use the spin model to extract semantic orientations of words.Each spin has a direction taking one of two values: up or down.Two neighboring spins tend to have the same direction from a energetic reason.Regarding each word as an electron and its semantic orientation as the spin of the electron, we construct a lexical network by connecting two words if, for example, one word appears in the gloss of the other word.Intuition behind this is that if a word is semantically oriented in one direction, then the words in its gloss tend to be oriented in the same direction.We construct a lexical network by linking two words if one word appears in the gloss of the other word.Each link belongs to one of two groups: the sameorientation links SL and the different-orientation links DL.If at least one word precedes a negation word (e.g., not) in the gloss of the other word, the link is a different-orientation link.Otherwise the links is a same-orientation link.We next set weights W = (wij) to links: where lij denotes the link between word i and word j, and d(i) denotes the degree of word i, which means the number of words linked with word i.Two words without connections are regarded as being connected by a link of weight 0.We call this network the gloss network (G).We construct another network, the glossthesaurus network (GT), by linking synonyms, antonyms and hypernyms, in addition to the the above linked words.Only antonym links are in DL.We enhance the gloss-thesaurus network with cooccurrence information extracted from corpus.As mentioned in Section 2, Hatzivassiloglou and McKeown (1997) used conjunctive expressions in corpus.Following their method, we connect two adjectives if the adjectives appear in a conjunctive form in the corpus.If the adjectives are connected by “and”, the link belongs to SL.If they are connected by “but”, the link belongs to DL.We call this network the gloss-thesaurus-corpus network (GTC).We suppose that a small number of seed words are given.In other words, we know beforehand the semantic orientations of those given words.We incorporate this small labeled dataset by modifying the previous update rule.Instead of QE(x, W) in Equation (2), we use the following function H(Q, x, W) : where [t] is 1 if t is negative, otherwise 0, and xz is calculated with the right-hand-side of Equation (6), where the penalty term α(xi − ai)2 in Equation (10) is ignored.We choose Q that minimizes this value.However, when a large amount of labeled data is unavailable, the value of pseudo leave-one-out error rate is not reliable.In such cases, we use magnetization m for hyper-parameter prediction: where L is the set of seed words, ai is the orientation of seed word i, and α is a positive constant.This expression means that if xi (i E L) is different from ai, the state is penalized.Using function H, we obtain the new update rule for xi (i E L) : i and anew iare the averages of xi respectively before and after update.What is discussed here was constructed with the reference to work by Inoue and Carlucci (2001), in which they applied the spin glass model to image restoration.Initially, the averages of the seed words are set according to their given orientations.The other averages are set to 0.When the difference in the value of the variational free energy is smaller than a threshold before and after update, we regard computation converged.The words with high final average values are classified as positive words.The words with low final average values are classified as negative words.The performance of the proposed method largely depends on the value of hyper-parameter Q.In order to make the method more practical, we propose criteria for determining its value.When a large labeled dataset is available, we can obtain a reliable pseudo leave-one-out error rate : At a high temperature, spins are randomly oriented (paramagnetic phase, m Pz� 0).At a low temperature, most of the spins have the same direction (ferromagnetic phase, m =� 0).It is known that at some intermediate temperature, ferromagnetic phase suddenly changes to paramagnetic phase.This phenomenon is called phase transition.Slightly before the phase transition, spins are locally polarized; strongly connected spins have the same polarity, but not in a global way.Intuitively, the state of the lexical network is locally polarized.Therefore, we calculate values of m with several different values of Q and select the value just before the phase transition.In our model, the semantic orientations of words are determined according to the averages values of the spins.Despite the heuristic flavor of this decision rule, it has a theoretical background related to maximizer of posterior marginal (MPM) estimation, or ‘finite-temperature decoding’ (Iba, 1999; Marroquin, 1985).In MPM, the average is the marginal distribution over xi obtained from the distribution over x.We should note that the finite-temperature decoding is quite different from annealing type algorithms or ‘zero-temperature decoding’, which correspond to maximum a posteriori (MAP) estimation and also often used in natural language processing (Cowie et al., 1992).Since the model estimation has been reduced to simple update calculations, the proposed model is similar to conventional spreading activation approaches, which have been applied, for example, to word sense disambiguation (Veronis and Ide, 1990).Actually, the proposed model can be regarded as a spreading activation model with a specific update rule, as long as we are dealing with 2-class model (2-Ising model).However, there are some advantages in our modelling.The largest advantage is its theoretical background.We have an objective function and its approximation method.We thus have a measure of goodness in model estimation and can use another better approximation method, such as Bethe approximation (Tanaka et al., 2003).The theory tells us which update rule to use.We also have a notion of magnetization, which can be used for hyperparameter estimation.We can use a plenty of knowledge, methods and algorithms developed in the field of statistical mechanics.We can also extend our model to a multiclass model (Q-Ising model).Another interesting point is the relation to maximum entropy model (Berger et al., 1996), which is popular in the natural language processing community.Our model can be obtained by maximizing the entropy of the probability distribution Q(x) under constraints regarding the energy function.We used glosses, synonyms, antonyms and hypernyms of WordNet (Fellbaum, 1998) to construct an English lexical network.For part-of-speech tagging and lemmatization of glosses, we used TreeTagger (Schmid, 1994).35 stopwords (quite frequent words such as “be” and “have”) are removed from the lexical network.Negation words include 33 words.In addition to usual negation words such as “not” and “never”, we include words and phrases which mean negation in a general sense, such as “free from” and “lack of”.The whole network consists of approximately 88,000 words.We collected 804 conjunctive expressions from Wall Street Journal and Brown corpus as described in Section 4.2.The labeled dataset used as a gold standard is General Inquirer lexicon (Stone et al., 1966) as in the work by Turney and Littman (2003).We extracted the words tagged with “Positiv” or “Negativ”, and reduced multiple-entry words to single entries.As a result, we obtained 3596 words (1616 positive words and 1980 negative words) 1.In the computation of 1Although we preprocessed in the same way as Turney and Littman, there is a slight difference between their dataset and our dataset.However, we believe this difference is insignificant. networks and four different sets of seed words.In the parentheses, the predicted value of Q is written.For cv, no value is written for Q, since 10 different values are obtained. accuracy, seed words are eliminated from these 3596 words.We conducted experiments with different values of 0 from 0.1 to 2.0, with the interval 0.1, and predicted the best value as explained in Section 4.3.The threshold of the magnetization for hyper-parameter estimation is set to 1.0 x 10−5.That is, the predicted optimal value of Q is the largest Q whose corresponding magnetization does not exceeds the threshold value.We performed 10-fold cross validation as well as experiments with fixed seed words.The fixed seed words are the ones used by Turney and Littman: 14 seed words {good, nice, excellent, positive, fortunate, correct, superior, bad, nasty, poor, negative, unfortunate, wrong, inferior}; 4 seed words {good, superior, bad, inferior}; 2 seed words {good, bad}.Table 1 shows the accuracy values of semantic orientation classification for four different sets of seed words and various networks.In the table, cv corresponds to the result of 10-fold cross validation, in which case we use the pseudo leave-one-out error for hyper-parameter estimation, while in other cases we use magnetization.In most cases, the synonyms and the cooccurrence information from corpus improve accuracy.The only exception is the case of 2 seed words, in which G performs better than GT.One possible reason of this inversion is that the computation is trapped in a local optimum, since a small number of seed words leave a relatively large degree of freedom in the solution space, resulting in more local optimal points.We compare our results with Turney and with various networks and four different sets of seed words.In the parenthesis, the actual best value of Q is written, except for cv.Littman’s results.With 14 seed words, they achieved 61.26% for a small corpus (approx.1 x 107 words), 76.06% for a medium-sized corpus (approx.2 × 109 words), 82.84% for a large corpus (approx.1 x 1011 words).Without a corpus nor a thesaurus (but with glosses in a dictionary), we obtained accuracy that is comparable to Turney and Littman’s with a medium-sized corpus.When we enhance the lexical network with corpus and thesaurus, our result is comparable to Turney and Littman’s with a large corpus.We examine how accurately our prediction method for Q works by comparing Table 1 above and Table 2 below.Our method predicts good Q quite well especially for 14 seed words.For small numbers of seed words, our method using magnetization tends to predict a little larger value.We also display the figure of magnetization and accuracy in Figure 1.We can see that the sharp change of magnetization occurs at around Q = 1.0 (phrase transition).At almost the same point, the classification accuracy reaches the peak.5.3 Precision for the Words with High Confidence We next evaluate the proposed method in terms of precision for the words that are classified with high confidence.We regard the absolute value of each average as a confidence measure and evaluate the top words with the highest absolute values of averages.The result of this experiment is shown in Figure 2, for 14 seed words as an example.The top 1000 words achieved more than 92% accuracy.This result shows that the absolute value of each average We also tested the shortest path method and the bootstrapping method on GTC and GT, and obtained low accuracies as expected in the discussion in Section 4. seeds proposed bootstrap 14 83.6 (0.8) 72.8 4 82.3 (0.9) 73.2 2 83.5 (0.7) 71.1 can work as a confidence measure of classification.In order to further investigate the model, we conduct experiments in restricted settings.We first construct a lexical network using only synonyms.We compare the spin model with the shortest-path method proposed by Kamps et al. (2004) on this network, because the shortestpath method cannot incorporate negative links of antonyms.We also restrict the test data to 697 adjectives, which is the number of examples that the shortest-path method can assign a non-zero orientation value.Since the shortest-path method is designed for 2 seed words, the method is extended to use the average shortest-path lengths for 4 seed words and 14 seed words.Table 3 shows the result.Since the only difference is their algorithms, we can conclude that the global optimization of the spin model works well for the semantic orientation extraction.We next compare the proposed method with a simple bootstrapping method proposed by Hu and Liu (2004).We construct a lexical network using synonyms and antonyms.We restrict the test data to 1470 adjectives for comparison of methods.The result in Table 4 also shows that the global optimization of the spin model works well for the semantic orientation extraction.We investigated a number of errors and concluded that there were mainly three types of errors.One is the ambiguity of word senses.For example, one of the glosses of “costly”is “entailing great loss or sacrifice”.The word “great” here means “large”, although it usually means “outstanding” and is positively oriented.Another is lack of structural information.For example, “arrogance” means “overbearing pride evidenced by a superior manner toward the weak”.Although “arrogance” is mistakingly predicted as positive due to the word “superior”, what is superior here is “manner”.The last one is idiomatic expressions.For example, although “brag” means “show off”, neither of “show” and “off” has the negative orientation.Idiomatic expressions often does not inherit the semantic orientation from or to the words in the gloss.The current model cannot deal with these types of errors.We leave their solutions as future work.We proposed a method for extracting semantic orientations of words.In the proposed method, we regarded semantic orientations as spins of electrons, and used the mean field approximation to compute the approximate probability function of the system instead of the intractable actual probability function.We succeeded in extracting semantic orientations with high accuracy, even when only a small number of seed words are available.There are a number of directions for future work.One is the incorporation of syntactic information.Since the importance of each word consisting a gloss depends on its syntactic role. syntactic information in glosses should be useful for classification.Another is active learning.To decrease the amount of manual tagging for seed words, an active learning scheme is desired, in which a small number of good seed words are automatically selected.Although our model can easily extended to a multi-state model, the effectiveness of using such a multi-state model has not been shown yet.Our model uses only the tendency of having the same orientation.Therefore we can extract semantic orientations of new words that are not listed in a dictionary.The validation of such extension will widen the possibility of application of our method.Larger corpora such as web data will improve performance.The combination of our method and the method by Turney and Littman (2003) is promising.Finally, we believe that the proposed model is applicable to other tasks in computational linguistics.
Extracting Relations With Integrated Information Using Kernel MethodsEntity relation detection is a form of information extraction that finds predefined relations between pairs of entities in text.This paper describes a relation detection approach that combines clues from different levels of syntactic processing using kernel methods.Information from three different levels of processing is considered: tokenization, sentence parsing and deep dependency analysis.Each source of information is represented by kernel functions.Then composite kernels are developed to integrate and extend individual kernels so that processing errors occurring at one level can be overcome by information from other levels.We present an evaluation of these methods on the 2004 ACE relation detection task, using Support Vector Machines, and show that each level of syntactic processing contributes useful information for this task.When evaluated on the official test data, our approach produced very competitive ACE value scores.We also compare the SVM with KNN on different kernels.Information extraction subsumes a broad range of tasks, including the extraction of entities, relations and events from various text sources, such as newswire documents and broadcast transcripts.One such task, relation detection, finds instances of predefined relations between pairs of entities, such as a Located-In relation between the entities Centre College and Danville, KY in the phrase Centre College in Danville, KY.The ‘entities’ are the individuals of selected semantic types (such as people, organizations, countries, ...) which are referred to in the text.Prior approaches to this task (Miller et al., 2000; Zelenko et al., 2003) have relied on partial or full syntactic analysis.Syntactic analysis can find relations not readily identified based on sequences of tokens alone.Even ‘deeper’ representations, such as logical syntactic relations or predicate-argument structure, can in principle capture additional generalizations and thus lead to the identification of additional instances of relations.However, a general problem in Natural Language Processing is that as the processing gets deeper, it becomes less accurate.For instance, the current accuracy of tokenization, chunking and sentence parsing for English is about 99%, 92%, and 90% respectively.Algorithms based solely on deeper representations inevitably suffer from the errors in computing these representations.On the other hand, low level processing such as tokenization will be more accurate, and may also contain useful information missed by deep processing of text.Systems based on a single level of representation are forced to choose between shallower representations, which will have fewer errors, and deeper representations, which may be more general.Based on these observations, Zhao et al. (2004) proposed a discriminative model to combine information from different syntactic sources using a kernel SVM (Support Vector Machine).We showed that adding sentence level word trigrams as global information to local dependency context boosted the performance of finding slot fillers for management succession events.This paper describes an extension of this approach to the identification of entity relations, in which syntactic information from sentence tokenization, parsing and deep dependency analysis is combined using kernel methods.At each level, kernel functions (or kernels) are developed to represent the syntactic information.Five kernels have been developed for this task, including two at the surface level, one at the parsing level and two at the deep dependency level.Our experiments show that each level of processing may contribute useful clues for this task, including surface information like word bigrams.Adding kernels one by one continuously improves performance.The experiments were carried out on the ACE RDR (Relation Detection and Recognition) task with annotated entities.Using SVM as a classifier along with the full composite kernel produced the best performance on this task.This paper will also show a comparison of SVM and KNN (k-Nearest-Neighbors) under different kernel setups.Many machine learning algorithms involve only the dot product of vectors in a feature space, in which each vector represents an object in the object domain.Kernel methods (Muller et al., 2001) can be seen as a generalization of feature-based algorithms, in which the dot product is replaced by a kernel function (or kernel) Ψ(X,Y) between two vectors, or even between two objects.Mathematically, as long as Ψ(X,Y) is symmetric and the kernel matrix formed by Ψ is positive semi-definite, it forms a valid dot product in an implicit Hilbert space.In this implicit space, a kernel can be broken down into features, although the dimension of the feature space could be infinite.Normal feature-based learning can be implemented in kernel functions, but we can do more than that with kernels.First, there are many wellknown kernels, such as polynomial and radial basis kernels, which extend normal features into a high order space with very little computational cost.This could make a linearly non-separable problem separable in the high order feature space.Second, kernel functions have many nice combination properties: for example, the sum or product of existing kernels is a valid kernel.This forms the basis for the approach described in this paper.With these combination properties, we can combine individual kernels representing information from different sources in a principled way.Many classifiers can be used with kernels.The most popular ones are SVM, KNN, and voted perceptrons.Support Vector Machines (Vapnik, 1998; Cristianini and Shawe-Taylor, 2000) are linear classifiers that produce a separating hyperplane with largest margin.This property gives it good generalization ability in high-dimensional spaces, making it a good classifier for our approach where using all the levels of linguistic clues could result in a huge number of features.Given all the levels of features incorporated in kernels and training data with target examples labeled, an SVM can pick up the features that best separate the targets from other examples, no matter which level these features are from.In cases where an error occurs in one processing result (especially deep processing) and the features related to it become noisy, SVM may pick up clues from other sources which are not so noisy.This forms the basic idea of our approach.Therefore under this scheme we can overcome errors introduced by one processing level; more particularly, we expect accurate low level information to help with less accurate deep level information.Collins et al. (1997) and Miller et al.(2000) used statistical parsing models to extract relational facts from text, which avoided pipeline processing of data.However, their results are essentially based on the output of sentence parsing, which is a deep processing of text.So their approaches are vulnerable to errors in parsing.Collins et al. (1997) addressed a simplified task within a confined context in a target sentence.Zelenko et al. (2003) described a recursive kernel based on shallow parse trees to detect personaffiliation and organization-location relations, in which a relation example is the least common subtree containing two entity nodes.The kernel matches nodes starting from the roots of two subtrees and going recursively to the leaves.For each pair of nodes, a subsequence kernel on their child nodes is invoked, which matches either contiguous or non-contiguous subsequences of node.Compared with full parsing, shallow parsing is more reliable.But this model is based solely on the output of shallow parsing so it is still vulnerable to irrecoverable parsing errors.In their experiments, incorrectly parsed sentences were eliminated.Culotta and Sorensen (2004) described a slightly generalized version of this kernel based on dependency trees.Since their kernel is a recursive match from the root of a dependency tree down to the leaves where the entity nodes reside, a successful match of two relation examples requires their entity nodes to be at the same depth of the tree.This is a strong constraint on the matching of syntax so it is not surprising that the model has good precision but very low recall.In their solution a bag-of-words kernel was used to compensate for this problem.In our approach, more flexible kernels are used to capture regularization in syntax, and more levels of syntactic information are considered.Kambhatla (2004) described a Maximum Entropy model using features from various syntactic sources, but the number of features they used is limited and the selection of features has to be a manual process.1 In our model, we use kernels to incorporate more syntactic information and let a Support Vector Machine decide which clue is crucial.Some of the kernels are extended to generate high order features.We think a discriminative classifier trained with all the available syntactic features should do better on the sparse data.ACE (Automatic Content Extraction)2 is a research and development program in information extraction sponsored by the U.S. Government.The 2004 evaluation defined seven major types of relations between seven types of entities.The entity types are PER (Person), ORG (Organization), FAC (Facility), GPE (Geo-Political Entity: countries, cities, etc.), LOC (Location), WEA (Weapon) and VEH (Vehicle).Each mention of an entity has a mention type: NAM (proper name), NOM (nominal) or 1 Kambhatla also evaluated his system on the ACE relation detection task, but the results are reported for the 2003 task, which used different relations and different training and test data, and did not use hand-annotated entities, so they cannot be readily compared to our results.PRO (pronoun); for example George W. Bush, the president and he respectively.The seven relation types are EMP-ORG (Employment/Membership/Subsidiary), PHYS (Physical), PER-SOC (Personal/Social), GPE-AFF (GPEAffiliation), Other-AFF (Person/ORG Affiliation), ART (Agent-Artifact) and DISC (Discourse).There are also 27 relation subtypes defined by ACE, but this paper only focuses on detection of relation types.Table 1 lists examples of each relation type. heads of the two entity arguments in a relation are marked.Types are listed in decreasing order of frequency of occurrence in the ACE corpus.Figure 1 shows a sample newswire sentence, in which three relations are marked.In this sentence, we expect to find a PHYS relation between Hezbollah forces and areas, a PHYS relation between Syrian troops and areas and an EMP-ORG relation between Syrian troops and Syrian.In our approach, input text is preprocessed by the Charniak sentence parser (including tokenization and POS tagging) and the GLARF (Meyers et al., 2001) dependency analyzer produced by NYU.Based on treebank parsing, GLARF produces labeled deep dependencies between words (syntactic relations such as logical subject and logical object).It handles linguistic phenomena like passives, relatives, reduced relatives, conjunctions, etc.That's because Israel was expected to retaliate against Hezbollah forces in areas controlled by Syrian troops.In our model, kernels incorporate information from tokenization, parsing and deep dependency analysis.A relation candidate R is defined as where arg1 and arg2 are the two entity arguments which may be related; seq=(t1, t2, ..., tn) is a token vector that covers the arguments and intervening words; link=(t1, t2, ..., tm) is also a token vector, generated from seq and the parse tree; path is a dependency path connecting arg1 and arg2 in the dependency graph produced by GLARF. path can be empty if no such dependency path exists.The difference between link and seq is that link only retains the “important” words in seq in terms of syntax.For example, all noun phrases occurring in seq are replaced by their heads.Words and constituent types in a stop list, such as time expressions, are also removed.A token T is defined as a string triple, where word, pos and base are strings representing the word, part-of-speech and morphological base form of T. Entity is a token augmented with other attributes, where tk is the token associated with E; type, subtype and mtype are strings representing the entity type, subtype and mention type of E. The subtype contains more specific information about an entity.For example, for a GPE entity, the subtype tells whether it is a country name, city name and so on.Mention type includes NAM, NOM and PRO.It is worth pointing out that we always treat an entity as a single token: for a nominal, it refers to its head, such as boys in the two boys; for a proper name, all the words are connected into one token, such as Bashar_Assad.So in a relation example R whose seq is (t1, t2, ..., tn), it is always true that arg1=t1 and arg2=tn.For names, the base form of an entity is its ACE type (person, organization, etc.).To introduce dependencies, we define a dependency token to be a token augmented with a vector of dependency arcs, DT=(word, pos, base, dseq), where dseq = (arc1, ... , arcn ).A dependency arc is ARC = (w, dw, label, e), where w is the current token; dw is a token connected by a dependency to w; and label and e are the role label and direction of this dependency arc respectively.From now on we upgrade the type of tk in arg1 and arg2 to be dependency tokens.Finally, path is a vector of dependency arcs, where l is the length of the path and arci (1<i<l) satisfies arc1.w=arg1.tk, arci+1.w=arci.dw and arcl.dw=arg2.tk.So path is a chain of dependencies connecting the two arguments in R. The arcs in it do not have to be in the same direction.Figure 2 shows a relation example generated from the text “... in areas controlled by Syrian troops”.In this relation example R, arg-, is ((“areas”, “NNS”, “area”, dseq), “LOC”, “Region”, “NOM”), and arg-,.dseq is ((OBJ, areas, in, 1), (OBJ, areas, controlled, 1)). arg2 is ((“troops”, “NNS”, “troop”, dseq), “ORG”, “Government”, “NOM”) and arg2.dseq = ((A-POS, troops, Syrian, 0), (SBJ, troops, controlled, 1)). path is ((OBJ, areas, controlled, 1), (SBJ, controlled, troops, 0)).The value 0 in a dependency arc indicates forward direction from w to dw, and 1 indicates backward direction.The seq and link sequences of R are shown in Figure 2.Some relations occur only between very restricted types of entities, but this is not true for every type of relation.For example, PER-SOC is a relation mainly between two person entities, while PHYS can happen between any type of entity and a GPE or LOC entity.In this section we will describe the kernels designed for different syntactic sources and explain the intuition behind them.We define two kernels to match relation examples at surface level.Using the notation just defined, we can write the two surface kernels as follows: KT is a kernel that matches two tokens.I(x, y) is a binary string match operator that gives 1 if x=y and 0 otherwise.Kernel Ψ1 matches attributes of two entity arguments respectively, such as type, subtype and lexical head of an entity.This is based on the observation that there are type constraints on the two arguments.For instance PER-SOC is a relation mostly between two person entities.So the attributes of the entities are crucial clues.Lexical information is also important to distinguish relation types.For instance, in the phrase U.S. president there is an EMP-ORG relation between president and U.S., while in a U.S. businessman there is a GPE-AFF relation between businessman and U.S. where Operator <t1, t2> concatenates all the string elements in tokens t1 and t2 to produce a new token.So Ψ2 is a kernel that simply matches unigrams and bigrams between the seq sequences of two relation examples.The information this kernel provides is faithful to the text. where min_len is the length of the shorter link sequence in R1 and R2.Ψ3 is a kernel that matches token by token between the link sequences of two relation examples.Since relations often occur in a short context, we expect many of them have similar link sequences.ψ 4 (R1 , R2)= Kpath (R1. path, R2 . path ), where Intuitively the dependency path connecting two arguments could provide a high level of syntactic regularization.However, a complete match of two dependency paths is rare.So this kernel matches the component arcs in two dependency paths in a pairwise fashion.Two arcs can match only when they are in the same direction.In cases where two paths do not match exactly, this kernel can still tell us how similar they are.In our experiments we placed an upper bound on the length of dependency paths for which we computed a non-zero kernel. where This kernel matches the local dependency context around the relation arguments.This can be helpful especially when the dependency path between arguments does not exist.We also hope the dependencies on each argument may provide some useful clues about the entity or connection of the entity to the context outside of the relation example.Having defined all the kernels representing shallow and deep processing results, we can define composite kernels to combine and extend the individual kern d Ψ3 covers the most important clues for this task: information about the two arguments and the word link between them.The polynomial extension is equivalent to adding pairs of features as new features.Intuitively this introduces new features like: the subtype of the first argument is a country name and the word of the second argument is president, which could be a good clue for an EMP-ORG relation.The polynomial kernel is down weighted by a normalization factor because we do not want the high order features to overwhelm the original ones.In our experiment, using polynomial kernels with degree higher than 2 does not produce better results.2) Full kernel This is the final kernel we used for this task, which is a combination of all the previous kernels.In our experiments, we set all the scalar factors to 1.Different values were tried, but keeping the original weight for each kernel yielded the best results for this task.All the individual kernels we designed are explicit.Each kernel can be seen as a matching of features and these features are enumerable on the given data.So it is clear that they are all valid kernels.Since the kernel function set is closed under linear combination and polynomial extension, the composite kernels are also valid.The reason we propose to use a feature-based kernel is that we can have a clear idea of what syntactic clues it represents and what kind of information it misses.This is important when developing or refining kernels, so that we can make them generate complementary information from different syntactic processing results.Experiments were carried out on the ACE RDR (Relation Detection and Recognition) task using hand-annotated entities, provided as part of the ACE evaluation.The ACE corpora contain documents from two sources: newswire (nwire) documents and broadcast news transcripts (bnews).In this section we will compare performance of different kernel setups trained with SVM, as well as different classifiers, KNN and SVM, with the same kernel setup.The SVM package we used is SVMlight.The training parameters were chosen using cross-validation.One-against-all classification was applied to each pair of entities in a sentence.When SVM predictions conflict on a relation example, the one with larger margin will be selected as the final answer.The ACE RDR training data contains 348 documents, 125K words and 4400 relations.It consists of both nwire and bnews documents.Evaluation of kernels was done on the training data using 5-fold cross-validation.We also evaluated the full kernel setup with SVM on the official test data, which is about half the size of the training data.All the data is preprocessed by the Charniak parser and GLARF dependency analyzer.Then relation examples are generated based these results.Table 2 shows the performance of the SVM on different kernel setups.The kernel setups in this experiment are incremental.From this table we can see that adding kernels continuously improves the performance, which indicates they provide additional clues to the previous setup.The argument kernel treats the two arguments as independent entities.The link sequence kernel introduces the syntactic connection between arguments, so adding it to the argument kernel boosted the performance.Setup F shows the performance of adding only dependency kernels to the argument kernel.The performance is not as good as setup B, indicating that dependency information alone is not as crucial as the link sequence. setups.Each setup adds one level of kernels to the previous one except setup F. Evaluated on the ACE training data with 5-fold cross-validation.Fscores marked by * are significantly better than the previous setup (at 95% confidence level).Another observation is that adding the bigram kernel in the presence of all other level of kernels improved both precision and recall, indicating that it helped in both correcting errors in other processing results and providing supplementary information missed by other levels of analysis.In another experiment evaluated on the nwire data only (about half of the training data), adding the bigram kernel improved F-score 0.5% and this improvement is statistically significant. different kernel setups.Types are ordered in decreasing order of frequency of occurrence in the ACE corpus.In SVM training, the same parameters were used for all 7 types.Table 3 shows the performance of SVM and KNN (k Nearest Neighbors) on different kernel setups.For KNN, k was set to 3.In the first setup of KNN, the two kernels which seem to contain most of the important information are used.It performs quite well when compared with the SVM result.The other two tests are based on the full kernel setup.For the two KNN experiments, adding more kernels (features) does not help.The reason might be that all kernels (features) were weighted equally in the composite kernel Φ2 and this may not be optimal for KNN.Another reason is that the polynomial extension of kernels does not have any benefit in KNN because it is a monotonic transformation of similarity values.So the results of KNN on kernel (Ψ1+Ψ3) and Φ1 would be exactly the same.We also tried different k for KNN and k=3 seems to be the best choice in either case.For the four major types of relations SVM does better than KNN, probably due to SVM’s generalization ability in the presence of large numbers of features.For the last three types with many fewer examples, performance of SVM is not as good as KNN.The reason we think is that training of SVM on these types is not sufficient.We tried different training parameters for the types with fewer examples, but no dramatic improvement obtained.We also evaluated our approach on the official ACE RDR test data and obtained very competitive scores.3 The primary scoring metric4 for the ACE evaluation is a 'value' score, which is computed by deducting from 100 a penalty for each missing and spurious relation; the penalty depends on the types of the arguments to the relation.The value scores produced by the ACE scorer for nwire and bnews test data are 71.7 and 68.0 repectively.The value score on all data is 70.1.5 The scorer also reports an F-score based on full or partial match of relations to the keys.The unweighted F-score for this test produced by the ACE scorer on all data is 76.0%.For this evaluation we used nearest neighbor to determine argument ordering and relation subtypes.The classification scheme in our experiments is one-against-all.It turned out there is not so much confusion between relation types.The confusion matrix of predictions is fairly clean.We also tried pairwise classification, and it did not help much.In this paper, we have shown that using kernels to combine information from different syntactic sources performed well on the entity relation detection task.Our experiments show that each level of syntactic processing contains useful information for the task.Combining them may provide complementary information to overcome errors arising from linguistic analysis.Especially, low level information obtained with high reliability helped with the other deep processing results.This design feature of our approach should be best employed when the preprocessing errors at each level are independent, namely when there is no dependency between the preprocessing modules.The model was tested on text with annotated entities, but its design is generic.It can work with noisy entity detection input from an automatic tagger.With all the existing information from other processing levels, this model can be also expected to recover from errors in entity tagging.Kernel functions have many nice properties.There are also many well known kernels, such as radial basis kernels, which have proven successful in other areas.In the work described here, only linear combinations and polynomial extensions of kernels have been evaluated.We can explore other kernel properties to integrate the existing syntactic kernels.In another direction, training data is often sparse for IE tasks.String matching is not sufficient to capture semantic similarity of words.One solution is to use general purpose corpora to create clusters of similar words; another option is to use available resources like WordNet.These word similarities can be readily incorporated into the kernel framework.To deal with sparse data, we can also use deeper text analysis to capture more regularities from the data.Such analysis may be based on newly-annotated corpora like PropBank (Kingsbury and Palmer, 2002) at the University of Pennsylvania and NomBank (Meyers et al., 2004) at New York University.Analyzers based on these resources can generate regularized semantic representations for lexically or syntactically related sentence structures.Although deeper analysis may even be less accurate, our framework is designed to handle this and still obtain some improvement in performance.This research was supported in part by the Defense Advanced Research Projects Agency under Grant N66001-04-1-8920 from SPAWAR San Diego, and by the National Science Foundation under Grant ITS-0325657.This paper does not necessarily reflect the position of the U.S. Government.We wish to thank Adam Meyers of the NYU NLP group for his help in producing deep dependency analyses.
A Composite Kernel To Extract Relations Between Entities With Both Flat And Structured FeaturesThis paper proposes a novel composite kernel for relation extraction.The composite kernel consists of two individual kernels: an entity kernel that allows for entity-related features and a convolution parse tree kernel that models syntactic information of relation examples.The motivation of our method is to fully utilize the nice properties of kernel methods to explore diverse knowledge for relation extraction.Our study illustrates that the composite kernel can effectively capture both flat and structured features without the need for extensive feature engineering, and can also easily scale to include more features.Evaluation on the ACE corpus shows that our method outperforms the previous best-reported methods and significantly outperforms previous two dependency tree kernels for relation extraction.The goal of relation extraction is to find various predefined semantic relations between pairs of entities in text.The research on relation extraction has been promoted by the Message Understanding Conferences (MUCs) (MUC, 19871998) and Automatic Content Extraction (ACE) program (ACE, 2002-2005).According to the ACE Program, an entity is an object or set of objects in the world and a relation is an explicitly or implicitly stated relationship among entities.For example, the sentence “Bill Gates is chairman and chief software architect of Microsoft Corporation.” conveys the ACE-style relation “EMPLOYMENT.exec” between the entities “Bill Gates” (PERSON.Name) and “Microsoft Corporation” (ORGANIZATION.Commercial).In this paper, we address the problem of relation extraction using kernel methods (Schölkopf and Smola, 2001).Many feature-based learning algorithms involve only the dot-product between feature vectors.Kernel methods can be regarded as a generalization of the feature-based methods by replacing the dot-product with a kernel function between two vectors, or even between two objects.A kernel function is a similarity function satisfying the properties of being symmetric and positive-definite.Recently, kernel methods are attracting more interests in the NLP study due to their ability of implicitly exploring huge amounts of structured features using the original representation of objects.For example, the kernels for structured natural language data, such as parse tree kernel (Collins and Duffy, 2001), string kernel (Lodhi et al., 2002) and graph kernel (Suzuki et al., 2003) are example instances of the wellknown convolution kernels1 in NLP.In relation extraction, typical work on kernel methods includes: Zelenko et al. (2003), Culotta and Sorensen (2004) and Bunescu and Mooney (2005).This paper presents a novel composite kernel to explore diverse knowledge for relation extraction.The composite kernel consists of an entity kernel and a convolution parse tree kernel.Our study demonstrates that the composite kernel is very effective for relation extraction.It also shows without the need for extensive feature engineering the composite kernel can not only capture most of the flat features used in the previous work but also exploit the useful syntactic structure features effectively.An advantage of our method is that the composite kernel can easily cover more knowledge by introducing more kernels.Evaluation on the ACE corpus shows that our method outperforms the previous bestreported methods and significantly outperforms the previous kernel methods due to its effective exploration of various syntactic features.The rest of the paper is organized as follows.In Section 2, we review the previous work.Section 3 discusses our composite kernel.Section 4 reports the experimental results and our observations.Section 5 compares our method with the 1 Convolution kernels were proposed for a discrete structure by Haussler (1999) in the machine learning field.This framework defines a kernel between input objects by applying convolution “sub-kernels” that are the kernels for the decompositions (parts) of the objects. previous work from the viewpoint of feature exploration.We conclude our work and indicate the future work in Section 6.Many techniques on relation extraction, such as rule-based (MUC, 1987-1998; Miller et al., 2000), feature-based (Kambhatla 2004; Zhou et al., 2005) and kernel-based (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), have been proposed in the literature.Rule-based methods for this task employ a number of linguistic rules to capture various relation patterns.Miller et al. (2000) addressed the task from the syntactic parsing viewpoint and integrated various tasks such as POS tagging, NE tagging, syntactic parsing, template extraction and relation extraction using a generative model.Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.These methods are very effective for relation extraction and show the bestreported performance on the ACE corpus.However, the problems are that these diverse features have to be manually calibrated and the hierarchical structured information in a parse tree is not well preserved in their parse tree-related features, which only represent simple flat path information connecting two entities in the parse tree through a path of non-terminals and a list of base phrase chunks.Prior kernel-based methods for this task focus on using individual tree kernels to exploit tree structure-related features.Zelenko et al. (2003) developed a kernel over parse trees for relation extraction.The kernel matches nodes from roots to leaf nodes recursively layer by layer in a topdown manner.Culotta and Sorensen (2004) generalized it to estimate similarity between dependency trees.Their tree kernels require the matchable nodes to be at the same layer counting from the root and to have an identical path of ascending nodes from the roots to the current nodes.The two constraints make their kernel high precision but very low recall on the ACE 2003 corpus.Bunescu and Mooney (2005) proposed another dependency tree kernel for relation extraction.Their kernel simply counts the number of common word classes at each position in the shortest paths between two entities in dependency trees.The kernel requires the two paths to have the same length; otherwise the kernel value is zero.Therefore, although this kernel shows performance improvement over the previous one (Culotta and Sorensen, 2004), the constraint makes the two dependency kernels share the similar behavior: good precision but much lower recall on the ACE corpus.The above discussion shows that, although kernel methods can explore the huge amounts of implicit (structured) features, until now the feature-based methods enjoy more success.One may ask: how can we make full use of the nice properties of kernel methods and define an effective kernel for relation extraction?In this paper, we study how relation extraction can benefit from the elegant properties of kernel methods: 1) implicitly exploring (structured) features in a high dimensional space; and 2) the nice mathematical properties, for example, the sum, product, normalization and polynomial expansion of existing kernels is a valid kernel (Schölkopf and Smola, 2001).We also demonstrate how our composite kernel effectively captures the diverse knowledge for relation extraction.In this section, we define the composite kernel and study the effective representation of a relation instance.Our composite kernel consists of an entity kernel and a convolution parse tree kernel.To our knowledge, convolution kernels have not been explored for relation extraction.(1) Entity Kernel: The ACE 2003 data defines four entity features: entity headword, entity type and subtype (only for GPE), and mention type while the ACE 2004 data makes some modifications and introduces a new feature “LDC mention type”.Our statistics on the ACE data reveals that the entity features impose a strong constraint on relation types.Therefore, we design a linear kernel to explicitly capture such features: where R1 and R2 stands for two relation instances, Ei means the ith entity of a relation instance, and KE(•,•) is a simple kernel function over the features of entities: where fi represents the ith entity feature, and the function C(•,•) returns 1 if the two feature values are identical and 0 otherwise.KE(•,•) returns the number of feature values in common of two entities.(2) Convolution Parse Tree Kernel: A convolution kernel aims to capture structured information in terms of substructures.Here we use the same convolution parse tree kernel as described in Collins and Duffy (2001) for syntactic parsing and Moschitti (2004) for semantic role labeling.Generally, we can represent a parse tree T by a vector of integer counts of each sub-tree type (regardless of its ancestors): φ(T) = (# subtree1(T), ..., # subtreei(T), ..., # subtreen(T) ) where # subtreei(T) is the occurrence number of the ith sub-tree type (subtreei) in T. Since the number of different sub-trees is exponential with the parse tree size, it is computationally infeasible to directly use the feature vectorφ(T) .To solve this computational issue, Collins and Duffy (2001) proposed the following parse tree kernel to calculate the dot product between the above high dimensional vectors implicitly. where N1 and N2 are the sets of nodes in trees T1 and T2, respectively, and ( ) I subtree i n is a function that is 1 iff the subtreei occurs with root at node n and zero otherwise, and ∆(n1,n2) is the number of the common subtrees rooted at n1 and n2, i.e.∆(n1, n2) = ∑i Isubtreei (n1 ) ⋅ Isubtreei (n2 ) ∆(n1 , n2) can be computed by the following recursive rules: where nc(n1) is the child number of n1, ch(n,j) is the jth child of node n andλ (0<λ <1) is the decay factor in order to make the kernel value less variable with respect to the subtree sizes.In addition, the recursive rule (3) holds because given two nodes with the same children, one can construct common sub-trees using these children and common sub-trees of further offspring.The parse tree kernel counts the number of common sub-trees as the syntactic similarity measure between two relation instances.The time complexity for computing this kernel is O( |N1  |⋅  |N2|) .In this paper, two composite kernels are defined by combing the above two individual kernels in the following ways: is the coefficient.Evaluation on the development set shows that this composite kernel yields the best performance when α is set to 0.4.Here, Kˆ(•,•) is the normalizedK(•,•), Kp(•,•) is the polynomial expansion of K(•,•) with degree d=2, i.e.Kp(•,•) = (K(•,•)+1)2 , and α is the coefficient.Evaluation on the development set shows that this composite kernel yields the best performance when α is set to 0.23.The polynomial expansion aims to explore the entity bi-gram features, esp. the combined features from the first and second entities, respectively.In addition, due to the different scales of the values of the two individual kernels, they are normalized before combination.This can avoid one kernel value being overwhelmed by that of another one.The entity kernel formulated by eqn.(1) is a proper kernel since it simply calculates the dot product of the entity feature vectors.The tree kernel formulated by eqn.(3) is proven to be a proper kernel (Collins and Duffy, 2001).Since kernel function set is closed under normalization, polynomial expansion and linear combination (Schölkopf and Smola, 2001), the two composite kernels are also proper kernels.A relation instance is encapsulated by a parse tree.Thus, it is critical to understand which portion of a parse tree is important in the kernel calculation.We study five cases as shown in Fig.1. mon sub-tree including the two entities.In other words, the sub-tree is enclosed by the shortest path linking the two entities in the parse tree (this path is also commonly-used as the path tree feature in the feature-based methods).Fig.1 illustrates different representations of an example relation instance.T1 is MCT for the relation instance, where the sub-tree circled by a dashed line is PT, which is also shown in T2 for clarity.The only difference between MCT and PT lies in that MCT does not allow partial production rules (for example, NP4PP is a partial production rule while NP4NP+PP is an entire production rule in the top of T2).For instance, only the most-right child in the most-left sub-tree [NP [CD 200] [JJ domestic] [E1-PER ...]] of T1 is kept in T2.By comparing the performance of T1 and T2, we can evaluate the effect of sub-trees with partial production rules as shown in T2 and the necessity of keeping the whole left and right context sub-trees as shown in T1 in relation extraction.T3 is CPT, where the two sub-trees circled by dashed lines are included as the context to T2 and make T3 context-sensitive.This is to evaluate whether the limited context information in CPT can boost performance.FPT in T4 is formed by removing the two circled nodes in T2.This is to study whether and how the elimination of single non-terminal nodes affects the performance of relation extraction. fits to 200 domestic partners of their own workers in New York”, where the phrase type “E1-PER” denotes that the current node is the 1st entity with type “PERSON”, and likewise for the others.The relation instance is excerpted from the ACE 2003 corpus, where a relation “SOCIAL.Other-Personal” exists between entities “partners” (PER) and “workers” (PER).We use Charniak’s parser (Charniak, 2001) to parse the example sentence.To save space, the FCPT is not shown here.Data: We use the English portion of both the ACE 2003 and 2004 corpora from LDC in our experiments.In the ACE 2003 data, the training set consists of 674 documents and 9683 relation instances while the test set consists of 97 documents and 1386 relation instances.The ACE 2003 data defines 5 entity types, 5 major relation types and 24 relation subtypes.The ACE 2004 data contains 451 documents and 5702 relation instances.It redefines 7 entity types, 7 major relation types and 23 subtypes.Since Zhao and Grishman (2005) use a 5-fold cross-validation on a subset of the 2004 data (newswire and broadcast news domains, containing 348 documents and 4400 relation instances), for comparison, we use the same setting (5-fold cross-validation on the same subset of the 2004 data, but the 5 partitions may not be the same) for the ACE 2004 data.Both corpora are parsed using Charniak’s parser (Charniak, 2001).We iterate over all pairs of entity mentions occurring in the same sentence to generate potential relation instances.In this paper, we only measure the performance of relation extraction models on “true” mentions with “true” chaining of coreference (i.e. as annotated by LDC annotators).Implementation: We formalize relation extraction as a multi-class classification problem.SVM is selected as our classifier.We adopt the one vs. others strategy and select the one with the largest margin as the final answer.The training parameters are chosen using cross-validation (C=2.4 (SVM); λ =0.4(tree kernel)).In our implementation, we use the binary SVMLight (Joachims, 1998) and Tree Kernel Tools (Moschitti, 2004).Precision (P), Recall (R) and F-measure (F) are adopted to measure the performance.In this subsection, we report the experiments of different kernel setups for different purposes. parse tree on relation extraction, we remove the entity-related information from parse trees by replacing the entity-related phrase types (“E1PER” and so on as shown in Fig.1) with “NP”.Table 1 compares the performance of 5 tree kernel setups on the ACE 2003 data using the tree structure information only.It shows that: ACE 2003 five major types using the parse tree structure information only (regardless of any entity-related information) kernel setups over the ACE major types of both the 2003 data (the numbers in parentheses) and the 2004 data (the numbers outside parentheses) the 2003 data although the ACE 2003 data is two times larger than the ACE 2004 data.This may be due to two reasons: 1) The ACE 2004 data defines two new entity types and re-defines the relation types and subtypes in order to reduce the inconsistency between LDC annotators.2) More importantly, the ACE 2004 data defines 43 entity subtypes while there are only 3 subtypes in the 2003 data.The detailed classification in the 2004 data leads to significant performance improvement of 6.2 (54.4-48.2) in Fmeasure over that on the 2003 data.Our composite kernel can achieve 77.3/65.6/70.9 and 76.1/68.4/72.1 in P/R/F over the ACE 2003/2004 major types, respectively. compare our method with previous work on the ACE 2002/2003/2004 data, respectively.They show that our method outperforms the previous methods and significantly outperforms the previous two dependency kernels4.This may be due to two reasons: 1) the dependency tree (Culotta and Sorensen, 2004) and the shortest path (Bunescu and Mooney, 2005) lack the internal hierarchical phrase structure information, so their corresponding kernels can only carry out node-matching directly over the nodes with word tokens; 2) the parse tree kernel has less constraints.That is, it is 4 Bunescu and Mooney (2005) used the ACE 2002 corpus, including 422 documents, which is known to have many inconsistencies than the 2003 version.Culotta and Sorensen (2004) used a generic ACE corpus including about 800 documents (no corpus version is specified).Since the testing corpora are in different sizes and versions, strictly speaking, it is not ready to compare these methods exactly and fairly.Therefore Table 3 is only for reference purpose.We just hope that we can get a few clues from this table. not restricted by the two constraints of the two dependency kernels (identical layer and ancestors for the matchable nodes and identical length of two shortest paths, as discussed in Section 2).The above experiments verify the effectiveness of our composite kernels for relation extraction.They suggest that the parse tree kernel can effectively explore the syntactic features which are critical for relation extraction. both the 2003 and 2004 data for the composite kernel by polynomial expansion (4) Error Analysis: Table 5 reports the error distribution of the polynomial composite kernel over the major types on the ACE data.It shows that 83.5%(198+115/198+115+62) / 85.8%(416 +171/416+171+96) of the errors result from relation detection and only 16.5%/14.2% of the errors result from relation characterization.This may be due to data imbalance and sparseness issues since we find that the negative samples are 8 times more than the positive samples in the training set.Nevertheless, it clearly directs our future work.In this section, we compare our method with the previous work from the feature engineering viewpoint and report some other observations and issues in our experiments.This is to explain more about why our method performs better and significantly outperforms the previous two dependency tree kernels from the theoretical viewpoint.(1) Compared with Feature-based Methods: The basic difference lies in the relation instance representation (parse tree vs. feature vector) and the similarity calculation mechanism (kernel function vs. dot-product).The main difference is the different feature spaces.Regarding the parse tree features, our method implicitly represents a parse tree by a vector of integer counts of each sub-tree type, i.e., we consider the entire sub-tree types and their occurring frequencies.In this way, the parse tree-related features (the path features and the chunking features) used in the featurebased methods are embedded (as a subset) in our feature space.Moreover, the in-between word features and the entity-related features used in the feature-based methods are also captured by the tree kernel and the entity kernel, respectively.Therefore our method has the potential of effectively capturing not only most of the previous flat features but also the useful syntactic structure features.(2) Compared with Previous Kernels: Since our method only counts the occurrence of each sub-tree without considering the layer and the ancestors of the root node of the sub-tree, our method is not limited by the constraints (identical layer and ancestors for the matchable nodes, as discussed in Section 2) in Culotta and Sorensen (2004).Moreover, the difference between our method and Bunescu and Mooney (2005) is that their kernel is defined on the shortest path between two entities instead of the entire subtrees.However, the path does not maintain the tree structure information.In addition, their kernel requires the two paths to have the same length.Such constraint is too strict.(1) Speed Issue: The recursively-defined convolution kernel is much slower compared to feature-based classifiers.In this paper, the speed issue is solved in three ways.First, the inclusion of the entity kernel makes the composite kernel converge fast.Furthermore, we find that the small portion (PT) of a full parse tree can effectively represent a relation instance.This significantly improves the speed.Finally, the parse tree kernel requires exact match between two subtrees, which normally does not occur very frequently.Collins and Duffy (2001) report that in practice, running time for the parse tree kernel is more close to linear (O(JN1J+JN2J), rather than O(JN1J*JN2J ).As a result, using the PC with Intel P4 3.0G CPU and 2G RAM, our system only takes about 110 minutes and 30 minutes to do training on the ACE 2003 (~77k training instances) and 2004 (~33k training instances) data, respectively.(2) Further Improvement: One of the potential problems in the parse tree kernel is that it carries out exact matches between sub-trees, so that this kernel fails to handle sparse phrases (i.e.“a car” vs. “a red car”) and near-synonymic grammar tags (for example, the variations of a verb (i.e. go, went, gone)).To some degree, it could possibly lead to over-fitting and compromise the performance.However, the above issues can be handled by allowing grammar-driven partial rule matching and other approximate matching mechanisms in the parse tree kernel calculation.Finally, it is worth noting that by introducing more individual kernels our method can easily scale to cover more features from a multitude of sources (e.g.Wordnet, gazetteers, etc) that can be brought to bear on the task of relation extraction.In addition, we can also easily implement the feature weighting scheme by adjusting the eqn.(2) and the rule (2) in calculating ∆(n1,n2) (see subsection 3.1).Kernel functions have nice properties.In this paper, we have designed a composite kernel for relation extraction.Benefiting from the nice properties of the kernel methods, the composite kernel could well explore and combine the flat entity features and the structured syntactic features, and therefore outperforms previous bestreported feature-based methods on the ACE corpus.To our knowledge, this is the first research to demonstrate that, without the need for extensive feature engineering, an individual tree kernel achieves comparable performance with the feature-based methods.This shows that the syntactic features embedded in a parse tree are particularly useful for relation extraction and which can be well captured by the parse tree kernel.In addition, we find that the relation instance representation (selecting effective portions of parse trees for kernel calculations) is very important for relation extraction.The most immediate extension of our work is to improve the accuracy of relation detection.This can be done by capturing more features by including more individual kernels, such as the WordNet-based semantic kernel (Basili et al., 2005) and other feature-based kernels.We can also benefit from machine learning algorithms to study how to solve the data imbalance and sparseness issues from the learning algorithm viewpoint.In the future work, we will design a more flexible tree kernel for more accurate similarity measure.Acknowledgements: We would like to thank Dr. Alessandro Moschitti for his great help in using his Tree Kernel Toolkits and fine-tuning the system.We also would like to thank the three anonymous reviewers for their invaluable suggestions.
Minimum Risk Annealing For Training Log-Linear Models10 restarts 1 restart 793 Optimization Procedure labeled dependency acc.[%] Slovenian Bulgarian Dutch Max. like.27.78 47.23 36.78 Min. error 22.52 54.72 36.78 Ann. min. risk 31.16 54.66 36.71 2: dependency accuracy on parsing 200sentence test corpora, after training 10 experts on 1000 senand fitting their weights 200 more.For Slovenian, minimum risk annealing is significantly better than the other training methods, while minimum error is significantly worse.For Bulgarian, both minimum error and annealed minimum risk training achieve significant gains over maximum likelihood, but are indistinguishable from each other.For Dutch, the three methods are indistinguishable. than either and in some cases significantly helped.Note, however, that annealed minimum risk training results in a deterministic classifier just as these other training procedures do.The orthogonal of Bayes risk decoding achieved gains on parsing (Goodman, 1996) and machine translation (Kumar and Byrne, 2004).In speech recognition, researchers have improved decoding by smoothing probability estimates numerically on heldout data in a manner reminiscent of annealing (Goel and Byrne, 2000).We are interested in applying our techniques for approximating nonlinear loss functions to MBR by performing the risk minimization inside the dynamic programming or other decoder.Another training approach that incorporates arloss functions is found in the in the margin-based-learning community (Taskar et al., 2004; Crammer et al., 2004).Like other max-margin techniques, these attempt to make the best hypothesis far away from the inferior ones.The distinction is in using a loss function to calculate the required margins.8 Conclusions Despite the challenging shape of the error surface, we have seen that it is practical to optimize task-specific error measures rather than optimizing likelihood—it produces lower-error systems.Different methods can be used to attempt this global, non-convex optimization.We showed that for MT, and sometimes for dependency parsing, an annealed minimum risk approach to optimization performs significantly better than a previous line-search method that does not smooth the error surface.It never does significantly worse.With such improved methods for minimizing error, we can hope to make better use of task-specific training criteria in NLP.References L. R. Bahl, P. F. Brown, P. V. de Souza, and R. L. Mercer.1988.A new algorithm for the estimation of hidden model parameters.In pages 493–496.E. Charniak and M. Johnson.2005.Coarse-to-fine n-best and maxent discriminative reranking.In pages 173–180.S. F. Chen and R. Rosenfeld.1999.A gaussian prior for smoothing maximum entropy models.Technical report, CS Dept., Carnegie Mellon University.K. Crammer, R. McDonald, and F. Pereira.2004.New large algorithms for structured prediction.In Structured Outputs M. Dreyer, D. A. Smith, and N. A. Smith.2006.Vine parsing and minimum risk reranking for speed and precision.In G. Elidan and N. Friedman.2005.Learning hidden variable The information bottleneck approach.6:81–127.V. Goel and W. J. Byrne.2000.Minimum Bayes-Risk auspeech recognition.Speech and Lan- 14(2):115–135.J. T. Goodman.1996.Parsing algorithms and metrics.In pages 177–183.Hinton.1999.Products of experts.In of volume 1, pages 1–6.K.-U.Hoffgen, H.-U.Simon, and K. S. Van Horn.1995. trainability of single neurons. of Computer and 50(1):114–125.D. S. Johnson and F. P. Preparata.1978.The densest hemiproblem.Comp.6(93–107).S. Katagiri, B.-H. Juang, and C.-H. Lee.1998.Pattern recognition using a family of design algorithms based upon the probabilistic descent method.86(11):2345–2373, November.P. Koehn, F. J. Och, and D. Marcu.2003.Statistical phrasetranslation.In pages 48–54.S. Kumar and W. Byrne.2004.Minimum bayes-risk decodfor statistical machine translation.In J. Lafferty, A. McCallum, and F. C. N. Pereira.2001.Conditional random fields: Probabilistic models for segmenting labeling sequence data.In F. J. Och.2003.Minimum error rate training in statistical translation.In pages 160–167.K. Papineni, S. Roukos, T. Ward, and W.-J.Zhu.2002.A method for automatic evaluation of machine In pages 311–318.K. A. Papineni.1999.Discriminative training via linear In A. Rao and K. Rose.2001.Deterministically annealed deof Hidden Markov Model speech recognizers. on Speech and Audio 9(2):111–126.K. Rose.1998.Deterministic annealing for clustering, compression, classification, regression, and related optimizaproblems.86(11):2210–2239.N. A. Smith and J. Eisner.2004.Annealing techniques for statistical language learning.In pages 486–493.Researchers in empirical natural language processing have expended substantial ink and effort in developing metrics to evaluate systems automatically against gold-standard corpora.The ongoing evaluation literature is perhaps most obvious in the machine translation community’s efforts to better BLEU (Papineni et al., 2002).Despite this research, parsing or machine translation systems are often trained using the much simpler and harsher metric of maximum likelihood.One reason is that in supervised training, the log-likelihood objective function is generally convex, meaning that it has a single global maximum that can be easily found (indeed, for supervised generative models, the parameters at this maximum may even have a closed-form solution).In contrast to the likelihood surface, the error surface for discrete structured prediction is not only riddled with local minima, but piecewise constant This work was supported by an NSF graduate research fellowship for the first author and by NSF ITR grant IIS0313193 and ONR grant N00014-01-1-0685.The views expressed are not necessarily endorsed by the sponsors.We thank Sanjeev Khudanpur, Noah Smith, Markus Dreyer, and the reviewers for helpful discussions and comments. and not everywhere differentiable with respect to the model parameters (Figure 1).Despite these difficulties, some work has shown it worthwhile to minimize error directly (Och, 2003; Bahl et al., 1988).We show improvements over previous work on error minimization by minimizing the risk or expected error—a continuous function that can be derived by combining the likelihood with any evaluation metric (§2).Seeking to avoid local minima, deterministic annealing (Rose, 1998) gradually changes the objective function from a convex entropy surface to the more complex risk surface (§3).We also discuss regularizing the objective function to prevent overfitting (§4).We explain how to compute expected loss under some evaluation metrics common in natural language tasks (§5).We then apply this machinery to training log-linear combinations of models for dependency parsing and for machine translation (§6).Finally, we note the connections of minimum risk training to max-margin training and minimum Bayes risk decoding (§7), and recapitulate our results (§8).In this work, we focus on rescoring with loglinear models.In particular, our experiments consider log-linear combinations of a relatively small number of features over entire complex structures, such as trees or translations, known in some previous work as products of experts (Hinton, 1999) or logarithmic opinion pools (Smith et al., 2005).A feature in the combined model might thus be a log probability from an entire submodel.Giving this feature a small or negative weight can discount a submodel that is foolishly structured, badly trained, or redundant with the other features.For each sentence xi in our training corpus S, we are given Ki possible analyses yi,i, ... yi,K,.(These may be all of the possible translations or parse trees; or only the Ki most probable under some other model; or only a random sample of size Ki.)Each analysis has a vector of real-valued features (i.e., factors, or experts) denoted fi,k.The score of the analysis yi,k is θ · fi,k, the dot product of its features with a parameter vector θ.For each sentence, we obtain a normalized probability distribution over the Ki analyses as We wish to adjust this model’s parameters θ to minimize the severity of the errors we make when using it to choose among analyses.A loss function Ly*(y) assesses a penalty for choosing y when y∗ is correct.We will usually write this simply as L(y) since y∗ is fixed and clear from context.For clearer exposition, we assume below that the total loss over some test corpus is the sum of the losses on individual sentences, although we will revisit that assumption in §5.One training criterion directly mimics test conditions.It looks at the loss incurred if we choose the best analysis of each xi according to the model: Since small changes in θ either do not change the best analysis or else push a different analysis to the top, this objective function is piecewise constant, hence not amenable to gradient descent.Och (2003) observed, however, that the piecewiseconstant property could be exploited to characterize the function exhaustively along any line in parameter space, and hence to minimize it globally along that line.By calling this global line minimization as a subroutine of multidimensional optimization, he was able to minimize (2) well enough to improve over likelihood maximization for training factored machine translation systems.Instead of considering only the best hypothesis for any θ, we can minimize risk, i.e., the expected loss under pθ across all analyses yi: This “smoothed” objective is now continuous and differentiable.However, it no longer exactly mimics test conditions, and it typically remains nonconvex, so that gradient descent is still not guaranteed to find a global minimum.Och (2003) found that such smoothing during training “gives almost identical results” on translation metrics.The simplest possible loss function is 0/1 loss, where L(y) is 0 if y is the true analysis y∗i and 1 otherwise.This loss function does not attempt to give partial credit.Even in this simple case, assuming P =6 NP, there exists no general polynomial-time algorithm for even approximating (2) to within any constant factor, even for Ki = 2 (Hoffgen et al., 1995, from Theorem 4.10.4).1 The same is true for for (3), since for Ki = 2 it can be easily shown that the min 0/1 risk is between 50% and 100% of the min 0/1 loss.Rather than minimizing a loss function suited to the task, many systems (especially for language modeling) choose simply to maximize the probability of the gold standard.The log of this likelihood is a convex function of the parameters θ: where y∗i is the true analysis of sentence xi.The only wrinkle is that pθ(y∗i  |xi) may be left undefined by equation (1) if y∗i is not in our set of Ki hypotheses.When maximizing likelihood, therefore, we will replace y∗i with the min-loss analysis in the hypothesis set; if multiple analyses tie 1Known algorithms are exponential but only in the dimensionality of the feature space (Johnson and Preparata, 1978). for this honor, we follow Charniak and Johnson (2005) in summing their probabilities.2 Maximizing (4) is equivalent to minimizing an upper bound on the expected 0/1 loss Ei(1 − pθ(yi  |xi)).Though the log makes it tractable, this remains a 0/1 objective that does not give partial credit to wrong answers, such as imperfect but useful translations.Most systems should be evaluated and preferably trained on less harsh metrics.To balance the advantages of direct loss minimization, continuous risk minimization, and convex optimization, deterministic annealing attempts the solution of increasingly difficult optimization problems (Rose, 1998).Adding a scale hyperparameter γ to equation (1), we have the following family of distributions: When γ = 0, all yi,k are equally likely, giving the uniform distribution; when γ = 1, we recover the model in equation (1); and as γ —* oc, we approach the winner-take-all Viterbi function that assigns probability 1 to the top-scoring analysis.For a fixed γ, deterministic annealing solves 2An alternative would be to artificially add yz (e.g., the reference translation(s)) to the hypothesis set during training.We then increase γ according to some schedule and optimize θ again.When γ is low, the smooth objective might allow us to pass over local minima that could open up at higher γ.Figure 3 shows how the smoothing is gradually weakened to reach the risk objective (3) as γ —* 1 and approach the true error objective (2) as γ —* oc.Our risk minimization most resembles the work of Rao and Rose (2001), who trained an isolatedword speech recognition system for expected word-error rate.Deterministic annealing has also been used to tackle non-convex likelihood surfaces in unsupervised learning with EM (Ueda and Nakano, 1998; Smith and Eisner, 2004).Other work on “generalized probabilistic descent” minimizes a similar objective function but with γ held constant (Katagiri et al., 1998).Although the entropy is generally higher at lower values of γ, it varies as the optimization changes θ.In particular, a pure unregularized loglinear model such as (5) is really a function of γ·θ, so the optimizer could exactly compensate for increased γ by decreasing the θ vector proportionately!3 Most deterministic annealing procedures, therefore, express a direct preference on the entropy H, and choose γ and θ accordingly: min Ep-Y,e[L(yi,k)] − T · H(pγ,θ) (7) γ,θ In place of a schedule for raising γ, we now use a cooling schedule to lower T from oc to −oc, thereby weakening the preference for high entropy.The Lagrange multiplier T on entropy is called “temperature” due to a satisfying connection to statistical mechanics.Once T is quite cool, it is common in practice to switch to raising γ directly and rapidly (quenching) until some convergence criterion is met (Rao and Rose, 2001).Informally, high temperature or γ < 1 smooths our model during training toward higher-entropy conditional distributions that are not so peaked at the desired analyses y* .Another reason for such smoothing is simply to prevent overfitting to these training examples.A typical way to control overfitting is to use a quadratic regularizing term, ||θ||2 or more generally Ed θ2d/2σ2d.Keeping this small keeps weights low and entropy high.We may add this regularizer to equation (6) or (7).In the maximum likelihood framework, we may subtract it from equation (4), which is equivalent to maximum a posteriori estimation with a diagonal Gaussian prior (Chen and Rosenfeld, 1999).The variance a2d may reflect a prior belief about the potential usefulness of feature d, or may be tuned on heldout data.Another simple regularization method is to stop cooling before T reaches 0 (cf.Elidan and Friedman (2005)).If loss on heldout data begins to increase, we may be starting to overfit.This technique can be used along with annealing or quadratic regularization and can achieve additional accuracy gains, which we report elsewhere (Dreyer et al., 2006).At each temperature setting of deterministic annealing, we need to minimize the expected loss on the training corpus.We now discuss how this expectation is computed.When rescoring, we assume that we simply wish to combine, in some way, statistics of whole sentences4 to arrive at the overall loss for the corpus.We consider evaluation metrics for natural language tasks from two broadly applicable classes: linear and nonlinear.A linear metric is a sum (or other linear combination) of the loss or gain on individual sentences.Accuracy—in dependency parsing, part-of-speech tagging, and other labeling tasks—falls into this class, as do recall, word error rate in ASR, and the crossing-brackets metric in parsing.Thanks to the linearity of expectation, we can easily compute our expected loss in equation (6) by adding up the expected loss on each sentence.Some other metrics involve nonlinear combinations over the sentences of the corpus.One common example is precision, P def = Pi ci/Pi ai, where ci is the number of correctly posited elements, and ai is the total number of posited elements, in the decoding of sentence i.(Depending on the task, the elements may be words, bigrams, labeled constituents, etc.)Our goal is to maximize P, so during a step of deterministic annealing, we need to maximize the expectation of P when the sentences are decoded randomly according to equation (5).Although this expectation is continuous and differentiable as a function of 0, unfortunately it seems hard to compute for any given 0.We observe however that an equivalent goal is to minimize − log P. Taking that as our loss function instead, equation (6) now needs to minimize the expectation of − log P,5 which decomposes somewhat more nicely: = E[log A] − E[log C] (8) where the integer random variables A = Pi ai and C = Pi ci count the number of posited and correctly posited elements over the whole corpus.To approximate E[g(A)], where g is any twicedifferentiable function (here g = log), we can approximate g locally by a quadratic, given by the Taylor expansion of g about A’s mean µA = E[A]: Here µA = Pi µai and Q2A = Pi Q2ai, since A is a sum of independent random variables ai (i.e., given the current model parameters 0, our randomized decoder decodes each sentence independently).In other words, given our quadratic approximation to g, E[g(A)] depends on the (true) distribution of A only through the single-sentence means µai and variances a2ai, which can be found by enumerating the Ki decodings of sentence i.The approximation becomes arbitrarily good as we anneal -y —* oc, since then Q2A —* 0 and E[g(A)] focuses on g near µA.For equation (8), and E[log C] is found similarly.Similar techniques can be used to compute the expected logarithms of some other non-linear metrics, such as F-measure (the harmonic mean of precision and recall)6 and Papineni et al. (2002)’s BLEU translation metric (the geometric mean of several precisions).In particular, the expectation of log BLEU distributes over its N + 1 summands: where Pn is the precision of the n-gram elements in the decoding.7 As is standard in MT research, we take wn = 1/N and N = 4.The first term in the BLEU score is the log brevity penalty, a continuous function of A1 (the total number of unigram tokens in the decoded corpus) that fires only if A1 < r (the average word count of the reference corpus).We again use a Taylor series to approximate the expected log brevity penalty.We mention an alternative way to compute (say) the expected precision C/A: integrate numerically over the joint density of C and A.How can we obtain this density?As (C, A) = Ei(ci, ai) is a sum of independent random length-2 vectors, its mean vector and 2 x 2 covariance matrix can be respectively found by summing the means and covariance matrices of the (ci, ai), each exactly computed from the distribution (5) over Ki hypotheses.We can easily approximate (C, A) by the (continuous) bivariate normal with that mean and covariance matrix8—or else accumulate an exact representation of its (discrete) probability mass function by a sequence of numerical convolutions.We tested the above training methods on two different tasks: dependency parsing and phrasebased machine translation.Since the basic setup was the same for both, we outline it here before describing the tasks in detail.In both cases, we start with 8 to 10 models (the “experts”) already trained on separate training data.To find the optimal coefficients 0 for a loglinear combination of these experts, we use separate development data, using the following procedure due to Och (2003): Our experiments simply compare three procedures at step 4.We may either Since these different optimization procedures will usually find different 0 at step 4, their K-best lists will diverge after the first iteration.For final testing, we selected among several variants of each procedure using a separate small heldout set.Final results are reported for a larger, disjoint test set.For our machine translation experiments, we trained phrase-based alignment template models of Finnish-English, French-English, and GermanEnglish, as follows.For each language pair, we aligned 100,000 sentence pairs from European Parliament transcripts using GIZA++.We then used Philip Koehn’s phrase extraction software to merge the GIZA++ alignments and to extract and score the alignment template model’s phrases (Koehn et al., 2003).The Pharaoh phrase-based decoder uses precisely the setup of this paper.It scores a candidate translation (including its phrasal alignment to the original text) as 0 • f, where f is a vector of the following 8 features: Our goal was to train the weights 0 of these 8 features.We used the method described above, employing the Pharaoh decoder at step 2 to generate the 200-best translations according to the current 0.As explained above, we compared three procedures at step 4: maximum log-likelihood by gradient ascent; minimum error using Och’s linesearch method; and annealed minimum risk.As our development data for training 0, we used 200 sentence pairs for each language pair.Since our methods can be tuned with hyperparameters, we used performance on a separate 200sentence held-out set to choose the best hyperparameter values.The hyperparameter levels for each method were distribution on [−1, 1] x [−1, 1] x • • • , when optimizing 0 at an iteration of step 4.10 by half at each step; then we quenched by doubling -y at each step.(We also ran experiments with quadratic regularization with all Qd at 0.5, 1, or 2 (§4) in addition to the entropy constraint.Also, instead of the entropy constraint, we simply annealed on -y while adding a quadratic regularization term.None of these regularized models beat the best setting of standard deterministic annealing on heldout or test data.)Final results on a separate 2000-sentence test set are shown in table 1.We evaluated translation using BLEU with one reference translation and ngrams up to 4.The minimum risk annealing procedure significantly outperformed maximum likelihood and minimum error training in all three language pairs (p < 0.001, paired-sample permutation test with 1000 replications).Minimum risk annealing generally outperformed minimum error training on the held-out set, regardless of the starting temperature T. However, higher starting temperatures do give better performance and a more monotonic learning curve (Figure 3), a pattern that held up on test data.(In the same way, for minimum error training, 10That is, we run step 4 from several starting points, finishing at several different points; we pick the finishing point with lowest development error (2).This reduces the sensitivity of this method to the starting value of 0.Maximum likelihood is not sensitive to the starting value of 0 because it has only a global optimum; annealed minimum risk is not sensitive to it either, because initially -y Pz� 0, making equation (6) flat. more random restarts give better performance and a more monotonic learning curve—see Figure 4.)Minimum risk annealing did not always win on the training set, suggesting that its advantage is not superior minimization but rather superior generalization: under the risk criterion, multiple lowloss hypotheses per sentence can help guide the learner to the right part of parameter space.Although the components of the translation and language models interact in complex ways, the improvement on Finnish-English may be due in part to the higher weight that minimum risk annealing found for the word penalty.That system is therefore more likely to produce shorter output like i have taken note of your remarks and i also agree with that . than like this longer output from the minimum-error-trained system: i have taken note ofyour remarks and i shall also agree with all that the union.We annealed using our novel expected-BLEU approximation from §5.We found this to perform significantly better on BLEU evaluation than if we trained with a “linearized” BLEU that summed per-sentence BLEU scores (as used in minimum Bayes risk decoding by Kumar and Byrne (2004)).We trained dependency parsers for three different languages: Bulgarian, Dutch, and Slovenian.11 Input sentences to the parser were already tagged for parts of speech.Each parser employed 10 experts, each parameterized as a globally normalized loglinear model (Lafferty et al., 2001).For example, the 9th component of the feature vector fz�k (which described the kth parse of the ith sentence) was the log of that parse’s normalized probability according to the 9th expert.Each expert was trained separately to maximize the conditional probability of the correct parse given the sentence.We used 10 iterations of gradient ascent.To speed training, for each of the first 9 iterations, the gradient was estimated on a (different) sample of only 1000 training sentences.We then trained the vector 0, used to combine the experts, to minimize the number of labeled dependency attachment errors on a 200-sentence development set.Optimization proceeded over lists of the 200-best parses of each sentence produced by a joint decoder using the 10 experts.Evaluating on labeled dependency accuracy on 200 test sentences for each language, we see that minimum error and annealed minimum risk training are much closer than for MT.For Bulgarian and Dutch, they are statistically indistinguishable using a paired-sample permutations test with 1000 replications.Indeed, on Dutch, all three optimization procedures produce indistinguishable results.On Slovenian, annealed minimum risk training does show a significant improvement over the other two methods.Overall, however, the results for this task are mediocre.We are still working on improving the underlying experts.We have seen that annealed minimum risk training provides a useful alternative to maximum likelihood and minimum error training.In our experiments, it never performed significantly worse 11For information on these corpora, see the CoNLL-X shared task on multilingual dependency parsing: http: //nextens.uvt.nl/~conll/. than either and in some cases significantly helped.Note, however, that annealed minimum risk training results in a deterministic classifier just as these other training procedures do.The orthogonal technique of minimum Bayes risk decoding has achieved gains on parsing (Goodman, 1996) and machine translation (Kumar and Byrne, 2004).In speech recognition, researchers have improved decoding by smoothing probability estimates numerically on heldout data in a manner reminiscent of annealing (Goel and Byrne, 2000).We are interested in applying our techniques for approximating nonlinear loss functions to MBR by performing the risk minimization inside the dynamic programming or other decoder.Another training approach that incorporates arbitrary loss functions is found in the structured prediction literature in the margin-based-learning community (Taskar et al., 2004; Crammer et al., 2004).Like other max-margin techniques, these attempt to make the best hypothesis far away from the inferior ones.The distinction is in using a loss function to calculate the required margins.Despite the challenging shape of the error surface, we have seen that it is practical to optimize task-specific error measures rather than optimizing likelihood—it produces lower-error systems.Different methods can be used to attempt this global, non-convex optimization.We showed that for MT, and sometimes for dependency parsing, an annealed minimum risk approach to optimization performs significantly better than a previous line-search method that does not smooth the error surface.It never does significantly worse.With such improved methods for minimizing error, we can hope to make better use of task-specific training criteria in NLP.
Fast Unsupervised Incremental ParsingThis paper describes an incremental parser and an unsupervised learning algorithm for inducing this parser from plain text.The parser uses a representation for syntactic structure similar to dependency links which is well-suited for incremental parsing.In contrast to previous unsupervised parsers, the parser does not use part-of-speech tags and both learning and parsing are local and fast, requiring no explicit clustering or global optimization.The parser is evaluated by converting its output into equivalent bracketing and improves on previously published results for unsupervised parsing from plain text.Grammar induction, the learning of the grammar of a language from unannotated example sentences, has long been of interest to linguists because of its relevance to language acquisition by children.In recent years, interest in unsupervised learning of grammar has also increased among computational linguists, as the difficulty and cost of constructing annotated corpora led researchers to look for ways to train parsers on unannotated text.This can either be semi-supervised parsing, using both annotated and unannotated data (McClosky et al., 2006) or unsupervised parsing, training entirely on unannotated text.The past few years have seen considerable improvement in the performance of unsupervised parsers (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006a; Bod, 2006b) and, for the first time, unsupervised parsers have been able to improve on the right-branching heuristic for parsing English.All these parsers learn and parse from sequences of part-of-speech tags and select, for each sentence, the binary parse tree which maximizes some objective function.Learning is based on global maximization of this objective function over the whole corpus.In this paper I present an unsupervised parser from plain text which does not use parts-of-speech.Learning is local and parsing is (locally) greedy.As a result, both learning and parsing are fast.The parser is incremental, using a new link representation for syntactic structure.Incremental parsing was chosen because it considerably restricts the search space for both learning and parsing.The representation the parser uses is designed for incremental parsing and allows a prefix of an utterance to be parsed before the full utterance has been read (see section 3).The representation the parser outputs can be converted into bracketing, thus allowing evaluation of the parser on standard treebanks.To achieve completely unsupervised parsing, standard unsupervised parsers, working from partof-speech sequences, need first to induce the partsof-speech for the plain text they need to parse.There are several algorithms for doing so (Sch¨utze, 1995; Clark, 2000), which cluster words into classes based on the most frequent neighbors of each word.This step becomes superfluous in the algorithm I present here: the algorithm collects lists of labels for each word, based on neighboring words, and then directly uses these labels to parse.No clustering is performed, but due to the Zipfian distribution of words, high frequency words dominate these lists and parsing decisions for words of similar distribution are guided by the same labels.Section 2 describes the syntactic representation used, section 3 describes the general parser algorithm and sections 4 and 5 complete the details by describing the learning algorithm, the lexicon it constructs and the way the parser uses this lexicon.Section 6 gives experimental results.The representation of syntactic structure which I introduce in this paper is based on links between pairs of words.Given an utterance and a bracketing of that utterance, shortest common cover link sets for the bracketing are defined.The original bracketing can be reconstructed from any of these link sets.An utterance is a sequence of words (x1, ..., x,,,) and a bracket is any sub-sequence (xi,..., xj) of consecutive words in the utterance.A set 13 of brackets over an utterance U is a bracketing of U if every word in U is in some bracket and for any X, Y E 13 either X n Y = 0, X C_ Y or Y C_ X (noncrossing brackets).The depth of a word x E U under a bracket B E 13 (x E B) is the maximal number of brackets X1, ... , X,,, E 13 such that x E X1 C ... C X,,, C B.A word x is agenerator of depth d of B in 13 if x is of minimal depth under B (among all words in B) and that depth is d. A bracket may have more than one generator.A common cover link over an utterance U is a triple x d* y where x, y E U, x 74 y and d is a nonnegative integer.The word x is the base of the link, the word y is its head and d is the depth of the link.The common cover link set RB associated with a bracketing 13 is the set of common cover links over U such that x d* y E RB iff the word x is a generator of depth d of the smallest bracket B E 13 such that x, y E B (see figure 1(a)).Given RB, a simple algorithm reconstructs the bracketing 13: for each word x and depth 0 < d, some � Some of the links in the common cover link set are redundant.The first redundancy is the result of brackets having more than one generator.The bracketing reconstruction algorithm outlined above can construct a bracket from the links based at any of its generators.The bracketing 13 can therefore be reconstructed from a subset R if, for every bracket B E 13, R contains the links based at least at one generator) of B.Such a set R is a representative subset of (see figure 1(b)).A second redundancy in the set follows from the linear transitivity of where if there is a This property implies that longer links can be deduced from shorter links.It is, therefore, sufficient to leave only the shortest necessary links in the set.Given a representative subset R of a shortest common cover link set of is constructed by removing any link which can be deduced from shorter links by linear transitivity.For each representative subset R C_ this defines a unique shortest common cover link set (see figure 1(c)).Given a shortest common cover link set the bracketing which it represents can be calculated by first using linear transitivity to deduce missing links and then applying the bracket reconstruction algorithm outlined above for R.U.Having defined a link-based representation of syntactic structure, it is natural to wonder what the relation is between this representation and standard dependency structures.The main differences between the two representations can all be seen in figure 2.The first difference is in the linking of the NP the boy.While the shortest common cover link set has an exocentric construction for this NP (that is, links going back and forth between the two words), the dependency structure forces us to decide which of the two words in the NP is its head.Considering that linguists have not been able to agree whether it is the determiner or the noun that is the head of an NP, it may be easier for a learning algorithm if it did not have to make such a choice.The second difference between the structures can be seen in the link from know to sleeps.In the shortest common cover link set, there is a path of links connecting know to each of the words separating it from sleeps, while in the dependency structure no such links exist.This property, which I will refer to as adjacency plays an important role in incremental parsing, as explained in the next section.The last main difference between the representations is the assignment of depth to the common cover links.In the present example, this allows us to distinguish between the attachment of the external (subject) and the internal (object) arguments of the verb.Dependencies cannot capture this difference without additional labeling of the links.In what follows, I will restrict common cover links to having depth 0 or 1.This restriction means that any tree represented by a shortest common cover link set will be skewed - every subtree must have a short branch.It seems that this is indeed a property of the syntax of natural languages.Building this restriction into the syntactic representation considerably reduces the search space for both parsing and learning.To calculate a shortest common cover link for an utterance, I will use an incremental parser.Incrementality means that the parser reads the words of the utterance one by one and, as each word is read, the parser is only allowed to add links which have one of their ends at that word.Words which have not yet been read are not available to the parser at this stage.This restriction is inspired by psycholinguistic research which suggests that humans process language incrementally (Crocker et al., 2000).If the incrementality of the parser roughly resembles that of human processing, the result is a significant restriction of parser search space which does not lead to too many parsing errors.The adjacency property described in the previous section makes shortest common cover link sets especially suitable for incremental parsing.Consider the example given in figure 2.When the word the is read, the parser can already construct a link from know to the without worrying about the continuation of the sentence.This link is part of the correct parse whether the sentence turns out to be I know the boy or I know the boy sleeps.A dependency parser, on the other hand, cannot make such a decision before the end of the sentence is reached.If the sentence is I know the boy then a dependency link has to be created from know to boy while if the sentence is I know the boy sleeps then such a link is wrong.This problem is known in psycholinguistics as the problem of reanalysis (Sturt and Crocker, 1996).Assume the incremental parser is processing a prefix (x1, ... , xk) of an utterance and has already deduced a set of links L for this prefix.It can now only add links which have one of their ends at xk and it may never remove any links.From the definitions in section 2.2 it is possible to derive an exact characterization of the links which may be added at each step such that the resulting link set represents some bracketing.It can be shown that any shortest common cover link set can be constructed incrementally under these conditions.As the full specification of these conditions is beyond the scope of this paper, I will only give the main condition, which is based on adjacency.It states that a link may be added from x to y only if for every z between x and y there is a path of links (in L) from x to z but no link from z to y.In the example in figure 2 this means that when the word sleeps is first read, a link to sleeps can be created from know, the and boy but not from I.Given these conditions, the parsing process is simple.At each step, the parser calculates a nonnegative weight (section 5) for every link which may be added between the prefix hx1, ... , xk−1i and xk.It then adds the link with the strongest positive weight and repeats the process (adding a link can change the set of links which may be added).When all possible links are assigned a zero weight by the parser, the parser reads the next word of the utterance and repeats the process.This is a greedy algorithm which optimizes every step separately.The weight function which assigns a weight to a candidate link is lexicalized: the weight is calculated based on the lexical entries of the words which are to be connected by the link.It is the task of the learning algorithm to learn the lexicon.The lexicon stores for each word x a lexical entry.Each such lexical entry is a sequence of adjacency points, holding statistics relevant to the decision whether to link x to some other word.These statistics are given as weights assigned to labels and linking properties.Each adjacency point describes a different link based at x, similar to the specification of the arguments of a word in dependency parsing.Let W be the set of words in the corpus.The set of labels L(W) = W × {0, 1} consists of two labels based on every word w: a class label (w, 0) (denoted by [w]) and an adjacency label (w, 1) (denoted by [w ] or [ w]).The two labels (w, 0) and (w, 1) are said to be opposite labels and, for l ∈ L(W), I write l−1 for the opposite of l. In addition to the labels, there is also a finite set P = {Stop, In*, In, Out} of linking properties.The Stop specifies the strength of non-attachment, In and Out specify the strength of inbound and outbound links and In* is an intermediate value in the induction of inbound and outbound strengths.A lexicon L is a function which assigns each word w ∈ W a lexical entry (... , Aw−2, Aw−1, Aw1 , Aw2 , ...).Each of the Aw i is an adjacency point.Each Aw i is a function Aw i : L(W) ∪ P → R which assigns each label in L(W) and each linking property in P a real valued strength.For each Awi , #(Awi ) is the count of the adjacency point: the number of times the adjacency point was updated.Based on this count, I also define a normalized version of Awi : Awi (l) = Awi (l)�#(Aw i ).Given a sequence of training utterances (Ut)0<t, the learner constructs a sequence of lexicons (Ls)0<s beginning with the zero lexicon L0 (which assigns a zero strength to all labels and linking properties).At each step, the learner uses the parsing function PL, based on the previously learned lexicon Ls to extend the parse L of an utterance Ut.It then uses the result of this parse step (together with the lexicon Ls) to create a new lexicon Ls+1 (it may be that Ls = Ls+1).This operation is a lexicon update.The process then continues with the new lexicon Ls+1.Any of the lexicons Ls constructed by the learner may be used for parsing any utterance U, but as s increases, parsing accuracy should improve.This learning process is open-ended: additional training text can always be added without having to re-run the learner on previous training data.To define a lexicon update, I extend the definition of an utterance to be U = h∅l, x1,... , xn, ∅ri where ∅l and ∅r are boundary markers.The property of adjacency can now be extended to include the boundary markers.A symbol α ∈ U is adjacent to a word x relative to a set of links L over U if for every word z between x and α there is a path of links in L from x to z but there is no link from z to α.In the following example, the adjacencies of x1 are ∅l, x2 and x3: x1 0 > x2 x3 x4 If a link is added from x2 to x3, x4 becomes adjacent to x1 instead of x3 (the adjacencies of x1 are then 0l, x2 and x4): x1 0 > x2 0 > x3 x4 The positions in the utterance adjacent to a word x are indexed by an index i such that i < 0 to the left of x, i > 0 to the right of x and |i |increases with the distance from x.The parser may only add a link from a word x to a word y adjacent to x (relative to the set of links already constructed).Therefore, the lexical entry of x should collect statistics about each of the adjacency positions of x.As seen above, adjacency positions may move, so the learner waits until the parser completes parsing the utterance and then updates each adjacency point Axi with the symbol a at the ith adjacency position of x (relative to the parse generated by the parser).It should be stressed that this update does not depend on whether a link was created from x to a.In particular, whatever links the parser assigns, Ax (−1) and Ax1 are always updated by the symbols which appear immediately before and after x.The following example should clarify the picture.Consider the fragment: put 0 = the �� �� 0 box on All the links in this example, including the absence of a link from box to on, depend on adjacency points of the form Ax(−1) and Ax1 which are updated independently of any links.Based on this alone and regardless of whether a link is created from put to on, Aput 2 will be updated by the word on, which is indeed the second argument of the verb put.The update of Axi by a is given by operations The update of Axi by a begins by incrementing the count: #(Axi ) += 1 If a is a boundary symbol (0l or 0r) or if x and a are words separated by stopping punctuation (full stop, question mark, exclamation mark, semicolon, comma or dash): (In practice, only l = [a] and the 10 strongest labels in AαSign(−i) are updated.Because of the exponential decay in the strength of labels in Aα Sign(−i), this is a good approximation.)If i = −1,1 and a is not a boundary or blocked by punctuation, simple bootstrapping takes place by updating the following properties: To understand the way the labels and properties are calculated, it is best to look at an example.The following table gives the linking properties and strongest labels for the determiner the as learned from the complete Wall Street Journal corpus (only Athe A strong class label [w] indicates that the word w frequently appears in contexts which are similar to the.A strong adjacency label [w ] (or [ w]) indicates that w either frequently appears next to the or that w frequently appears in the same contexts as words which appear next to the.The property Stop counts the number of times a boundary appeared next to the.Because the can often appear at the beginning of an utterance but must be followed by a noun or an adjective, it is not surprising that Stop is stronger than any label on the left but weaker than all labels on the right.In general, it is unlikely that a word has an outbound link on the side on which its Stop strength is stronger than that of any label.The opposite is not true: a label stronger than Stop indicates an attachment but this may also be the result of an inbound link, as in the following entry for to, where the strong labels on the left are a result of an inbound link: to For this reason, the learning process is based on the property •Ax i which indicates where a link is not possible.Since an outbound link on one word is inbound on the other, the inbound/outbound properties of each word are then calculated by a simple bootstrapping process as an average of the opposite properties of the neighboring words.At each step, the parser must assign a non-negative weight to every candidate link x � y which may d be added to an utterance prefix (x1,... , xk), and the link with the largest (non-zero) weight (with a preference for links between xk−1 and xk) is added to the parse.The weight could be assigned directly based on the In and Out properties of either x or y but this method is not satisfactory for three reasons: first, the values of these properties on low frequency words are not reliable; second, the values of the properties on x and y may conflict; third, some words are ambiguous and require different linking in different contexts.To solve these problems, the weight of the link is taken from the values of In and Out on the best matching label between x and y.This label depends on both words and is usually a frequent word with reliable statistics.It serves as a prototype for the relation between x and y.A label l is a matching label between Ax i and AySign(−i) if Axi(l) > Axi (Stop) and either l = (y, 1) or AySign(−i)(l−1) > 0.The best matching label at Axi is the matching label l such that the match strength min(�Axi (l), �AySign(−i)(l−1)) is maximal (if l = (y, 1) then �AySign(−i)(l−1) is defined to be 1).In practice, as before, only the top 10 labels in Axi and AySign(−i) are considered.The best matching label from x to y is calculated between Axi and AySign(−i) such that Axi is on the same side of x as y and was either already used to create a link or is the first adjacency point on that side of x which was not yet used.This means that the adjacency points on each side have to be used one by one, but may be used more than once.The reason is that optional arguments of x usually do not have an adjacency point of their own but have the same labels as obligatory arguments of x and can share their adjacency point.The Axi with the strongest matching label is selected, with a preference for the unused adjacency point.As in the learning process, label matching is blocked between words which are separated by stopping punctuation.The best matching label l = (w, S) from x to y can be either a class (S = 0) or an adjacency (S = 1) label at Axi .If it is a class label, w can be seen as taking the place of x and all words separating it from y (which are already linked to x).If l is an adjacency label, w can be seen to take the place of y.The calculation of the weight Wt(x *d y) of the link from x to y is therefore based on the strengths of the In and Out properties of Awσ where Q = Sign(i) if l = (w, 0) and Q = Sign(−i) if l = (w, 1).In addition, the weight is bounded from above by the best label match strength, s(l): Wt(x �y) = min(s(l), AQ (In*)) where if AQ (In*) < 0 and AQ (Out) < 0 then d = 1 and otherwise d = 0. y inside the smallest bracket covering x.Such links are therefore created in the second case above, when the attachment indication is mixed.To explain the third case, recall that s(l) > 0 means that the label l is stronger than Stop on AZ .This implies a link unless the properties of w block it.One way in which w can block the link is to have a positive strength for the link in the opposite direction.Another way in which the properties of w can block the link is if l = (w, 0) and AQ (Out) < 0, that is, if the learning process has explicitly determined that no outbound link from w (which represents x in this case) is possible.The same conclusion cannot be drawn from a negative value for the In property when l = (w, 1) because, as with standard dependencies, a word determines its outbound links much more strongly than its inbound links.
A fully Bayesian approach to unsupervised part-of-speech taggingUnsupervised learning of linguistic structure is a difficult problem.A common approach is to define a generative model and maximize the probability of the hidden structure given the observed data.Typically, this is done using maximum-likelihood estimation (MLE) of the model parameters.We show using part-of-speech tagging that a fully Bayesian approach can greatly improve performance.Rather than estimating a single set of parameters, the Bayesian approach integrates over all possible parameter values.This difference ensures that the learned structure will have high probability over a range of possible parameters, and permits the use of priors favoring the sparse distributions that are typical of natural language.Our model has the structure of a standard trigram HMM, yet its accuracy is closer to that of a state-of-the-art discriminative model (Smith and Eisner, 2005), up to 14 percentage points better than MLE.We find improvements both when training from data alone, and using a tagging dictionary.Unsupervised learning of linguistic structure is a difficult problem.Recently, several new model-based approaches have improved performance on a variety of tasks (Klein and Manning, 2002; Smith and Eisner, 2005).Nearly all of these approaches have one aspect in common: the goal of learning is to identify the set of model parameters that maximizes some objective function.Values for the hidden variables in the model are then chosen based on the learned parameterization.Here, we propose a different approach based on Bayesian statistical principles: rather than searching for an optimal set of parameter values, we seek to directly maximize the probability of the hidden variables given the observed data, integrating over all possible parameter values.Using part-of-speech (POS) tagging as an example application, we show that the Bayesian approach provides large performance improvements over maximum-likelihood estimation (MLE) for the same model structure.Two factors can explain the improvement.First, integrating over parameter values leads to greater robustness in the choice of tag sequence, since it must have high probability over a range of parameters.Second, integration permits the use of priors favoring sparse distributions, which are typical of natural language.These kinds of priors can lead to degenerate solutions if the parameters are estimated directly.Before describing our approach in more detail, we briefly review previous work on unsupervised POS tagging.Perhaps the most well-known is that of Merialdo (1994), who used MLE to train a trigram hidden Markov model (HMM).More recent work has shown that improvements can be made by modifying the basic HMM structure (Banko and Moore, 2004), using better smoothing techniques or added constraints (Wang and Schuurmans, 2005), or using a discriminative model rather than an HMM (Smith and Eisner, 2005).Non-model-based approaches have also been proposed (Brill (1995); see also discussion in Banko and Moore (2004)).All of this work is really POS disambiguation: learning is strongly constrained by a dictionary listing the allowable tags for each word in the text.Smith and Eisner (2005) also present results using a diluted dictionary, where infrequent words may have any tag.Haghighi and Klein (2006) use a small list of labeled prototypes and no dictionary.A different tradition treats the identification of syntactic classes as a knowledge-free clustering problem.Distributional clustering and dimensionality reduction techniques are typically applied when linguistically meaningful classes are desired (Sch¨utze, 1995; Clark, 2000; Finch et al., 1995); probabilistic models have been used to find classes that can improve smoothing and reduce perplexity (Brown et al., 1992; Saul and Pereira, 1997).Unfortunately, due to a lack of standard and informative evaluation techniques, it is difficult to compare the effectiveness of different clustering methods.In this paper, we hope to unify the problems of POS disambiguation and syntactic clustering by presenting results for conditions ranging from a full tag dictionary to no dictionary at all.We introduce the use of a new information-theoretic criterion, variation of information (Meilˇa, 2002), which can be used to compare a gold standard clustering to the clustering induced from a tagger’s output, regardless of the cluster labels.We also evaluate using tag accuracy when possible.Our system outperforms an HMM trained with MLE on both metrics in all circumstances tested, often by a wide margin.Its accuracy in some cases is close to that of Smith and Eisner’s (2005) discriminative model.Our results show that the Bayesian approach is particularly useful when learning is less constrained, either because less evidence is available (corpus size is small) or because the dictionary contains less information.In the following section, we discuss the motivation for a Bayesian approach and present our model and search procedure.Section 3 gives results illustrating how the parameters of the prior affect results, and Section 4 describes how to infer a good choice of parameters from unlabeled data.Section 5 presents results for a range of corpus sizes and dictionary information, and Section 6 concludes.In model-based approaches to unsupervised language learning, the problem is formulated in terms of identifying latent structure from data.We define a model with parameters 0, some observed variables w (the linguistic input), and some latent variables t (the hidden structure).The goal is to assign appropriate values to the latent variables.Standard approaches do so by selecting values for the model parameters, and then choosing the most probable variable assignment based on those parameters.For example, maximum-likelihood estimation (MLE) seeks parameters 0� such that where P(w|0) = & P(w, t|0).Sometimes, a non-uniform prior distribution over 0 is introduced, in which case 0� is the maximum a posteriori (MAP) solution for 0: The values of the latent variables are then taken to be those that maximize P(t|w, �0).In contrast, the Bayesian approach we advocate in this paper seeks to identify a distribution over latent variables directly, without ever fixing particular values for the model parameters.The distribution over latent variables given the observed data is obtained by integrating over all possible values of 0: This distribution can be used in various ways, including choosing the MAP assignment to the latent variables, or estimating expected values for them.To see why integrating over possible parameter values can be useful when inducing latent structure, consider the following example.We are given a coin, which may be biased (t = 1) or fair (t = 0), each with probability .5.Let 0 be the probability of heads.If the coin is biased, we assume a uniform distribution over 0, otherwise 0 = .5.We observe w, the outcomes of 10 coin flips, and we wish to determine whether the coin is biased (i.e. the value of t).Assume that we have a uniform prior on B, with p(B) = 1 for all B ∈ [0, 1].First, we apply the standard methodology of finding the MAP estimate for B and then selecting the value of t that maximizes P(t|w, B).In this case, an elementary calculation shows that the MAP estimate is B = nH/10, where nH is the number of heads in w (likewise, nT is the number of tails).Consequently, P(t|w, �B) favors t = 1 for any sequence that does not contain exactly five heads, and assigns equal probability tot = 1 and t = 0 for any sequence that does contain exactly five heads — a counterintuitive result.In contrast, using some standard results in Bayesian analysis we can show that applying Equation 3 yields approach is sensitive to the robustness of a choice of t to the value of B, as illustrated in Figure 1.Even though a sequence (Figure 1 (a)), P(t = B) is only greater than 0.5 for a small range of B around B (Figure 1 (b)), meaning that the choice oft = 1 is not very robust to variation in B.In contrast, a sequence with nH = 8 favors t = 1 for a wide range of B around B.By integrating over B, Equation 3 takes into account the consequences of possible variation in B.Another advantage of integrating over B is that it permits the use of linguistically appropriate priors.In many linguistic models, including HMMs, the distributions over variables are multinomial.For a multinomial with parameters B = ... , BK), a natural choice of prior is the K-dimensional Dirichlet distribution, which is conjugate to the For simplicity, we initially assume that all K parameters (also known as hyperparameters) of the Dirichlet distribution are equal to Q, i.e. the Diri chlet is symmetric.The value of Q determines which parameters B will have high probability: when Q = 1, all parameter values are equally likely; when Q > 1, multinomials that are closer to uniform are prior is conjugate to a distribution if the posterior has the same form as the pri d B as a function of B. mation.For a sequence of draws x = ... , xn) from a multinomial distribution B with observed counts ... , nK, a symmetric prior over B yields the MAP estimate Bk = When Q 1, standard MLE techniques such as EM can be used to find the MAP estimate simply by adding of size Q 1 to each of the expected counts nk at each iteration.However, when Q < 1, the values of B that set one or more of the Bk equal to 0 can have infinitely high posterior probability, meaning that MAP estimation can yield degenerate solutions.If, instead of estimating B, we integrate over all possible values, we no longer encounter such difficulties.Instead, the probability that outcome xi value of a latent variable, t, from observed data, w, chooses a value of t robust to uncertainty in B.(a) Posterior distribution on B given w. (b) Probability preferred; and when Q < 1, high probability is assigned to sparse multinomials, where one or more parameters are at or near 0.Typically, linguistic structures are characterized by sparse distributions (e.g., POS tags are followed with high probability by only a few other tags, and have highly skewed output distributions).Consequently, it makes sense to use a Dirichlet prior with Q < 1.However, as noted by Johnson et al. (2007), this choice of Q leads to difficulties with MAP estiwhere nk is the number of times k occurred in x−i.2.3 Inference See MacKay and Peto (1995) for a derivation.To perform inference in our model, we use Gibbs 2.2 Model Definition sampling (Geman and Geman, 1984), a stochastic Our model has the structure of a standard trigram procedure that produces samples from the posterior HMM, with the addition of symmetric Dirichlet pri- distribution P(t|w, α, β) a P(w|t, β)P(t|α).We ors over the transition and output distributions: initialize the tags at random, then iteratively resamti|ti−1 = t,ti−2 = t′, τ(t,t′) — Mult(τ(t,t′)) ple each tag according to its conditional distribution wi|ti = t, ω(t) — Mult(ω(t)) given the current values of all other tags.Exchangeτ(t,t′)|α — Dirichlet(α) ability allows us to treat the current counts of the ω(t)|β — Dirichlet(β) other tag trigrams and outputs as “previous” obserwhere ti and wi are the ith tag and word.We assume vations.The only complication is that resampling that sentence boundaries are marked with a distin- a tag changes the identity of three trigrams at once, guished tag.For a model with T possible tags, each and we must account for this in computing its condiof the transition distributions τ(t,t′) has T compo- tional distribution.The sampling distribution for ti nents, and each of the output distributions ω(t) has is given in Figure 2.Wt components, where Wt is the number of word In Bayesian statistical inference, multiple samples types that are permissible outputs for tag t. We will from the posterior are often used in order to obtain use τ and ω to refer to the entire transition and out- statistics such as the expected values of model variput parameter sets.This model assumes that the ables.For POS tagging, estimates based on multiprior over state transitions is the same for all his- ple samples might be useful if we were interested in, tories, and the prior over output distributions is the for example, the probability that two words have the same for all states.We relax the latter assumption in same tag.However, computing such probabilities Section 4. across all pairs of words does not necessarily lead to Under this model, Equation 5 gives us a consistent clustering, and the result would be diffin(ti−2,ti−1,ti) + α cult to evaluate.Using a single sample makes stanP(ti|t−i, α) = (6) dard evaluation methods possible, but yields subn(ti−2,ti−1) + Tα optimal results because the value for each tag is samn(ti,wi) + β pled from a distribution, and some tags will be asP(wi|ti, t−i,w−i,β) = (7) signed low-probability values.Our solution is to n(ti) + Wtiβ treat the Gibbs sampler as a stochastic search prowhere n(ti−2,ti−1,ti) and n(ti,wi) are the number of cedure with the goal of identifying the MAP tag seoccurrences of the trigram (ti−2,ti−1,ti) and the quence.This can be done using tempering (annealtag-word pair (ti, wi) in the i — 1 previously gener- ing), where a temperature of φ is equivalent to raisated tags and words.Note that, by integrating out ing the probabilities in the sampling distribution to the parameters τ and ω, we induce dependencies the power of 1 φ.As φ approaches 0, even a single between the variables in the model.The probabil- sample will provide a good MAP estimate. ity of generating a particular trigram tag sequence 3 Fixed Hyperparameter Experiments (likewise, output) depends on the number of times 3.1 Method that sequence (output) has been generated previ- Our initial experiments follow in the tradition begun ously.Importantly, trigrams (and outputs) remain by Merialdo (1994), using a tag dictionary to conexchangeable: the probability of a set of trigrams strain the possible parts of speech allowed for each (outputs) is the same regardless of the order in which word.(This also fixes Wt, the number of possible it was generated.The property of exchangeability is words for tag t.) The dictionary was constructed by crucial to the inference algorithm we describe next. listing, for each word, all tags found for that word in 747 the entire WSJ treebank.For the experiments in this section, we used a 24,000-word subset of the treebank as our unlabeled training corpus.54.5% of the tokens in this corpus have at least two possible tags, with the average number of tags per token being 2.3.We varied the values of the hyperparameters α and Q and evaluated overall tagging accuracy.For comparison with our Bayesian HMM (BHMM) in this and following sections, we also present results from the Viterbi decoding of an HMM trained using MLE by running EM to convergence (MLHMM).Where direct comparison is possible, we list the scores reported by Smith and Eisner (2005) for their conditional random field model trained using contrastive estimation (CRF/CE).2 For all experiments, we ran our Gibbs sampling algorithm for 20,000 iterations over the entire data set.The algorithm was initialized with a random tag assignment and a temperature of 2, and the temperature was gradually decreased to .08.Since our inference procedure is stochastic, our reported results are an average over 5 independent runs.Results from our model for a range of hyperparameters are presented in Table 1.With the best choice of hyperparameters (α = .003, Q = 1), we achieve average tagging accuracy of 86.8%.This far surpasses the MLHMM performance of 74.5%, and is closer to the 90.1% accuracy of CRF/CE on the same data set using oracle parameter selection.The effects of α, which determines the probabil2Results of CRF/CE depend on the set of features used and the contrast neighborhood.In all cases, we list the best score reported for any contrast neighborhood using trigram (but no spelling) features.To ensure proper comparison, all corpora used in our experiments consist of the same randomized sets of sentences used by Smith and Eisner.Note that training on sets of contiguous sentences from the beginning of the treebank consistently improves our results, often by 1-2 percentage points or more.MLHMM scores show less difference between randomized and contiguous corpora.BHMM as a function of the hyperparameters α and Q.Results are averaged over 5 runs on the 24k corpus with full tag dictionary.Standard deviations in most cases are less than .5. ity of the transition distributions, are stronger than the effects of Q, which determines the probability of the output distributions.The optimal value of .003 for α reflects the fact that the true transition probability matrix for this corpus is indeed sparse.As α grows larger, the model prefers more uniform transition probabilities, which causes it to perform worse.Although the true output distributions tend to be sparse as well, the level of sparseness depends on the tag (consider function words vs. content words in particular).Therefore, a value of Q that accurately reflects the most probable output distributions for some tags may be a poor choice for other tags.This leads to the smaller effect of Q, and suggests that performance might be improved by selecting a different Q for each tag, as we do in the next section.A final point worth noting is that even when α = Q = 1 (i.e., the Dirichlet priors exert no influence) the BHMM still performs much better than the MLHMM.This result underscores the importance of integrating over model parameters: the BHMM identifies a sequence of tags that have high probability over a range of parameter values, rather than choosing tags based on the single best set of parameters.The improved results of the BHMM demonstrate that selecting a sequence that is robust to variations in the parameters leads to better performance.In our initial experiments, we experimented with different fixed values of the hyperparameters and reported results based on their optimal values.However, choosing hyperparameters in this way is timeconsuming at best and impossible at worst, if there is no gold standard available.Luckily, the Bayesian approach allows us to automatically select values for the hyperparameters by treating them as additional variables in the model.We augment the model with priors over the hyperparameters (here, we assume an improper uniform prior), and use a single Metropolis-Hastings update (Gilks et al., 1996) to resample the value of each hyperparameter after each iteration of the Gibbs sampler.Informally, to update the value of hyperparameter α, we sample a proposed new value α′ from a normal distribution with p = α and a = .1α.The probability of accepting the new value depends on the ratio between P(t|w, α) and P(t|w, α′) and a term correcting for the asymmetric proposal distribution.Performing inference on the hyperparameters allows us to relax the assumption that every tag has the same prior on its output distribution.In the experiments reported in the following section, we used two different versions of our model.The first version (BHMM1) uses a single value of Q for all word classes (as above); the second version (BHMM2) uses a separate Qj for each tag class j.In this set of experiments, we used the full tag dictionary (as above), but performed inference on the hyperparameters.Following Smith and Eisner (2005), we trained on four different corpora, consisting of the first 12k, 24k, 48k, and 96k words of the WSJ corpus.For all corpora, the percentage of ambiguous tokens is 54%-55% and the average number of tags per token is 2.3.Table 2 shows results for the various models and a random baseline (averaged by the various models on different sized corpora.BHMM1 and BHMM2 use hyperparameter inference; CRF/CE uses parameter selection based on an unlabeled development set.Standard deviations (a) for the BHMM results fell below those shown for each corpus size. over 5 random tag assignments).Hyperparameter inference leads to slightly lower scores than are obtained by oracle hyperparameter selection, but both versions of BHMM are still far superior to MLHMM for all corpus sizes.Not surprisingly, the advantages of BHMM are most pronounced on the smallest corpus: the effects of parameter integration and sensible priors are stronger when less evidence is available from the input.In the limit as corpus size goes to infinity, the BHMM and MLHMM will make identical predictions.In unsupervised learning, it is not always reasonable to assume that a large tag dictionary is available.To determine the effects of reduced or absent dictionary information, we ran a set of experiments inspired by those of Smith and Eisner (2005).First, we collapsed the set of 45 treebank tags onto a smaller set of 17 (the same set used by Smith and Eisner).We created a full tag dictionary for this set of tags from the entire treebank, and also created several reduced dictionaries.Each reduced dictionary contains the tag information only for words that appear at least d times in the training corpus (the 24k corpus, for these experiments).All other words are fully ambiguous between all 17 classes.We ran tests with d = 1, 2, 3, 5, 10, and oc (i.e., knowledge-free syntactic clustering).With standard accuracy measures, it is difficult to variation of information between clusterings induced by the assigned and gold standard tags as the amount of information in the dictionary is varied.Standard deviations (Q) for the BHMM results fell below those shown in each column.The percentage of ambiguous tokens and average number of tags per token for each value of d is also shown. evaluate the quality of a syntactic clustering when no dictionary is used, since cluster names are interchangeable.We therefore introduce another evaluation measure for these experiments, a distance metric on clusterings known as variation of information (Meilˇa, 2002).The variation of information (VI) between two clusterings C (the gold standard) and C′ (the found clustering) of a set of data points is a sum of the amount of information lost in moving from C to C′, and the amount that must be gained.It is defined in terms of entropy H and mutual information I: V I(C, C′) = H(C) + H(C′) − 2I(C, C′).Even when accuracy can be measured, VI may be more informative: two different tag assignments may have the same accuracy but different VI with respect to the gold standard if the errors in one assignment are less consistent than those in the other.Table 3 gives the results for this set of experiments.One or both versions of BHMM outperform MLHMM in terms of tag accuracy for all values of d, although the differences are not as great as in earlier experiments.The differences in VI are more striking, particularly as the amount of dictionary information is reduced.When ambiguity is greater, both versions of BHMM show less confusion with respect to the true tags than does MLHMM, and BHMM2 performs the best in all circumstances.The confusion matrices in Figure 3 provide a more intuitive picture of the very different sorts of clusterings produced by MLHMM and BHMM2 when no tag dictionary is available.Similar differences hold to a lesser degree when a partial dictionary is provided.With MLHMM, different tokens of the same word type are usually assigned to the same cluster, but types are assigned to clusters more or less at random, and all clusters have approximately the same number of types (542 on average, with a standard deviation of 174).The clusters found by BHMM2 tend to be more coherent and more variable in size: in the 5 runs of BHMM2, the average number of types per cluster ranged from 436 to 465 (i.e., tokens of the same word are spread over fewer clusters than in MLHMM), with a standard deviation between 460 and 674.Determiners, prepositions, the possessive marker, and various kinds of punctuation are mostly clustered coherently.Nouns are spread over a few clusters, partly due to a distinction found between common and proper nouns.Likewise, modal verbs and the copula are mostly separated from other verbs.Errors are often sensible: adjectives and nouns are frequently confused, as are verbs and adverbs.The kinds of results produced by BHMM1 and BHMM2 are more similar to each other than to the results of MLHMM, but the differences are still informative.Recall that BHMM1 learns a single value for Q that is used for all output distributions, while BHMM2 learns separate hyperparameters for each cluster.This leads to different treatments of difficult-to-classify low-frequency items.In BHMM1, these items tend to be spread evenly among all clusters, so that all clusters have similarly sparse output distributions.In BHMM2, the system creates one or two clusters consisting entirely of very infrequent items, where the priors on these clusters strongly prefer uniform outputs, and all other clusters prefer extremely sparse outputs (and are more coherent than in BHMM1).This explains the difference in VI between the two systems, as well as the higher accuracy of BHMM1 for d > 3: the single Q discourages placing lowfrequency items in their own cluster, so they are more likely to be clustered with items that have similar transition probabilities.The problem of junk clusters in BHMM2 might be alleviated by using a non-uniform prior over the hyperparameters to encourage some degree of sparsity in all clusters.In this paper, we have demonstrated that, for a standard trigram HMM, taking a Bayesian approach to POS tagging dramatically improves performance over maximum-likelihood estimation.Integrating over possible parameter values leads to more robust solutions and allows the use of priors favoring sparse distributions.The Bayesian approach is particularly helpful when learning is less constrained, either because less data is available or because dictionary information is limited or absent.For knowledgefree clustering, our approach can also be extended through the use of infinite models so that the number of clusters need not be specified in advance.We hope that our success with POS tagging will inspire further research into Bayesian methods for other natural language learning tasks.
Unsupervised Coreference Resolution in a Nonparametric Bayesian ModelWe present an unsupervised, nonparametric Bayesian approach to coreference resolution which models both global entity identity across a corpus as well as the sequential anaphoric structure within each document.While most existing coreference work is driven by pairwise decisions, our model is fully generative, producing each mention from a combination of global entity properties and local attentional state.Despite being unsupervised, our system achieves a 70.3 on the MUC-6 test set, broadly in the range of some recent supervised results.Referring to an entity in natural language can broadly be decomposed into two processes.First, speakers directly introduce new entities into discourse, entities which may be shared across discourses.This initial reference is typically accomplished with proper or nominal expressions.Second, speakers refer back to entities already introduced.This anaphoric reference is canonically, though of course not always, accomplished with pronouns, and is governed by linguistic and cognitive constraints.In this paper, we present a nonparametric generative model of a document corpus which naturally connects these two processes.Most recent coreference resolution work has focused on the task of deciding which mentions (noun phrases) in a document are coreferent.The dominant approach is to decompose the task into a collection of pairwise coreference decisions.One then applies discriminative learning methods to pairs of mentions, using features which encode properties such as distance, syntactic environment, and so on (Soon et al., 2001; Ng and Cardie, 2002).Although such approaches have been successful, they have several liabilities.First, rich features require plentiful labeled data, which we do not have for coreference tasks in most domains and languages.Second, coreference is inherently a clustering or partitioning task.Naive pairwise methods can and do fail to produce coherent partitions.One classic solution is to make greedy left-to-right linkage decisions.Recent work has addressed this issue in more global ways.McCallum and Wellner (2004) use graph partioning in order to reconcile pairwise scores into a final coherent clustering.Nonetheless, all these systems crucially rely on pairwise models because clusterlevel models are much harder to work with, combinatorially, in discriminative approaches.Another thread of coreference work has focused on the problem of identifying matches between documents (Milch et al., 2005; Bhattacharya and Getoor, 2006; Daume and Marcu, 2005).These methods ignore the sequential anaphoric structure inside documents, but construct models of how and when entities are shared between them.1 These models, as ours, are generative ones, since the focus is on cluster discovery and the data is generally unlabeled.In this paper, we present a novel, fully generative, nonparametric Bayesian model of mentions in a document corpus.Our model captures both withinand cross-document coreference.At the top, a hierarchical Dirichlet process (Teh et al., 2006) captures cross-document entity (and parameter) sharing, while, at the bottom, a sequential model of salience captures within-document sequential structure.As a joint model of several kinds of discourse variables, it can be used to make predictions about either kind of coreference, though we focus experimentally on within-document measures.To the best of our ability to compare, our model achieves the best unsupervised coreference performance.We adopt the terminology of the Automatic Context Extraction (ACE) task (NIST, 2004).For this paper, we assume that each document in a corpus consists of a set of mentions, typically noun phrases.Each mention is a reference to some entity in the domain of discourse.The coreference resolution task is to partition the mentions according to referent.Mentions can be divided into three categories, proper mentions (names), nominal mentions (descriptions), and pronominal mentions (pronouns).In section 3, we present a sequence of increasingly enriched models, motivating each from shortcomings of the previous.As we go, we will indicate the performance of each model on data from ACE 2004 (NIST, 2004).In particular, we used as our development corpus the English translations of the Arabic and Chinese treebanks, comprising 95 documents and about 3,905 mentions.This data was used heavily for model design and hyperparameter selection.In section 5, we present final results for new test data from MUC-6 on which no tuning or development was performed.This test data will form our basis for comparison to previous work.In all experiments, as is common, we will assume that we have been given as part of our input the true mention boundaries, the head word of each mention and the mention type (proper, nominal, or pronominal).For the ACE data sets, the head and mention type are given as part of the mention annotation.For the MUC data, the head was crudely chosen to be the rightmost mention token, and the mention type was automatically detected.We will not assume any other information to be present in the data beyond the text itself.In particular, unlike much related work, we do not assume gold named entity recognition (NER) labels; indeed we do not assume observed NER labels or POS tags at all.Our primary performance metric will be the MUC F1 measure (Vilain et al., 1995), commonly used to evaluate coreference systems on a within-document basis.Since our system relies on sampling, all results are averaged over five random runs.In this section, we present a sequence of generative coreference resolution models for document corpora.All are essentially mixture models, where the mixture components correspond to entities.As far as notation, we assume a collection of I documents, each with Ji mentions.We use random variables Z to refer to (indices of) entities.We will use 0z to denote the parameters for an entity z, and 0 to refer to the concatenation of all such 0z.X will refer somewhat loosely to the collection of variables associated with a mention in our model (such as the head or gender).We will be explicit about X and 0z shortly.Our goal will be to find the setting of the entity indices which maximize the posterior probability: where Z, X, and 0 denote all the entity indices, observed values, and parameters of the model.Note that we take a Bayesian approach in which all parameters are integrated out (or sampled).The inference task is thus primarily a search problem over the index labels Z.The Weir Group1, whose2 headquarters3 is in the US4, is a large, specialized corporation5 investing in the area of electricity generation.This power plant6, which7 will be situated in Rudong8, Jiangsu9, has an annual generation capacity of 2.4 million kilowatts.The Weir Group1, whose1 headquarters2 is in the US3, is a large, specialized corporation4 investing in the area of electricity generation.This power plant5, which1 will be situated in Rudong6, Jiangsu7, has an annual generation capacity of 2.4 million kilowatts.The Weir Group1, whose1 headquarters2 is in the US3, is a large, specialized corporation4 investing in the area of electricity generation.This power plant5, which5 will be situated in Rudong6, Jiangsu7, has an annual generation capacity of 2.4 million kilowatts.Our first, overly simplistic, corpus model is the standard finite mixture of multinomials shown in figure 1(a).In this model, each document is independent save for some global hyperparameters.Inside each document, there is a finite mixture model with a fixed number K of components.The distribution Q over components (entities) is a draw from a symmetric Dirichlet distribution with concentration α.For each mention in the document, we choose a component (an entity index) z from Q.Entity z is then associated with a multinomial emission distribution over head words with parameters Oh, which are drawn from a symmetric Dirichlet over possible mention heads with concentration AH.2 Note that here the X for a mention consists only of the mention head H. As we enrich our models, we simultaneously develop an accompanying Gibbs sampling procedure to obtain samples from P(Z|X).3 For now, all heads H are observed and all parameters (Q and 0) can be integrated out analytically: for details see Teh et al. (2006).The only sampling is for the values of Zi,j, the entity index of mention j in document i.The relevant conditional distribution is:4 where Hi,j is the head of mention j in document i.Expanding each term, we have the contribution of the prior: 2In general, we will use a subscripted A to indicate concentration for finite Dirichlet distributions.Unless otherwise specified, A concentration parameters will be set to a−4 and omitted from diagrams.3One could use the EM algorithm with this model, but EM will not extend effectively to the subsequent models.4Here, Z−i,j denotes Z − {Zi,j} where nz is the number of elements of Z−i,j with entity index z.Similarly we have for the contribution of the emissions: where nh,z is the number of times we have seen head h associated with entity index z in (Z, H−i,j).A clear drawback of the finite mixture model is the requirement that we specify a priori a number of entities K for a document.We would like our model to select K in an effective, principled way.A mechanism for doing so is to replace the finite Dirichlet prior on Q with the non-parametric Dirichlet process (DP) prior (Ferguson, 1973).5 Doing so gives the model in figure 1(b).Note that we now list an infinite number of mixture components in this model since there can be an unbounded number of entities.Rather than a finite Q with a symmetric Dirichlet distribution, in which draws tend to have balanced clusters, we now have an infinite Q.However, most draws will have weights which decay exponentially quickly in the prior (though not necessarily in the posterior).Therefore, there is a natural penalty for each cluster which is actually used.With Z observed during sampling, we can integrate out Q and calculate P(Zi,j|Z−i,j) analytically, using the Chinese restaurant process representation: where znew is a new entity index not used in Z−i,j and nz is the number of mentions that have entity index z.Aside from this change, sampling is identical to the finite mixture case, though with the number of clusters actually occupied in each sample drifting upwards or downwards.This model yielded a 54.5 Fi on our development data.6 This model is, however, hopelessly crude, capturing nothing of the structure of coreference.Its largest empirical problem is that, unsurprisingly, pronoun mentions such as he are given their own clusters, not labeled as coreferent with any non-pronominal mention (see figure 2(a)).While an entity-specific multinomial distribution over heads makes sense for proper, and some nominal, mention heads, it does not make sense to generate pronominal mentions this same way.I.e., all entities can be referred to by generic pronouns, the choice of which depends on entity properties such as gender, not the specific entity.We therefore enrich an entity’s parameters 0 to contain not only a distribution over lexical heads 0h, but also distributions (0t, 09, 0n) over properties, where 0t parametrizes a distribution over entity types (PER, LOC, ORG, MISC), and 09 for gender (MALE, FEMALE, NEUTER), and 0n for number (SG, PL).7 We assume each of these property distributions is drawn from a symmetric Dirichlet distribution with small concentration parameter in order to encourage a peaked posterior distribution.Previously, when an entity z generated a mention, it drew a head word from 0hz .It now undergoes a more complex and structured process.It first draws an entity type T, a gender G, a number N from the distributions 0t, 09, and 0n, respectively.Once the properties are fetched, a mention type M is chosen (proper, nominal, pronoun), according to a global multinomial (again with a symmetric Dirichlet prior and parameter AM).This corresponds to the (temporary) assumption that the speaker makes a random i.i.d. choice for the type of each mention.Our head model will then generate a head, conditioning on the entity, its properties, and the mention type, as shown in figure 3(b).If M is not a pronoun, the head is drawn directly from the entity head multinomial with parameters 0hz .Otherwise, it is drawn based on a global pronoun head distribution, conditioning on the entity properties and parametrized by 9.Formally, it is given by: Although we can observe the number and gender draws for some mentions, like personal pronouns, there are some for which properties aren’t observed (e.g., it).Because the entity property draws are not (all) observed, we must now sample the unobserved ones as well as the entity indices Z.For instance, we could sample Ti,j, the entity type of pronominal mention j in document i, using, P(Ti,j|Z, N, G, H, T−i,j) ∝ P(Ti,j|Z)P(Hi,j|T, N, G, H), where the posterior distributions on the right hand side are straightforward because the parameter priors are all finite Dirichlet.Sampling G and N are identical.Of course we have prior knowledge about the relationship between entity type and pronoun head choice.For example, we expect that he is used for mentions with T = PERSON.In general, we assume that for each pronominal head we have a list of compatible entity types, which we encode via the prior on 0.We assume 0 is drawn from a Dirichlet distribution where each pronoun head is given a synthetic count of (1 + AP) for each (t, g, n) where t is compatible with the pronoun and given AP otherwise.So, while it will be possible in the posterior to use he to refer to a non-person, it will be biased towards being used with persons.This model gives substantially improved predictions: 64.1 Fi on our development data.As can be seen in figure 2(b), this model does correct the systematic problem of pronouns being considered their own entities.However, it still does not have a preference for associating pronominal references to entities which are in any way local.We would like our model to capture how mention types are generated for a given entity in a robust and somewhat language independent way.The choice of entities may reasonably be considered to be independent given the mixing weights Q, but how we realize an entity is strongly dependent on context (Ge et al., 1998).In order to capture this in our model, we enrich it as shown in figure 4.As we proceed through a document, generating entities and their mentions, we maintain a list of the active entities and their saliences, or activity scores.Every time an entity is mentioned, we increment its activity score by 1, and every time we move to generate the next mention, all activity scores decay by a constant factor of 0.5.This gives rise to an ordered list of entity activations, L, where the rank of an entity decays exponentially as new mentions are generated.We call this list a salience list.Given a salience list, L, each possible entity z has some rank on this list.We discretize these ranks into five buckets S: TOP (1), HIGH (23), MID (4-6), LOW (7+), and NONE.Given the entity choices Z, both the list L and buckets S are deterministic (see figure 4).We assume that the mention type M is conditioned on S as shown in figure 4.We note that correctly sampling an entity now requires that we incorporate terms for how a change will affect all future salience values.This changes our sampling equation for existing entities: where the product ranges over future mentions in the document and Si,j, is the value of future salience feature given the setting of all entities, including setting the current entity Zi,j to z.A similar equation holds for sampling a new entity.Note that, as discussed below, this full product can be truncated as an approximation.This model gives a 71.5 Fi on our development data.Table 1 shows the posterior distribution of the mention type given the salience feature.This model fixes many anaphora errors and in particular fixes the second anaphora error in figure 2(c).One advantage of a fully generative approach is that we can allow entities to be shared between documents in a principled way, giving us the capacity to do cross-document coreference.Moreover, sharing across documents pools information about the properties of an entity across documents.We can easily link entities across a corpus by assuming that the pool of entities is global, with global mixing weights Qo drawn from a DP prior with concentration parameter 'y.Each document uses the same global entities, but each has a documentspecific distribution Qi drawn from a DP centered on Q0 with concentration parameter α.Up to the point where entities are chosen, this formulation follows the basic hierarchical Dirichlet process prior of Teh et al. (2006).Once the entities are chosen, our model for the realization of the mentions is as before.This model is depicted graphically in figure 5.Although it is possible to integrate out Q0 as we did the individual Qi, we instead choose for efficiency and simplicity to sample the global mixture distribution Q0 from the posterior distribution P(Q0|Z).8 The mention generation terms in the model and sampler are unchanged.In the full hierarchical model, our equation (1) for sampling entities, ignoring the salience component of section 3.4, becomes: where Qz0 is the probability of the entity z under the sampled global entity distribution and Qu0 is the unknown component mass of this distribution.The HDP layer of sharing improves the model’s predictions to 72.5 F1 on our development data.We should emphasize that our evaluation is of course per-document and does not reflect cross-document coreference decisions, only the gains through crossdocument sharing (see section 6.2).Up until now, we’ve discussed Gibbs sampling, but we are not interested in sampling from the posterior P(Z|X), but in finding its mode.Instead of sampling directly from the posterior distribution, we instead sample entities proportionally to exponentiated entity posteriors.The exponent is given by exp ci k−1, where i is the current round number (starting at i = 0), c = 1.5 and k = 20 is the total number of sampling epochs.This slowly raises the posterior exponent from 1.0 to ec.In our experiments, we found this procedure to outperform simulated annealing.We also found sampling the T, G, and N variables to be particularly inefficient, so instead we maintain soft counts over each of these variables and use these in place of a hard sampling scheme.We also found that correctly accounting for the future impact of salience changes to be particularly inefficient.However, ignoring those terms entirely made negligible difference in final accuracy.9We present our final experiments using the full model developed in section 3.As in section 3, we use true mention boundaries and evaluate using the MUC F1 measure (Vilain et al., 1995).All hyperparameters were tuned on the development set only.The document concentration parameter α was set by taking a constant proportion of the average number of mentions in a document across the corpus.This number was chosen to minimize the squared error between the number of proposed entities and true entities in a document.It was not tuned to maximize the F1 measure.A coefficient of 0.4 was chosen.The global concentration coefficient -y was chosen to be a constant proportion of αM, where M is the number of documents in the corpus.We found 0.15 to be a good value using the same least-square procedure.The values for these coefficients were not changed for the experiments on the test sets.Our main evaluation is on the standard MUC-6 formal test set.10 The standard experimental setup for this data is a 30/30 document train/test split.Training our system on all 60 documents of the training and test set (as this is in an unsupervised system, the unlabeled test documents are present at training time), but evaluating only on the test documents, gave 63.9 F1 and is labeled MUC-6 in table 2(a).One advantage of an unsupervised approach is that we can easily utilize more data when learning a model.We demonstrate the effectiveness of this fact by evaluating on the MUC-6 test documents with increasing amounts of unannotated training data.We first added the 191 documents from the MUC-6 dryrun training set (which were not part of the training data for official MUC-6 evaluation).This model gave 68.0 F1 and is labeled +DRYRUN-TRAIN in table 2(a).We then added the ACE ENGLISH-NWIRE training data, which is from a different corpora than the MUC-6 test set and from a different time period.This model gave 70.3 F1 and is labeled +ENGLISHNWIRE in table 2(a).Our results on this test set are surprisingly comparable to, though slightly lower than, some recent supervised systems.McCallum and Wellner (2004) report 73.4 F1 on the formal MUC-6 test set, which is reasonably close to our best MUC-6 number of 70.3 F1.McCallum and Wellner (2004) also report a much lower 91.6 F1 on only proper nouns mentions.Our system achieves a 89.8 F1 when evaluation is restricted to only proper mentions.11 The et al. (2004).A mention is proper if it is annotated with NER information.It is a pronoun if the head is on the list of English pronouns.Otherwise, it is a nominal mention.Note we do not use the NER information for any purpose but determining whether the mention is proper.11The best results we know on the MUC-6 test set using the standard setting are due to Luo et al. (2004) who report a 81.3 Fl (much higher than others).However, it is not clear this is a comparable number, due to the apparent use of gold NER features, which provide a strong clue to coreference.Regardless, it is unsurprising that their system, which has many rich features, would outperform ours. closest comparable unsupervised system is Cardie and Wagstaff (1999) who use pairwise NP distances to cluster document mentions.They report a 53.6 F1 on MUC6 when tuning distance metric weights to maximize F1 on the development set.We also performed experiments on ACE 2004 data.Due to licensing restrictions, we did not have access to the ACE 2004 formal development and test sets, and so the results presented are on the training sets.We report results on the newswire section (NWIRE in table 2b) and the broadcast news section (BNEWS in table 2b).These datasets include the prenominal mention type, which is not present in the MUC6 data.We treated prenominals analogously to the treatment of proper and nominal mentions.We also tested our system on the Chinese newswire and broadcast news sections of the ACE 2004 training sets.Our relatively higher performance on Chinese compared to English is perhaps due to the lack of prenominal mentions in the Chinese data, as well as the presence of fewer pronouns compared to English.Our ACE results are difficult to compare exactly to previous work because we did not have access to the restricted formal test set.However, we can perform a rough comparison between our results on the training data (without coreference annotation) to supervised work which has used the same training data (with coreference annotation) and evaluated on the formal test set.Denis and Baldridge (2007) report 67.1 F1 and 69.2 F1 on the English NWIRE and BNEWS respectively using true mention boundaries.While our system underperforms the supervised systems, its accuracy is nonetheless promising.The largest source of error in our system is between coreferent proper and nominal mentions.The most common examples of this kind of error are appositive usages e.g.George W. Bush, president of the US, visited Idaho.Another error of this sort can be seen in figure 2, where the corporation mention is not labeled coreferent with the The Weir Group mention.Examples such as these illustrate the regular (at least in newswire) phenomenon that nominal mentions are used with informative intent, even when the entity is salient and a pronoun could have been used unambiguously.This aspect of nominal mentions is entirely unmodeled in our system.Since we do not have labeled cross-document coreference data, we cannot evaluate our system’s crossdocument performance quantitatively.However, in addition to observing the within-document gains from sharing shown in section 3, we can manually inspect the most frequently occurring entities in our corpora.Table 3 shows some of the most frequently occurring entities across the English ACE NWIRE corpus.Note that Bush is the most frequent entity, though his (and others’) nominal cluster president is mistakenly its own entity.Merging of proper and nominal clusters does occur as can be seen in table 3.We can use our model to for unsupervised NER tagging: for each proper mention, assign the mode of the generating entity’s distribution over entity types.Note that in our model the only way an entity becomes associated with an entity type is by the pronouns used to refer to it.12 If we evaluate our system as an unsupervised NER tagger for the proper mentions in the MUC-6 test set, it yields a 12Ge et al. (1998) exploit a similar idea to assign gender to proper mentions. per-label accuracy of 61.2% (on MUC labels).Although nowhere near the performance of state-ofthe-art systems, this result beats a simple baseline of always guessing PERSON (the most common entity type), which yields 46.4%.This result is interesting given that the model was not developed for the purpose of inferring entity types whatsoever.We have presented a novel, unsupervised approach to coreference resolution: global entities are shared across documents, the number of entities is determined by the model, and mentions are generated by a sequential salience model and a model of pronounentity association.Although our system does not perform quite as well as state-of-the-art supervised systems, its performance is in the same general range, despite the system being unsupervised.
cdec: A Decoder Alignment and Learning Framework for Finite-State and Context-Free Translation Modelspresent an open source framework for decoding, aligning with, and training a number of statistical machine translation models, including word-based models, phrase-based models, and models based on synchronous context-free grammars.Using a single unified internal representation for translation forests, the decoder strictly separates model-specific translation logic from general rescoring, pruning, and inference algorithms.From this unified representation, the decoder can not only the 1or translations, but also alignments to a reference, or the quantities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques.Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders.The dominant models used in machine translation and sequence tagging are formally based on either weighted finite-state transducers (FSTs) or weighted synchronous context-free grammars (SCFGs) (Lopez, 2008).Phrase-based models (Koehn et al., 2003), lexical translation models (Brown et al., 1993), and finite-state conditional random fields (Sha and Pereira, 2003) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007).We introduce a software package called cdec that manipulates both classes in a unified way.1 Although open source decoders for both phrasebased and hierarchical translation models have been available for several years (Koehn et al., 2007; Li et al., 2009), their extensibility to new models and algorithms is limited by two significant design flaws that we have avoided with cdec.First, their implementations tightly couple the translation, language model integration (which we call rescoring), and pruning algorithms.This makes it difficult to explore alternative translation models without also re-implementing rescoring and pruning logic.In cdec, model-specific code is only required to construct a translation forest (§3).General rescoring (with language models or other models), pruning, inference, and alignment algorithms then apply to the unified data structure (§4).Hence all model types benefit immediately from new algorithms (for rescoring, inference, etc.); new models can be more easily prototyped; and controlled comparison of models is made easier.Second, existing open source decoders were designed with the traditional phrase-based parameterization using a very small number of dense features (typically less than 10). cdec has been designed from the ground up to support any parameterization, from those with a handful of dense features up to models with millions of sparse features (Blunsom et al., 2008; Chiang et al., 2009).Since the inference algorithms necessary to compute a training objective (e.g. conditional likelihood or expected BLEU) and its gradient operate on the unified data structure (§5), any model type can be trained using with any of the supported training criteria.The software package includes general function optimization utilities that can be used for discriminative training (§6).These features are implemented without compromising on performance.We show experimentally that cdec uses less memory and time than comparable decoders on a controlled translation task (§7).The decoding pipeline consists of two phases.The first (Figure 1) transforms input, which may be represented as a source language sentence, lattice (Dyer et al., 2008), or context-free forest (Dyer and Resnik, 2010), into a translation forest that has been rescored with all applicable models.In cdec, the only model-specific logic is confined to the first step in the process where an input string (or lattice, etc.) is transduced into the unified hypergraph representation.Since the model-specific code need not worry about integration with rescoring models, it can be made quite simple and efficient.Furthermore, prior to language model integration (and distortion model integration, in the case of phrase based translation), pruning is unnecessary for most kinds of models, further simplifying the model-specific code.Once this unscored translation forest has been generated, any non-coaccessible states (i.e., states that are not reachable from the goal node) are removed and the resulting structure is rescored with language models using a user-specified intersection/pruning strategy (§4) resulting in a rescored translation forest and completing phase 1.The second phase of the decoding pipeline (depicted in Figure 2) computes a value from the rescored forest: 1- or k-best derivations, feature expectations, or intersection with a target language reference (sentence or lattice).The last option generates an alignment forest, from which a word alignment or feature expectations can be extracted.Most of these values are computed in a time complexity that is linear in the number of edges and nodes in the translation hypergraph using cdec’s semiring framework (§5).Alignment is the process of determining if and how a translation model generates a (source, target) string pair.To compute an alignment under a translation model, the phase 1 translation hypergraph is reinterpreted as a synchronous contextfree grammar and then used to parse the target sentence.2 This results in an alignment forest, which is a compact representation of all the derivations of the sentence pair under the translation model.From this forest, the Viterbi or maximum a posteriori word alignment can be generated.This alignment algorithm is explored in depth by Dyer (2010).Note that if the phase 1 forest has been pruned in some way, or the grammar does not derive the sentence pair, the target intersection parse may fail, meaning that an alignment will not be recoverable.Recent research has proposed a unified representation for the various translation and tagging formalisms that is based on weighted logic programming (Lopez, 2009).In this view, translation (or tagging) deductions have the structure of a context-free forest, or directed hypergraph, where edges have a single head and 0 or more tail nodes (Nederhof, 2003).Once a forest has been constructed representing the possible translations, general inference algorithms can be applied.In cdec’s translation hypergraph, a node represents a contiguous sequence of target language words.For SCFG models and sequential tagging models, a node also corresponds to a source span and non-terminal type, but for word-based and phrase-based models, the relationship to the source string (or lattice) may be more complicated.In a phrase-based translation hypergraph, the node will correspond to a source coverage vector (Koehn et al., 2003).In word-based models, a single node may derive multiple different source language coverages since word based models impose no requirements on covering all words in the input.Figure 3 illustrates two example hypergraphs, one generated using a SCFG model and other from a phrase-based model.Edges are associated with exactly one synchronous production in the source and target language, and alternative translation possibilities are expressed as alternative edges.Edges are further annotated with feature values, and are annotated with the source span vector the edge corresponds to.An edge’s output label may contain mixtures of terminal symbol yields and positions indicating where a child node’s yield should be substituted.2The parser is smart enough to detect the left-branching grammars generated by lexical translation and tagging models, and use a more efficient intersection algorithm. specifies what path is taken from the input (one of the bold ovals) to a unified translation hypergraph.The highlighted path is the workflow used in the test reported in §7.In the case of SCFG grammars, the edges correspond simply to rules in the synchronous grammar.For non-SCFG translation models, there are two kinds of edges.The first have zero tail nodes (i.e., an arity of 0), and correspond to word or phrase translation pairs (with all translation options existing on edges deriving the same head node), or glue rules that glue phrases together.For tagging, word-based, and phrase-based models, these are strictly arranged in a monotone, leftbranching structure.The design of cdec separates the creation of a translation forest from its rescoring with a language models or similar models.3 Since the structure of the unified search space is context free (§3), we use the logic for language model rescoring described by Chiang (2007), although any weighted intersection algorithm can be applied.The rescoring models need not be explicitly represented as FSTs—the state space can be inferred.Although intersection using the Chiang algorithm runs in polynomial time and space, the resulting rescored forest may still be too large to represent completely. cdec therefore supports three pruning strategies that can be used during intersection: full unpruned intersection (useful for tagging models to incorporate, e.g., Markov features, but not generally practical for translation), cube pruning, and cube growing (Huang and Chiang, 2007).Semirings are a useful mathematical abstraction for dealing with translation forests since many useful quantities can be computed using a single linear-time algorithm but with different semirings.A semiring is a 5-tuple (K, ®, ®, 0,1) that indicates the set from which the values will be drawn, K, a generic addition and multiplication operation, ® and ®, and their identities 0 and 1.Multiplication and addition must be associative.Multiplication must distribute over addition, and v ® 0 must equal 0.Values that can be computed using the semirings include the number of derivations, the expected translation length, the entropy of the translation posterior distribution, and the expected values of feature functions (Li and Eisner, 2009).Since semirings are such a useful abstraction, cdec has been designed to facilitate implementation of new semirings.Table 1 shows the C++ representation used for semirings.Note that because of our representation, built-in types like double, int, and bool (together with their default operators) are semirings.Beyond these, the type prob t is provided which stores the logarithm of the value it represents, which helps avoid underflow and overflow problems that may otherwise be encountered.A generic first-order expectation semiring is also provided (Li and Eisner, 2009).Three standard algorithms parameterized with semirings are provided: INSIDE, OUTSIDE, and INSIDEOUTSIDE, and the semiring is specified using C++ generics (templates).Additionally, each algorithm takes a weight function that maps from hypergraph edges to a value in K, making it possible to use many different semirings without altering the underlying hypergraph.Although Viterbi and k-best extraction algorithms are often expressed as INSIDE algorithms with the tropical semiring, cdec provides a separate derivation extraction framework that makes use of a < operator (Huang and Chiang, 2005).Thus, many of the semiring types define not only the elements shown in Table 1 but T::operator< as well.The k-best extraction algorithm is also parameterized by an optional predicate that can filter out derivations at each node, enabling extraction of only derivations that yield different strings as in Huang et al. (2006).Two training pipelines are provided with cdec.The first, called Viterbi envelope semiring training, VEST, implements the minimum error rate training (MERT) algorithm, a gradient-free optimization technique capable of maximizing arbitrary loss functions (Och, 2003).Rather than computing an error surface using kbest approximations of the decoder search space, cdec’s implementation performs inference over the full hypergraph structure (Kumar et al., 2009).In particular, by defining a semiring whose values are sets of line segments, having an addition operation equivalent to union, and a multiplication operation equivalent to a linear transformation of the line segments, Och’s line search can be computed simply using the INSIDE algorithm.Since the translation hypergraphs generated by cdec may be quite large making inference expensive, the logic for constructing error surfaces is factored according to the MapReduce programming paradigm (Dean and Ghemawat, 2004), enabling parallelization across a cluster of machines.Implementations of the BLEU and TER loss functions are provided (Papineni et al., 2002; Snover et al., 2006).In addition to the widely used MERT algorithm, cdec also provides a training pipeline for discriminatively trained probabilistic translation models (Blunsom et al., 2008; Blunsom and Osborne, 2008).In these models, the translation model is trained to maximize conditional log likelihood of the training data under a specified grammar.Since log likelihood is differentiable with respect to the feature weights in an exponential model, it is possible to use gradient-based optimization techniques to train the system, enabling the parameterization of the model using millions of sparse features.While this training approach was originally proposed for SCFG-based translation models, it can be used to train any model type in cdec.When used with sequential tagging models, this pipeline is identical to traditional sequential CRF training (Sha and Pereira, 2003).Both the objective (conditional log likelihood) and its gradient have the form of a difference in two quantities: each has one term that is computed over the translation hypergraph which is subtracted from the result of the same computation over the alignment hypergraph (refer to Figures 1 and 2).The conditional log likelihood is the difference in the log partition of the translation and alignment hypergraph, and is computed using the INSIDE algorithm.The gradient with respect to a particular feature is the difference in this feature’s expected value in the translation and alignment hypergraphs, and can be computed using either INSIDEOUTSIDE or the expectation semiring and INSIDE.Since a translation forest is generated as an intermediate step in generating an alignment forest (§2) this computation is straightforward.Since gradient-based optimization techniques may require thousands of evaluations to converge, the batch training pipeline is split into map and reduce components, facilitating distribution over very large clusters.Briefly, the cdec is run as the map function, and sentence pairs are mapped over.The reduce function aggregates the results and performs the optimization using standard algorithms, including LBFGS (Liu et al., 1989), RPROP (Riedmiller and Braun, 1993), and stochastic gradient descent.Table 2 compares the performance of cdec, Hiero, and Joshua 1.3 (running with 1 or 8 threads) decoding using a hierarchical phrase-based translation grammar and identical pruning settings.4 Figure 4 shows the cdec configuration and weights file used for this test.The workstation used has two 2GHz quad-core Intel Xenon processors, 32GB RAM, is running Linux kernel version 2.6.18 and gcc version 4.1.2.All decoders use SRI’s language model toolkit, version 1.5.9 (Stolcke, 2002).Joshua was run on the Sun HotSpot JVM, version 1.6.0 12.A hierarchical phrase-based translation grammar was extracted for the NIST MT03 Chinese-English translation using a suffix array rule extractor (Lopez, 2007).A non-terminal span limit of 15 was used, and all decoders were configured to use cube pruning with a limit of 30 candidates at each node and no further pruning.All decoders produced a BLEU score between 31.4 and 31.6 (small differences are accounted for by different tie-breaking behavior and OOV handling). formalism=scfg grammar=grammar.mt03.scfg.gz add pass through rules=true scfg max span limit=15 feature function=LanguageModel en.3gram.pruned.lm.gz -o 3 feature function=WordPenalty intersection strategy=cube pruning cubepruning pop limit=30 8 Future work C. Dyer.2010.Two monolingual parses are better than one (synchronous parse).In Proc. of HLT-NAACL. cdec continues to be under active development.We are taking advantage of its modular design to study alternative algorithms for language model integration.Further training pipelines are under development, including minimum risk training using a linearly decomposable approximation of BLEU (Li and Eisner, 2009), and MIRA training (Chiang et al., 2009).All of these will be made publicly available as the projects progress.We are also improving support for parallel training using Hadoop (an open-source implementation of MapReduce).This work was partially supported by the GALE program of the Defense Advanced Research Projects Agency, Contract No.HR0011-06-2-001.Any opinions, findings, conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of the sponsors.Further support was provided the EuroMatrix project funded by the European Commission (7th Framework Programme).Discussions with Philipp Koehn, Chris Callison-Burch, Zhifei Li, Lane Schwarz, and Jimmy Lin were likewise crucial to the successful execution of this project.
Planning Coherent Multisentential TextThe relations are used as plans; their intended effects are interpreted as the goals they achieve.In other words, in order to bring about the state which both speaker and hearer know that the purpose of know that they both know it, etc.), the structurer uses Purpose as a plan and tries to satisfy its constraints.In this system, constraints and goals are interfor example, in the event that believed not by the satellite constraint of the resimply becomes the goal to achieve (BMB (RESULT Similarly, the propo- Ow s SCAN-1 ?ACT-2)) (DMB (08.1 are interpreted as the goal to find some element that could legitimately take place of In order to enable the relations to nest recursome relations' nucleuses and satellites contain requirements that specify additional relations, such as examples, contrasts, etc.Of course, these additional requirements may only be included if such material can coherently follow the content of the nucleus or satellite.The question of ordering such additional constituents is still under investigation.The question of whether such additional material should be included at all is not addressed; the structurer tries to say everything it is given.The structurer produces all coherent paragraphs (that is, coherent as defined by the relations) that satisfy the given goal(s) for any set of input elements.For example, paragraph (b) is produced to the initial goal S H (SEQUENCE goal is produced by PEA, together with the appropriate representation ele- (ASK-I. in response to the does the system a program?.Different initial goals will result in different paragraphs.Each paragraph is represented as a tree in which branch points are RST relations and leaves are input elements.Figure 1 is the tree for para- (b).It contains the relations (signalled by 'then' and 'finally&quot;), Elaboration (&quot;in particular&quot;), and Purpose (&quot;in order to&quot;).In the corresponding paragraph produced by Penman, the relations' characteristic words or phrases (boldfaced below) appear between the blocks of text they relate: [The system asks the user to tell it the characteristic of the program to be system applies to the progran2.](0 system scans the proo order to opportunities to apply transformations to the system resolves [It confirms the enhancewith the [it performs the enhancement.ho 166 input sentence generator --ot update agenda choose final plan get next bud RST relations expand bud grow tree Figure 2: Hierarchical Planning Structurer 6-The Structurer As stated above, the structurer is a simplified top-down hierarchical expansion planner (see Figure 2).It operates as follows: given one or more communicative goals, it finds RST relations whose intended effects match (some of) these goals; it then inspects which of the input elements match the nucleus and subgoal constraints for each relation.Unmatched constraints become subgoals which are posted on an agenda for the next level of planning.The tree can be expanded in either depth-first or breadth-first fashion.Eventually, process bottoms out when either: (a) all input elements have been used and unsatisfied subgoals remain (in which case the structurer could request more input with desired properties from the encapsulating system); or (b) all goals are satisfied.If more than one plan (i.e., paragraph tree structure) is produced, the results are ordered by preferring trees with the minimum unused number of input elements and the minimum number of remaining unsatisfied subgoals.The best tree is then traversed in left-to-right order; leaves provide input to Penman to be generated in English and relations at branch points provide typical interclausal relation words or phrases.In this way the structurer performs top-down goal refinement down to the level of the input elements. and Further Work This work is also being tested in a completely separate domain: the generation of text in a multimedia system that answers database queries.Penman produces the following description of the ship Knox (where CTG 070.10 designates a group of ships): Knox is en route in rendezvous with CTG 070.10, arriving in Pearl Harbor on 4/24, for port visit until 4/30.In this text, each clause (en route, rendezvous, arrive, visit) is a separate input element; the structurer linked them using the relations Sequence and Purpose (the same Purpose as shown above; it is signalled by &quot;in order to&quot;).However, Penman can also be made to produce (d).Knox is en route in order to rendezvous with CTG 070.10.It will arrive in Pearl Harbor on 4/24.It will be on port visit until 4/30.The problem is clear: how should sentences in the paragraph be scoped?At present, avoiding any claims about a theory, the structurer can feed 167 Penman either extreme: make everything one sentence, or make each input element a separate sentence.However, neither extreme is satisfactory; as is clear from paragraph (b), &quot;short&quot; spans of text can be linked and &quot;long&quot; ones left separate.A simple way to implement this is to count the number of leaves under each branch (nucleus or satellite) in the paragraph structure tree.Another shortcoming is the treatment of input elements as indivisible entities.This shortcoming is a result of factoring out the problem of aggregation as a separate text planning task.Chunking together input elements (to eliminate detail) or taking them apart (to be more detailed) has received scant mention — see Ellovy 871, and for the related problem of paraphrase see [Schank 75] — but this task should interact with text structuring in order to provide text that is both optimally detailed and coherent.At the present time, only about 20% of the RST relations have been formalized to the extent that they can be used by the structurer.This formalization process is difficult, because it goes handin-hand with the development of terms with which to characterize the relations' goals/constraints.Though the formalization can never be completely finalized — who can hope to represent something like motivation or justification complete with all ramifications?— the hope is that, by having the requirements stated in rather basic terms, the relations will be easily adaptable to any new representation scheme and domain.(It should be noted, of course, that, to be useful, these formalizations need only be as specific and as detailed as the domain model and representation requires.)In addition, the availability of a set of communicative goals more detailed than just say or ask (for example), should make it easier for programs that require output text to interface with the generator.This is one focus of current text planning work at ISI.The example texts in this paper are generated by Penman, a systemic grammar-based generator with larger coverage than probably any other existing text generator.Penman was developed at ISI (see [Mann & Matthiessen 831, [Mann 83], [Matthiessen 841).The input to Penman is produced by PEA (Programming Enhancement Advisor; see [Moore 87]), a program that inspects a user's LISP program and suggests enhancements.PEA is being developed to interact with the user in order to answer his or her questions about the suggested enhancements.Its theoretical focus is the production of explanations over extended interactions in ways that are superior to the simple goal-tree traversal of systems such as TYRESIAS ([Davis 76]) and MYCIN ([Shortliffe 761).In answer to the question how does the system enhance a program?, the following text (not generated by Penman) is not satisfactory: ...because you have to work too hard to make sense of it.In contrast, using the same propositions (now rearranged and linked with appropriate connectives), paragraph (b) (generated by Penman) is far easier to understand: (b).The system asks the user to tell it the characteristic of the program to be enhanced.Then the system applies transformations to the program.In particular, the system scans the program in order to find opportunities to apply transformations to the program.Then the system resolves conflicts.It confirms the enhancement with the user.Finally, it performs the enhancement.Clearly, you do not get coherent text simply by stringing together sentences, even if they are related — note especially the underlined text in (b) and its corresponding three propositions in (a).The goal of this paper is to describe a method of planning paragraphs to be coherent while avoiding unintended spurious effects that result from the juxtaposition of unrelated pieces of text.This planning work, which can be called text structuring, must obviously be done before the actual generating of language can begin.Text structuring is one of a number of pre-generation text planning tasks.For some of the other tasks Penman has special-purpose domain-specific solutions.They include: The problem of text coherence can be characterized in specific terms as follows.Assuming that input elements are sentence- or clause-sized chunks of representation, the permutation set of the input elements defines the space of possible paragraphs.A simplistic, brute-force way to achieve coherent text would be to search this space and pick out the coherent paragraphs.This search would be factorially expensive.For example, in paragraph (b) above, the 7 input clusters received from PEA provide 7!= 5,040 candidate paragraphs.However, by utilizing the constraints imposed by coherence, one can formulate operators that guide the search and significantly limit the search to a manageable size.In the example, the operators described below produced only 3 candidate paragraphs.Then, from this set of remaining candidates, the best paragraph can be found by applying a relatively simple evaluation metric.The contention of this paper is that, exercising proper care, the coherence relations that hold between successive pieces of text can be formulated as the abovementioned search operators and used in a hierarchical-expansion planner to limit the search and to produce structures describing the coherent paragraphs.The illustrate this contention, the Penman text structurer is a simplified top-down planner (as described first by Pacerdoti 771).It uses a formalized version of the relations of Rhetorical Structure Theory (see immediately below) as plans.Its output is one (or more) tree(s) that describe the structure(s) of coherent paragraphs built from the input elements.Input elements are the leaves of the tree(s); they are sent to the Penman generator to be transformed into sentences.The heart of the problem is obviously coherence.Coherent text can be defined as text in which the hearer knows how each part of the text relates to the whole; i.e., (a) the hearer knows why it is said, and (b) the hearer can relate the semantics of each part to a single overarching framework.In 1978, Hobbs ((Hobbs 78, 79, 82]) recognized that in coherent text successive pieces of text are related in a specified set of ways.He produced a set of relations organized into four categories, which he postulated as the four types of phenomena that occur during conversation.His argument, unfortunately, contains a number of shortcomings; not only is the categorization not well-motivated, but the list of relations is incomplete.In her thesis work, McKeown took a different approach ([McKeown 82]).She defined a set of relatively static schemas that represent the structure of stereotypical paragraphs for describing objects.In essence, these schemas are paragraph templates; coherence is enforced by the correct nesting and filling-in of templates.No explicit theory of coherence was offered.Mann and Thompson, alter a wide-ranging study involving hundreds of paragraphs, proposed that a set of 20 relations suffice to represent the relations that hold within the texts that normally occur in English ([Mann di Thompson 87, 86, 83]).These relations, called RST (rhetorical structure theory), are used recursively; the assumption (never explicitly stated) is that a paragraph is only coherent if all its parts can eventually be made to fit into one overarching relation.The enterprise was completely descriptive; no formal definition of the relations or justification for their completeness were given.However, the relations do include most of Hobbs's relations and support McKeown's schemas.A number of similar descriptions exist.The description of how parts of purposive text can relate goes back at least to Aristotle ((Aristotle Both Grimes and Shepherd categorize typical intersentential relations ([Grimes 75] and [Shepherd 26]).Hovy ([llovy 86]) describes a program that uses some relations to slant text.As defined by Mann and Thompson, RST relations hold between two successive pieces of text (at the lowest level, between two clauses; at the highest level, between two parts that make up a paragraph)' .Therefore, each relation has two parts, a nucleus and a satellite.To determine the applicability of the relation, each part has a set of constraints on the entities that can be related.Relations may also have requirements on the combination of the two parts.In addition, each relation has an effect field, which is intended to denote the conditions which the speaker is attempting to achieve.In formalizing these relations and using them generatively to plan paragraphs, rather than analytically to describe paragraph structure, a shift of focus is required.Relations must be seen as plans — the operators that guide the search through the permutation space.The nucleus and satellite constraints become requirements that must be met by any piece of text before it can be used in the relation (i.e., before it can be coherently juxtaposed with the preceding text).The effect field contains a description of the intended effect of the relation (i.e., the goal that the plan achieves, if properly executed).Since the goals in generation are communicative, the intended effect must be seen as the inferences that the speaker is licensed to make about the hearer's knowledge after the successful completion of the relation.Since the relations are used as plans, and since their satellite and nucleus constraints must be reformulated as subgoals to the structurer, these constraints are best represented in terms of the communicative intent of the speaker.That is, they are best represented in terms of what the hearer will know — i.e., what inferences the hearer would run — upon being told the nucleus or satellite filler.As it turns out, suitable terms for this purpose are provided by the formal theory of rational interaction currently being developed by, among others, Cohen, Levesque, and Perrault.For example, in (Cohen & Levesque 851, Cohen and Levesque present a proof that the indirect speech act of requesting can be derived from the following basic modal operators 'This is not strictly true; a small number of relations, such as Sequence, relate more than two pieces of text.However, for ease of use, they have been implemented as binary relations in the structurer. as well as from a few other operators such as AND and OR.They then define summaries as, essentially, speech act operators with activating conditions (gates) and effects.These summaries closely resemble, in structure, the RST plans described here, with gates corresponding to satellite and nucleus constraints and effects to intended effects.The RST relation Purpose expresses the relation between an action and its intended result: Purpose For example, when used to produce the sentence The system scans the program in order to find opportunities to apply transformations to the program, this relation is instantiated asThe system 'wants&quot; to find themThis is the purpose of the scanning The elements SCAN-I, OPP-1, etc., are part of a network provided to the Penman structurer by PEA.These elements are defined as propositions in a property-inheritance network of the usual kind written in NIKL aSchmolze dc Lipkis 831, [Kaczmarek et al. 861), a descendant of KL.. ONE Prachman 78D.Some input for this example sentence is: The relations are used as plans; their intended effects are interpreted as the goals they achieve.In other words, in order to bring about the state in which both speaker and hearer know that OPP-1 is the purpose of SCAN-1 (and know that they both know it, etc.), the structurer uses Purpose as a plan and tries to satisfy its constraints.In this system, constraints and goals are interchangable; for example, in the event that (RESULT SCAN-1 FIND-1) is believed not known by the hearer, satellite constraint 3 of the Purpose relation simply becomes the goal to achieve (BMB S H (RESULT SCAN-1 FIND-1)).Similarly, the propositions Ow s H (RESULT SCAN-1 ?ACT-2)) (DMB S H (08.1 ?ACT-2 oPP-1)) are interpreted as the goal to find some element that could legitimately take the place of ?ACT-2.In order to enable the relations to nest recursively, some relations' nucleuses and satellites contain requirements that specify additional relations, such as examples, contrasts, etc.Of course, these additional requirements may only be included if such material can coherently follow the content of the nucleus or satellite.The question of ordering such additional constituents is still under investigation.The question of whether such additional material should be included at all is not addressed; the structurer tries to say everything it is given.The structurer produces all coherent paragraphs (that is, coherent as defined by the relations) that satisfy the given goal(s) for any set of input elements.For example, paragraph (b) is produced to satisfy the initial goal (BY13 S H (SEQUENCE ASK-1 ?NEM).This goal is produced by PEA, together with the appropriate representation elements (ASK-I.SCAN-1, etc.) in response to the question how does the system enhance a program?.Different initial goals will result in different paragraphs.Each paragraph is represented as a tree in which branch points are RST relations and leaves are input elements.Figure 1 is the tree for paragraph (b).It contains the relations Sequence (signalled by 'then' and 'finally&quot;), Elaboration (&quot;in particular&quot;), and Purpose (&quot;in order to&quot;).In the corresponding paragraph produced by Penman, the relations' characteristic words or phrases (boldfaced below) appear between the blocks of text they relate: [The system asks the user to tell it the characteristic of the program to be enhanced.](.)Then [the system applies transformations to the progran2.](0 In particular, [the system scans the program] o in order to [find opportunities to apply transformations to the program.[(d) Then [the system resolves confiicts.](e) [It confirms the enhancement with the user.[().)Finally, [it performs the enhancement.ho As stated above, the structurer is a simplified top-down hierarchical expansion planner (see Figure 2).It operates as follows: given one or more communicative goals, it finds RST relations whose intended effects match (some of) these goals; it then inspects which of the input elements match the nucleus and subgoal constraints for each relation.Unmatched constraints become subgoals which are posted on an agenda for the next level of planning.The tree can be expanded in either depth-first or breadth-first fashion.Eventually, the structuring process bottoms out when either: (a) all input elements have been used and unsatisfied subgoals remain (in which case the structurer could request more input with desired properties from the encapsulating system); or (b) all goals are satisfied.If more than one plan (i.e., paragraph tree structure) is produced, the results are ordered by preferring trees with the minimum unused number of input elements and the minimum number of remaining unsatisfied subgoals.The best tree is then traversed in left-to-right order; leaves provide input to Penman to be generated in English and relations at branch points provide typical interclausal relation words or phrases.In this way the structurer performs top-down goal refinement down to the level of the input elements.This work is also being tested in a completely separate domain: the generation of text in a multimedia system that answers database queries.Penman produces the following description of the ship Knox (where CTG 070.10 designates a group of ships): (c).Knox is en route in order to rendezvous with CTG 070.10, arriving in Pearl Harbor on 4/24, for port visit until 4/30.In this text, each clause (en route, rendezvous, arrive, visit) is a separate input element; the structurer linked them using the relations Sequence and Purpose (the same Purpose as shown above; it is signalled by &quot;in order to&quot;).However, Penman can also be made to produce (d).Knox is en route in order to rendezvous with CTG 070.10.It will arrive in Pearl Harbor on 4/24.It will be on port visit until 4/30.The problem is clear: how should sentences in the paragraph be scoped?At present, avoiding any claims about a theory, the structurer can feed Penman either extreme: make everything one sentence, or make each input element a separate sentence.However, neither extreme is satisfactory; as is clear from paragraph (b), &quot;short&quot; spans of text can be linked and &quot;long&quot; ones left separate.A simple way to implement this is to count the number of leaves under each branch (nucleus or satellite) in the paragraph structure tree.Another shortcoming is the treatment of input elements as indivisible entities.This shortcoming is a result of factoring out the problem of aggregation as a separate text planning task.Chunking together input elements (to eliminate detail) or taking them apart (to be more detailed) has received scant mention — see Ellovy 871, and for the related problem of paraphrase see [Schank 75] — but this task should interact with text structuring in order to provide text that is both optimally detailed and coherent.At the present time, only about 20% of the RST relations have been formalized to the extent that they can be used by the structurer.This formalization process is difficult, because it goes handin-hand with the development of terms with which to characterize the relations' goals/constraints.Though the formalization can never be completely finalized — who can hope to represent something like motivation or justification complete with all ramifications?— the hope is that, by having the requirements stated in rather basic terms, the relations will be easily adaptable to any new representation scheme and domain.(It should be noted, of course, that, to be useful, these formalizations need only be as specific and as detailed as the domain model and representation requires.)In addition, the availability of a set of communicative goals more detailed than just say or ask (for example), should make it easier for programs that require output text to interface with the generator.This is one focus of current text planning work at ISI.For help with Penman, Robert Albano, John Bateman, Bob Kasper, Christian Matthiessen, Lynn Poulton, and Richard Whitney.For help with the input, Bill Mann and Johanna Moore.For general comments, all the above, and Cede Paris, Stuart Shapiro, and Norm Sondheimer.
Evaluating Discourse Processing AlgorithmsIn order to take steps towards establishing a methodology for evaluating Natural Language systems, we conducted a case study.We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues.We present the quantitative results of handsimulating these algorithms, but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general.We illustrate the general difficulties encountered with quantitative evaluation.These are problems with: (a) allowing for underlying assumptions, (b) determining how to handle underspecifications, and (c) evaluating the contribution of false positives and error chaining.In the course of developing natural language interfaces, computational linguists are often in the position of evaluating different theoretical approaches to the analysis of natural language (NL).They might want to (a) evaluate and improve on a current system, (b) add a capability to a system that it didn't previously have, (c) combine modules from different systems.Consider the goal of adding a discourse component to a system, or evaluating and improving one that is already in place.A discourse module might combine theories on, e.g., centering or local focusing [GJW83, Sid79], global focus [Gro77], coherence relations[Hob85], event' reference [Web86], intonational structure [PH87], system vs. user beliefs [Po186], plan or intent recognition or production [Coh78, AP86, SI81], control[WS88], or complex syntactic structures [Pri85].How might one evaluate the relative contributions of each of these factors or compare two approaches to the same problem?In order to take steps towards establishing a methodology for doing this type of comparison, we conducted a case study.We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the cospecifiers of pronouns in naturally occurring texts and dialogues[Hob76b, BFP87].Thus there are two parts to this paper: we present the quantitative results of hand-simulating these algorithms (henceforth Hobbs algorithm and BFP algorithm), but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general.We illustrate the general difficulties encountered with quantitative evaluation.These are problems with: (a) allowing for underlying assumptions, (b) determining how to handle underspecifications, and (c) evaluating the contribution of false positives and error chaining.Although both algorithms are part of theories of discourse that posit the interaction of the algorithm with an inference or intentional component, we will not use reasoning in tandem with the algorithm's operation.We have made this choice because we want to be able to analyse the performance of the algorithms across different domains.We focus on the linguistic basis of these approaches, using only selectional restrictions, so that our analysis is independent of the vagaries of a particular knowledge representation.Thus what we are evaluating is the extent to which these algorithms suffice to narrow the search of an inference component'.This analysis gives us 'But note the definition of success in section 2.1. some indication of the contribution of syntactic constraints, task structure and global focus to anaphoric processing.The data on which we compare the algorithms are important if we are to evaluate claims of generality.If we look at types of NL input, one clear division is between textual and interactive input.A related, though not identical factor is whether the language being analysed is produced by more than one person, although this distinction may be conflated in textual material such as novels that contain reported conversations.Within two-person interactive dialogues, there are the task-oriented masterslave type, where all the expertise and hence much of the initiative, rests with one person.In other twoperson dialogues, both parties may contribute discourse entities to the conversation on a more equal basis.Other factors of interest are whether the dialogues are human-to-human or human-to-computer, as well as the modality of communication, e.g. spoken or typed, since some researchers have indicated that dialogues, and particularly uses of reference within them, vary along these dimensions [Coh84, Tho80, GSBC86, D389, WS89].We analyse the performance of the algorithms on three types of data.Two of the samples are those that Hobbs used when developing his algorithm.One is an excerpt from a novel and the other a sample of journalistic writing.The remaining sample is a set of 5 human-human, keyboard-mediated, task-oriented dialogues about the assembly of a plastic water pump [Coh84].This covers only a subset of the above types.Obviously it would be instructive to conduct a similar analysis on other textual types.When embarking on such a comparison, it would be convenient to assume that the inputs to the algorithms are identical and compare their outputs.Unfortunately since researchers do not even agree on which phenomena can be explained syntactically and which semantically, the boundaries between two modules are rarely the same in NL systems.In this case the BFP centering algorithm and Hobbs algorithm both make ASSUMPTIONS about other system components.These are, in some sense, a further specification of the operation of the algorithms that must be made in order to hand-simulate the algorithms.There are two major sets of assumptions, based on discourse segmentation and syntactic representation.We attempt to make these explicit for each algorithm and pinpoint where the algorithms might behave differently were these assumptions not well-founded.In addition, there may be a number of UNDERSPECIFICATIONS in the descriptions of the algorithms.These often arise because theories that attempt to categorize naturally occurring data and algorithms based on them will always be prey to previously unencountered examples.For example, since the BFP salience hierarchy for discourse entities is based on grammatical relation, an implicit assumption is that an utterance only has one subject.However the novel Wheels has many examples of reported dialogue such as She continued, unperturbed, &quot;Mr. Vale quotes the Bible about air pollution.&quot; One might wonder whether the subject is She or Mr. Vale.In some cases, the algorithm might need to be further specificied in order to be able to process any of the data, whereas in others they may just highlight where the algorithm needs to be modified (see section 3.2).In general we count underspecifications as failures.Finally, it may not be clear what the DEFINITION OF SUCCESS is.In particular it is not clear what to do in those cases where an algorithm produces multiple or partial interpretations.In this situation a system might flag the utterance as ambiguous and draw in support from other discourse components.This arises in the present analysis for two reasons: (1) the constraints given by [GJW86] do not always allow one to choose a preferred interpretation, (2) the BFP algorithm proposes equally ranked interpretations in parallel.This doesn't happen with the Hobbs algorithm because it proposes interpretations in a sequential manner, one at a time.We chose to count as a failure those situations in which the BFP algorithm only reduces the number of possible interpretations, but Hobbs algorithm stops with a correct interpretation.This ignores the fact that Hobbs may have rejected a number of interpretations before stopping.We also have not needed to make a decision on how to score an algorithm that only finds one interpretation for an utterance that humans find ambiguous.The centering algorithm as defined by Brennan, Friedman and Pollard, (BFP algorithm), is derived from a set of rules and constraints put forth by Grosz, Josh i and Weinstein [GJW83, 0JW86].We shall not reproduce this algorithm here (See [BFP87]).There are two main structures in the centering algorithm, the CB, the BACKWARD LOOKING CENTER, which is what the discourse is 'about', and an ordered list, CF, of FORWARD LOOKING CENTERS, which are the discourse entities available to the next utterance for pronominalization.The centering framework predicts that in a local coherent stretch of dialogue, speakers will prefer to CONTINUE talking about the same discourse entity, that the CB will be the highest ranked entity of the previous utterance's forward centers that is realized in the current utterance, and that if anything is pronominalized the CB must be.In the centering framework, the order of the forward-centers list is intended to reflect the salience of discourse entities.The BFP algorithm orders this list by grammatical relation of the complements of the main verb, i.e. first the subject, then object, then indirect object, then other subcategorized-for complements, then noun phrases found in adjunct clauses.This captures the intuition that subjects are more salient than other discourse entities.The BFP algorithm added linguistic constraints on CONTRA-INDEXING to the centering framework.These constraints are exemplified by the fact that, in the sentence he likes him, the entity cospecified by he cannot be the same as that cospecified by him.We say that he and him are CONTRA-INDEXED.The BFP algorithm depends on semantic processing to precompute these constraints, since they are derived from the syntactic structure, and depend on some notion of c-command[Rei76].The other assumption that is dependent on syntax is that the the representations of discourse entities can be marked with the grammatical function through which they were realized, e.g. subject.The BFP algorithm assumes that some other mechanism can structure both written texts and taskoriented dialogues into hierarchical segments.The present concern is not with whether there might be a grammar of discourse that determines this structure, or whether it is derived from the cues that cooperative speakers give hearers to aid in processing.Since centering is a local phenomenon and is intended to operate within a segment, we needed to deduce a segmental structure in order to analyse the data.Speaker's intentions, task structure, cue words like O.K. now.., intonational properties of utterances, coherence relations, the scoping of modal operators, and mechanisms for shifting control between discourse participants have all been proposed as ways of determining discourse segmentation [Gro77, GSSG, Rei85, PH87, HL87, Hob78, Hob85, Rob88, WS88].Here, we use a combination of orthography, anaphora distribution, cue words and task structure.The rules are: BFP never state that cospecifiers for pronouns within the same segment are preferred over those in previous segments, but this is an implicit assumption, since this line of research is derived from Sidner's work on local focusing.Segment initial utterances therefore are the only situation where the BFP algorithm will prefer a within-sentence noun phrase as the cospecifier of a pronoun.The Hobbs algorithm is based on searching for a pronoun's co-specifier in the syntactic parse tree of input sentences [Hob76b].We reproduce this algorithm in full in the appendix along with an example.Hobbs algorithm operates on one sentence at a time, but the structure of previous sentences in the discourse is available.It is stated in terms of searches on parse trees.When looking for an intrasentential antecedent, these searches are conducted in a left-toright, breadth-first manner.However, when looking for a pronoun's antecedent within a sentence, it will go sequentially further and further up the tree to the left of the pronoun, and that failing will look in the previous sentence.Hobbs does not assume a segmentation of discourse structure in this algorithm; the algorithm will go back arbitrarily far in the text to find an antecedent.In more recent work, Hobbs uses the notion of COHERENCE RELATIONS to structure the discourse [HM87].The order by which Hobbs' algorithm traverses the parse tree is the closest thing in his framework to predictions about which discourse entities are salient.In the main it prefers co-specifiers for pronouns that are within the same sentence, and also ones that are closer to the pronoun in the sentence.This amounts to a claim that different discourse entities are salient, depending on the position of a pronoun in a sentence.When seeking an intersentential cospecification, Hobbs algorithm searches the parse tree of the previous utterance breadth-first, from left to right.This predicts that entities realized in subject position are more salient, since even if an adjunct clause linearly precedes the main subject, any noun phrases within it will be deeper in the parse tree.This also means that objects and indirect objects will be among the first possible antecedents found, and in general that the depth of syntactic embedding is an important determiner of discourse prominence.Miming to the assumptions about syntax, we note that Hobbs assumes that one can produce the correct syntactic structure for an utterance, with all adjunct phrases attached at the proper point of the parse tree.In addition, in order to obey linguistic constraints on coreference, the algorithm depends on the existence of a N parse tree node, which denotes a noun phrase without its determiner (See the example in the Appendix).Hobbs algorithm procedurally encodes contra-indexing constraints by skipping over NP nodes whose N node dominates the part of the parse tree in which the pronoun is found, which means that he cannot guarantee that two contraindexed pronouns will not choose the same NP as a co-specifier.Hobbs also assumes that his algorithm can somehow collect discourse entities mentioned alone into sets as co-specifiers of plural anaphors.Hobbs discusses at length other assumptions that he makes about the capabilities of an interpretive process that operates before the algorithm [Hob761)].This includes such things as being able to recover syntactically recoverable omitted text, such as elided verb phrases, and the identities of the speakers and hearers in a dialogue.A major component of any discourse algorithm is the prediction of which entities are salient, even though all the factors that contribute to the salience of a discourse entity have not been identified [Pri81, Pri85, BF83, HTD86].So an obvious question is when the two algorithms actually make different predictions.The main difference is that the choice of a co-specifier for a pronoun in the Hobbs algorithm depends in part on the position of that pronoun in the sentence.In the centering framework, no matter what criteria one uses to order the forward-centers list, pronouns take the most salient entities as antecedents, irrespective of that pronoun's position.Hobbs ordering of entities from a previous utterance varies from BFP in that possessors come before case-marked objects and indirect objects, and there may be some other differences as well but none of them were relevant to the analysis that follows.The effects ot some of the assumptions are measurable and we will attempt to specify exactly what these effects are, however some are not, e.g. we cannot measure the effect of Hobbs' syntax assumption since it is difficult to say how likely one is to get the wrong parse.We adopt the set collection assumption for both algorithms as well as the ability to recover the identity of speakers and hearers in dialogue.The texts on which the algorithms are analysed are the first chapter of Arthur Hailey's novel Wheels, and the July 7, 1975 edition of Newsweek.The sentences in Wheels are short and simple with long sequences consisting of reported conversation, so it is similar to a conversational text.The articles from Newsweek are typical of journalistic writing.For each text, the first 100 occurrences of singular and plural thirdperson pronouns were used to test the performance of the algorithms.The task-dialogues contain a total of 81 uses of it and no other pronouns except for I and you.In the figures below note that possessives like his are counted along with he and that accusatives like him and her are counted as he and she2.We performed three analyses on the quantitative results.A comparison of the two algorithms on each data set individually and an overall analysis on the three data sets combined revealed no significant differences in the performance of the two algorithms (x2 = 3.25, not significant).In addition for each algorithm alone we tested whether there were significant differences in performance for different textual types.Both of the algorithms performed significantly worse on the task dialogues (x2 = 22.05 for Hobbs, x2 = 21.55 for BFP, p < 0.05).We might wonder with what confidence we should view these numbers.A significant factor that must be considered is the contribution of FALSE POSITIVES and ERROR CHAINING.A FALSE POSITIVE is when an algorithm gets the right answer for the wrong reason.A very simple example of this phenomena is illustrated by this sequence from one of the task dialogues.The first it in Expi refers to the pump.Hobbs algorithm gets the right antecedent for it in Exp3, which is the little handle, but then fails on it in Exp4, whereas the BFP algorithm has the pump centered at Expi and continues to select that as the antecedent for it throughout the text.This means BFP gets the wrong co-specifier in Exp3 but this error allows it to get the correct co-specifier in Exp4.Another type of false positive example is &quot;Everybody and HIS brother suddenly wants to be the President's friend,&quot; said one aide.Hobbs gets this correct as long as one is willing to accept that Everybody is really the antecedent of his.It seems to me that this might be an idiomatic use.ERROR CHAINING refers to the fact that once an algorithm makes an error, other errors can result.Consider: Sorry no luck.Expi: I bet IT's the stupid red thing.Exp2: Take IT out.Cli2: Ok.IT is stuck.In this example once an algorithm fails at Expi it will fail on Exp2 and Cli2 as well since the choices of a cospecifier in the following examples are dependent on the choice in Expi.It isn't possible to measure the effect of false positives, since in some sense they are subjective judgements.However one can and should measure the effects of error chaining, since reporting numbers that correct for error chaining is misleading, but if the error that produced the error chain can be corrected then the algorithm might show a significant improvement.In this analysis, error chains contributed 22 failures to Hobbs' algorithm and 19 failures to BFP.The numbers presented in the previous section are intuitively unsatisfying.They tell us nothing about what makes the algorithms more or less general, or how they might be improved.In addition, given the assumptions that we needed to make in order to produce them, one might wonder to what extent the data is a result of these assumptions.Figure 1 also fails to indicate whether the two algorithms missed the same examples or are covering a different set of phenomena, i.e. what the relative distribution of the successes and failures are.But having done the hand-simulation in order to produce such numbers, all of this information is available.In this section we will first discuss the relative importance of various factors that go into producing the numbers above, then discuss if the algorithms can be modified since the flexibility of a framework in allowing one to make modifications is an important dimension of evaluation.The figures 2, 3 and 4 show for each pronominal category, the distribution of successes and failures for both algorithms.Since the main purpose of evaluation must be to improve the theory that we are evaluating, the most interesting cases are the ones on which the algorithms' performance varies and those that neither algorithm gets correct.We discuss these below.In the Wheels data, 4 examples rest on the assumption that the identities of speakers and hearers is recoverable.For example in The GM president smiled.&quot;Except Henry will be damned forceful and the papers won't print all HIS language.&quot;, getting the his correct here depends on knowing that it is the GM president speaking.Only 4 examples rest on being able to produce collections or discourse entities, and 2 of these occurred with an explicit instruction to the hearer to produce such a collection by using the phrase them both.There are 21 cases that Hobbs gets that BFP don't, and of these these a few classes stand out.In every case the relevant factor is Hobbs' preference for intrasentential co-specifiers.One class, (n = 3), is exemplified by Put the little black ring into the the large blue CAP with the hole in IT.All three involved using the preposition with in a descriptive adjunct on a noun phrase.It may be that with-adjuncts are common in visual descriptions, since they were only found in our data in the task dialogues, and a quick inspection of Gross's task-oriented dialogues revealed some as well[Deu74].Another class, (n = 7), are possessives.In some cases the possessive co-specified with the subject of the sentence, e.g.The SENATE took time from ITS paralyzing New Hampshire election debate to vote agreement, and in others it was within a relative clause and co-specified with the subject of that clause, e.g.The auto industry should be able to produce a totally safe, defect-free CAR that doesn't pollute ITS environment.Other cases seem to be syntactically marked subject matching with constructions that link two S clauses (n = 8).These are uses of more-than in e.g. but Chamberlain grossed about $8.3 million morethan HE could have made by selling on the home front.There also are S-if-S cases, as in Mondale said: &quot;I think THE MAFIA would be broke if IT conducted all its business that way.&quot; We also have subject matching in AS-AS examples as in ... and the resulting EXPOSURE to daylight has become as uncomfortable as IT was unaccustomed, as well as in sentential complements, such as But another liberal, Minnesota's Walter MONDALE, said HE had found a lot of incompetence in the agency's operations.The fact that quite a few of these are also marked with But may be significant.In terms of the possible effects that we noted earlier, the DEFINITION OF SUCCESS (see section 2.1 favors Hobbs (n = 2).Consider: K: Next take the red piece that is the smallest and insert it into the hole in the side of the large plastic tube.IT goes in the hole nearest the end with the engravings on IT.The Hobbs algorithm will correctly choose the end as the antecedent for the second it.The BFP algorithm on the other hand will get two interpretations, one in which the second it co-specifies the red piece and one in which it co-specifies the end.They are both CONTINUING interpretations since the first it co-specifies the CB, but the constraints don't make a choice.All of the examples on which BFP succeed and Hobbs fails have to do with extended discussion of one discourse entity.For instance: Expi: Now take the blue cap with the two prongs sticking out (Cu = blue cap) Exp2: and fit the little piece of pink plastic on IT.Ok?(Cs= blue cap) Clii: ok. Exp3: Insert the rubber ring into that blue cap.(Ca= blue cap) Exp4: Now screw IT onto the cylinder.On this example, Hobbs fails by choosing the cospecifier of it in Exp4 to be the rubber ring, even 2 5 6 though the whole segment has been about the blue cap.Another example from the novel WHEELS is given below.On this one Hobbs gets the first use of he but then misses the next four, as a result of missing the second one by choosing a housekeeper as the cospecifier for HIS...An executive vice-president of Ford was preparing to leave for Detroit Metropolitan Airport.HE had already breakfasted, alone.A housekeeper had brought a tray to HIS desk in the softly lighted study where, since 5 a.m., HE had been alternately reading memoranda (mostly on special blue stationery which Ford vice-presidents used in implementing policy) and dictating crisp instructions into a recording machine.HE had scarcely looked up, either as the mail arrived, or while eating, as HE accomplished in an hour what would have taken...Since an executive vice-president is centered in the first sentence, and continued in each following sentence, the BFP algorithm will correctly choose the cospecifier.Among the examples that neither algorithm gets correctly are 20 examples from the task dialogues of it referring to the global focus, the pump.In 15 cases, these shifts to global focus are marked syntactically with a cue word such as Now, and are not marked in 5 cases.Presumably they are felicitous since the pump is visually salient.Besides the global focus cases, pronominal references to entities that were not linguistically introduced are rare.The only other example is an implicit reference to 'the problem' of the pump not working: Clii: Sorry no luck.Expi: I bet IT's the stupid red thing.We have only two examples of sentential or VP anaphora altogether, such as Madam Chairwoman, said Colby at last, I am trying to run a secret intelligence service.IT was a forlorn hope.Neither Hobbs algorithm nor BFP attempt to cover these examples.Three of the examples are uses of it that seem to be lexicalized with certain verbs, e.g.They hit IT off real well.One can imagine these being treated as phrasal lexical items, and therefore not handled by an anaphoric processing component[AS89].Most of the interchanges in the task dialogues consist of the client responding to commands with cues such as O.K. or Ready to let the expert know when they have completed a task.When both parties contribute discourse entities to the common ground, both algorithms may fail (n = 4).Consider: Expi: Now we have a little red piece left Exp2: and I don't know what to do with IT.CHI: Well, there is a hole in the green plunger inside the cylinder.Exp3: I don't think IT goes in THERE.Exp4: I think IT may belong in the blue cap onto which you put the pink piece of plastic.In Exp3, one might claim that it and there are contraindexed, and that there can be properly resolved to a hole, so that it cannot be any of the noun phrases in the prepositional phrases that modify a hole, but whether any theory of contra-indexing actually give us this is questionable.The main factor seems to be that even though Expi is not syntactically a question, the little red piece is the focus of a question, and as such is in focus despite the fact that the syntactic construction there is supposedly focuses a hole in the green plunger ...[Sid79].These examples suggest that a questioned entity is left focused until the point in the dialogue at which the question is resolved.The fact that well has been noted as a marker of response to questions supports this analysis[Sch87].Thus the relevant factor here may be the switching of control among discourse participants [WS88].These mixed-initiative features make these sequences inherently different than text.Task structure in the pump dialogues is an important factor especially as it relates to the use of global focus.Twenty of the cases on which both algorithms fail are references to the pump, which is the global focus.We can include a global focus in the centering framework, as a separate notion from the current CB.This means that in the 15 out of 20 cases where the shift to global focus is identifiably marked with a cue-word such as now, the segment rules will allow BFP to get the global focus examples. forward centers list, as Sidner does in her algorithm for local focusing [Sid79].This lets BFP get the two examples of event anaphora.Hobbs discusses the fact that his algorithm cannot be modified to get event anaphora in [Hob76b].Another interesting fact is that in every case in which Hobbs' algorithm gets the correct co-specifier and BFP didn't, the relevant factor is Hobbs' preference for intrasentential co-specifiers.One view on these cases may be that these are not discourse anaphora, but there seems to be no principled way to make this distinction.However, Carter has proposed some extensions to Sidner's algorithm for local focusing that seem to be relevant here(chap.6, [Car87]).He argues that intra-sentential candidates (ISCs) should be preferred over candidates from the previous utterance, ONLY in the cases where no discourse center has been established or the discourse center is rejected for syntactic or selectional reasons.He then uses Hobbs algorithm to produce an ordering of these ISCs.This is compatible with the centering framework since it is underspecified as to whether one should always choose to establish a discourse center with a co-specifier from a previous utterance.If we adopt Carter's rule into the centering framework, we find that of the 21 cases that Hobbs gets that BFP don't, in 7 cases there is no discourse center established, and in another 4 the current center can be rejected on the basis of syntactic or sortal information.Of these Carter's rule clearly gets 5, and another 3 seem to rest on whether one might want to establish a discourse entity from a previous utterance.Since the addition of this constraint does not allow BFP to get any examples that neither algorithm got, it seems that this combination is a way of making the best out of both algorithms.The addition of these modifications changes the quantitative results.See the Figure 5.However, the statistical analyses still show that there is no significant difference in the performance of the algorithms in general.It is also still the case that the performance of each algorithm significantly varies depending on the data.The only significant difference as a result of the modifications is that the BFP algorithm now performs significantly better on the pump dialogues alone (x2 = 4.31,p < .05).We can benefit in two ways from performing such evaluations: (a) we get general results on a methodology for doing evaluation, (b) we discover ways we can improve current theories.A split of evaluation efforts into quantitative versus qualitative is incoherent.We cannot trust the results of a quantitative evaluation without doing a considerable amount of qualitative analyses and we should perform our qualitative analyses on those components that make a significant contribution to the quantitative results; we need to be able to measure the effect of various factors.These measurements must be made by doing comparisons at the data level.In terms of general results, we have identified some factors that make evaluations of this type more complicated and which might lead us to evaluate solely quantitative results with care.These are: (a) To decide how to evaluate UNDERSPECIFICATIONS and the contribution of ASSUMPTIONS, and (b) To determine the effects of FALSE POSITIVES and ERROR CHAINING.We advocate an approach in which the contribution of each underspecification and assumption is tabulated as well as the effect of error chains.If a principled way could be found to identify false positives, their effect should be reported as well as part of any quantitative evaluation.In addition, we have taken a few steps towards determining the relative importance of different factors to the successful operation of discourse modules.The percent of successes that both algorithms get indicates that syntax has a strong influence, and that at the very least we can reduce the amount of inference required.In 59% to 82% of the cases both algorithms get the correct result.This probably means that in a large number of cases there was no potential conflict of co-specifiers.In addition, this analysis has shown, that at least for task-oriented dialogues global focus is a significant factor, and in general discourse structure is more important in the task dialogues.However simple devices such as cue words may go a long way toward determining this structure.Finally, we should note that doing evaluations such as this allows us to determine the GENERALITY of our approaches.Since the performance of both Hobbs and BFP varies according to the type of the text, and in fact was significantly worse on the task dialogues than on the texts, we might question how their performance would vary on other inputs.An annotated corpus comprising some of the various NL input types such as those I discussed in the introduction would go a long way towards giving us a basis against which we could evaluate the generality of our theories.David Carter, Phil Cohen, Nick Haddock, Jerry Hobbs, Aravind Joshi, Don Knuth, Candy Sidner, Phil Stenton, Bonnie Webber, and Steve Whittaker have provided valuable insights toward this endeavor and critical comments on a multiplicity of earlier versions of this paper.Steve Whittaker advised me on the statistical analyses.I would like to thank Jerry Hobbs for encouraging me to do this in the first place.ers plans.In Proc.International Joint Conference on Artificial Intelligence, pages 203-208, Vancouver, BC, Canada, 1981.[Sid79] Candace L. Sidner.Toward a computational theory of definite anaphora comprehension in English.Technical Report AITR-537, MIT, 1979.
Statistical Decision-Tree Models For ParsingSyntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text, as is evidenced by their poor performance on domains like the Wall Street Journal, and by the movement away from parsing-based approaches to textprocessing in general.In this paper, I describe SPATTER, a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result.This work is based on the following premises: (1) grammars are too complex and detailed to develop manually for most interesting domains; (2) parsing models must rely heavily on lexical and contextual information to analyze sentences accurately; and (3) existing n-grain modeling techniques are inadequate for parsing models.In experiments comparing SPATTER with IBM's computer manuals parser, SPATTER significantly outperforms the grammar-based parser.Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures, SPATachieves 86% precision, and 1.3 crossing brackets per sentence for sentences of 40 words or less, and 91% precision, 90% recall, and 0.5 crossing brackets for sentences between 10 and 20 words in length.Parsing a natural language sentence can be viewed as making a sequence of disambiguation decisions: determining the part-of-speech of the words, choosing between possible constituent structures, and selecting labels for the constituents.Traditionally, disambiguation problems in parsing have been addressed by enumerating possibilities and explicitly declaring knowledge which might aid the disambiguation process.However, these approaches have proved too brittle for most interesting natural language problems.This work addresses the problem of automatically discovering the disambiguation criteria for all of the decisions made during the parsing process, given the set of possible features which can act as disambiguators.The candidate disambiguators are the words in the sentence, relationships among the words, and relationships among constituents already constructed in the parsing process.Since most natural language rules are not absolute, the disambiguation criteria discovered in this work are never applied deterministically.Instead, all decisions are pursued non-deterministically according to the probability of each choice.These probabilities are estimated using statistical decision tree models.The probability of a complete parse tree (T) of a sentence (S) is the product of each decision (c11) conditioned on all previous decisions: Each decision sequence constructs a unique parse, and the parser selects the parse whose decision sequence yields the highest cumulative probability.By combining a stack decoder search with a breadthfirst algorithm with probabilistic pruning, it is possible to identify the highest-probability parse for any sentence using a reasonable amount of memory and time.The claim of this work is that statistics from a large corpus of parsed sentences combined with information-theoretic classification and training algorithms can produce an accurate natural language parser without the aid of a complicated knowledge base or grammar.This claim is justified by constructing a parser, called SPATTER (Statistical PATTErn Recognizer), based on very limited linguistic information, and comparing its performance to a state-of-the-art grammar-based parser on a common task.It remains to be shown that an accurate broad-coverage parser can improve the performance of a text processing application.This will be the subject of future experiments.One of the important points of this work is that statistical models of natural language should not be restricted to simple, context-insensitive models.In a problem like parsing, where long-distance lexical information is crucial to disambiguate interpretations accurately, local models like probabilistic context-free grammars are inadequate.This work illustrates that existing decision-tree technology can be used to construct and estimate models which selectively choose elements of the context which contribute to disambiguation decisions, and which have few enough parameters to be trained using existing resources.I begin by describing decision-tree modeling, showing that decision-tree models are equivalent to interpolated n-gram models.Then I briefly describe the training and parsing procedures used in SPATTER.Finally, I present some results of experiments comparing SPATTER with a grammarian's rulebased statistical parser, along with more recent results showing SPATTER applied to the Wall Street Journal domain.Much of the work in this paper depends on replacing human decision-making skills with automatic decision-making algorithms.The decisions under consideration involve identifying constituents and constituent labels in natural language sentences.Grammarians, the human decision-makers in parsing, solve this problem by enumerating the features of a sentence which affect the disambiguation decisions and indicating which parse to select based on the feature values.The grammarian is accomplishing two critical tasks: identifying the features which are relevant to each decision, and deciding which choice to select based on the values of the relevant features.Decision-tree classification algorithms account for both of these tasks, and they also accomplish a third task which grammarians classically find difficult.By assigning a probability distribution to the possible choices, decision trees provide a ranking system which not only specifies the order of preference for the possible choices, but also gives a measure of the relative likelihood that each choice is the one which should be selected.A decision tree is a decision-making device which assigns a probability to each of the possible choices based on the context of the decision: P(f Ih), where f is an element of the future vocabulary (the set of choices) and h is a history (the context of the decision).This probability P(flh) is determined by asking a sequence of questions qi q2...q„ about the context, where the ith question asked is uniquely determined by the answers to the i —1 previous questions.For instance, consider the part-of-speech tagging problem.The first question a decision tree might ask is: If the answer is the, then the decision tree needs to ask no more questions; it is clear that the decision tree should assign the tag f = determiner with probability 1.If, instead, the answer to question 1 is bear, the decision tree might next ask the question: If the answer to question 2 is determiner, the decision tree might stop asking questions and assign the tag f = noun with very high probability, and the tag f = verb with much lower probability.However, if the answer to question 2 is noun, the decision tree would need to ask still more questions to get a good estimate of the probability of the tagging decision.The decision tree described in this paragraph is shown in Figure 1.Each question asked by the decision tree is represented by a tree node (an oval in the figure) and the possible answers to this question are associated with branches emanating from the node.Each node defines a probability distribution on the space of possible decisions.A node at which the decision tree stops asking questions is a leaf node.The leaf nodes represent the unique states in the decision-making problem, i.e. all contexts which lead to the same leaf node have the same probability distribution for the decision.A decision-tree model is not really very different from an interpolated n-gram model.In fact, they are equivalent in representational power.The main differences between the two modeling techniques are how the models are parameterized and how the parameters are estimated.First, let's be very clear on what we mean by an n-grain model.Usually, an n-gram model refers to a Markov process where the probability of a particular token being generating is dependent on the values of the previous n — 1 tokens generated by the same process.By this definition, an n-gram model has I WI&quot; parameters, where IWI is the number of unique tokens generated by the process.However, here let's define an n-gram model more loosely as a model which defines a probability distribution on a random variable given the values of n-1 random variables, P(.flhi h2 .• . hn-1).There is no assumption in the definition that any of the random variables F or Hi range over the same vocabulary.The number of parameters in this n-gram model is IFI H IHil.Using this definition, an n-gram model can be represented by a decision-tree model with n — 1 questions.For instance, the part-of-speech tagging model P(t11w2t1—iti-2) can be interpreted as a 4gram model, where HI is the variable denoting the word being tagged, 112 is the variable denoting the tag of the previous word, and H3 is the variable denoting the tag of the word two words back.Hence, this 4-gram tagging model is the same as a decisiontree model which always asks the sequence of 3 questions: But can a decision-tree model be represented by an n-gram model?No, but it can be represented by an interpolated n-gram model.The proof of this assertion is given in the next section.The standard approach to estimating an n-gram model is a two step process.The first step is to count the number of occurrences of each n-gram from a training corpus.This process determines the empirical distribution, The second step is smoothing the empirical distribution using a separate, held-out corpus.This step improves the empirical distribution by finding statistically unreliable parameter estimates and adjusting them based on more reliable information.A commonly-used technique for smoothing is deleted interpolation._ Deleted interpolation estimates a model P(flhi 112 .• • hn-1) by using a linear combination of empirical models P(fihki hk, . h,.), where m < n and k1_1 < ki < n for all i < m. For example, a model P(f Ihi h2h3) might be interpolated as follows: where E Ai (hi h2h3) = 1 for all histories hi h2h3.The optimal values for the Ai functions can be estimated using the forward-backward algorithm (Baum, 1972).A decision-tree model can be represented by an interpolated n-gram model as follows.A leaf node in a decision tree can be represented by the sequence of question answers, or history values, which leads the decision tree to that leaf.Thus, a leaf node defines a probability distribution based on values of those questions: P(f Ihkihh ... hk..), where m < n and ki_I < k < n, and where hk, is the answer to one of the questions asked on the path from the root to the leaf.'But this is the same as one of the terms in the interpolated n-gram model.So, a decision 'Note that in a decision tree, the leaf distribution is not affected by the order in which questions are asked.Asking about Pi, followed by h2 yields the same future distribution as asking about h2 followed by h1. tree can be defined as an interpolated n-grain model where the Ai function is defined as: 1 if hki hk, hk_ is a leaf, Ai (hki hk, • • • hk,.)= o otherwise.The point of showing the equivalence between ngram models and decision-tree models is to make clear that the power of decision-tree models is not in their expressiveness, but instead in how they can be automatically acquired for very large modeling problems.As n grows, the parameter space for an n-gram model grows exponentially, and it quickly becomes computationally infeasible to estimate the smoothed model using deleted interpolation.Also, as n grows large, the likelihood that the deleted interpolation process will converge to an optimal or even near-optimal parameter setting becomes vanishingly small.On the other hand, the decision-tree learning algorithm increases the size of a model only as the training data allows.Thus, it can consider very large history spaces, i.e. n-gram models with very large n. Regardless of the value of n, the number of parameters in the resulting model will remain relatively constant, depending mostly on the number of training examples.The leaf distributions in decision trees are empirical estimates, i.e. relative-frequency counts from the training data.Unfortunately, they assign probability zero to events which can possibly occur.Therefore, just as it is necessary to smooth empirical ngram models, it is also necessary to smooth empirical decision-tree models.The decision-tree learning algorithms used in this work were developed over the past 15 years by the IBM Speech Recognition group (Bahl et al., 1989).The growing algorithm is an adaptation of the CART algorithm in (Breiman et al., 1984).For detailed descriptions and discussions of the decisiontree algorithms used in this work, see (Magerman, 1994).An important point which has been omitted from this discussion of decision trees is the fact that only binary questions are used in these decision trees.A question which has k values is decomposed into a sequence of binary questions using a classification tree on those k values.For example, a question about a word is represented as 30 binary questions.These 30 questions are determined by growing a classification tree on the word vocabulary as described in (Brown et al., 1992).The 30 questions represent 30 different binary partitions of the word vocabulary, and these questions are defined such that it is possible to identify each word by asking all 30 questions.For more discussion of the use of binary decision-tree questions, see (Magerman, 1994).The SPATTER parsing algorithm is based on interpreting parsing as a statistical pattern recognition process.A parse tree for a sentence is constructed by starting with the sentence's words as leaves of a tree structure, and labeling and extending nodes these nodes until a single-rooted, labeled tree is constructed.This pattern recognition process is driven by the decision-tree models described in the previous section.A parse tree can be viewed as an n-ary branching tree, with each node in a tree labeled by either a non-terminal label or a part-of-speech label.If a parse tree is interpreted as a geometric pattern, a constituent is no more than a set of edges which meet at the same tree node.For instance, the noun phrase, &quot;a brown cow,&quot; consists of an edge extending to the right from &quot;a,&quot; an edge extending to the left from &quot;cow,&quot; and an edge extending straight up from &quot;brown&quot;.In SPATTER, a parse tree is encoded in terms of four elementary components, or features: words, tags, labels, and extensions.Each feature has a fixed vocabulary, with each element of a given feature vocabulary having a unique representation.The word feature can take on any value of any word.The tag feature can take on any value in the part-of-speech tag set.The label feature can take on any value in the non-terminal set.The extension can take on any of the following five values: right - the node is the first child of a constituent; left - the node is the last child of a constituent; up - the node is neither the first nor the last child of a constituent; unary - the node is a child of a unary constituent; root - the node is the root of the tree.For an n word sentence, a parse tree has n leaf nodes, where the word feature value of the ith leaf node is the ith word in the sentence.The word feature value of the internal nodes is intended to contain the lexical head of the node's constituent.A deterministic lookup table based on the label of the internal node and the labels of the children is used to approximate this linguistic notion.The SPATTER representation of the sentence is shown in Figure 3.The nodes are constructed bottom-up from left-to-right, with the constraint that no constituent node is constructed until all of its children have been constructed.The order in which the nodes of the example sentence are constructed is indicated in the figure.Each code used by the PC is listed SPATTER consists of three main decision-tree models: a part-of-speech tagging model, a nodeextension model, and a node-labeling model.Each of these decision-tree models are grown using the following questions, where X is one of word, tag, label, or extension, and Y is either left and right: For each of the nodes listed above, the decision tree could also ask about the number of children and span of the node.For the tagging model, the values of the previous two words and their tags are also asked, since they might differ from the head words of the previous two constituents.The training algorithm proceeds as follows.The training corpus is divided into two sets, approximately 90% for tree growing and 10% for tree smoothing.For each parsed sentence in the tree growing corpus, the correct state sequence is traversed.Each state transition from si to si+i is an event; the history is made up of the answers to all of the questions at state si and the future is the value of the action taken from state si to state s.1.Each event is used as a training example for the decisiontree growing process for the appropriate feature's tree (e.g. each tagging event is used for growing the tagging tree, etc.).After the decision trees are grown, they are smoothed using the tree smoothing corpus using a variation of the deleted interpolation algorithm described in (Magerman, 1994).The parsing procedure is a search for the highest probability parse tree.The probability of a parse is just the product of the probability of each of the actions made in constructing the parse, according to the decision-tree models.Because of the size of the search space, (roughly 0(1711N1), where IT1 is the number of part-ofspeech tags, n is the number of words in the sentence, and 'NI is the number of non-terminal labels), it is not possible to compute the probability of every parse.However, the specific search algorithm used is not very important, so long as there are no search errors.A search error occurs when the the highest probability parse found by the parser is not the highest probability parse in the space of all parses.SPATTER's search procedure uses a two phase approach to identify the highest probability parse of a sentence.First, the parser uses a stack decoding algorithm to quickly find a complete parse for the sentence.Once the stack decoder has found a complete parse of reasonable probability (> 10—s), it switches to a breadth-first mode to pursue all of the partial parses which have not been explored by the stack decoder.In this second mode, it can safely discard any partial parse which has a probability lower than the probability of the highest probability completed parse.Using these two search modes, SPATTER guarantees that it will find the highest probability parse.The only limitation of this search technique is that, for sentences which are modeled poorly, the search might exhaust the available memory before completing both phases.However, these search errors conveniently occur on sentences which SPATTER is likely to get wrong anyway, so there isn't much performance lossed due to the search errors.Experimentally, the search algorithm guarantees the highest probability parse is found for over 96% of the sentences parsed.In the absence of an NL system, SPATTER can be evaluated by comparing its top-ranking parse with the treebank analysis for each test sentence.The parser was applied to two different domains, IBM Computer Manuals and the Wall Street Journal.The first experiment uses the IBM Computer Manuals domain, which consists of sentences extracted from IBM computer manuals.The training and test sentences were annotated by the University of Lancaster.The Lancaster treebank uses 195 part-ofspeech tags and 19 non-terminal labels.This treebank is described in great detail in (Black et al., 1993).The main reason for applying SPATTER to this domain is that IBM had spent the previous ten years developing a rule-based, unification-style probabilistic context-free grammar for parsing this domain.The purpose of the experiment was to estimate SPATTER's ability to learn the syntax for this domain directly from a treebank, instead of depending on the interpretive expertise of a grammarian.The parser was trained on the first 30,800 sentences from the Lancaster treebank.The test set included 1,473 new sentences, whose lengths range from 3 to 30 words, with a mean length of 13.7 words.These sentences are the same test sentences used in the experiments reported for IBM's parser in (Black et al., 1993).In (Black et al., 1993), IBM's parser was evaluated using the 0-crossingbrackets measure, which represents the percentage of sentences for which none of the constituents in the parser's parse violates the constituent boundaries of any constituent in the correct parse.After over ten years of grammar development, the IBM parser achieved a 0-crossing-brackets score of 69%.On this same test set, SPATTER scored 76%.The experiment is intended to illustrate SPATTER's ability to accurately parse a highly-ambiguous, large-vocabulary domain.These experiments use the Wall Street Journal domain, as annotated in the Penn Treebank, version 2.The Penn Treebank uses 46 part-of-speech tags and 27 non-terminal labels.2 The WSJ portion of the Penn Treebank is divided into 25 sections, numbered 00 - 24.In these experiments, SPATTER was trained on sections 02 - 21, which contains approximately 40,000 sentences.The test results reported here are from section 00, which contains 1920 sentences.3 Sections 01, 22, 23, and 24 will be used as test data in future experiments.The Penn Treebank is already tokenized and sentence detected by human annotators, and thus the test results reported here reflect this.SPATTER parses word sequences, not tag sequences.Furthermore, SPATTER does not simply pre-tag the sentences and use only the best tag sequence in parsing.Instead, it uses a probabilistic model to assign tags to the words, and considers all possible tag sequences according to the probability they are assigned by the model.No information about the legal tags for a word are extracted from the test corpus.In fact, no information other than the words is used from the test corpus.For the sake of efficiency, only the sentences of 40 words or fewer are included in these experiments.4 For this test set, SPATTER takes on average 12 seconds per sentence on an SGI R4400 with 160 megabytes of RAM.To evaluate SPATTER's performance on this domain, I am using the PARSEVAL measures, as defined in (Black et al., 1991): Precision no. of correct constituents in SPATTER parse no. of constituents in SPATTER parse Recall no. of correct constituents in SPATTER parse no. of constituents in treebank parse Crossing Brackets no. of constituents which violate constituent boundaries with a constituent in the treebank parse.The precision and recall measures do not consider constituent labels in their evaluation of a parse, since the treebank label set will not necessarily coincide with the labels used by a given grammar.Since SPATTER uses the same syntactic label set as the Penn Treebank, it makes sense to report labelled precision and labelled recall.These measures are computed by considering a constituent to be correct if and only if it's label matches the label in the treebank.Table 1 shows the results of SPATTER evaluated against the Penn Treebank on the Wall Street Journal section 00.Figures 5, 6, and 7 illustrate the performance of SPATTER as a function of sentence length.SPATTER's performance degrades slowly for sentences up to around 28 words, and performs more poorly and more erratically as sentences get longer.Figure 4 indicates the frequency of each sentence length in the test corpus. function of sentence length for Wall Street Journal experiments.Regardless of what techniques are used for parsing disambiguation, one thing is clear: if a particular piece of information is necessary for solving a disambiguation problem, it must be made available to the disambiguation mechanism.The words in the sentence are clearly necessary to make parsing decisions, and in some cases long-distance structural information is also needed.Statistical models for parsing need to consider many more features of a sentence than can be managed by n-gram modeling techniques and many more examples than a human can keep track of.The SPATTER parser illustrates how large amounts of contextual information can be incorporated into a statistical model for parsing by applying decision-tree learning algorithms to a large annotated corpus.
Semeval-2012 Task 8: Cross-lingual Textual Entailment for Content SynchronizationThis paper presents the first round of the on Textual Entailment for organized within SemEval-2012.The task was designed to promote research on semantic inference over texts written in different languages, targeting at the same time a real application scenario.Participants were presented with datasets for different language pairs, where multi-directional entailment relations (“forward”, “backward”, “bidirectional”, “no entailment”) had to be identified.We report on the training and test data used for evaluation, the process of their creation, the participating systems (10 teams, 92 runs), the approaches adopted and the results achieved.The cross-lingual textual entailment task (Mehdad et al., 2010) addresses textual entailment (TE) recognition (Dagan and Glickman, 2004) under the new dimension of cross-linguality, and within the new challenging application scenario of content synchronization.Cross-linguality represents a dimension of the TE recognition problem that has been so far only partially investigated.The great potential for integrating monolingual TE recognition components into NLP architectures has been reported in several areas, including question answering, information retrieval, information extraction, and document summarization.However, mainly due to the absence of cross-lingual textual entailment (CLTE) recognitioncomponents, similar improvements have not been achieved yet in any cross-lingual application.The CLTE task aims at prompting research to fill this gap.Along such direction, research can now benefit from recent advances in other fields, especially machine translation (MT), and the availability of: i) large amounts of parallel and comparable corpora in many languages, ii) open source software to compute word-alignments from parallel corpora, and iii) open source software to set up MT systems.We believe that all these resources can positively contribute to develop inference mechanisms for multilingual data.Content synchronization represents a challenging application scenario to test the capabilities of advanced NLP systems.Given two documents about the same topic written in different languages (e.g.Wiki pages), the task consists of automatically detecting and resolving differences in the information they provide, in order to produce aligned, mutually enriched versions of the two documents.Towards this objective, a crucial requirement is to identify the information in one page that is either equivalent or novel (more informative) with respect to the content of the other.The task can be naturally cast as an entailment recognition problem, where bidirectional and unidirectional entailment judgments for two text fragments are respectively mapped into judgments about semantic equivalence and novelty.Alternatively, the task can be seen as a machine translation evaluation problem, where judgments about semantic equivalence and novelty depend on the possibility to fully or partially translate a text fragment into the other.The recent advances on monolingual TE on the one hand, and the methodologies used in Statistical Machine Translation (SMT) on the other, offer promising solutions to approach the CLTE task.In line with a number of systems that model the RTE task as a similarity problem (i.e. handling similarity scores between T and H as useful evidence to draw entailment decisions), the standard sentence and word alignment programs used in SMT offer a strong baseline for CLTE.However, although representing a solid starting point to approach the problem, similarity-based techniques are just approximations, open to significant improvements coming from semantic inference at the multilingual level (e.g. cross-lingual entailment rules such as “perro”→“animal”).Taken in isolation, similaritybased techniques clearly fall short of providing an effective solution to the problem of assigning directions to the entailment relations (especially in the complex CLTE scenario, where entailment relations are multi-directional).Thanks to the contiguity between CLTE, TE and SMT, the proposed task provides an interesting scenario to approach the issues outlined above from different perspectives, and large room for mutual improvement.Given a pair of topically related text fragments (T1 and T2) in different languages, the CLTE task consists of automatically annotating it with one of the following entailment judgments (see Figure 1 for Spanish/English examples of each judgment): In this task, both T1 and T2 are assumed to be true statements.Although contradiction is relevant from an application-oriented perspective, contradictory pairs are not present in the dataset created for the first round of the task.Four CLTE corpora have been created for the following language combinations: Spanish/English (SP-EN), Italian/English (IT-EN), French/English (FR-EN), German/English (DE-EN).The datasets are released in the XML format shown in Figure 1.The dataset was created following the crowdsourcing methodology proposed in (Negri et al., 2011), which consists of the following steps: only the pairs where the difference between the number of words in T1 and T2 (length diff) was below a fixed threshold (10 words) were retained.1 The final result is a monolingual English dataset annotated with multi-directional entailment judgments, which are well distributed over length diff values ranging from 0 to 9; To ensure the good quality of the datasets, all the collected pairs were manually checked and corrected when necessary.Only pairs with agreement between two expert annotators were retained.The final result is a multilingual parallel entailment corpus, where T1s are in 5 different languages (i.e.English, Spanish, German, Italian, and French), and T2s are in English.It’s worth mentioning that the monolingual English corpus, a by-product of our data collection methodology, will be publicly released as a further contribution to the research community.2 Each dataset consists of 1,000 pairs (500 for training and 500 for test), balanced across the four entailment judgments (bidirectional, forward, backward, and no entailment).For each language combination, the distribution of the four entailment judgments according to length diff is shown in Figure 2.Vertical bars represent, for each length diff value, the proportion of pairs belonging to the four entailment classes.As can be seen, the length diff constraint applied to the length difference in the monolingual English pairs (step 3 of the creation process) is substantially reflected in the cross-lingual datasets for all language combinations.In fact, as shown in Table 1, the majority of the pairs is always included in the same length diff range (approximately [-5,+5]) and, within this range, the distribution of the four classes is substantially uniform.Our assumption is that such data distribution makes entailment judgments based on mere surface features such as sentence length ineffective, thus encouraging the development of alternative, deeper processing strategies.Evaluation results have been automatically computed by comparing the entailment judgments returned by each system with those manually assigned by human annotators.The metric used for systems’ ranking is accuracy over the whole test set, i.e. the number of correct judgments out of the total number of judgments in the test set.Additionally, we calculated precision, recall, and F1 measures for each of the four entailment judgment categories taken separately.These scores aim at giving participants the possibility to gain clearer insights into their system’s behavior on the entailment phenomena relevant to the task.For each language combination, two baselines considering the length difference between T1 and T2 have been calculated (besides the trivial 0.25 accuracy score obtained by assigning each test pair in the balanced dataset to one of the four classes): judgments returned by the two classifiers are composed into a single multi-directional judgment (“YES-YES”=“bidirectional”, “YESNO”=“forward”, “NO-YES”=“backward”, “NO-NO”=“no entailment”); Both the baselines have been calculated with the LIBSVM package (Chang and Lin, 2011), using a linear kernel with default parameters.Baseline results are reported in Table 2.Although the four CLTE datasets are derived from the same monolingual EN-EN corpus, baseline results present slight differences due to the effect of translation into different languages.Participants were allowed to submit up to five runs for each language combination.A total of 17 teams registered to participate in the task and downloaded the training set.Out of them, 12 downloaded the test set and 10 (including one of the task organizers) submitted valid runs.Eight teams produced submissions for all the language combinations, while two teams participated only in the SP-EN task.In total, 92 runs have been submitted and evaluated (29 for SP-EN, and 21 for each of the other language pairs).Despite the novelty and the difficulty of the problem, these numbers demonstrate the interest raised by the task, and the overall success of the initiative.Accuracy results are reported in Table 3.As can be seen from the table, overall accuracy scores are quite different across language pairs, with the highest result on SP-EN (0.632), which is considerably higher than the highest score on DE-EN (0.558).This might be due to the fact that most of the participating systems rely on a “pivoting” approach that addresses CLTE by automatically translating T1 in the same language of T2 (see Section 6).Regarding the DE-EN dataset, pivoting methods might be penalized by the lower quality of MT output when German T1s are translated into English.The comparison with baselines results leads to interesting observations.First of all, while all systems significantly outperform the lowest 1-class baseline (0.25), both other baselines are surprisingly hard to beat.This shows that, despite the effort in keeping the distribution of the entailment classes uniform across different length diff values, eliminating the correlation between sentences’ length and correct entailment decisions is difficult.As a consequence, although disregarding semantic aspects of the problem, features considering such information are quite effective.In general, systems performed better on the SPEN dataset, with most results above the binary baseline (8 out of 10), and half of the systems above the multi-class baseline.For the other language pairs the results are lower, with only 3 out of 8 participants above the two baselines in all datasets.Average results reflect this situation: the average scores are always above the binary baseline, whereas only the SP-EN average result is higher than the multiclass baseline(0.44 vs. 0.43).To better understand the behaviour of each system (also in relation to the different language combinations), Table 4 provides separate precision, recall, and F1 scores for each entailment judgment, calculated over the best runs of each participating team.Overall, the results suggest that the “bidirectional” and “no entailment” categories are more problematic than “forward” and “backward” judgments.For most datasets, in fact, systems’ performance on “bidirectional” and “no entailment” is significantly lower, typically on recall.Except for the DE-EN dataset (more problematic on “forward”), also average F1 results on these judgments are lower.This might be due to the fact that, for all datasets, the vast majority of “bidirectional” and “no entailment” judgments falls in a length diff range where the distribution of the four classes is more uniform (see Figure 2).Similar reasons can justify the fact that “backward” entailment results are consistently higher on all datasets.Compared with “forward” entailment, these judgments are in fact less scattered across the entire length diff range (i.e. less intermingled with the other classes).A rough classification of the approaches adopted by participants can be made along two orthogonal dimensions, namely: Concerning the former dimension, most of the systems (6 out of 10) adopted a pivoting approach, relying on Google Translate (4 systems), Microsoft Bing Translator (1), or a combination of Google, Bing, and other MT systems (1) to produce English T2s.Regarding the latter dimension, the compositional approach was preferred to multi-class classification (6 out of 10).The best performing system relies on a “hybrid” approach (combining monolingual and cross-lingual alignments) and a compositional strategy.Besides the frequent recourse to MT tools, other resources used by participants include: on-line dictionaries for the translation of single words, word alignment tools, part-of-speech taggers, NP chunkers, named entity recognizers, stemmers, stopwords lists, and Wikipedia as an external multilingual corpus.More in detail: BUAP [pivoting, compositional] (Vilari˜no et al., 2012) adopts a pivoting method based on translating T1 into the language of T2 and vice versa (Google Translate3 and the OpenOffice Thesaurus4).Similarity measures (e.g.Jaccard index) and rules are respectively used to annotate the two resulting sentence pairs with entailment judgments and combine them in a single decision.CELI [cross lingual, compositional & multiclass] (Kouylekov, 2012) uses dictionaries for word matching, and a multilingual corpus extracted from Wikipedia for term weighting.Word overlap and similarity measures are then used in different approaches to the task.In one run (Run 1), they are used to train a classifier that assigns separate entailment judgments for each direction.Such judgments are finally composed into a single one for each pair.In the other runs, the same features are used for multi-class classification.DirRelCond3 [cross lingual, compositional] (Perini, 2012) uses bilingual dictionaries (Freedict5 and WordReference6) to translate content words into English.Then, entailment decisions are taken combining directional relatedness scores between words in both directions (Perini, 2011).FBK [cross lingual, compositional & multiclass] (Mehdad et al., 2012a) uses cross-lingual matching features extracted from lexical phrase tables, semantic phrase tables, and dependency relations (Mehdad et al., 2011; Mehdad et al., 2012b; Mehdad et al., 2012c).The features are used for multi-class and binary classification using SVMs.HDU [hybrid, compositional] (W¨aschle and Fendrich, 2012) uses a combination of binary classifiers for each entailment direction.The classifiers use both monolingual alignment features based on METEOR (Banerjee and Lavie, 2005) alignments (translations obtained from Google Translate), and cross-lingual alignment features based on GIZA++ (Och and Ney, 2000) (word alignments learned on Europarl).ICT [pivoting, compositional] (Meng et al., 2012) adopts a pivoting method (using Google Translate and an in-house hierarchical MT system), and the open source EDITS system (Kouylekov and Negri, 2010) to calculate similarity scores between monolingual English pairs.Separate unidirectional entailment judgments obtained from binary classifier are combined to return one of the four valid CLTE judgments.JU-CSE-NLP [pivoting, compositional] (Neogi et al., 2012) uses Microsoft Bing translator7 to produce monolingual English pairs.Separate lexical mapping scores are calculated (from T1 to T2 and vice-versa) considering different types of information and similarity metrics.Binary entailment decisions are then heuristically combined into single decisions.Sagan [pivoting, multi-class] (Castillo and Cardenas, 2012) adopts a pivoting method using Google Translate, and trains a monolingual system based on a SVM multi-class classifier.A CLTE corpus derived from the RTE-3 dataset is also used as a source of additional training material.SoftCard [pivoting, multi-class] (Jimenez et al., 2012) after automatic translation with Google Translate, uses SVMs to learn entailment decisions based on information about the cardinality of: T1, T2, their intersection and their union.Cardinalities are computed in different ways, considering tokens in T1 and T2, their IDF, and their similarity (computed with edit-distance) UAlacant [pivoting, multi-class] (Espl`a-Gomis et al., 2012) exploits translations obtained from Google Translate, Microsoft Bing translator, and the Apertium open-source MT platform (Forcada et al., 2011).8 Then, a multi-class SVM classifier is used to take entailment decisions using information about overlapping sub-segments as features.Despite the novelty of the problem and the difficulty to capture multi-directional entailment relations across languages, the first round of the Crosslingual Textual Entailment for Content Synchronization task organized within SemEval-2012 was a successful experience.This year a new interesting challenge has been proposed, a benchmark for four language combinations has been released, baseline results have been proposed for comparison, and a monolingual English dataset has been produced as a by-product which can be useful for monolingual TE research.The interest shown by participants was encouraging: 10 teams submitted a total of 92 runs for all the language pairs proposed.Overall, the results achieved on all datasets are encouraging, with best systems significantly outperforming the proposed baselines.It is worth observing that the nature of the task, which lies between semantics and machine translation, led to the participation of teams coming from both these communities, showing interesting opportunities for integration and mutual improvement.The proposed approaches reflect this situation, with teams traditionally working on MT now dealing with entailment, and teams traditionally participating in the RTE challenges now dealing with cross-lingual alignment techniques.Our ambition, for the future editions of the CLTE task, is to further consolidate the bridge between the semantics and MT communities.This work has been partially supported by the ECfunded project CoSyne (FP7-ICT-4-24853).The authors would also like to acknowledge Giovanni Moretti from CELCT for evaluation scripts and technical assistance, and the volunteer translators that contributed to the creation of the dataset:
Building A Discourse-Tagged Corpus In The Framework Of Rhetorical Structure Theoryand Anne Anderson.1997.The reliability of a dialogue structure coding Linguistics 13-32.Giacomo Ferrari.1998.Preliminary steps toward the creation of a discourse and text In of the First International Conference on LanguageThe advent of large-scale collections of annotated data has marked a paradigm shift in the research community for natural language processing.These corpora, now also common in many languages, have accelerated development efforts and energized the community.Annotation ranges from broad characterization of document-level information, such as topic or relevance judgments (Voorhees and Harman, 1999; Wayne, 2000) to discrete analysis of a wide range of linguistic phenomena.However, rich theoretical approaches to discourse/text analysis (Van Dijk and Kintsch, 1983; Meyer, 1985; Grosz and Sidner, 1986; Mann and Thompson, 1988) have yet to be applied on a large scale.So far, the annotation of discourse structure of documents has been applied primarily to identifying topical segments (Hearst, 1997), inter-sentential relations (Nomoto and Matsumoto, 1999; Ts’ou et al., 2000), and hierarchical analyses of small corpora (Moser and Moore, 1995; Marcu et al., 1999).In this paper, we recount our experience in developing a large resource with discourse-level annotation for NLP research.Our main goal in undertaking this effort was to create a reference corpus for community-wide use.Two essential considerations from the outset were that the corpus needed to be consistently annotated, and that it would be made publicly available through the Linguistic Data Consortium for a nominal fee to cover distribution costs.The paper describes the challenges we faced in building a corpus of this level of complexity and scope – including selection of theoretical approach, annotation methodology, training, and quality assurance.The resulting corpus contains 385 documents of American English selected from the Penn Treebank (Marcus et al., 1993), annotated in the framework of Rhetorical Structure Theory.We believe this resource holds great promise as a rich new source of textlevel information to support multiple lines of research for language understanding applications.Two principle goals underpin the creation of this discourse-tagged corpus: 1) The corpus should be grounded in a particular theoretical approach, and 2) it should be sufficiently large enough to offer potential for wide-scale use – including linguistic analysis, training of statistical models of discourse, and other computational linguistic applications.These goals necessitated a number of constraints to our approach.The theoretical framework had to be practical and repeatable over a large set of documents in a reasonable amount of time, with a significant level of consistency across annotators.Thus, our approach contributes to the community quite differently from detailed analyses of specific discourse phenomena in depth, such as anaphoric relations (Garside et al., 1997) or style types (Leech et al., 1997); analysis of a single text from multiple perspectives (Mann and Thompson, 1992); or illustrations of a theoretical model on a single representative text (Britton and Black, 1985; Van Dijk and Kintsch, 1983).Our annotation work is grounded in the Rhetorical Structure Theory (RST) framework (Mann and Thompson, 1988).We decided to use RST for three reasons: can play a crucial role in building natural language generation systems (Novy, 1993; Moore and Paris, 1993; Moore, 1995) and text summarization systems (Marcu, 2000); can be used to increase the naturalness of machine translation outputs (Marcu et al. 2000); and can be used to build essayscoring systems that provide students with discourse-based feedback (Burstein et al., 2001).We suspect that RST trees can be exploited successfully in the context of other applications as well.In the RST framework, the discourse structure of a text can be represented as a tree defined in terms of four aspects: supporting or background unit of information.Below, we describe the protocol that we used to build consistent RST annotations.The first step in characterizing the discourse structure of a text in our protocol is to determine the elementary discourse units (EDUs), which are the minimal building blocks of a discourse tree.Mann and Thompson (1988, p. 244) state that “RST provides a general way to describe the relations among clauses in a text, whether or not they are grammatically or lexically signalled.” Yet, applying this intuitive notion to the task of producing a large, consistently annotated corpus is extremely difficult, because the boundary between discourse and syntax can be very blurry.The examples below, which range from two distinct sentences to a single clause, all convey essentially the same meaning, packaged in different ways: In Example 1, there is a consequential relation between the first and second sentences.Ideally, we would like to capture that kind of rhetorical information regardless of the syntactic form in which it is conveyed.However, as examples 2-4 illustrate, separating rhetorical from syntactic analysis is not always easy.It is inevitable that any decision on how to bracket elementary discourse units necessarily involves some compromises.Reseachers in the field have proposed a number of competing hypotheses about what constitutes an elementary discourse unit.While some take the elementary units to be clauses (Grimes, 1975; Givon, 1983; Longacre, 1983), others take them to be prosodic units (Hirschberg and Litman, 1993), turns of talk (Sacks, 1974), sentences (Polanyi, 1988), intentionally defined discourse segments (Grosz and Sidner, 1986), or the “contextually indexed representation of information conveyed by a semiotic gesture, asserting a single state of affairs or partial state of affairs in a discourse world,” (Polanyi, 1996, p.5).Regardless of their theoretical stance, all agree that the elementary discourse units are non-overlapping spans of text.Our goal was to find a balance between granularity of tagging and ability to identify units consistently on a large scale.In the end, we chose the clause as the elementary unit of discourse, using lexical and syntactic clues to help determine boundaries: Relative clauses, nominal postmodifiers, or clauses that break up other legitimate EDUs, are treated as embedded discourse units: a relaxation of controls on exports to the Soviet bloc,] [is questioning...]wsj_2326 Finally, a small number of phrasal EDUs are allowed, provided that the phrase begins with a strong discourse marker, such as because, in spite of, as a result of, according to.We opted for consistency in segmenting, sacrificing some potentially discourse-relevant phrases in the process.Once the elementary units of discourse have been determined, adjacent spans are linked together via rhetorical relations creating a hierarchical structure.Relations may be mononuclear or multinuclear.Mononuclear relations hold between two spans and reflect the situation in which one span, the nucleus, is more salient to the discourse structure, while the other span, the satellite, represents supporting information.Multinuclear relations hold among two or more spans of equal weight in the discourse structure.A total of 53 mononuclear and 25 multinuclear relations were used for the tagging of the RST Corpus.The final inventory of rhetorical relations is data driven, and is based on extensive analysis of the corpus.Although this inventory is highly detailed, annotators strongly preferred keeping a higher level of granularity in the selections available to them during the tagging process.More extensive analysis of the final tagged corpus will demonstrate the extent to which individual relations that are similar in semantic content were distinguished consistently during the tagging process.The 78 relations used in annotating the corpus can be partitioned into 16 classes that share some type of rhetorical meaning: Attribution, Background, Cause, Comparison, Condition, Contrast, Elaboration, Enablement, Evaluation, Explanation, Joint, Manner-Means, Topic-Comment, Summary, Temporal, TopicChange.For example, the class Explanation includes the relations evidence, explanationargumentative, and reason, while TopicComment includes problem-solution, questionanswer, statement-response, topic-comment, and comment-topic.In addition, three relations are used to impose structure on the tree: textualorganization, span, and same-unit (used to link parts of units separated by embedded units or spans).Our methodology for annotating the RST Corpus builds on prior corpus work in the Rhetorical Structure Theory framework by Marcu et al. (1999).Because the goal of this effort was to build a high-quality, consistently annotated reference corpus, the task required that we employ people as annotators whose primary professional experience was in the area of language analysis and reporting, provide extensive annotator training, and specify a rigorous set of annotation guidelines.The annotators hired to build the corpus were all professional language analysts with prior experience in other types of data annotation.They underwent extensive hands-on training, which took place roughly in three phases.During the orientation phase, the annotators were introduced to the principles of Rhetorical Structure Theory and the discourse-tagging tool used for the project (Marcu et al., 1999).The tool enables an annotator to segment a text into units, and then build up a hierarchical structure of the discourse.In this stage of the training, the focus was on segmenting hard copy texts into EDUs, and learning the mechanics of the tool.In the second phase, annotators began to explore interpretations of discourse structure, by independently tagging a short document, based on an initial set of tagging guidelines, and then meeting as a group to compare results.The initial focus was on resolving segmentation differences, but over time this shifted to addressing issues of relations and nuclearity.These exploratory sessions led to enhancements in the tagging guidelines.To reinforce new rules, annotators re-tagged the document.During this process, we regularly tracked interannotator agreement (see Section 4.2).In the final phase, the annotation team concentrated on ways to reduce differences by adopting some heuristics for handling higher levels of the discourse structure.Wiebe et al. (1999) present a method for automatically formulating a single best tag when multiple judges disagree on selecting between binary features.Because our annotators had to select among multiple choices at each stage of the discourse annotation process, and because decisions made at one stage influenced the decisions made during subsequent stages, we could not apply Wiebe et al.’s method.Our methodology for determining the “best” guidelines was much more of a consensus-building process, taking into consideration multiple factors at each step.The final tagging manual, over 80 pages in length, contains extensive examples from the corpus to illustrate text segmentation, nuclearity, selection of relations, and discourse cues.The manual can be downloaded from the following web site: http://www.isi.edu/~marcu/discourse.The actual tagging of the corpus progressed in three developmental phases.During the initial phase of about four months, the team created a preliminary corpus of 100 tagged documents.This was followed by a one-month reassessment phase, during which we measured consistency across the group on a select set of documents, and refined the annotation rules.At this point, we decided to proceed by pre-segmenting all of the texts on hard copy, to ensure a higher overall quality to the final corpus.Each text was presegmented by two annotators; discrepancies were resolved by the author of the tagging guidelines.In the final phase (about six months) all 100 documents were re-tagged with the new approach and guidelines.The remainder of the corpus was tagged in this manner.Annotators developed different strategies for analyzing a document and building up the corresponding discourse tree.There were two basic orientations for document analysis – hard copy or graphical visualization with the tool.Hard copy analysis ranged from jotting of notes in the margins to marking up the document into discourse segments.Those who preferred a graphical orientation performed their analysis simultaneously with building the discourse structure, and were more likely to build the discourse tree in chunks, rather than incrementally.We observed a variety of annotation styles for the actual building of a discourse tree.Two of the more representative styles are illustrated below. discourse tree by immediately attaching the current node to a previous node.When building the tree in this fashion, the annotator must anticipate the upcoming discourse structure, possibly for a large span.Yet, often an appropriate choice of relation for an unseen segment may not be obvious from the current (rightmost) unit that needs to be attached.That is why annotators typically used this approach on short documents, but resorted to other strategies for longer documents.2.The annotator segments multiple units at a time, then builds discourse sub-trees for each sentence.Adjacent sentences are then linked, and larger sub-trees begin to emerge.The final tree is produced by linking major chunks of the discourse Corp.]18 [This is in part because of the effect]19 [of having to average the number of shares outstanding,]20 [she said.]21 [In addition,]22 [Mrs. Lidgerwood said,]23 [Norfolk is likely to draw down its cash initially]24 [to finance the purchases]25 [and thus forfeit some interest income.]26 wsj_1111 The discourse sub-tree for this text fragment is given in Figure 1.Using Style 1 the annotator, upon segmenting unit [17], must anticipate the upcoming example relation, which spans units [17-26].However, even if the annotator selects an incorrect relation at that point, the tool allows great flexibility in changing the structure of the tree later on.Using Style 2, the annotator segments each sentence, and builds up corresponding sub-trees for spans [16], [17-18], [19-21] and [22-26].The *elaboration-object-attribute-embedded structure.This strategy allows the annotator to see the emerging discourse structure more globally; thus, it was the preferred approach for longer documents.Consider the text fragment below, consisting of four sentences, and 11 EDUs: [Still, analysts don’t expect the buy-back to significantly affect per-share earnings in the short term.]16 [The impact won’t be that great,]17 [said Graeme Lidgerwood of First Boston second and third sub-trees are then linked via an explanation-argumentative relation, after which, the fourth sub-tree is linked via an elaborationadditional relation.The resulting span [17-26] is finally attached to node [16] as an example satellite.A number of steps were taken to ensure the quality of the final discourse corpus.These involved two types of tasks: checking the validity of the trees and tracking inter-annotator consistency.Annotators reviewed each tree for syntactic and semantic validity.Syntactic checking involved ensuring that the tree had a single root node and comparing the tree to the document to check for missing sentences or fragments from the end of the text.Semantic checking involved reviewing nuclearity assignments, as well as choice of relation and level of attachment in the tree.All trees were checked with a discourse parser and tree traversal program which often identified errors undetected by the manual validation process.In the end, all of the trees worked successfully with these programs.We tracked inter-annotator agreement during each phase of the project, using a method developed by Marcu et al. (1999) for computing kappa statistics over hierarchical structures.The kappa coefficient (Siegel and Castellan, 1988) has been used extensively in previous empirical studies of discourse (Carletta et al., 1997; Flammia and Zue, 1995; Passonneau and Litman, 1997).It measures pairwise agreement among a set of coders who make category judgments, correcting for chance expected agreement.The method described in Marcu et al. (1999) maps hierarchical structures into sets of units that are labeled with categorial judgments.The strengths and shortcomings of the approach are also discussed in detail there.Researchers in content analysis (Krippendorff, 1980) suggest that values of kappa > 0.8 reflect very high agreement, while values between 0.6 and 0.8 reflect good agreement.Table 1 shows average kappa statistics reflecting the agreement of three annotators at various stages of the tasks on selected documents.Different sets of documents were chosen for each stage, with no overlap in documents.The statistics measure annotation reliability at four levels: elementary discourse units, hierarchical spans, hierarchical nuclearity and hierarchical relation assignments.At the unit level, the initial (April 00) scores and final (January 01) scores represent agreement on blind segmentation, and are shown in boldface.The interim June and November scores represent agreement on hard copy pre-segmented texts.Notice that even with pre-segmenting, the agreement on units is not 100% perfect, because of human errors that occur in segmenting with the tool.As Table 1 shows, all levels demonstrate a marked improvement from April to November (when the final corpus was completed), ranging from about 0.77 to 0.92 at the span level, from 0.70 to 0.88 at the nuclearity level, and from 0.60 to 0.79 at the relation level.In particular, when relations are combined into the 16 rhetoricallyrelated classes discussed in Section 2.2, the November results of the annotation process are extremely good.The Fewer-Relations column shows the improvement in scores on assigning relations when they are grouped in this manner, with November results ranging from 0.78 to 0.82.In order to see how much of the improvement had to do with pre-segmenting, we asked the same three annotators to annotate five previously unseen documents in January, without reference to a pre-segmented document.The results of this experiment are given in the last row of Table 1, and they reflect only a small overall decline in performance from the November results.These scores reflect very strong agreement and represent a significant improvement over previously reported results on annotating multiple texts in the RST framework (Marcu et al., 1999).Table 2 reports final results for all pairs of taggers who double-annotated four or more documents, representing 30 out of the 53 documents that were double-tagged.Results are based on pre-segmented documents.Our team was able to reach a significant level of consistency, even though they faced a number of challenges which reflect differences in the agreement scores at the various levels.While operating under the constraints typical of any theoretical approach in an applied environment, the annotators faced a task in which the complexity increased as support from the guidelines tended to decrease.Thus, while rules for segmenting were fairly precise, annotators relied on heuristics requiring more human judgment to assign relations and nuclearity.Another factor is that the cognitive challenge of the task increases as the tree takes shape.It is relatively straightforward for the annotator to make a decision on assignment of nuclearity and relation at the inter-clausal level, but this becomes more complex at the intersentential level, and extremely difficult when linking large segments.This tension between task complexity and guideline under-specification resulted from the practical application of a theoretical model on a broad scale.While other discourse theoretical approaches posit distinctly different treatments for various levels of the discourse (Van Dijk and Kintsch, 1983; Meyer, 1985), RST relies on a standard methodology to analyze the document at all levels.The RST relation set is rich and the concept of nuclearity, somewhat interpretive.This gave our annotators more leeway in interpreting the higher levels of the discourse structure, thus introducing some stylistic differences, which may prove an interesting avenue of future research.The RST Corpus consists of 385 Wall Street Journal articles from the Penn Treebank, representing over 176,000 words of text.In order to measure inter-annotator consistency, 53 of the documents (13.8%) were double-tagged.The documents range in size from 31 to 2124 words, with an average of 458.14 words per document.The final tagged corpus contains 21,789 EDUs with an average of 56.59 EDUs per document.The average number of words per EDU is 8.1.The articles range over a variety of topics, including financial reports, general interest stories, business-related news, cultural reviews, editorials, and letters to the editor.In selecting these documents, we partnered with the Linguistic Data Consortium to select Penn Treebank texts for which the syntactic bracketing was known to be of high caliber.Thus, the RST Corpus provides an additional level of linguistic annotation to supplement existing annotated resources.For details on obtaining the corpus, annotation software, tagging guidelines, and related documentation and resources, see: http://www.isi.edu/~marcu/discourse.A growing number of groups have developed or are developing discourse-annotated corpora for text.These can be characterized both in terms of the kinds of features annotated as well as by the scope of the annotation.Features may include specific discourse cues or markers, coreference links, identification of rhetorical relations, etc.The scope of the annotation refers to the levels of analysis within the document, and can be characterized as follows: topical segments (Hearst, 1997), linking of large text segments via specific relations (Ferrari, 1998; Rebeyrolle, 2000), or defining text objects with a text architecture (Pery-Woodley and Rebeyrolle, 1998).Developing corpora with these kinds of rich annotation is a labor-intensive effort.Building the RST Corpus involved more than a dozen people on a full or part-time basis over a oneyear time frame (Jan. – Dec. 2000).Annotation of a single document could take anywhere from 30 minutes to several hours, depending on the length and topic.Re-tagging of a large number of documents after major enhancements to the annotation guidelines was also time consuming.In addition, limitations of the theoretical approach became more apparent over time.Because the RST theory does not differentiate between different levels of the tree structure, a fairly fine-grained set of relations operates between EDUs and EDU clusters at the macrolevel.The procedural knowledge available at the EDU level is likely to need further refinement for higher-level text spans along the lines of other work which posits a few macro-level relations for text segments, such as Ferrari (1998) or Meyer (1985).Moreover, using the RST approach, the resultant tree structure, like a traditional outline, imposed constraints that other discourse representations (e.g., graph) would not.In combination with the tree structure, the concept of nuclearity also guided an annotator to capture one of a number of possible stylistic interpretations.We ourselves are eager to explore these aspects of the RST, and expect new insights to appear through analysis of the corpus.We anticipate that the RST Corpus will be multifunctional and support a wide range of language engineering applications.The added value of multiple layers of overt linguistic phenomena enhancing the Penn Treebank information can be exploited to advance the study of discourse, to enhance language technologies such as text summarization, machine translation or information retrieval, or to be a testbed for new and creative natural language processing techniques.
Phrasal Cohesion And Statistical Machine TranslationThere has been much interest in using phrasal movement to improve statistical machine translation.We explore how well phrases cohere across two languages, specifically English and French, and examine the particular conditions under which they do not.We demonstrate that while there are cases where coherence is poor, there are many regularities which can be exploited by a statistical machine translation system.We also compare three variant syntactic representations to determine which one has the best properties with respect to cohesion.Statistical machine translation (SMT) seeks to develop mathematical models of the translation process whose parameters can be automatically estimated from a parallel corpus.The first work in SMT, done at IBM (Brown et al., 1993), developed a noisy-channel model, factoring the translation process into two portions: the translation model and the language model.The translation model captures the translation of source language words into the target language and the reordering of those words.The language model ranks the outputs of the translation model by how well they adhere to the syntactic constraints of the target language.1 The prime deficiency of the IBM model is the reordering component.Even in the most complex of 'Though usually a simple word n-gram model is used for the language model. the five IBM models, the reordering operation pays little attention to context and none at all to higherlevel syntactic structures.Many attempts have been made to remedy this by incorporating syntactic information into translation models.These have taken several different forms, but all share the basic assumption that phrases in one language tend to stay together (i.e. cohere) during translation and thus the word-reordering operation can move entire phrases, rather than moving each word independently.(Yarowsky et al., 2001) states that during their work on noun phrase bracketing they found a strong cohesion among noun phrases, even when comparing English to Czech, a relatively free word order language.Other than this, there is little in the SMT literature to validate the coherence assumption.Several studies have reported alignment or translation performance for syntactically augmented translation models (Wu, 1997; Wang, 1998; Alshawi et al., 2000; Yamada and Knight, 2001; Jones and Havrilla, 1998) and these results have been promising.However, without a focused study of the behavior of phrases across languages, we cannot know how far these models can take us and what specific pitfalls they face.The particulars of cohesion will clearly depend upon the pair of languages being compared.Intuitively, we expect that while French and Spanish will have a high degree of cohesion, French and Japanese may not.It is also clear that if the cohesion between two closely related languages is not high enough to be useful, then there is no hope for these methods when applied to distantly related languages.For this reason, we have examined phrasal cohesion for French and English, two languages which are fairly close syntactically but have enough differences to be interesting.An alignment is a mapping between the words in a string in one language and the translations of those words in a string in another language.Given an English string, , and a French string, , an alignment a can be represented by .Each is a set of indices into where indicates that word in the French sentence is aligned with word in the English sentence. indicates that English word has no corresponding French word.Given an alignment and an English phrase covering words , the span is a pair where the first element is and the second element is .Thus, the span includes all words between the two extrema of the alignment, whether or not they too are part of the translation.If phrases cohere perfectly across languages, the span of one phrase will never overlap the span of another.If two spans do overlap, we call this a crossing.Figure 1 shows an example of an English parse along with the alignment between the English and French words (shown with dotted lines).The English word “not” is aligned to the two French words “ne” and “pas” and thus has a span of [1,3].The main English verb “change” is aligned to the French “modifie” and has a span of [2,2].The two spans overlap and thus there is a crossing.This definition is asymmetric (i.e. what is a crossing when moving from English to French is not guaranteed to be a crossing when moving from French to English).However, we only pursue translation direction since that is the one for which we have parsed data.To calculate spans, we need aligned pairs of English and French sentences along with parses for the English sentences.Our aligned data comes from a corpus described in (Och and Ney, 2000) which contains 500 sentence pairs randomly selected from the Canadian Hansard corpus and manually aligned.The alignments are of two types: sure (S) and possible (P).S alignments are those which are unambiguous while P alignments are those which are less certain.P alignments often appear when a phrase in one language translates as a unit into a phrase in the other language (e.g. idioms, free translations, missing function words) but can also be the result of genuine ambiguity.When two annotators disagree, the union of the P alignments produced by each annotator is recorded as the P alignment in the corpus.When an S alignment exists, there will always also exist a P alignment such that P S. The English sentences were parsed using a state-of-the-art statistical parser (Charniak, 2000) trained on the University of Pennsylvania Treebank (Marcus et al., 1993). je invoque le R`eglement Since P alignments often align phrasal translations, the number of crossings when P alignments are used will be artificially inflated.For example, in Figure 2 note that every pair of English and French words under the verb phrase is aligned.This will generate five crossings, one each between the pairs VBP-PP, IN-NP, NP -PP, NN-DT, and IN-NP .However, what is really happening is that the whole verb phrase is first being moved without crossing anything else and then being translated as a unit.For our purposes we want to count this example as producing zero crossings.To accomplish this, we defined a simple heuristic to detect phrasal translations so we can filter them if desired.After calculating the French spans from the English parses and alignment information, we counted crossings for all pairs of child constituents in each constituent in the sentence, maintaining separate counts for those involving the head constituent of the phrase and for crossings involving modifiers only.We did this while varying conditions along two axes: alignment type and phrasal translation filtering.Recalling the two different types of alignments, S and P, we examined three different conditions: S alignments only, P alignments only, or S alignments where present falling back to P alignments (S P).For each of these conditions, we counted crossings both with and without using the phrasal translation filter.For a given alignment type S,S P,P, let if phrases and cross each other and otherwise.Let if the phrasal translation filter is turned off.If the filter is on, , modifier constituents , and child constituents and for a particular alignment type , the number of head crossings and modifier crossings can be calculated recursively:Table 1 shows the average number of crossings per sentence.The table is split into two sections, one for results when the phrasal filter was used and one for when it was not.“Alignment Type” refers to whether we used S, P or S P as the alignment data.The “Head Crossings” line shows the results when comparing the span of the head constituent of a phrase with the spans of its modifier constituents, and “Modifier Crossings” refers to the case where we compare the spans of pairs of modifiers.The “Phrasal Translations” line shows the average number of phrasal translations detected per sentence.For S alignments, the results are quite promising, with an average of only 0.236 head crossings per sentence and an even smaller average for modifier crossings (0.056).However, these results are overly optimistic since often many words in a sentence will not have an S alignment at all, such as “coming”, “in”, and “before” in following example: the full report will be coming in before the fall le rapport complet sera d´epos´e de ici le automne prochain When we use P alignments for these unaligned words (the S P case), we get a more meaningful result.Both types of crossings are much more frequent (4.790 for heads and 0.88 for modifiers) and Then, for a given phrase with head constituent if and are part of a phrasal translation in alignment otherwise phrasal translation filtering has a much larger effect (reducing head average to 2.772 and modifier average to 0.516).Phrasal translations account for almost half of all crossings, on average.This effect is even more pronounced in the case where we use P alignments only.This reinforces the importance of phrasal translation in the development of any translation system.Even after filtering, the number of crossings in the S P case is quite large.This is discouraging, however there are reasons why this result should be looked on as more of an upper bound than anything precise.For one thing, there are cases of phrasal translation which our heuristic fails to recognize, an example of which is shown in Figure 3.The alignment of “explorer” with “this” and “matter” seems to indicate that the intention of the annotator was to align the phrase “work this matter out”, as a unit, to “de explorer la question”.However, possibly due to an error during the coding of the alignment, “work” and “out” align with “de” (indicated by the solid lines) while “this” and “matter” do not.This causes the phrasal translation heuristic to fail resulting in a crossing where there should be none.Also, due to the annotation guidelines, P alignments are not as consistent as would be ideal.Recall that in cases of annotator disagreement, the P alignment is taken to be the union of the P alignments of both annotators.Thus, it is possible for the P alignment to contain two mutually conflicting alignments.These composite alignments will likely generate crossings even where the alignments of each individual annotator would not.While reflecting genuine ambiguity, an SMT system would likely pursue only one of the alternatives and only a portion of the crossings would come into play.Our results show a significantly larger number of head crossings than modifier crossings.One possibility is that this is due to most phrases having a head and modifier pair to test, while many do not have multiple modifiers and therefore there are fewer opportunities for modifier crossings.Thus, it is informative to examine how many potential crossings actually turn out to be crossings.Table 2 provides this result in the form of the percentage of crossing tests which result in detection of a crossing.To calculate this, we kept totals for the number of head ( ) and modifier ( ) crossing tests performed as well as the number of phrasal translations detected ( ).Note that when the phrasal translation filter is turned on, these totals differ for each of the different alignment types (S, S P, and P).The percentages are calculated after summing over all sentencesin the corpus: There are still many more crossings in the S P and P alignments than in the S alignments.The S alignment has 1.58% head crossings while the S P and P alignments have 32.16% and 35.47% respectively, with similar relative percentages for modifier crossings.Also as before, half to two-thirds of crossings in the S P and P alignments are due to phrasal translations.More interestingly, we see that modifier crossings remain significantly less prevalent than head crossings (e.g.14.45% vs. 32.16% for the S P case) and that this is true uniformly across all parameter settings.This indicates that heads are more intimately involved with their modifiers than modifiers are with each other and therefore are more likely to be involved in semi-phrasal constructions.Since it is clear that crossings are too prevalent to ignore, it is informative to try to understand exactly what constructions give rise to them.To that end, we examined by hand all of the head crossings produced using the S alignments with phrasal filtering.Table 3 shows the results of this analysis.The first thing to note is that by far most of the crossings do not reflect lack of phrasal cohesion between the two languages.Instead, they are caused either by errors in the syntactic analysis or the fact that translation as done by humans is a much richer process than just replication of the source sentence in another language.Sentences are reworded, clauses are reordered, and sometimes human translators even make mistakes.Errors in syntactic analysis consist mostly of attachment errors.Rewording and reordering accounted for a large number of crossings as well.In most of the cases of rewording (see Figure 4) or relorsque nous avons pr´epar´e le budget , nous avons pris cela en consid´eration ordering (see Figure 5) a more “parallel” translation would also be valid.Thus, while it would be difficult for a statistical model to learn from these examples, there is nothing to preclude production of a valid translation from a system using phrasal movement in the reordering phase.The rewording and reordering examples were so varied that we were unable to find any regularities which might be exploited by a translation model.Among the cases which do result from language differences, the most common is the “ne ... pas” construction (e.g.Figure 1).Fifteen percent of the 86 total crossings are due to this construction.Because “ne ... pas” wraps around the verb, it will always result in a crossing.However, the types of syntactic structures (categorized as context-free grammar rules) which are present in cases of negation are rather restricted.Of the 47 total distinct syntactic structures which resulted in crossings, only three of them involved negation.In addition, the crossings associated with these particular structures were unambiguously caused by negation (i.e. for each structure, only negation-related crossings were present).Next most common is the case where the English contains a modal verb which is aligned with the main verb in the French.In the example in Figure 6, “will be” is aligned to “sera” (indicated by the solid lines) and because of the constituent structure of the English parse there is a crossing.As with negation, this type of crossing is quite regular, resulting uniquely from only two different syntactic structures.Many of the causes listed above are related to verb phrases.In particular, some of the adverb-related crossings (e.g.Figure 1) and all of the modal-related crossings (e.g.Figure 6) are artifacts of the nested verb phrase structure of our parser.This nesting usually does not provide any extra information beyond what could be gleaned from word order.Therefore, we surmised that flattening verb phrases would eliminate some types of crossings without reducing the utility of the parse.The flattening operation consists of identifying all nested verb phrases and splicing the children of the nested phrase into the parent phrase in its place.This procedure is applied recursively until there are no nested verb phrases.An example is shown in Figure 8.Crossings can be calculated as before.Adverbs are a third common cause, as they typically follow the verb in French while preceding it in English.Figure 7 shows an example where the span of “simplement” overlaps with the span of the verb phrase beginning with “tells” (indicated by the solid lines).Unlike negation and modals, this case is far less regular.It arises from six different syntactic constructions and two of those constructions are implicated in other types of crossings as well.Flattening reduces the number of potential head crossings while increasing the number of potential modifier crossings.Therefore, we would expect to see a comparable change to the number of crossings measured, and this is exactly what we find, as shown in Tables 4 and 5.For example, for S P alignments, the average number of head crossings decreases from 2.772 to 2.252, while the average number of modifier crossings increases from 0.516 to 0.86.We see similar behavior when we look at the percentage of crossings per chance (Tables 6 and 7).For the same alignment type, the percentage of head crossings decreases from 18.61% to 15.12%, while the percentage of modifier crossings increases from 8.47% to 10.59%.One thing to note, however, is that the total number of crossings of both types detected in the corpus decreases as compared to the baseline, and thus the benefits to head crossings outweigh the detriments to modifier crossings.Our intuitions about the cohesion of syntactic structures follow from the notion that translation, as a meaning-preserving operation, preserves the dependencies between words, and that syntactic structures encode these dependencies.Therefore, dependency structures should cohere as well as, or better than, their corresponding syntactic structures.To examine the validity of this, we extracted dependency structures from the parse trees (with flattened verb phrases) and calculated crossings for them.Figure 9 shows a parse tree and its corresponding dependency structure.The procedure for counting modifier crossings in a dependency structure is identical to the procedure for parse trees.For head crossings, the only difference is that rather than comparing spans of two siblings, we compare the spans of a child and its parent.Again focusing on the S P alignment case, we see that the average number of head crossings (see Table 4) continues to decrease compared to the previous case (from 2.252 to 1.88), and that the average number of modifier crossings (see Table 5) continues to increase (from 0.86 to 1.498).This time, however, the percentages for both types of crossings (see Tables 6 and 7) decrease relative to the case of flattened verb phrases (from 15.12% to 12.62% for heads and from 10.59% to 9.22% for modifiers).The percentage of modifier crossings is still higher than in the base case (9.22% vs. 8.47%).Overall, however, the dependency representation has the best cohesion properties. ernment.We would like to thank Franz Och for providing us with the manually annotated data used in these experiments.We have examined the issue of phrasal cohesion between English and French and discovered that while there is less cohesion than we might desire, there is still a large amount of regularity in the constructions where breakdowns occur.This reassures us that reordering words by phrasal movement is a reasonable strategy.Many of the initially daunting number of crossings were due to non-linguistic reasons, such as rewording during translation or errors in syntactic analysis.Among the rest, there are a small number of syntactic constructions which result in the majority of the crossings examined in our analysis.One practical result of this skewed distribution is that one could hope to discover the major problem areas for a new language pair by manually aligning a small number of sentences.This information could be used to filter a training corpus to remove sentences which would cause problems in training the translation model, or for identifying areas to focus on when working to improve the model itself.We are interested in examining different language pairs as the opportunity arises.We have also examined the differences in cohesion between Treebank-style parse trees, trees with flattened verb phrases, and dependency structures.Our results indicate that the highest degree of cohesion is present in dependency structures.Therefore, in an SMT system which is using some type of phrasal movement during reordering, dependency structures should produce better results than raw parse trees.In the future, we plan to explore this hypothesis in an actual translation system.The work reported here was supported in part by the Defense Advanced Research Projects Agency under contract number N66001-00-C-8008.The views and conclusions contained in this document are those of the author and should not be interpreted as necessarily representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the United States Gov
The Parallel Grammar ProjectWe report on the Parallel Grammar (ParGram) project which uses the XLE parser and grammar development platform for six languages: English,Large-scale grammar development platforms are expensive and time consuming to produce.As such, a desideratum for the platforms is a broad utilization scope.A grammar development platform should be able to be used to write grammars for a wide variety of languages and a broad range of purposes.In this paper, we report on the Parallel Grammar (ParGram) project (Butt et al., 1999) which uses the XLE parser and grammar development platform (Maxwell and Kaplan, 1993) for six languages: English, French, German, Japanese, Norwegian, and Urdu.All of the grammars use the Lexical-Functional Grammar (LFG) formalism which produces c(onstituent)structures (trees) and f(unctional)-structures (AVMs) as the syntactic analysis.LFG assumes a version of Chomsky’s Universal Grammar hypothesis, namely that all languages are structured by similar underlying principles.Within LFG, f-structures are meant to encode a language universal level of analysis, allowing for crosslinguistic parallelism at this level of abstraction.Although the construction of c-structures is governed by general wellformedness principles, this level of analysis encodes language particular differences in linear word order, surface morphological vs. syntactic structures, and constituency.The ParGram project aims to test the LFG formalism for its universality and coverage limitations and to see how far parallelism can be maintained across languages.Where possible, the analyses produced by the grammars for similar constructions in each language are parallel.This has the computational advantage that the grammars can be used in similar applications and that machine translation (Frank, 1999) can be simplified.The results of the project to date are encouraging.Despite differences between the languages involved and the aims and backgrounds of the project groups, the ParGram grammars achieve a high level of parallelism.This parallelism applies to the syntactic analyses produced, as well as to grammar development itself: the sharing of templates and feature declarations, the utilization of common techniques, and the transfer of knowledge and technology from one grammar to another.The ability to bundle grammar writing techniques, such as templates, into transferable technology means that new grammars can be bootstrapped in a relatively short amount of time.There are a number of other large-scale grammar projects in existence which we mention briefly here.The LS-GRAM project (Schmidt et al., 1996), funded by the EU-Commission under LRE (Linguistic Research and Engineering), was concerned with the development of grammatical resources for nine European languages: Danish, Dutch, English, French, German, Greek, Italian, Portuguese, and Spanish.The project started in January 1994 and ended in July 1996.Development of grammatical resources was carried out in the framework of the Advanced Language Engineering Platform (ALEP).The coverage of the grammars implemented in LSGRAM was, however, much smaller than the coverage of the English (Riezler et al., 2002) or German grammar in ParGram.An effort which is closer in spirit to ParGram is the implemention of grammar development platforms for HPSG.In the Verbmobil project (Wahlster, 2000), HPSG grammars for English, German, and Japanese were developed on two platforms: LKB (Copestake, 2002) and PAGE.The PAGE system, developed and maintained in the Language Technology Lab of the German National Research Center on Artificial Intelligence DFKI GmbH, is an advanced NLP core engine that facilitates the development of grammatical and lexical resources, building on typed feature logics.To evaluate the HPSG platforms and to compare their merits with those of XLE and the ParGram projects, one would have to organize a special workshop, particularly as the HPSG grammars in Verbmobil were written for spoken language, characterized by short utterances, whereas the LFG grammars were developed for parsing technical manuals and/or newspaper texts.There are some indications that the German and English grammars in ParGram exceed the HPSG grammars in coverage (see (Crysmann et al., 2002) on the German HPSG grammar).This paper is organized as follows.We first provide a history of the project.Then, we discuss how parallelism is maintained in the project.Finally, we provide a summary and discussion.The ParGram project began in 1994 with three languages: English, French, and German.The grammar writers worked closely together to solidify the grammatical analyses and conventions.In addition, as XLE was still in development, its abilities grew as the size of the grammars and their needs grew.After the initial stage of the project, more languages were added.Because Japanese is typologically very different from the initial three European languages of the project, it represented a challenging case.Despite this typological challenge, the Japanese grammar has achieved broad coverage and high performance within a year and a half.The South Asian language Urdu also provides a widely spoken, typologically distinct language.Although it is of Indo-European origin, it shares many characteristics with Japanese such as verb-finality, relatively free word order, complex predicates, and the ability to drop any argument (rampant pro-drop).Norwegian assumes a typological middle position between German and English, sharing different properties with each of them.Both the Urdu and the Norwegian grammars are still relatively small.Each grammar project has different goals, and each site employs grammar writers with different backgrounds and skills.The English, German, and Japanese projects have pursued the goal of having broad coverage, industrial grammars.The Norwegian and Urdu grammars are smaller scale but are experimenting with incorporating different kinds of information into the grammar.The Norwegian grammar includes a semantic projection; their analyses produce not only c- and f-structures, but also semantic structures.The Urdu grammar has implemented a level of argument structure and is testing various theoretical linguistic ideas.However, even when the grammars are used for different purposes and have different additional features, they have maintained their basic parallelism in analysis and have profited from the shared grammar writing techniques and technology.Table (1) shows the size of the grammars.The first figure is the number of left-hand side categories in phrase-structure rules which compile into a collection of finite-state machines with the listed number of states and arcs.Maintaining parallelism in grammars being developed at different sites on typologically distinct languages by grammar writers from different linguistic traditions has proven successful.At project meetings held twice a year, analyses of sample sentences are compared and any differences are discussed; the goal is to determine whether the differences are justified or whether the analyses should be changed to maintain parallelism.In addition, all of the fstructure features and their values are compared; this not only ensures that trivial differences in naming conventions do not arise, but also gives an overview of the constructions each language covers and how they are analyzed.All changes are implemented before the next project meeting.Each meeting also involves discussion of constructions whose analysis has not yet been settled on, e.g., the analysis of partitives or proper names.If an analysis is agreed upon, all the grammars implement it; if only a tentative analysis is found, one grammar implements it and reports on its success.For extremely complicated or fundamental issues, e.g., how to represent predicate alternations, subcommittees examine the issue and report on it at the next meeting.The discussion of such issues may be reopened at successive meetings until a concensus is reached.Even within a given linguistic formalism, LFG for ParGram, there is usually more than one way to analyze a construction.Moreover, the same theoretical analysis may have different possible implementations in XLE.These solutions often differ in efficiency or conceptual simplicity and one of the tasks within the ParGram project is to make design decisions which favor one theoretical analysis and concomitant implementation over another.Whenever possible, the ParGram grammars choose the same analysis and the same technical solution for equivalent constructions.This was done, for example, with imperatives.Imperatives are always assigned a null pronominal subject within the fstructure and a feature indicating that they are imperatives, as in (2).Another example of this type comes from the analysis of specifiers.Specifiers include many different types of information and hence can be analyzed in a number of ways.In the ParGram analysis, the c-structure analysis is left relatively free according to language particular needs and slightly varying theoretical assumptions.For instance, the Norwegian grammar, unlike the other grammars, implements the principles in (Bresnan, 2001) concerning the relationship between an X -based c-structure and the f-structure.This allows Norwegian specifiers to be analyzed as functional heads of DPs etc., whereas they are constituents of NPs in the other grammars.However, at the level of f-structure, this information is part of a complex SPEC feature in all the grammars.Thus parallelism is maintained at the level of f-structure even across different theoretical preferences.An example is shown in (3) for Norwegian and English in which the SPEC consists of a QUANT(ifier) and a POSS(essive) (SPEC can also contain information about DETerminers and DEMONstratives).(3) a. alle mine hester (Norwegian) all my horses ‘all my horses’ Interrogatives provide an interesting example because they differ significantly in the c-structures of the languages, but have the same basic f-structure.This contrast can be seen between the German example in (4) and the Urdu one in (5).In German, the interrogative word is in first position with the finite verb second; English and Norwegian pattern like German.In Urdu the verb is usually in final position, but the interrogative can appear in a number of positions, including following the verb (5c).Despite these differences in word order and hence in c-structure, the f-structures are parallel, with the interrogative being in a FOCUS-INT and the sentence having an interrogative STMT-TYPE, as in (6).In the project grammars, many basic constructions are of this type.However, as we will see in the next section, there are times when parallelism is not possible and not desirable.Even in these cases, though, the grammars which can be parallel are; so, three of the languages might have one analysis, while three have another.Parallelism is not maintained at the cost of misrepresenting the language.This is reflected by the fact that the c-structures are not parallel because word order varies widely from language to language, although there are naming conventions for the nodes.Instead, the bulk of the parallelism is in the f-structure.However, even in the f-structure, situations arise in which what seems to be the same construction in different languages do not have the same analysis.An example of this is predicate adjectives, as in (7). it TOP red ‘It is red.’ In English, the copular verb is considered the syntactic head of the clause, with the pronoun being the subject and the predicate adjective being an XCOMP.However, in Japanese, the adjective is the mainpredicate, with the pronoun being the subject.As such, these receive the non-parallel analyses seen in (8a) for Japanese and (8b) for English.Another situation that arises is when a feature or construction is syntactically encoded in one language, but not another.In such cases, the information is only encoded in the languages that need it.The equivalence captured by parallel analyses is not, for example, translational equivalence.Rather, parallelism involves equivalence with respect to grammatical properties, e.g. construction types.One consequence of this is that a typologically consistent use of grammatical terms, embodied in the feature names, is enforced.For example, even though there is a tradition for referring to the distinction between the pronouns he and she as a gender distinction in English, this is a different distinction from the one called gender in languages like German, French, Urdu, and Norwegian, where gender refers to nominal agreement classes.Parallelism leads to the situation where the feature GEND occurs in German, French, Urdu, and Norwegian, but not in English and Japanese.That is, parallelism does not mean finding the same features in all languages, but rather using the same features in the same way in all languages, to the extent that they are justified there.A French example of grammatical gender is shown in (9); note that determiner, adjective, and participle agreement is dependent on the gender of the noun.The f-structure for the nouns crayon and plume are as in (10) with an overt GEND feature.(9) a.Le petit crayon est cass´e.(French) the-M little-M pencil-M is broken-M. ‘The little pencil is broken.’ b.La petite plume est cass´ee.(French) the-F little-F pen-F is broken-F. ‘The little pen is broken.’ F-structures for the equivalent words in English and Japanese will not have a GEND feature.A similar example comes from Japanese discourse particles.It is well-known that Japanese has syntactic encodings for information such as honorification.The verb in the Japanese sentence (11a) encodes information that the subject is respected, while the verb in (11b) shows politeness from the writer (speaker) to the reader (hearer) of the sentence.The f-structures for the verbs in (11) are as in A final example comes from English progressives, as in (13).In order to distinguish these two forms, the English grammar uses a PROG feature within the tense/aspect system.(13b) shows the fstructure for (13a.ii).However, this distinction is not found in the other languages.For example, (14a) is used to express both (13a.i) and (13a.ii) in German.(14) a. Er weinte.(German) he cried ‘He cried.’ As seen in (14b), the German f-structure is left underspecified for PROG because there is no syntactic reflex of it.If such a feature were posited, rampant ambiguity would be introduced for all past tense forms in German.Instead, the semantics will determine whether such forms are progressive.Another type of situation arises when one language provides evidence for a certain feature space or type of analysis that is neither explicitly mirrored nor explicitly contradicted by another language.In theoretical linguistics, it is commonly acknowledged that what one language codes overtly may be harder to detect for another language.This situation has arisen in the ParGram project.Case features fall under this topic.German, Japanese, and Urdu mark NPs with overt case morphology.In comparison, English, French, and Norwegian make relatively little use of case except as part of the pronominal system.Nevertheless, the f-structure analyses for all the languages contain a case feature in the specification of noun phrases.This “overspecification” of information expresses deeper linguistic generalizations and keeps the fstructural analyses as parallel as possible.In addition, the features can be put to use for the isolated phenomena in which they do play a role.For example, English does not mark animacy grammatically in most situations.However, providing a ANIM + feature to known animates, such as people’s names and pronouns, allows the grammar to encode information that is relevant for interpretation.Consider the relative pronoun who in (15).(15) a. the girl[ANIM +] who[ANIM +] left b. the box[ANIM +] who[ANIM +] left The relative pronoun has a ANIM + feature that is assigned to the noun it modifies by the relative clause rules.As such, a noun modified by a relative clause headed by who is interpreted as animate.In the case of canonical inanimates, as in (15b), this will result in a pragmatically odd interpretation, which is encoded in the f-structure.Teasing apart these different phenomena crosslinguistically poses a challenge that the ParGram members are continually engaged in.As such, we have developed several methods to help maintain parallelism.The parallelism among the grammars is maintained in a number of ways.Most of the work is done during two week-long project meetings held each year.Three main activities occur during these meetings: comparison of sample f-structures, comparison of features and their values, and discussions of new or problematic constructions.A month before each meeting, the host site chooses around fifteen sentences whose analysis is to be compared at the meeting.These can be a random selection or be thematic, e.g., all dealing with predicatives or with interrogatives.The sentences are then parsed by each grammar and the output is compared.For the more recent grammars, this may mean adding the relevant rules to the grammars, resulting in growth of the grammar; for the older grammars, this may mean updating a construction that has not been examined in many years.Another approach that was taken at the beginning of the project was to have a common corpus of about 1,000 sentences that all of the grammars were to parse.For the English, French, and German grammars, this was an aligned tractor manual.The corpus sentences were used for the initial f-structure comparisons.Having a common corpus ensured that the grammars would have roughly the same coverage.For example, they all parsed declarative and imperative sentences.However, the nature of the corpus can leave major gaps in coverage; in this case, the manual contained no interrogatives.The XLE platform requires that a grammar declare all the features it uses and their possible values.Part of the Urdu feature table is shown in (16) (the notation has been simplified for expository purposes).As seen in (16) for QUANT, attributes which take other attributes as their values must also be declared.An example of such a feature was seen in (3b) for SPEC which takes QUANT and POSS features, among others, as its values.(16) PRON-TYPE: pers poss null .PROPER: date location name title .PSEM: locational directional .PTYPE: sem nosem .QUANT-FORM .The feature declarations of all of the languages are compared feature by feature to ensure parallelism.The most obvious use of this is to ensure that the grammars encode the same features in the same way.For example, at a basic level, one feature declaration might have specified GEN for gender while the others had chosen the name GEND; this divergence in naming is regularized.More interesting cases arise when one language uses a feature and another does not for analyzing the same phenomena.When this is noticed via the feature-table comparison, it is determined why one grammar needs the feature and the other does not, and thus it may be possible to eliminate the feature in one grammar or to add it to another.On a deeper level, the feature comparison is useful for conducting a survey of what constructions each grammar has and how they are implemented.For example, if a language does not have an ADEGREE (adjective degree) feature, the question will arise as to whether the grammar analyzes comparative and superlative adjectives.If they do not, then they should be added and should use the ADEGREE feature; if they do, then the question arises as to why they do not have this feature as part of their analysis.Finally, there is the discussion of problematic constructions.These may be constructions that already have analyses which had been agreed upon in the past but which are not working properly now that more data has been considered.More frequently, they are new constructions that one of the grammars is considering adding.Possible analyses for the construction are discussed and then one of the grammars will incorporate the analysis to see whether it works.If the analysis works, then the other grammars will incorporate the analysis.Constructions that have been discussed in past ParGram meetings include predicative adjectives, quantifiers, partitives, and clefts.Even if not all of the languages have the construction in question, as was the case with clefts, the grammar writers for that language may have interesting ideas on how to analyze it.These group discussions have proven particularly useful in extending grammar coverage in a parallel fashion.Once a consensus is reached, it is the responsibility of each grammar to make sure that its analyses match the new standard.As such, after each meeting, the grammar writers will rename features, change analyses, and implement new constructions into their grammars.Most of the basic work has now been accomplished.However, as the grammars expand coverage, more constructions need to be integrated into the grammars, and these constructions tend to be ones for which there is no standard analysis in the linguistic literature; so, differences can easily arise in these areas.The experiences of the ParGram grammar writers has shown that the parallelism of analysis and implementation in the ParGram project aids further grammar development efforts.Many of the basic decisions about analyses and formalism have already been made in the project.Thus, the grammar writer for a new language can use existing technology to bootstrap a grammar for the new language and can parse equivalent constructions in the existing languages to see how to analyze a construction.This allows the grammar writer to focus on more difficult constructions not yet encountered in the existing grammars.Consider first the Japanese grammar which was started in the beginning of 2001.At the initial stage, the work of grammar development involved implementing the basic constructions already analyzed in the other grammars.It was found that the grammar writing techniques and guidelines to maintain parallelism shared in the ParGram project could be efficiently applied to the Japanese grammar.During the next stage, LFG rules needed for grammatical issues specific to Japanese have been gradually incorporated, and at the same time, the biannual ParGram meetings have helped significantly to keep the grammars parallel.Given this system, in a year and a half, using two grammar writers, the Japanese grammar has attained coverage of 99% for 500 sentences of a copier manual and 95% for 10,000 sentences of an eCRM (Voice-of-Customer) corpus.Next consider the Norwegian grammar which joined the ParGram group in 1999 and also emphasized slightly different goals from the other groups.Rather than prioritizing large textual coverage from the outset, the Norwegian group gave priority to the development of a core grammar covering all major construction types in a principled way based on the proposals in (Bresnan, 2001) and the inclusion of a semantic projection in addition to the f-structure.In addition, time was spent on improving existing lexical resources ( 80,000 lemmas) and adapting them to the XLE format.Roughly two man-years has been spent on the grammar itself.The ParGram cooperation on parallelism has ensured that the derived fstructures are interesting in a multilingual context, and the grammar will now serve as a basis for grammar development in other closely related Scandinavian languages.Thus, the ParGram project has shown that it is possible to use a single grammar development platform and a unified methodology of grammar writing to develop large-scale grammars for typologically different languages.The grammars’ analyses show a large degree of parallelism, despite being developed at different sites.This is achieved by intensive meetings twice a year.The parallelism can be exploited in applications using the grammars: the fewer the differences, the simpler a multilingual application can be (see (Frank, 1999) on a machine-translation prototype using ParGram).
An Evaluation Exercise For Word AlignmentBiBr.EF.1 BiBr.EF.2 BiBr.EF.3 BiBr.EF.4 BiBr.EF.5 BiBr.EF.6 BiBr.EF.7 BiBr.EF.8 Limited Unlimited Unlimited Limited Unlimited Unlimited Limited Unlimited baseline of bilingual bracketing baseline of bilingual bracketing + English POS tagging baseline of bilingual bracketing + English POS tagging and base NP reverse direction of BiBr.EF.1 reverse direction of BiBr.EF.2 reverse direction of BiBr.EF.3 intersection of BiBr.EF.1 & BiBr.EF.3 intersection of BiBr.EF.3 & BiBr.EF.6 ProAlign.EF.1 Unlimited cohesion between source and target language + English parser + distributional similarity for English words Ralign.EF.1 Limited Giza (IBM Model 2) + recursive parallel segmentation UMD.EF.1 Limited IBM Model 2, trained with 1/20 of the corpus, distortion 2, iterations 4 XRCE.Base.EF.1 XRCE.Nolem.EF.2 XRCE.Nolem.EF.3 Limited GIZA++ (IBM Model 4) with English and French lemmatizer GIZA++ only (IBM Model 4), trained with 1/4 of the corpus GIZA++ only (IBM Model 4), trained with 1/2 of the corpus Table 2: Short description for English-French systems System Resources Description BiBr.RE.1 BiBr.RE.2 BiBr.RE.3 Limited Unlimited Unlimited baseline of bilingual bracketing baseline of bilingual bracketing + English POS tagging baseline of bilingual bracketing + English POS tagging and base NPThe task of word alignment consists of finding correspondences between words and phrases in parallel texts.Assuming a sentence aligned bilingual corpus in languages L1 and L2, the task of a word alignment system is to indicate which word token in the corpus of language L1 corresponds to which word token in the corpus of language L2.As part of the HLT/NAACL 2003 workshop on ”Building and Using Parallel Texts: Data Driven Machine Translation and Beyond”, we organized a shared task on word alignment, where participating teams were provided with training and test data, consisting of sentence aligned parallel texts, and were asked to provide automatically derived word alignments for all the words in the test set.Data for two language pairs were provided: (1) EnglishFrench, representing languages with rich resources (20 million word parallel texts), and (2) Romanian-English, representing languages with scarce resources (1 million word parallel texts).Similar with the Machine Translation evaluation exercise organized by NIST1, two subtasks were defined, with teams being encouraged to participate in both subtasks. use any resources in addition to those provided.Such resources had to be explicitly mentioned in the system description.Test data were released one week prior to the deadline for result submissions.Participating teams were asked to produce word alignments, following a common format as specified below, and submit their output by a certain deadline.Results were returned to each team within three days of submission.The word alignment result files had to include one line for each word-to-word alignment.Additionally, lines in the result files had to follow the format specified in Fig.1.While the SIP and confidence fields overlap in their meaning, the intent of having both fields available is to enable participating teams to draw their own line on what they consider to be a Sure or Probable alignment.Both these fields were optional, with some standard values assigned by default.Consider the following two aligned sentences: where: o confidence is a real number, in the range (0-1] (1 meaning highly confident, 0 meaning not confident); this field is optional, and by default confidence number of 1 was assumed. aligned (English token 4 aligned with French token 4), and counts towards the final evaluation figures.Alternatively, systems could also provide an SIP marker and/or a confidence score, as shown in the following example: with missing SIP fields considered by default to be S, and missing confidence scores considered by default 1.The annotation guide and illustrative word alignment examples were mostly drawn from the Blinker Annotation Project.Please refer to (Melamed, 1999, pp.169–182) for additional details.[French]: <s snum=18> fixe moi ton salaire , et je te le donnerai .</s> and he said from the English sentence has no corresponding translation in French, and therefore all these words are aligned with the token id 0.... 18 1 0 18 2 0 18 3 0 18 4 0 ... since the words do not correspond one to one, and yet the two phrases mean the same thing in the given context, the phrases should be linked as wholes, by linking each word in one to each word in another.For the example above, this translates into 12 wordto-word alignments:The shared task included two different language pairs: the alignment of words in English-French parallel texts, and in Romanian-English parallel texts.For each language pair, training data were provided to participants.Systems relying only on these resources were considered part of the Limited Resources subtask.Systems making use of any additional resources (e.g. bilingual dictionaries, additional parallel corpora, and others) were classified under the Unlimited Resources category.Two sets of training data were made available. for pages containing potential parallel translations were manually identified (mainly from the archives of Romanian newspapers).Next, texts were automatically downloaded and sentence aligned.A manual verification of the alignment was also performed.These data collection process resulted in a corpus of about 850,000 Romanian words, and about 900,000 English words.All data were pre-tokenized.For English and French, we used a version of the tokenizers provided within the EGYPT Toolkit2.For Romanian, we used our own tokenizer.Identical tokenization procedures were used for training, trial, and test data.Two sets of trial data were made available at the same time training data became available.Trial sets consisted of sentence aligned texts, provided together with manually determined word alignments.The main purpose of these data was to enable participants to better understand the format required for the word alignment result files.Trial sets consisted of 37 English-French, and 17 Romanian-English aligned sentences.A total of 447 English-French aligned sentences (Och and Ney, 2000), and 248 Romanian-English aligned sentences were released one week prior to the deadline.Participants were required to run their word alignment systems on these two sets, and submit word alignments.Teams were allowed to submit an unlimited number of results sets for each language pair.The gold standard for the two language pair alignments were produced using slightly different alignment procedures, which allowed us to study different schemes for producing gold standards for word aligned data.For English-French, annotators where instructed to assign a Sure or Probable tag to each word alignment they produced.The intersection of the Sure alignments produced by the two annotators led to the final Sure aligned set, while the reunion of the Probable alignments led to the final Probable aligned set.The Sure alignment set is guaranteed to be a subset of the Probable alignment set.The annotators did not produce any NULL alignments.Instead, we assigned NULL alignments as a default backup mechanism, which forced each word to belong to at least one alignment.The English-French aligned data were produced by Franz Och and Hermann Ney (Och and Ney, 2000).For Romanian-English, annotators were instructed to assign an alignment to all words, with specific instructions as to when to assign a NULL alignment.Annotators were not asked to assign a Sure or Probable label.Instead, we had an arbitration phase, where a third annotator judged the cases where the first two annotators disagreed.Since an inter-annotator agreement was reached for all word alignments, the final resulting alignments were considered to be Sure alignments.Evaluations were performed with respect to four different measures.Three of them – precision, recall, and Fmeasure – represent traditional measures in Information Retrieval, and were also frequently used in previous word alignment literature.The fourth measure was originally introduced by (Och and Ney, 2000), and proposes the notion of quality of word alignment.Given an alignment A, and a gold standard alignment ~, each such alignment set eventually consisting of two sets As, .Ap, and 9s, 9p corresponding to Sure and Probable alignments, the following measures are defined (where T is the alignment type, and can be set to either S or P).Each word alignment submission was evaluated in terms of the above measures.Moreover, we conducted two sets of evaluations for each submission: • NULL-Align, where each word was enforced to belong to at least one alignment; if a word did not belong to any alignment, a NULL Probable alignment was assigned by default.This set of evaluations pertains to full coverage word alignments.We conducted therefore 14 evaluations for each submission file: AER, Sure/Probable Precision, Sure/Probable Recall, and Sure/Probable F-measure, with a different figure determined for NULL-Align and NO-NULL-Align alignments.Seven teams from around the world participated in the word alignment shared task.Table 1 lists the names of the participating systems, the corresponding institutions, and references to papers in this volume that provide detailed descriptions of the systems and additional analysis of their results.All seven teams participated in the Romanian-English subtask, and five teams participated in the English-French subtask.3 There were no restrictions placed on the number of submissions each team could make.This resulted in a total of 27 submissions from the seven teams, where 14 sets of results were submitted for the English-French subtask, and 13 for the Romanian-English subtask.Of the 27 total submissions, there were 17 in the Limited resources subtask, and 10 in the Unlimited resources subtask.Tables 2 and 3 show all of the submissions for each team in the two subtasks, and provide a brief description of their approaches.While each participating system was unique, there were a few unifying themes.Four teams had approaches that relied (to varying degrees) on an IBM model of statistical machine translation (Brown et al., 1993).UMD was a straightforward implementation of IBM Model 2, BiBr employed a boosting procedure in deriving an IBM Model 1 lexicon, Ralign used IBM Model 2 as a foundation for their recursive splitting procedure, and XRCE used IBM Model 4 as a base for alignment with lemmatized text and bilingual lexicons.Two teams made use of syntactic structure in the text to be aligned.ProAlign satisfies constraints derived from a dependency tree parse of the English sentence being aligned.BiBr also employs syntactic constraints that must be satisfied.However, these come from parallel text that has been shallowly parsed via a method known as bilingual bracketing.Three teams approached the shared task with baseline or prototype systems.Fourday combines several intuitive baselines via a nearest neighbor classifier, RACAI carries out a greedy alignment based on an automatically extracted dictionary of translations, and UMD’s implementation of IBM Model 2 provides an experimental platform for their future work incorporating prior knowledge about cognates.All three of these systems were developed within a short period of time before and during the shared task.Tables 4 and 5 list the results obtained by participating systems in the Romanian-English task.Similarly, results obtained during the English-French task are listed in Tables 6 and 7.For Romanian-English, limited resources, XRCE systems (XRCE.Nolem-56k.RE.2 and XRCE.Trilex.RE.3) seem to lead to the best results.These are systems that are based on GIZA++, with or without additional resources (lemmatizers and lexicons).For unlimited resources, ProAlign.RE.1 has the best performance.For English-French, Ralign.EF.1 has the best performance for limited resources, while ProAlign.EF.1 has again the largest number of top ranked figures for unlimited resources.To make a cross-language comparison, we paid particular attention to the evaluation of the Sure alignments, since these were collected in a similar fashion (an agreement had to be achieved between two different annotators).The results obtained for the English-French Sure alignments are significantly higher (80.54% best Fmeasure) than those for Romanian-English Sure alignments (71.14% best F-measure).Similarly, AER for English-French (5.71% highest error reduction) is clearly better than the AER for Romanian-English (28.86% highest error reduction).This difference in performance between the two data sets is not a surprise.As expected, word alignment, like many other NLP tasks (Banko and Brill, 2001), highly benefits from large amounts of training data.Increased performance is therefore expected when larger training data sets are available.The only evaluation set where Romanian-English data leads to better performance is the Probable alignments set.We believe however that these figures are not directly comparable, since the English-French Probable alignments were obtained as a reunion of the alignments assigned by two different annotators, while for the Romanian-English Probable set two annotators had to reach an agreement (that is, an intersection of their individual alignment assignments).Interestingly, in an overall evaluation, the limited resources systems seem to lead to better results than those with unlimited resources.Out of 28 different evaluation figures, 20 top ranked figures are provided by systems with limited resources.This suggests that perhaps using a large number of additional resources does not seem to improve a lot over the case when only parallel texts are employed.Ranked results for all systems are plotted in Figures 2 and 3.In the graphs, systems are ordered based on their AER scores.System names are preceded by a marker to indicate the system type: L stands for Limited Resources, and U stands for Unlimited Resources.A shared task on word alignment was organized as part of the HLT/NAACL 2003 Workshop on Building and Using Parallel Texts.In this paper, we presented the task definition, and resources involved, and shortly described the participating systems.The shared task included Romanian-English and English-French sub-tasks, and drew the participation of seven teams from around the world.Comparative evaluations of results led to interesting insights regarding the impact on performance of (1) various alignment algorithms, (2) large or small amounts of training data, and (3) type of resources available.Data and evaluation software used in this exercise are available online at http://www.cs.unt.edu/˜rada/wpt.There are many people who contributed greatly to making this word alignment evaluation task possible.We are grateful to all the participants in the shared task, for their hard work and involvement in this evaluation exercise.Without them, all these comparative analyses of word alignment techniques would not be possible.We are very thankful to Franz Och from ISI and Hermann Ney from RWTH Aachen for kindly making their English-French word aligned data available to the workshop participants; the Hansards made available by Ulrich Germann from ISI constituted invaluable data for the English-French shared task.We would also like to thank the student volunteers from the Department of English, Babes-Bolyai University, Cluj-Napoca, Romania who helped creating the Romanian-English word aligned data.We are also grateful to all the Program Committee members of the current workshop, for their comments and suggestions, which helped us improve the definition of this shared task.In particular, we would like to thank Dan Melamed for suggesting the two different subtasks (limited and unlimited resources), and Michael Carl and Phil Resnik for initiating interesting discussions regarding phrase-based evaluations.
The Senseval-3 English Lexical Sample TaskThis paper presents the task definition, resources, participating systems, and comparative results for the English lexical sample task, which was orgaas part of the evaluation exercise.The task drew the participation of 27 teams from around the world, with a total of 47 systems.We describe in this paper the task definition, resources, participating systems, and comparative results for the English lexical sample task, which was organized as part of the SENSEVAL-3 evaluation exercise.The goal of this task was to create a framework for evaluation of systems that perform targeted Word Sense Disambiguation.This task is a follow-up to similar tasks organized during the SENSEVAL-1 (Kilgarriff and Palmer, 2000) and SENSEVAL-2 (Preiss and Yarowsky, 2001) evaluations.The main changes in this year’s evaluation consist of a new methodology for collecting annotated data (with contributions from Web users, as opposed to trained lexicographers), and a new sense inventory used for verb entries (Wordsmyth).2 Building a Sense Tagged Corpus with Volunteer Contributions over the Web The sense annotated corpus required for this task was built using the Open Mind Word Expert system (Chklovski and Mihalcea, 2002) 1.To overcome the current lack of sense tagged data and the limitations imposed by the creation of such data using trained lexicographers, the OMWE system enables the collection of semantically annotated corpora over the Web.Sense tagged examples are collected using a Web-based application that allows contributors to annotate words with their meanings.The tagging exercise proceeds as follows.For each target word the system extracts a set of sentences from a large textual corpus.These examples are presented to the contributors, who are asked to select the most appropriate sense for the target word in each sentence.The selection is made using checkboxes, which list all possible senses of the current target word, plus two additional choices, “unclear” and “none of the above.” Although users are encouraged to select only one meaning per word, the selection of two or more senses is also possible.The results of the classification submitted by other users are not presented to avoid artificial biases.Similar to the annotation scheme used for the English lexical sample at SENSEVAL-2, we use a “tag until two agree” scheme, with an upper bound on the number of annotations collected for each item set to four.The data set used for the SENSEVAL-3 English lexical sample task consists of examples extracted from the British National Corpus (BNC).Earlier versions of OMWE also included data from the Penn Treebank corpus, the Los Angeles Times collection as provided during TREC conferences (http://trec.nist.gov), and Open Mind Common Sense (http://commonsense.media.mit.edu).The sense inventory used for nouns and adjectives is WordNet 1.7.1 (Miller, 1995), which is consistent with the annotations done for the same task during SENSEVAL-2.Verbs are instead annotated with senses from Wordsmyth (http://www.wordsmyth.net/).The main reason motivating selection of a different sense inventory is the weak verb performance of systems participating in the English lexical sample in SENSEVAL-2, which may be due to the high number of senses defined for verbs in the WordNet sense inventory.By choosing a different set of senses, we hope to gain insight into the dependence of difficulty of the sense disambiguation task on sense inventories.Table 1 presents the number of words under each part of speech, and the average number of senses for each class.For this evaluation exercise, we decided to isolate the task of semantic tagging from the task of identifying multi-word expressions; we applied a filter that removed all examples pertaining to multi-word expressions prior to the tagging phase.Consequently, the training and test data sets made available for this task do not contain collocations as possible target words, but only single word units.This is a somewhat different definition of the task as compared to previous similar evaluations; the difference may have an impact on the overall performance achieved by systems participating in the task.The inter-tagger agreement obtained so far is closely comparable to the agreement figures previously reported in the literature.Kilgarriff (2002) mentions that for the SENSEVAL-2 nouns and adjectives there was a 66.5% agreement between the first two taggings (taken in order of submission) entered for each item.About 12% of that tagging consisted of multi-word expressions and proper nouns, which are usually not ambiguous, and which are not considered during our data collection process.So far we measured a 62.8% inter-tagger agreement between the first two taggings for single word tagging, plus close-to-100% precision in tagging multi-word expressions and proper nouns (as mentioned earlier, this represents about 12% of the annotated data).This results in an overall agreement of about 67.3% which is reasonable and closely comparable with previous figures.Note that these figures are collected for the entire OMWE data set build so far, which consists of annotated data for more than 350 words.In addition to raw inter-tagger agreement, the kappa statistic, which removes from the agreement rate the amount of agreement that is expected by chance(Carletta, 1996), was also determined.We measure two figures: micro-average , where number of senses, agreement by chance, and are determined as an average for all words in the set, and macro-average , where inter-tagger agreement, agreement by chance, and are individually determined for each of the words in the set, and then combined in an overall average.With an average of five senses per word, the average value for the agreement by chance is measured at 0.20, resulting in a micro- statistic of 0.58.For macro- estimations, we assume that word senses follow the distribution observed in the OMWE annotated data, and under this assumption, the macro- is 0.35.27 teams participated in this word sense disambiguation task.Tables 2 and 3 list the names of the participating systems, the corresponding institutions, and the name of the first author – which can be used as reference to a paper in this volume, with more detailed descriptions of the systems and additional analysis of the results.There were no restrictions placed on the number of submissions each team could make.A total number of 47 submissions were received for this task.Tables 2 and 3 show all the submissions for each team, gives a brief description of their approaches, and lists the precision and recall obtained by each system under fine and coarse grained evaluations.The precision/recall baseline obtained for this task under the “most frequent sense” heuristic is 55.2% (fine grained) and 64.5% (coarse grained).The performance of most systems (including several unsupervised systems, as listed in Table 3) is significantly higher than the baseline, with the best system performing at 72.9% (79.3%) for fine grained (coarse grained) scoring.Not surprisingly, several of the top performing systems are based on combinations of multiple classifiers, which shows once again that voting schemes that combine several learning algorithms outperform the accuracy of individual classifiers.The English lexical sample task in SENSEVAL3 featured English ambiguous words that were to be tagged with their most appropriate WordNet or Wordsmyth sense.The objective of this task was to: (1) Determine feasibility of reliably finding the English lexical sample Word Sense Disambiguation task.Precision and recall figures are provided for both fine grained and coarse grained scoring.Corresponding team and reference to system description (in this volume) are indicated for the first system for each team. appropriate sense for words with various degrees of polysemy, using different sense inventories; and (2) Determine the usefulness of sense annotated data collected over the Web (as opposed to other traditional approaches for building semantically annotated corpora).The results of 47 systems that participated in this event tentatively suggest that supervised machine learning techniques can significantly improve over the most frequent sense baseline, and also that it is possible to design unsupervised techniques for reliable word sense disambiguation.Additionally, this task has highlighted creation of testing and training data by leveraging the knowledge of Web volunteers.The training and test data sets used in this exercise are available online from http://www.senseval.org and http://teach-computers.org.Many thanks to all those who contributed to the Open Mind Word Expert project, making this task possible.In particular, we are grateful to Gwen Lenker – our most productive contributor.We are also grateful to all the participants in this task, for their hard work and involvement in this evaluation exercise.Without them, all these comparative analyses would not be possible.We are indebted to the Princeton WordNet team, for making WordNet available free of charge, and to Robert Parks from Wordsmyth, for making available the verb entries used in this evaluation.We are particularly grateful to the National Science Foundation for their support under research grant IIS-0336793, and to the University of North Texas for a research grant that provided funding for contributor prizes.
Emotions Evoked by Common Words and Phrases: Using Mechanical Turk to Create an Emotion LexiconEven though considerable attention has been given to semantic orientation of words and the creation of large polarity lexicons, research in emotion analysis has had to rely on limited and small emotion lexicons.In this paper, we show how we create a high-quality, moderate-sized emotion lexicon using Mechanical Turk.In addition to questions about emotions evoked by terms, we show how the inclusion of a word choice question can discourage malicious data entry, help identify instances where the annotator may not be familiar with the target term (allowing us to reject such annotations), and help obtain annotations at sense level (rather than at word level).We perform an extensive analysis of the annotations to better understand the distribution of emotions evoked by terms of different parts of speech.We identify which emotions tend to be evoked simultaneously by the same term and show that certain emotions indeed go hand in hand.When analyzing text, automatically detecting emotions such as joy, sadness, fear, anger, and surprise is useful for a number of purposes, including identifying blogs that express specific emotions towards the topic of interest, identifying what emotion a newspaper headline is trying to evoke, and devising automatic dialogue systems that respond appropriately to different emotional states of the user.Often different emotions are expressed through different words.For example, delightful and yummy indicate the emotion of joy, gloomy and cry are indicative of sadness, 26 shout and boiling are indicative of anger, and so on.Therefore an emotion lexicon—a list of emotions and words that are indicative of each emotion—is likely to be useful in identifying emotions in text.Words may evoke different emotions in different contexts, and the emotion evoked by a phrase or a sentence is not simply the sum of emotions conveyed by the words in it, but the emotion lexicon will be a useful component for any sophisticated emotion detecting algorithm.The lexicon will also be useful for evaluating automatic methods that identify the emotions evoked by a word.Such algorithms may then be used to automatically generate emotion lexicons in languages where no such lexicons exist.As of now, high-quality high-coverage emotion lexicons do not exist for any language, although there are a few limited-coverage lexicons for a handful of languages, for example, the WordNet Affect Lexicon (WAL) (Strapparava and Valitutti, 2004) for six basic emotions and the General Inquirer (GI) (Stone et al., 1966), which categorizes words into a number of categories, including positive and negative semantic orientation.Amazon has an online service called Mechanical Turk that can be used to obtain a large amount of human annotation in an efficient and inexpensive manner (Snow et al., 2008; Callison-Burch, 2009).1 However, one must define the task carefully to obtain annotations of high quality.Several checks must be placed to ensure that random and erroneous annotations are discouraged, rejected, and re-annotated.In this paper, we show how we compiled a moderate-sized English emotion lexicon by manual annotation through Amazon’s Mechanical Turk service.This dataset, which we will call EmoLez, is many times as large as the only other known emotion lexicon, WordNet Affect Lexicon.More importantly, the terms in this lexicon are carefully chosen to include some of the most frequent nouns, verbs, adjectives, and adverbs.Beyond unigrams, it has a large number of commonly used bigrams.We also include some words from the General Inquirer and some from WordNet Affect Lexicon, to allow comparison of annotations between the various resources.We perform an extensive analysis of the annotations to answer several questions that have not been properly addressed so far.For instance, how hard is it for humans to annotate words with the emotions they evoke?What percentage of commonly used terms, in each part of speech, evoke an emotion?Are emotions more commonly evoked by nouns, verbs, adjectives, or adverbs?Is there a correlation between the semantic orientation of a word and the emotion it evokes?Which emotions tend to go together; that is, which emotions are evoked simultaneously by the same term?This work is intended to be a pilot study before we create a much larger emotion lexicon with tens of thousands of terms.We focus on the emotions of joy, sadness, anger, fear, trust, disgust, surprise, and anticipation— argued by many to be the basic and prototypical emotions (Plutchik, 1980).Complex emotions can be viewed as combinations of these basic emotions.WordNet Affect Lexicon (Strapparava and Valitutti, 2004) has a few hundred words annotated with the emotions they evoke.2 It was created by manually identifying the emotions of a few seed words and then marking all their WordNet synonyms as having the same emotion.The General Inquirer (Stone et al., 1966) has 11,788 words labeled with 182 categories of word tags, including positive and negative semantic orientation.3 It also has certain other affect categories, such as pleasure, arousal, feeling, and pain but these have not been exploited to a significant degree by the natural language processing community.Work in emotion detection can be roughly classified into that which looks for specific emotion denoting words (Elliott, 1992), that which determines tendency of terms to co-occur with seed words whose emotions are known (Read, 2004), that which uses hand-coded rules (Neviarouskaya et al., 2009), and that which uses machine learning and a number of emotion features, including emotion denoting words (Alm et al., 2005).Much of this recent work focuses on six emotions studied by Ekman (1992).These emotions— joy, sadness, anger, fear, disgust, and surprise— are a subset of the eight proposed in Plutchik (1980).We focus on the Plutchik emotions because the emotions can be naturally paired into opposites—joy– sadness, anger–fear, trust–disgust, and anticipationsurprise.Natural symmetry apart, we believe that prior work on automatically computing word–pair antonymy (Lin et al., 2003; Mohammad et al., 2008; Lobanova et al., 2010) can now be leveraged in automatic emotion detection.In the subsections below we present the challenges in obtaining high-quality emotion annotation, how we address those challenges, how we select the target terms, and the questionnaire we created for the annotators.Words used in different senses can evoke different emotions.For example, the word shout evokes a different emotion when used in the context of admonishment, than when used in “Give me a shout if you need any help.” Getting human annotations on word senses is made complicated by decisions about which sense-inventory to use and what level of granularity the senses must have.On the one hand, we do not want to choose a fine-grained sense-inventory because then the number of word–sense combinations will become too large and difficult to easily distinguish, and on the other hand we do not want to work only at the word level because when used in different senses a word may evoke different emotions.Yet another challenge is how best to convey a word sense to the annotator.Long definitions will take time to read and limit the number of annotations we can obtain for the same amount of resources.Further, we do not want to bias the annotator towards an emotion through the definition.We want the users to annotate a word only if they are already familiar with it and know its meanings.And lastly, we must ensure that malicious and erroneous annotations are rejected.In order to overcome the challenges described above, before asking the annotators questions about what emotions are evoked by a target term, we first present them with a word choice problem pertaining to the target.They are provided with four different words and asked which word is closest in meaning to the target.This single question serves many purposes.Through this question we convey the word sense for which annotations are to be provided, without actually providing annotators with long definitions.If an annotator is not familiar with the target word and still attempts to answer questions pertaining to the target, or is randomly clicking options in our questionnaire, then there is a 75% chance that they will get the answer to this question wrong, and we can discard all responses pertaining to this target term by the annotator (that is, we discard answers to the emotion questions provided by the annotator for this target term).We generated these word choice problems automatically using the Macquarie Thesaurus (Bernard, 1986).Published thesauri, such as Roget’s and Macquarie, divide the vocabulary into about a thousand categories, which may be interpreted as coarse senses.If a word has more than one sense, then it can be found in more than one thesaurus category.Each category also has a head word which best captures the meaning of the category.Most of the target terms chosen for annotation are restricted to those that are listed in exactly one thesaurus category.The word choice question for a target term is automatically generated by selecting the following four alternatives (choices): the head word of the thesaurus category pertaining to the target term (the correct answer); and three other head words of randomly selected categories (the distractors).The four alternatives are presented to the annotator in random order.Only a small number of the words in the WordNet Affect Lexicon are listed in exactly one thesaurus category (have one sense), and so we included target terms that occurred in two thesaurus categories as well.For these questions, we listed head words from both the senses (categories) as two of the alternatives (probability of a random choice being correct is 50%).Depending on the alternative chosen, we can thus determine the sense for which the subsequent emotion responses are provided by the annotator.In order to generate an emotion lexicon, we first identify a list of words and phrases for which we want human annotations.We chose the Macquarie Thesaurus as our source pool for unigrams and bigrams.Any other published dictionary would have worked well too.However, apart from over 57,000 commonly used English word types, the Macquarie Thesaurus also has entries for more than 40,000 commonly used phrases.From this list of unigrams and bigrams we chose those that occur frequently in the Google n-gram corpus (Brants and Franz, 2006).Specifically we chose the 200 most frequent n-grams in the following categories: noun unigrams, noun bigrams, verb unigrams, verb bigrams, adverb unigrams, adverb bigrams, adjective unigrams, adjective bigrams, words in the General Inquirer that are marked as having a negative semantic orientation, words in General Inquirer that are marked as having a positive semantic orientation.When selecting these sets, we ignored terms that occurred in more than one Macquarie Thesaurus category.Lastly, we chose all words from each of the six emotion categories in the WordNet Affect Lexicon that had at most two senses in the thesaurus (occurred in at most two thesaurus categories).The first and second column of Table 1 list the various sets of target terms as well as the number of terms in each set for which annotations were requested.EmoLexUni stands for all the unigrams taken from the thesaurus.EmoLexBi refers to all the bigrams.EmoLexGI are all the words taken from the General Inquirer.EmoLexWAL are all the words taken from the WordNet Affect Lexicon.An entity submitting a task to Mechanical Turk is called the requester.A requester first breaks the task into small independently solvable units called HITs (Human Intelligence Tasks) and uploads them on the Mechanical Turk website.The requester specifies the compensation that will be paid for solving each HIT.The people who provide responses to these HITs are called Turkers.The requester also specifies the number of different Turkers that are to annotate each HIT.The annotation provided by a Turker for a HIT is called an assignment.We created Mechanical Turk HITs for each of the terms specified in Table 1.Each HIT has a set of questions, all of which are to be answered by the same person.We requested five different assignments for each HIT (each HIT is to be annotated by five different Turkers).Different HITS may be attempted by different Turkers, and a Turker may attempt as many HITs as they wish.Below is an example HIT for the target word “startle”.Title: Emotions evoked by words Reward per HIT: $0.04 [Questions 5 to 11 are similar to 4, except that joy is replaced with one of the other seven emotions: sadness (failure and heart-break); fear (horror and scary); anger (rage and shouting); trust (faith and integrity); disgust (gross and cruelty); surprise (startle and sudden); anticipation (expect and eager).]Before going live, the survey was approved by the ethics committee at the National Research Council Canada.The first set of emotion annotations on Mechanical Turk were completed in about nine days.The Turkers spent a minute on average to answer the questions in a HIT.This resulted in an hourly pay of slightly more than $2.Once the assignments were collected, we used automatic scripts to validate the annotations.Some assignments were discarded because they failed certain tests (described below).A subset of the discarded assignments were officially rejected (the Turkers were not paid for these assignments) because instructions were not followed.About 500 of the 10,880 assignments (2,176 x 5) included at least one unanswered question.These assignments were discarded and rejected.More than 85% of the remaining assignments had the correct answer for the word choice question.This was a welcome result showing that, largely, the annotations were done in a responsible manner.We discarded all assignments that had the wrong answer for the word choice question.If an annotator obtained an overall score that is less than 66.67% on the word choice questions (that is, got more than one out of three wrong), then we assumed that, contrary to instructions, HITs for words not familiar to the annotator were attempted.We discarded and rejected all assignments by such annotators (not just the assignments for which they got the word choice question wrong).HITs pertaining to all the discarded assignments were uploaded for a second time on Mechanical Turk and the validation process was repeated.After the second round, we had three or more valid assignments for 2081 of the 2176 target terms.We will refer to this set of assignments as the master set.We create the emotion lexicon from this master set containing 9892 assignments from about 1000 Turkers who attempted 1 to 450 assignments each.About 100 of them provided 20 or more assignments each (more than 7000 assignments in all).The master set has, on average, about 4.75 assignments for each of the 2081 target terms.(See Table 1 for more details.)The different emotion annotations for a target term were consolidated by determining the majority class of emotion intensities.For a given term– emotion pair, the majority class is that intensity level that is chosen most often by the Turkers to represent the degree of emotion evoked by the word.Ties are broken by choosing the stronger intensity level.Table 2 lists the percent of 2081 target terms assigned a majority class of no, weak, moderate, and strong emotion.For example, it tells us that 7.6% of the target terms strongly evoke joy.The table also presents an average of the numbers in each column (micro average).Observe that the percentages for individual emotions do not vary greatly from the average.The last row lists the percent of target terms that evoke some emotion (any of the eight) at the various intensity levels.We calculated this using the intensity level of the strongest emotion expressed by each target.Observe that 30.1% of the target terms strongly evoke at least one of the eight basic emotions.Even though we asked Turkers to annotate emotions at four levels of intensity, practical NLP applications often require only two levels—evoking particular emotion (evocative) or not (non-evocative).For each target term–emotion pair, we convert the four-level annotations into two-level annotations by placing all no- and weak-intensity assignments in the non-evocative bin, all moderate- and strongintensity assignments in the evocative bin, and then choosing the bin with the majority assignments.Table 3 gives percent of target terms considered to be evocative.The last row in the table gives the percentage of terms evocative of some emotion (any of the eight).Table 4 shows how many terms in each category are evocative of the different emotions.Table 4 shows that a sizable percent of nouns, verbs, adjectives, and adverbs are evocative.Adverbs and adjectives are some of the most emotion inspiring terms and this is not surprising considering that they are used to qualify a noun or a verb.Anticipation, trust, and joy come through as the most common emotions evoked by terms of all four parts of speech.The EmoLexWAL rows are particularly interesting because they serve to determine how much the Turker annotations match annotations in the Wordnet Affect Lexicon (WAL).The most common Turker-determined emotion for each of these rows is marked in bold.Observe that WAL anger terms are mostly marked as anger evocative, joy terms as joy evocative, and so on.The EmoLexWAL rows also indicate which emotions get confused for which, or which emotions tend to be evoked simultaneously by a term.Observe that anger terms tend also to be evocative of disgust.Similarly, fear and sadness go together, as do joy, trust, and anticipation.The EmoLexGI rows rightly show that words marked as negative in the General Inquirer, mostly evoke negative emotions (anger, fear, disgust, and sadness).Observe that the percentages for trust and joy are much lower.On the other hand, positive words evoke anticipation, joy, and trust.In order to analyze how often the annotators agreed with each other, for each term–emotion pair, we calculated the percentage of times the majority class has size 5 (all Turkers agree), size 4 (all but one agree), size 3, and size 2.Observe that for more than 50% of the terms, at least four annotators agree with each other.Table 5 presents these agreement values.Since many NLP systems may rely only on two intensity values (evocative or non-evocative), we also calculate agreement at that level (Table 6).Observe that for more than 50% of the terms, all five annotators agree with each other, and for more than 80% of the terms, at least four annotators agree.This shows a high degree of agreement on emotion annotations despite no real control over the educational background and qualifications of the annotators.We consolidate the semantic orientation (polarity) annotations in a manner identical to the process for emotion annotations.Table 7 lists the percent of 2081 target terms assigned a majority class of no, weak, moderate, and strong semantic orientation.For example, it tells us that 16% of the target terms are strongly negative.The last row in the table lists the percent of target terms that have some semantic orientation (positive or negative) at the various intensity levels.Observe that 35% of the target terms are strongly evaluative (positively or negatively).Just as in the case for emotions, practical NLP applications often require only two levels of semantic orientation—having particular semantic orientation or not (evaluative) or not (non-evaluative).For each target term–emotion pair, we convert the fourlevel semantic orientation annotations into two-level ones, just as we did for the emotions.Table 8 gives percent of target terms considered to be evaluative.The last row in the table gives the percentage of terms evaluative with respect to some semantic orientation (positive or negative).Table 9 shows how many terms in each category are positively and negatively evaluative.Observe in Table 9 that, across the board, a sizable number of terms are evaluative with respect to some semantic orientation.Interestingly unigram nouns have a markedly lower proportion of negative terms, and a much higher proportion of positive terms.It may be argued that the default semantic orientation of noun concepts is positive, and that usually it takes a negative adjective to make the phrase negative.The EmoLexGI rows in the two tables show that words marked as having a negative semantic orientation in the General Inquirer are mostly marked as negative by the Turkers.And similarly, the positives in GI are annotated as positive.Again, this is confirmation that the quality of annotation obtained is high.The EmoLexWAL rows show that anger, disgust, fear, and sadness terms tend not to have a positive semantic orientation and are mostly negative.In contrast, and expectedly, the joy terms are positive.The surprise terms are more than twice as likely to be positive than negative.In order to analyze how often the annotators agreed with each other, for each term–emotion pair, we calculated the percentage of times the majority class has size 5 (all Turkers agree), size 4 (all but one agree), size 3, and size 2.Table 10 presents these agreement values.Observe that for more than 50% of the terms, at least four annotators agree with each other.Table 11 gives agreement values at the twointensity level.Observe that for more than 50% of the terms, all five annotators agree with each other, and for more than 80% of the terms, at least four annotators agree.We showed how Mechanical Turk can be used to create a high-quality, moderate-sized, emotion lexicon for a very small cost (less than US$500).Notably, we used automatically generated word choice questions to detect and reject erroneous annotations and to reject all annotations by unqualified Turkers and those who indulge in malicious data entry.We compared a subset of our lexicon with existing gold standard data to show that the annotations obtained are indeed of high quality.A detailed analysis of the (evaluative and non-evaluative): Percent of 2081 terms for which the majority class size was 3, 4, and 5. lexicon revealed insights into how prevalent emotion bearing terms are among common unigrams and bigrams.We also identified which emotions tend to be evoked simultaneously by the same term.The lexicon is available for free download.4 Since this pilot experiment with about 2000 target terms was successful, we will now obtain emotion annotations for tens of thousands of English terms.We will use the emotion lexicon to identify emotional tone of larger units of text, such as newspaper headlines and blog posts.We will also use it to evaluate automatically generated lexicons, such as the polarity lexicons by Turney and Littman (2003) and Mohammad et al. (2009).We will explore the variance in emotion evoked by near-synonyms, and also how common it is for words with many meanings to evoke different emotions in different senses.This research was funded by the National research Council Canada (NRC).Thanks to Diana Inkpen and Diman Ghazi for early discussions on emotion.Thanks to Joel Martin for encouragement and support.Thanks to Norm Vinson and the Ethics Committee at NRC for examining, guiding, and approving the survey.And last but not least, thanks to the more than 1000 anonymous people who answered the emotion survey with diligence and care.
Sentiment Analysis of Twitter DataWe examine sentiment analysis on Twitter data.The contributions of this paper are: (1) We introduce POS-specific prior polarity features.(2) We explore the use of a tree kernel to obviate the need for tedious feature engineering.The new features (in conjunction with previously proposed features) and the tree kernel perform approximately at the same level, both outperforming the state-of-the-art baseline.Microblogging websites have evolved to become a source of varied kind of information.This is due to nature of microblogs on which people post real time messages about their opinions on a variety of topics, discuss current issues, complain, and express positive sentiment for products they use in daily life.In fact, companies manufacturing such products have started to poll these microblogs to get a sense of general sentiment for their product.Many times these companies study user reactions and reply to users on microblogs.One challenge is to build technology to detect and summarize an overall sentiment.In this paper, we look at one such popular microblog called Twitter and build models for classifying “tweets” into positive, negative and neutral sentiment.We build models for two classification tasks: a binary task of classifying sentiment into positive and negative classes and a 3-way task of classifying sentiment into positive, negative and neutral classes.We experiment with three types of models: unigram model, a feature based model and a tree kernel based model.For the feature based model we use some of the features proposed in past literature and propose new features.For the tree kernel based model we design a new tree representation for tweets.We use a unigram model, previously shown to work well for sentiment analysis for Twitter data, as our baseline.Our experiments show that a unigram model is indeed a hard baseline achieving over 20% over the chance baseline for both classification tasks.Our feature based model that uses only 100 features achieves similar accuracy as the unigram model that uses over 10,000 features.Our tree kernel based model outperforms both these models by a significant margin.We also experiment with a combination of models: combining unigrams with our features and combining our features with the tree kernel.Both these combinations outperform the unigram baseline by over 4% for both classification tasks.In this paper, we present extensive feature analysis of the 100 features we propose.Our experiments show that features that have to do with Twitter-specific features (emoticons, hashtags etc.) add value to the classifier but only marginally.Features that combine prior polarity of words with their parts-of-speech tags are most important for both the classification tasks.Thus, we see that standard natural language processing tools are useful even in a genre which is quite different from the genre on which they were trained (newswire).Furthermore, we also show that the tree kernel model performs roughly as well as the best feature based models, even though it does not require detailed feature engineering.We use manually annotated Twitter data for our experiments.One advantage of this data, over previously used data-sets, is that the tweets are collected in a streaming fashion and therefore represent a true sample of actual tweets in terms of language use and content.Our new data set is available to other researchers.In this paper we also introduce two resources which are available (contact the first author): 1) a hand annotated dictionary for emoticons that maps emoticons to their polarity and 2) an acronym dictionary collected from the web with English translations of over 5000 frequently used acronyms.The rest of the paper is organized as follows.In section 2, we discuss classification tasks like sentiment analysis on micro-blog data.In section 3, we give details about the data.In section 4 we discuss our pre-processing technique and additional resources.In section 5 we present our prior polarity scoring scheme.In section 6 we present the design of our tree kernel.In section 7 we give details of our feature based approach.In section 8 we present our experiments and discuss the results.We conclude and give future directions of research in section 9.Sentiment analysis has been handled as a Natural Language Processing task at many levels of granularity.Starting from being a document level classification task (Turney, 2002; Pang and Lee, 2004), it has been handled at the sentence level (Hu and Liu, 2004; Kim and Hovy, 2004) and more recently at the phrase level (Wilson et al., 2005; Agarwal et al., 2009).Microblog data like Twitter, on which users post real time reactions to and opinions about “everything”, poses newer and different challenges.Some of the early and recent results on sentiment analysis of Twitter data are by Go et al. (2009), (Bermingham and Smeaton, 2010) and Pak and Paroubek (2010).Go et al. (2009) use distant learning to acquire sentiment data.They use tweets ending in positive emoticons like “:)” “:-)” as positive and negative emoticons like “:(” “:-(” as negative.They build models using Naive Bayes, MaxEnt and Support Vector Machines (SVM), and they report SVM outperforms other classifiers.In terms of feature space, they try a Unigram, Bigram model in conjunction with parts-of-speech (POS) features.They note that the unigram model outperforms all other models.Specifically, bigrams and POS features do not help.Pak and Paroubek (2010) collect data following a similar distant learning paradigm.They perform a different classification task though: subjective versus objective.For subjective data they collect the tweets ending with emoticons in the same manner as Go et al. (2009).For objective data they crawl twitter accounts of popular newspapers like “New York Times”, “Washington Posts” etc.They report that POS and bigrams both help (contrary to results presented by Go et al. (2009)).Both these approaches, however, are primarily based on ngram models.Moreover, the data they use for training and testing is collected by search queries and is therefore biased.In contrast, we present features that achieve a significant gain over a unigram baseline.In addition we explore a different method of data representation and report significant improvement over the unigram models.Another contribution of this paper is that we report results on manually annotated data that does not suffer from any known biases.Our data is a random sample of streaming tweets unlike data collected by using specific queries.The size of our hand-labeled data allows us to perform crossvalidation experiments and check for the variance in performance of the classifier across folds.Another significant effort for sentiment classification on Twitter data is by Barbosa and Feng (2010).They use polarity predictions from three websites as noisy labels to train a model and use 1000 manually labeled tweets for tuning and another 1000 manually labeled tweets for testing.They however do not mention how they collect their test data.They propose the use of syntax features of tweets like retweet, hashtags, link, punctuation and exclamation marks in conjunction with features like prior polarity of words and POS of words.We extend their approach by using real valued prior polarity, and by combining prior polarity with POS.Our results show that the features that enhance the performance of our classifiers the most are features that combine prior polarity of words with their parts of speech.The tweet syntax features help but only marginally.Gamon (2004) perform sentiment analysis on feeadback data from Global Support Services survey.One aim of their paper is to analyze the role of linguistic features like POS tags.They perform extensive feature analysis and feature selection and demonstrate that abstract linguistic analysis features contributes to the classifier accuracy.In this paper we perform extensive feature analysis and show that the use of only 100 abstract linguistic features performs as well as a hard unigram baseline.Twitter is a social networking and microblogging service that allows users to post real time messages, called tweets.Tweets are short messages, restricted to 140 characters in length.Due to the nature of this microblogging service (quick and short messages), people use acronyms, make spelling mistakes, use emoticons and other characters that express special meanings.Following is a brief terminology associated with tweets.Emoticons: These are facial expressions pictorially represented using punctuation and letters; they express the user’s mood.Target: Users of Twitter use the “@” symbol to refer to other users on the microblog.Referring to other users in this manner automatically alerts them.Hashtags: Users usually use hashtags to mark topics.This is primarily done to increase the visibility of their tweets.We acquire 11,875 manually annotated Twitter data (tweets) from a commercial source.They have made part of their data publicly available.For information on how to obtain the data, see Acknowledgments section at the end of the paper.They collected the data by archiving the real-time stream.No language, location or any other kind of restriction was made during the streaming process.In fact, their collection consists of tweets in foreign languages.They use Google translate to convert it into English before the annotation process.Each tweet is labeled by a human annotator as positive, negative, neutral or junk.The “junk” label means that the tweet cannot be understood by a human annotator.A manual analysis of a random sample of tweets labeled as “junk” suggested that many of these tweets were those that were not translated well using Google translate.We eliminate the tweets with junk label for experiments.This leaves us with an unbalanced sample of 8,753 tweets.We use stratified sampling to get a balanced data-set of 5127 tweets (1709 tweets each from classes positive, negative and neutral).In this paper we introduce two new resources for pre-processing twitter data: 1) an emoticon dictionary and 2) an acronym dictionary.We prepare the emoticon dictionary by labeling 170 emoticons listed on Wikipedia1 with their emotional state.For example, “:)” is labeled as positive whereas “:=(” is labeled as negative.We assign each emoticon a label from the following set of labels: Extremely-positive, Extremely-negative, Positive, Negative, and Neutral.We compile an acronym dictionary from an online resource.2 The dictionary has translations for 5,184 acronyms.For example, lol is translated to laughing out loud.We pre-process all the tweets as follows: a) replace all the emoticons with a their sentiment polarity by looking up the emoticon dictionary, b) replace all URLs with a tag ||U||, c) replace targets (e.g.“@John”) with tag ||T||, d) replace all negations (e.g. not, no, never, n’t, cannot) by tag “NOT”, and e) replace a sequence of repeated characters by three characters, for example, convert coooooooool to coool.We do not replace the sequence by only two characters since we want to differentiate between the regular usage and emphasized usage of the word.Acronym English expansion gr8, gr8t great lol laughing out loud rotf rolling on the floor bff best friend forever We present some preliminary statistics about the data in Table 3.We use the Stanford tokenizer (Klein and Manning, 2003) to tokenize the tweets.We use a stop word dictionary3 to identify stop words.All the other words which are found in WordNet (Fellbaum, 1998) are counted as English words.We use the standard tagset defined by the Penn Treebank for identifying punctuation.We record the occurrence of three standard twitter tags: emoticons, URLs and targets.The remaining tokens are either non English words (like coool, zzz etc.) or other symbols.In Table 3 we see that 38.3% of the tokens are stop words, 30.1% of the tokens are found in WordNet and 1.2% tokens are negation words.11.8% of all the tokens are punctuation marks excluding exclamation marks which make up for 2.8% of all tokens.In total, 84.1% of all tokens are tokens that we expect to see in a typical English language text.There are 4.2% tags that are specific to Twitter which include emoticons, target, hastags and “RT” (retweet).The remaining 11.7% tokens are either words that cannot be found in WordNet (like Zzzzz, kewl) or special symbols which do not fall in the category of Twitter tags.A number of our features are based on prior polarity of words.For obtaining the prior polarity of words, we take motivation from work by Agarwal et al. (2009).We use Dictionary of Affect in Language (DAL) (Whissel, 1989) and extend it using WordNet.This dictionary of about 8000 English language words assigns every word a pleasantness score (E R) between 1 (Negative) - 3 (Positive).We first normalize the scores by diving each score my the scale (which is equal to 3).We consider words with polarity less than 0.5 as negative, higher than 0.8 as positive and the rest as neutral.If a word is not directly found in the dictionary, we retrieve all synonyms from Wordnet.We then look for each of the synonyms in DAL.If any synonym is found in DAL, we assign the original word the same pleasantness score as its synonym.If none of the synonyms is present in DAL, the word is not associated with any prior polarity.For the given data we directly found prior polarity of 81.1% of the words.We find polarity of other 7.8% of the words by using WordNet.So we find prior polarity of about 88.9% of English language words.We design a tree representation of tweets to combine many categories of features in one succinct convenient representation.For calculating the similarity between two trees we use a Partial Tree (PT) kernel first proposed by Moschitti (2006).A PT kernel calculates the similarity between two trees by comparing all possible sub-trees.This tree kernel is an instance of a general class of convolution kernels.Convolution Kernels, first introduced by Haussler (1999), can be used to compare abstract objects, like strings, instead of feature vectors.This is because these kernels involve a recursive calculation over the “parts” of abstract object.This calculation is made computationally efficient by using Dynamic Programming techniques.By considering all possible combinations of fragments, tree kernels capture any possible correlation between features and categories of features.Figure 1 shows an example of the tree structure we design.This tree is for a synthesized tweet: @Fernando this isn’t a great day for playing the HARP!:).We use the following procedure to convert a tweet into a tree representation: Initialize the main tree to be “ROOT”.Then tokenize each tweet and for each token: a) if the token is a target, emoticon, exclamation mark, other punctuation mark, or a negation word, add a leaf node to the “ROOT” with the corresponding tag.For example, in the tree in Figure 1 we add tag ||T  ||(target) for “@Fernando”, add tag “NOT” for the token “n’t”, add tag “EXC” for the exclamation mark at the end of the sentence and add ||P ||for the emoticon representing positive mood. b) if the token is a stop word, we simply add the subtree “ (STOP (‘stop-word’))” to “ROOT”.For instance, we add a subtree corresponding to each of the stop words: this, is, and for. c) if the token is an English language word, we map the word to its part-of-speech tag, calculate the prior polarity of the word using the procedure described in section 5 and add the subtree (EW (‘POS’ ‘word’ ‘prior polarity’)) to the “ROOT”.For example, we add the subtree (EW (JJ great POS)) for the word great.“EW” refers to English word. d) For any other token <token> we add subtree “(NE (<token>))” to the “ROOT”.“NE” refers to non-English.The PT tree kernel creates all possible subtrees and compares them to each other.These subtrees include subtrees in which non-adjacent branches become adjacent by excising other branches, though order is preserved.In Figure 1, we show some of the tree fragments that the PT kernel will attempt to compare with tree fragments from other trees.For example, given the tree (EW (JJ) (great) (POS)), the PT kernel will use (EW (JJ) (great) (POS)), (EW (great) (POS)), (EW (JJ) (POS)), (EW (JJ) (great)), (EW (JJ)), (EW (great)), (EW (POS)), (EW), (JJ), (great), and (POS).This means that the PT tree kernel attempts to use full information, and also abstracts away from specific information (such as the lexical item).In this manner, it is not necessary to create by hand features at all levels of abstraction.We propose a set of features listed in Table 4 for our experiments.These are a total of 50 type of features.We calculate these features for the whole tweet and for the last one-third of the tweet.In total we get 100 additional features.We refer to these features as Senti-features throughout the paper.Our features can be divided into three broad categories: ones that are primarily counts of various features and therefore the value of the feature is a natural number E N. Second, features whose value is a real number E R. These are primarily features that capture the score retrieved from DAL.Thirdly, features whose values are boolean E B.These are bag of words, presence of exclamation marks and capitalized text.Each of these broad categories is divided into two subcategories: Polar features and Non-polar features.We refer to a feature as polar if we calculate its prior polarity either by looking it up in DAL (extended through WordNet) or in the emoticon dictionary.All other features which are not associated with any prior polarity fall in the Nonpolar category.Each of Polar and Non-polar features is further subdivided into two categories: POS and Other.POS refers to features that capture statistics about parts-of-speech of words and Other refers to all other types of features.In reference to Table 4, row f1 belongs to the category Polar POS and refers to the count of number of positive and negative parts-of-speech (POS) in a tweet, rows f2, f3, f4 belongs to the category Polar Other and refers to count of number of negation words, count of words that have positive and negative prior polarity, count of emoticons per polarity type, count of hashtags, capitalized words and words with exclamation marks associated with words that have prior polarity, row f5 belongs to the category Non-Polar POS and refers to counts of different parts-of-speech tags, rows f6, f7 belong to the category Non-Polar Other and refer to count of number of slangs, latin alphabets, and other words without polarity.It also relates to special terms such as the number of hashtags, URLs, targets and newlines.Row f8 belongs to the category Polar POS and captures the summation of prior polarity scores of words with POS of JJ, RB, VB and NN.Similarly, row f9 belongs to the category Polar Other and calculates the summation of prior polarity scores of all words, row f10 refers to the category Non-Polar Other and calculates the percentage of tweet that is capitalized.Finally, row f11 belongs to the category NonPolar Other and refers to presence of exclamation and presence of capitalized words as features.In this section, we present experiments and results for two classification tasks: 1) Positive versus Negative and 2) Positive versus Negative versus Neutral.For each of the classification tasks we present three models, as well as results for two combinations of these models: For the unigram plus Senti-features model, we present feature analysis to gain insight about what kinds of features are adding most value to the model.We also present learning curves for each of the models and compare learning abilities of models when provided limited data.Experimental-Set-up: For all our experiments we use Support Vector Machines (SVM) and report averaged 5-fold cross-validation test results.We tune the C parameter for SVM using an embedded 5-fold cross-validation on the training data of each fold, i.e. for each fold, we first run 5-fold cross-validation only on the training data of that fold for different values of C. We pick the setting that yields the best cross-validation error and use that C for determining test error for that fold.As usual, the reported accuracies is the average over the five folds.This is a binary classification task with two classes of sentiment polarity: positive and negative.We use a balanced data-set of 1709 instances for each class and therefore the chance baseline is 50%.We use a unigram model as our baseline.Researchers report state-of-the-art performance for sentiment analysis on Twitter data using a unigram model (Go et al., 2009; Pak and Paroubek, 2010).Table 5 compares the performance of three models: unigram model, feature based model using only 100 Senti-features, and the tree kernel model.We report mean and standard deviation of 5-fold test accuracy.We observe that the tree kernels outperform the unigram and the Senti-features by 2.58% and 2.66% respectively.The 100 Senti-features described in Table 4 performs as well as the unigram model that uses about 10,000 features.We also experiment with combination of models.Combining unigrams with Senti-features outperforms the combination of kernels with Senti-features by 0.78%.This is our best performing system for the positive versus negative task, gaining about 4.04% absolute gain over a hard unigram baseline.Table 6 presents classifier accuracy and F1measure when features are added incrementally.We start with our baseline unigram model and subsequently add various sets of features.First, we add all non-polar features (rows f5, f6, f7, f10, f11 in Table 4) and observe no improvement in the performance.Next, we add all part-of-speech based features (rows f1, f8) and observe a gain of 3.49% over the unigram baseline.We see an additional increase in accuracy by 0.55% when we add other prior polarity features (rows f2, f3, f4, f9 in Table 4).From these experiments we conclude that the most important features in Senti-features are those that involve prior polarity of parts-of-speech.All other features play a marginal role in achieving the best performing system.In fact, we experimented by using unigrams with only prior polarity POS features and achieved a performance of 75.1%, which is only slightly lower than using all Senti-features.In terms of unigram features, we use Information Gain as the attribute evaluation metric to do feature selection.In Table 7 we present a list of unigrams that consistently appear as top 15 unigram features across all folds.Words having positive or negative prior polarity top the list.Emoticons also appear as important unigrams.Surprisingly though, the word for appeared as a top feature.A preliminary analysis revealed that the word for appears as frequently in positive tweets as it does in negative tweets.However, tweets containing phrases like for you and for me tend to be positive even in the absence of any other explicit prior polarity words.Owing to previous research, the URL appearing as a top feature is less surprising because Go et al. (2009) report that tweets containing URLs tend to be positive.The learning curve for the 2-way classification task is in Figure 2.The curve shows that when limited data is used the advantages in the performance of our best performing systems is even more pronounced.This implies that with limited amount of training data, simply using unigrams has a critical disadvantage, while both tree kernel and unigram model with our features exhibit promising performance.This is a 3-way classification task with classes of sentiment polarity: positive, negative and neutral.We use a balanced data-set of 1709 instances for each class and therefore the chance baseline is 33.33%.For this task the unigram model achieves a gain of 23.25% over chance baseline.Table 8 compares the performance of our three models.We report mean and standard deviation of 5-fold test accuracy.We observe that the tree kernels outperform the unigram and the Senti-features model by 4.02% and 4.29% absolute, respectively.We note that this difference is much more pronounced comparing to the two way classification task.Once again, our 100 Senti-features perform almost as well as the unigram baseline which has about 13,000 features.We also experiment with the combination of models.For this classification task the combination of tree kernel with Senti-features outperforms the combination of unigrams with Senti-features by a small margin.This is our best performing system for the 3-way classification task, gaining 4.25% over the unigram baseline.The learning curve for the 3-way classification task is similar to the curve of the 2-way classification task, and we omit it.Table 9 presents classifier accuracy and F1measure when features are added incrementally.We start with our baseline unigram model and subsequently add various sets of features.First, we add all non-polar features (rows f5, f6, f7, f10 in Table 4) and observe an small improvement in the performance.Next, we add all part-of-speech based features and observe a gain of 3.28% over the unigram baseline.We see an additional increase in accuracy by 0.64% when we add other prior polarity features (rows f2, f3, f4, f9 in Table 4).These results are in line with our observations for the 2-way classification task.Once again, the main contribution comes from features that involve prior polarity of parts-ofspeech.The top ranked unigram features for the 3-way classification task are mostly similar to that of the 2-way classification task, except several terms with neutral polarity appear to be discriminative features, such as to, have, and so.We presented results for sentiment analysis on Twitter.We use previously proposed state-of-the-art unigram model as our baseline and report an overall gain of over 4% for two classification tasks: a binary, positive versus negative and a 3-way positive versus negative versus neutral.We presented a comprehensive set of experiments for both these tasks on manually annotated data that is a random sample of stream of tweets.We investigated two kinds of models: tree kernel and feature based models and demonstrate that both these models outperform the unigram baseline.For our feature-based approach, we do feature analysis which reveals that the most important features are those that combine the prior polarity of words and their parts-of-speech tags.We tentatively conclude that sentiment analysis for Twitter data is not that different from sentiment analysis for other genres.In future work, we will explore even richer linguistic analysis, for example, parsing, semantic analysis and topic modeling.Agarwal and Rambow are funded by NSF grant IIS-0713548.Vovsha is funded by NSF grant IIS-0916200.We would like to thank NextGen Invent (NGI) Corporation for providing us with the Twitter data.Please contact Deepak Mittal (deepak.mittal@ngicorportion.com) about obtaining the data.
Disambiguating Noun Groupings With Respect To Wordnet SensesWord groupings useful for language processing tasks are increasingly available, as thesauri appear on-line, and as distributional word clustering techniques improve.However, for many tasks, one is in relationships among word words.This paper presents a method for automatic sense disambiguation of nouns appearing within sets of related nouns — the kind of data one finds in on-line thesauri, or as the output of distributional clustering algorithms.Disambiguation is performed with respect to WordNet senses, which are fairly fine-grained; however, the method also permits the assignment of higher-level WordNet categories rather than sense labels.The method is illustrated primarily by example, though results of a more rigorous evaluation are also presented.Word groupings useful for language processing tasks are increasingly available, as thesauri appear on-line, and as distributional techniques become increasingly widespread (e.g.(Bensch and Savitch, 1992; Brill, 1991; Brown et al., 1992; Grefenstette, 1994; McKeown and Hatzivassiloglou, 1993; Pereira et al., 1993; Schtitze, 1993)).However, for many tasks, one is interested in relationships among word senses, not words.Consider, for example, the cluster containing attorney, counsel, trial, court, and judge, used by Brown et al. (1992) to illustrate a &quot;semantically sticky&quot; group of words.As is often the case where sense ambiguity is involved, we as readers impose the most coherent interpretation on the words within the group without being aware that we are doing so.Yet a computational system has no choice but to consider other, more awkward possibilities — for example, this cluster might be capturing a distributional relationship between advice (as one sense of counsel) and royalty (as one sense of court).This would be a mistake for many applications, such as query expansion in information retrieval, where a surfeit of false connections can outweigh the benefits obtained by using lexical knowledge.One obvious solution to this problem would be to extend distributional grouping methods to word senses.For example, one could construct vector representations of senses on the basis of their co-occurrence with words or with other senses.Unfortunately, there are few corpora annotated with word sense information, and computing reliable statistics on word senses rather than words will require more data, rather than less.1 Furthermore, one widely available example of a large, manually sense-tagged corpus — the WordNet group's annotated subset of the Brown corpus2 — vividly illustrates the difficulty in obtaining suitable data.lActually, this depends on the fine-grainedness of sense distinctions; clearly one could annotate corpora with very high level semantic distinctions For example, Basili et al. (1994) take such a coarse-grained approach, utilizing on the order of 10 to 15 semantic tags for a given domain.I assume throughout this paper that finer-grained distinctions than that are necessary.It is quite small, by current corpus standards (on the order of hundreds of thousands of words, rather than millions or tens of millions); the direct annotation methodology used to create it is labor intensive (Marcus et al. (1993) found that direct annotation takes twice as long as automatic tagging plus correction, for partof-speech annotation); and the output quality reflects the difficulty of the task (inter-annotator disagreement is on the order of 10%, as contrasted with the approximately 3% error rate reported for part-of-speech annotation by Marcus et al.).There have been some attempts to capture the behavior of semantic categories in a distributional setting, despite the unavailability of sense-annotated corpora.For example, Hearst and Schtitze (1993) take steps toward a distributional treatment of WordNet-based classes, using Schtitze's (1993) approach to constructing vector representations from a large co-occurrence matrix.Yarowsky's (1992) algorithm for sense disambiguation can be thought of as a way of determining how Roget's thesaurus categories behave with respect to contextual features.And my own treatment of selectional constraints (Resnik, 1993) provides a way to describe the plausibility of co-occurrence in terms of WordNet's semantic categories, using co-occurrence relationships mediated by syntactic structure.In each case, one begins with known semantic categories (WordNet synsets, Roget's numbered classes) and non-sense-annotated text, and proceeds to a distributional characterization of semantic category behavior using co-occurrence relationships.This paper begins from a rather different starting point.As in the above-cited work, there is no presupposition that sense-annotated text is available.Here, however, I make the assumption that word groupings have been obtained through some black box procedure, e.g. from analysis of unannotated text, and the goal is to annotate the words within the groupings post hoc using a knowledge-based catalogue of senses.If successful, such an approach has obvious benefits: one can use whatever sources of good word groupings are available — primarily unsupervised word clustering methods, but also on-line thesauri and the like — without folding in the complexity of dealing with word senses at the same time.3 The resulting sense groupings should be useful for a variety of purposes, although ultimately this work is motivated by the goal of sense disambiguation for unrestricted text using unsupervised methods.Let us state the problem as follows.We are given a set of words W = {w1,., /D}, with each word wz having an associated set Si = {si,i, , of possible senses.We assume that there exists some set W' C U Si, representing the set of word senses that an ideal human judge would conclude belong to the group of senses corresponding to the word grouping W. The goal is then to define a membership function co that takes si,j, wi, and W as its arguments and computes a value in [0, 1], representing the confidence with which one can state that sense si,3 belongs in sense grouping W'.4 Note that, in principle, nothing precludes the possibility that multiple senses of a word are included in WI.Example.Consider the following word group:5 burglars thief rob mugging stray robbing lookout chase crate thieves Restricting our attention to noun senses in WordNet, only lookout and crate are polysemous.Treating this word group as W, one would expect cp to assign a value of 1 to the unique senses of the monosemous words, and to assign a high value to lookout's sense as lookout, lookout man, sentinel, sentry, watch, scout: a person employed to watch for something to happen.Low (or at least lower) values of co would be expected for the senses of lookout that correspond to an observation tower, or to the activity of watching.Crate's two WordNet senses correspond to the physical object and the quantity (i.e., crateful, as in &quot;a crateful of oranges&quot;); my own intuition is that the first of these would more properly be included in HP than the second, and should therefore receive a higher value of cc, though of course neither I nor any other individual really constitutes an &quot;ideal human judge.&quot; The core of the disambiguation algorithm is a computation of semantic similarity using the WordNet taxonomy, a topic recently investigated by a number of people (Leacock and Chodorow, 1994; Resnik, 1995; Sussna, 1993).In this paper, I restrict my attention to WordNet's IS-A taxonomy for nouns, and take an approach in which semantic similarity is evaluated on the basis of the information content shared by the items being compared.The intuition behind the approach is simple: the more similar two words are, the more informative will be the most specific concept that subsumes them both.(That is, their least upper bound in the taxonomy; here a concept corresponds to a WordNet synset.)The traditional method of evaluating similarity in a semantic network by measuring the path length between two nodes (Lee et al., 1993; Rada et at, 1989) also captures this, albeit indirectly, when the semantic network is just an IS-A hierarchy: if the minimal path of IS-A links between two nodes is long, that means it is necessary to go high in the taxonomy, to more abstract concepts, in order to find their least upper bound.However, there are problems with the simple path-length definition of semantic similarity, and experiments using WordNet show that other measures of semantic similarity, such as the one employed here, provide a better match to human similarity judgments than simple path length does (Resnik, 1995).Given two words w1 and w2, their semantic similarity is calculated as where subsumers(wi, w2) is the set of WordNet synsets that subsume (i.e., are ancestors of) both w1 and w2, in any sense of either word.The concept c that maximizes the expression in (1) will be referred to as the most informative subsumer of wi and w2.Although there are many ways to associate probabilities with taxonomic classes, it is reasonable to require that concept probability be non-decreasing as one moves higher in the taxonomy; i.e., that ci c2 implies Pr(c2) > Pr(ci ).This guarantees that &quot;more abstract&quot; does indeed mean &quot;less informative,&quot; defining informativeness in the traditional way in terms of log likelihood.Probability estimates are derived from a corpus by computing where words(c) is the set of nouns having a sense subsumed by concept C. Probabilities are then computed simply as relative frequency: where N is the total number of noun instances observed.Singular and plural forms are counted as the same noun, and nouns not covered by WordNet are ignored.Although the WordNet noun taxonomy has multiple root nodes, a single, &quot;virtual&quot; root node is assumed to exist, with the original root nodes as its children.Note that by equations (1) through (3), if two senses have the virtual root node as their only upper bound then their similarity value is 0.Example.The following table shows the semantic similarity computed for several word pairs, in each case shown with the most informative subsumer.6 Probabilities were estimated using the Penn Treebank version of the Brown corpus.The pairs come from an example given by Church and Hanks (1989), illustrating the words that human subjects most frequently judged as being associated with the word doctor.(The word sick also appeared on the list, but is excluded here because it is not a noun.)Word 1 Word 2 Similarity Most Informative Subsumer doctor nurse 9.4823 (health professional) doctor lawyer 7.2240 (professional person) doctor man 2.9683 (person, individual) doctor medicine 1.0105 (entity) doctor hospital 1.0105 (entity) doctor health 0.0 virtual root doctor sickness 0.0 virtual root Doctors are minimally similar to medicine and hospitals, since these things are all instances of &quot;something having concrete existence, living or nonliving&quot; (WordNet class (entity)), but they are much more similar to lawyers, since both are kinds of professional people, and even more similar to nurses, since both are professional people working specifically within the health professions.Notice that similarity is a more specialized notion than association or relatedness: doctors and sickness may be highly associated, but one would not judge them to be particularly similar.The disambiguation algorithm for noun groups is inspired by the observation that when two polysemous words are similar, their most informative subsumer provides information about which sense of each word is the relevant one.In the above table, for example, both doctor and nurse are polysemous: WordNet records doctor not only as a kind of health professional, but also as someone who holds a Ph.D., and nurse can mean not only a health professional but also a nanny.When the two words are considered together, however, the shared element of meaning for the two relevant senses emerges in the form of the most informative subsumer.It may be that other pairings of possible senses also share elements of meaning (for example, doctor/Ph.D. and nurse/nanny are both descendants of (person, individual)).However, in cases like those illustrated above, the more specific or informative the shared ancestor is, the more strongly it suggests which senses come to mind when the words are considered together.The working hypothesis in this paper is that this holds true in general.Turning that observation into an algorithm requires two things: a way to assign credit to word senses based on similarity with co-occurring words, and a tractable way to generalize to the case where more than two polysemous words are involved.The algorithm given in Figure 1 does both quite straightforwardly.Algorithm.Given W = {w[1], , w[n]l, a set of nouns: for i and j = lion, with i <j v[i, j) = sim(w[i], w[j]) c[i, j] = the most informative subsumer for w[i] and w[j] fork = 1 to num_senses(w[i]) if c[i, j] is an ancestor of sense[i, increment support[i, k] by v[i, j] fork' = 1 to num_senses(wW) if c[i, j] is an ancestor of sense[i, k'] increment support[j, k'] by v Ei, increment normalization[i] by v[i, j] increment normalization[j] by vii, j] This algorithm considers the words in W pairwise, avoiding the tractability problems in considering all possible combinations of senses for the group (0 (ma) if each word had m senses).For each pair considered, the most informative subsumer is identified, and this pair is only considered as supporting evidence for those senses that are descendants of that concept.Notice that by equation (1), support [i, k] is a sum of log probabilities, and therefore preferring senses with high support is equivalent to optimizing a product of probabilities.Thus considering words pairwise in the algorithm reflects a probabilistic independence assumption.Example.The most informative subsumer for doctor and nurse is (health professional), and therefore that pairing contributes support to the sense of doctor as an M.D., but not a Ph.D.Similarly, it contributes support to the sense of nurse as a health professional, but not a nanny.The amount of support contributed by a pairwise comparison is proportional to how informative the most informative subsumer is.Therefore the evidence for the senses of a word will be influenced more by more similar words and less by less similar words.By the time this process is completed over all pairs, each sense of each word in the group has had the potential of receiving supporting evidence from a pairing with every other word in the group.The value assigned to that sense is then the proportion of support it did receive, out of the support possible.(The latter is kept track of by array normalization in the pseudocode.)Discussion.The intuition behind this algorithm is essentially the same intuition exploited by Lesk (1986), Sussna (1993), and others: the most plausible assignment of senses to multiple co-occurring words is the one that maximizes relatedness of meaning among the senses chosen.Here I make an explicit comparison with Sussna's approach, since it is the most similar of previous work.Sussna gives as an example of the problem he is solving the following paragraph from the corpus of 1963 Time magazine articles used in information retrieval research (uppercase in the Time corpus, lowercase here for readability; punctuation is as it appears in the original corpus): the allies after nassau in december 1960, the u.s . first proposed to help nato develop its own nuclear strike force. but europe made no attempt to devise a plan. last week, as they studied the nassau accord between president kennedy and prime minister macmillan, europeans saw emerging the first outlines of the nuclear nato that the u.s . wants and will support. it all sprang from the anglo-u.s . crisis over cancellation of the bug-ridden skybolt missile, and the u.s . offer to supply britain and france with the proved polaris (time, dec .28) From this, Sussna extracts the following noun grouping to disambiguate: allies strike force attempt plan week accord president prime minister outlines support crisis cancellation bug missile france polaris time These are the non-stopword nouns in the paragraph that appear in WordNet (he used version 1.2).The description of Sussna's algorithm for disambiguating noun groupings like this one is similar to the one proposed here, in a number of ways: relatedness is characterized in terms of a semantic network (specifically WordNet); the focus is on nouns only; and evaluations of semantic similarity (or, in Sussna's case, semantic distance) are the basis for sense selection.However, there are some important differences, as well.First, unlike Sussna's proposal, this algorithm aims to disambiguate groupings of nouns already established (e.g. by clustering, or by manual effort) to be related, as opposed to groupings of nouns that happen to appear near each other in running text (which may or may not reflect relatedness based on meaning).This provides some justification for restricting attention to similarity (reflected by the scaffolding of IS-A links in the taxonomy), as opposed to the more general notion of association.Second, this difference is reflected algorithmically by the fact that Sussna uses not only IS-A links but also other WordNet links such as PART-OF.Third, unlike Sussna's algorithm, the semantic similarity/distance computation here is not based on path length, but on information content, a choice that I have argued for elsewhere (Resnik, 1993; Resnik, 1995).Fourth, the combinatorics are handled differently: Sussna explores analyzing all sense combinations (and living with the exponential complexity), as well as the alternative of sequentially &quot;freezing&quot; a single sense for each of wi , , w2_1 and using those choices, assumed to be correct, as the basis for disambiguating wi.The algorithm presented here falls between those two alternatives.A final, important difference between this algorithm and previous algorithms for sense disambiguation is that it offers the possibility of assigning higher-level WordNet categories rather than lowest-level sense labels.It is a simple modification to the algorithm to assign values of co not only to synsets directly containing words in W, but to any ancestors of those synsets — one need only let the list of synsets associated with each word wi (i.e, Si in the problem statement of Section 2.1) also include any synset that is an ancestor of any synset containing word wi.Assuming that num senses (w[i] ) and s ens e [ , k] are reinterpreted accordingly, the algorithm will compute co not only for the synsets directly including words in W, but also for any higher-level abstractions of them.Example.Consider the word group doctor, nurse, lawyer.If one were to include all subsuming concepts for each word, rather than just the synsets of which they are directly members, the concepts with non-zero values of co would be as follows: Given assignments of co at all levels of abstraction, one obvious method of semantic annotation is to assign the highest-level concept for which co is at least as large as the sense-specific value of cp.For instance, in the previous example, one would assign the annotation (health professional) to both doctor and nurse (thus explicitly capturing a generalization about their presence in the word group, at the appropriate level of abstraction), and the annotation (professional) to lawyer.In this section I present a number of examples for evaluation by inspection.In each case, I give the source of the noun grouping, the grouping itself, and for each word a description of word senses together with their values of y).Distributional cluster (Brown et al., 1992): head, body, hands, eye, voice, arm, seat, hair, mouth As noted in Section 2.1, this group represents a set of words similar to burglar, according to Schtitze's method for deriving vector representation from corpus behavior.In this case, words rob and robbing were excluded because they were not nouns in WordNet.The word stray probably should be excluded also, since it most likely appears on this list as an adjective (as in &quot;stray bullet&quot;).Machine-generated thesaurus entry (Grefenstette, 1994): method, test, mean, procedure, technique I chose this grouping at random from a thesaurus created automatically by Grefenstette's syntacticodistributional methods, using the MED corpus of medical abstracts as its source.The group comes from from the thesaurus entry for the word method.Note that mean probably should be means.There is a tradition in sense disambiguation of taking particularly ambiguous words and evaluating a system's performance on those words.Here I look at one such case, the word line; the goal is to see what sense the algorithm chooses when considering the word in the contexts of each of the Roget's Thesaurus classes in which it appears, where a &quot;class&quot; includes all the nouns in one of the numbered categories.7 The following list provides brief descriptions of the 25 senses of line in WordNet: Since line appears in 13 of the numbered categories in Roget's thesaurus, a full description of the values of co would be too large for the present paper.Indeed, showing all the nouns in the numbered categories would take up too much space: they average about 70 nouns apiece.Instead, I identify the numbered category, and give the three WordNet senses of line for which co was greatest.Qualitatively, the algorithm does a good job in most of the categories.The reader might find it an interesting exercise to try to decide which of the 25 senses he or she would choose, especially in the cases where the algorithm did less well (e.g. categories #200, #203, #466).The previous section provided illustrative examples, demonstrating the performance of the algorithm on some interesting cases.In this section, I present experimental results using a more rigorous evaluation methodology.Input for this evaluation came from the numbered categories of Roget's.Test instances consisted of a noun group (i.e., all the nouns in a numbered category) together with a single word in that group to be disambiguated.To use an example from the previous section, category #590 (&quot;Writing&quot;) contains the following: writing, chirography, penmanship, quill driving, typewriting, writing, manuscript, MS, these presents, stroke of the pen, dash of the pen, coupe de plume, line, headline, pen and ink, letter, uncial writing, cuneiform character, arrowhead, Ogham, Runes, hieroglyphic, contraction, Devanagari, Nagari, script, shorthand, stenography, secret writing, writing in cipher, cryptography, stenography, copy, transcript, rescript, rough copy, fair copy, handwriting, signature, sign manual, autograph, monograph, holograph, hand, fist, calligraphy, good hand, running hand, flowing hand, cursive hand, legible hand, bold hand, bad hand, crampedhand, crabbed hand, illegible hand, scribble, ill-formed letters, pothooks and hangers, stationery, pen, quill, goose quill, pencil, style, paper, foolscap, parchment, vellum, papyrus, tablet, slate, marble, pillar, table, blackboard, ink bottle, ink horn, ink pot, ink stand, ink well, typewriter, transcription, inscription, superscription, graphology, composition, authorship, writer, scribe, amanuensis, scrivener, secretary, clerk, penman, copyist, transcriber, quill driver, stenographer, typewriter, typist, writer for the press Any word or phrase in that group that appears in the noun taxonomy for WordNet would be a candidate as a test instance — for example, line, or secret writing.The test set, chosen at random, contained 125 test cases.(Note that because of the random choice, there were some cases where more than one test instance came from the same numbered category.)Two human judges were independently given the test cases to disambiguate.For each case, they were given the full set of nouns in the numbered category (as shown above) together with descriptions of the WordNet senses for the word to be disambiguated (as, for example, the list of 25 senses for line given in the previous section, though thankfully few words have that many senses!).It was a forced-choice task; that is, the judge was required to choose exactly one sense.In addition, for each judgment, the judge was required to provide a confidence value for this decision, ranging from 0 (not at all confident) to 4 (highly confident).Results are presented here individually by judge.For purposes of evaluation, test instances for which the judge had low confidence (i.e. confidence ratings of 0 or 1) were excluded.For Judge 1, there were 99 test instances with sufficiently high confidence to be considered.As a baseline, ten runs were done selecting senses by random choice, with the average percent correct being 34.8%, standard deviation 3.58.As an upper bound, Judge 2 was correct on 65.7% of those test instances.The disambiguation algorithm shows considerable progress toward this upper bound, with 58.6% correct.For Judge 2, there were 86 test instances with sufficiently high confidence to be considered.As a baseline, ten runs were done selecting senses by random choice, with the average percent correct being 33.3%, standard deviation 3.83.As an upper bound, Judge 1 was correct on 68.6% of those test instances.Again, the disambiguation algorithm performs well, with 60.5% correct.The results of the evaluation are extremely encouraging, especially considering that disambiguating word senses to the level of fine-grainedness found in WordNet is quite a bit more difficult than disambiguation to the level of homographs (Hearst, 1991; Cowie et al., 1992).A note worth adding: it is not clear that the &quot;exact match&quot; criterion — that is, evaluating algorithms by the percentage of exact matches of sense selection against a human-judged baseline — is the right task.In particular, in many tasks it is at least as important to avoid inappropriate senses than to select exactly the right one.This would be the case in query expansion for information retrieval, for example, where indiscriminately adding inappropriate words to a query can degrade performance (Voorhees, 1994).The examples presented in Section 3 are encouraging in this regard: in addition to performing well at the task of assigning a high score to the best sense, it does a good job of assigning low scores to senses that are clearly inappropriate.Regardless of the criterion for success, the algorithm does need further evaluation.Immediate plans include a larger scale version of the experiment presented here, involving thesaurus classes, as well as a similarly designed evaluation of how the algorithm fares when presented with noun groups produced by distributional clustering.In addition, I plan to explore alternative measures of semantic similarity, for example an improved variant on simple path length that has been proposed by Leacock and Chodorow (1994).Ultimately, this algorithm is intended to be part of a suite of techniques used for disambiguating words in running text with respect to WordNet senses.I would argue that success at that task will require combining knowledge of the kind that WordNet provides, primarily about relatedness of meaning, with knowledge of the kind best provided by corpora, primarily about usage in context.The difficulty with the latter kind of knowledge is that, until now, the widespread success in characterizing lexical behavior in terms of distributional relationships has applied at the level of words — indeed, word forms — as opposed to senses.This paper represents a step toward getting as much leverage as possible out of work within that paradigm, and then using it to help determine relationships among word senses, which is really where the action is.
Cascaded Grammatical Relation AssignmentIn this paper we discuss cascaded Memory- Based grammatical relations assignment.In the first stages of the cascade, we find chunks of several types (NP,VP,ADJP,ADVP,PP) and label them with their adverbial function (e.g. local, temporal).In the last stage, we assign grammatical relations to pairs of chunks.We studied the effect of adding several levels to this cascaded classifier and we found that even the less performing chunkers enhanced the performance of the relation finder.When dealing with large amounts of text, finding structure in sentences is often a useful preprocessing step.Traditionally, full parsing is used to find structure in sentences.However, full parsing is a complex task and often provides us with more information then we need.For many tasks detecting only shallow structures in a sentence in a fast and reliable way is to be preferred over full parsing.For example, in information retrieval it can be enough to find only simple NPs and VPs in a sentence, for information extraction we might also want to find relations between constituents as for example the subject and object of a verb.In this paper we discuss some Memory-Based (MB) shallow parsing techniques to find labeled chunks and grammatical relations in a sentence.Several MB modules have been developed in previous work, such as: a POS tagger (Daelemans et al., 1996), a chunker (Veenstra, 1998; Tjong Kim Sang and Veenstra, 1999) and a grammatical relation (GR) assigner (Buchholz, 1998).The questions we will answer in this paper are: Can we reuse these modules in a cascade of classifiers?What is the effect of cascading?Will errors at a lower level percolate to higher modules?Recently, many people have looked at cascaded and/or shallow parsing and OR assignment.Abney (1991) is one of the first who proposed to split up parsing into several cascades.He suggests to first find the chunks and then the dependecies between these chunks.Crefenstette (1996) describes a cascade of finite-state transducers, which first finds noun and verb groups, then their heads, and finally syntactic functions.Brants and Skut (1998) describe a partially automated annotation tool which constructs a complete parse of a sentence by recursively adding levels to the tree.(Collins, 1997; Ratnaparkhi, 1997) use cascaded processing for full parsing with good results.Argamon et at.(1998) applied Memory-Based Sequence Learning (MBSL) to NP chunking and subject/object identification.However, their subject and object finders are independent of their chunker (i.e. not cascaded).Drawing from this previous work we will explicitly study the effect of adding steps to the grammatical relations assignment cascade.Through experiments with cascading several classifiers, we will show that even using imperfect classifiers can improve overall performance of the cascaded classifier.We illustrate this claim on the task of finding grammatical relations (e.g. subject, object, locative) to verbs in text.The CR assigner uses several sources of information step by step such as several types of XP chunks (NP, VP, PP, ADJP and ADVP), and adverbial functions assigned to these chunks (e.g. temporal, local).Since not all of these entities are predicted reliably, it is the question whether each source leads to an improvement of the overall GR assignment.In the rest of this paper we will first briefly describe Memory-Based Learning in Section 2.In Section 3.1, we discuss the chunking classifiers that we later use as steps in the cascade.Section 3.2 describes the basic GR classifier.Section 3.3 presents the architecture and results of the cascaded GR assignment experiments.We discuss the results in Section 4 and conclude with Section 5.Memory-Based Learning (MBL) keeps all training data in memory and only abstracts at classification time by extrapolating a class from the most similar item(s) in memory.In recent work Daelemans et at.(1999b) have shown that for typical natural language processing tasks, this approach is at an advantage because it also &quot;remembers&quot; exceptional, low-frequency cases which are useful to extrapolate from.Moreover, automatic feature weighting in the similarity metric of an MB learner makes the approach well-suited for domains with large numbers of features from heterogeneous sources, as it embodies a smoothing-by-similarity method when data is sparse (Zavrel and Daelemans, 1997).We have used the following MBL algorithms': test item and each memory item is defined as the number of features for which they have a different value (overlap metric).IB1-IG : IB1 with information gain (an information-theoretic notion measuring the reduction of uncertainty about the class to be predicted when knowing the value of a feature) to weight the cost of a feature value mismatch during comparison.IGTree : In this variant, a decision tree is created with features as tests, and ordered according to the information gain of the features, as a heuristic approximation of the computationally more expensive IB1 variants.For more references and information about these algorithms we refer to (Daelemans et al., 1998; Daelemans et al., 1999b).For other memory-based approaches to parsing, see (Bod, 1992) and (Sekine, 1998).In this section we describe the stages of the cascade.The very first stage consists of a MemoryBased Part-of-Speech Tagger (MBT) for which we refer to (Daelemans et al., 1996).The next three stages involve determining boundaries and labels of chunks.Chunks are nonrecursive, non-overlapping constituent parts of sentences (see (Abney, 1991)).First, we simultaneously chunk sentences into: NP-, VP: Prep-, ADJP- and APVP-chunks.As these chunks are non-overlapping, no words can belong to more than one chunk, and thus no conflicts can arise.Prep-chunks are the prepositional part of PPs, thus excluding the nominal part.Then we join a Prep-chunk and one — or more coordinated — NP-chunks into a PPchunk.Finally, we assign adverbial function (ADVFUNC) labels (e.g. locative or temporal) to all chunks.In the last stage of the cascade, we label several types of grammatical relations between pairs of words in the sentence.The data for all our experiments was extracted from the Penn Treebank II Wall Street Journal (WSJ) corpus (Marcus et al., 1993).For all experiments, we used sections 00-19 as training material and 20-24 as test material.See Section 4 for results on other train/test set splittings.For evaluation of our results we use the precision and recall measures.Precision is the percentage of predicted chunks/relations that are actually correct, recall is the percentage of correct chunks/relations that are actually found.For convenient comparisons of only one value, we also list the Fo---1 value (C.J.van Rijsbergen, 1979).(i32+1)pree.rec with /3 1 In the first experiment described in this section, the task is to segment the sentence into chunks and to assign labels to these chunks.This process of chunking and labeling is carried out by assigning a tag to each word in a sentence leftto-right.Ramshaw and Marcus (1995) first assigned a chunk tag to each word in the sentence: I for inside a chunk, 0 for outside a chunk, and B for inside a chunk, but the preceding word is in another chunk.As we want to find more than one kind of chunk, we have to further differentiate the JOB tags as to which kind of chunk (NP, VP, Prep, ADJP or ADVP) the word is in.With the extended JOB tag set at hand we can tag the sentence: After having found Prep-, NP- and other chunks, we collapse Preps and NPs to PPs in a second step.While the GR assigner finds relations between VPs and other chunks (cf.Section 3.2), the PP chunker finds relations between prepositions and NPs 2 in a way similar to OR. assignment (see Section 3.2).In the last chunking/labeling step, we assign adverbial functions to chunks.The classes are the adverbial function labels from the treebank: LOC (locative), TMP (temporal), DIR.(directional), PRP (purpose and reason), MNR (manner), EXT (extension) or &quot;2 for none of the former.Table 1 gives an overview of the results of the chunking-labeling experiments, using the following algorithms, determined by validation on the train set: IBI-IG for XP-chunking and IGTree for PP-chunking and ADVFUNCs assignment.In grammatical relation assignment we assign a GR to pairs of words in a sentence.In our 2PPs containing anything else than NPs (e.g.'without bringing his wife) are not searched for. ments.NP-,VP-, ADJP-, ADVP- and Prepchunks are found simultaneously, but for convenience, precision and recall values are given separately for each type of chunk. experiments, one of these words is always a verb, since this yields the most important GRs.The other word is the head of the phrase which is annotated with this grammatical relation in the treebank.A preposition is the head of a PP, a noun of an NP and so on.Defining relations to hold between heads means that the algorithm can, for example, find a subject relation between a noun and a verb without necessarily having to make decisions about the precise boundaries of the subject NP.Suppose we had the POS-tagged sentence shown in Figure 1 and we wanted the algorithm to decide whether, and if so how, Miller (henceforth: the focus) is related to the first verb organized.We then construct an instance for this pair of words by extracting a set of feature values from the sentence.The instance contains information about the verb and the focus: a feature for the word form and a feature for the POS of both.It also has similar features for the local context of the focus.Experiments on the training data suggest an optimal context width of two elements to the left and one to the right.In the present case, elements are words or punctuation signs.In addition to the lexical and the local context information, we include superficial information about clause structure: The first feature indicates the distance from the verb to the focus, counted in elements.A negative distance means that the focus is to the left of the verb.The second feature contains the number of other verbs between the verb and the focus.The third feature is the number of intervening commas.The features were chosen by manual 6-7, 8-9 and 12-13 describe the context words, Features 10-11 the focus word.Empty contexts are indicated by the value &quot;-&quot; for all features.&quot;feature engineering&quot;.Table 2 shows the complete instance for Miller-organized in row 5, together with the other first four instances for the sentence.The class is mostly &quot;-&quot;, to indicate that the word does not have a direct grammatical relation to organized.Other possible classes are those from a list of more than 100 different labels found in the treebank.These are combinations of a syntactic category and zero, one or more functions, e.g.NP-SBJ for subject, NP-PRD for predicative object, NP for (in)direct object3, PP-LOC for locative PP adjunct, PP-LOC-CLR for subcategorised locative PP, etcetera.According to their information gain values, features are ordered with decreasing importance as follows: 11,13, 10, 1, 2, 8, 12, 9, 6 , 4 , 7 , 3 , 5.Intuitively,. this ordering makes sense.The most important feature is the POS of the focus, because this determines whether it can have a GR to a verb at all (fninctuation cannot) and what kind of relation is possible.The POS of the following word is important, because e.g. a noun followed by a noun is probably not the head of an NP and will therefore not have a direct GR to the verb.The word itself may be important if it is e.g. a preposition, a pronoun or a clearly temporal/local adverb.Features 1 and 2 give some indication of the complexity of the structure intervening between the focus and the verb.The more complex this structure, the lower the probability that the focus and the verb are related.Context further away is less important than near context.To test the effects of the chunking steps from Section 3.1 on this task, we will now construct instances based on more structured input text, like that in Figure 2.This time, the focus is described by five features instead of two, for the additional information: which type of chunk it is in, what, the preposition is if it is in a PP chunk, and what the adverbial function is, if any.We still have a context of two elements left, one right, but elements are now defined to be either chunks, or words outside any chunk, or punctuation.Each chunk in the context is represented by its last word (which is the semantically most important word in most cases), by the POS of the last word, and by the type of chunk.The distance feature is adapted to the new definition of element, too, and instead of counting intervening verbs, we now count intervening VP chunks.Figure 3 shows the first five instances for the sentence in Figure 2.Class value&quot; &quot; again means the focus is not directly related to the verb&quot; (but to some other verb or a non-verbal element).According to their information gain values, features are ordered in decreasing importance as follows: 16, 15, 12, 14, 11, 2, 1, 19, 10, 9, 13, 18, 6, 17, 8, 4, 7, 3, 5.Comparing this to the earlier feature ordering, we see that most of the new features are distance and intervening VPs and commas.Features 4 and 5 show the verb and its POS.Features 6-8, 9-11 and 17-19 describe the context words/chunks, Features 12-16 the focus chunk.Empty contexts are indicated by the &quot;-&quot; for all features. very important, thereby justifying their introduction.Relative to the other &quot;old&quot; features, the structural features 1 and 2 have gained importance, probably because more structure is available in the input to represent.In principle, we would have to construct one instance for each possible pair of a verb and a focus word in the sentence.However, we restrict instances to those where there is at most one other verb/VP chunk between the verb and the focus, in case the focus precedes the verb, and no other verb in case the verb precedes the focus.This restriction allows, for example, for a relative clause on the subject (as in our example sentence).In the training data, 97.9% of the related pairs fulfill this condition (when counting VP chunks).Experiments on the training data showed that increasing the admitted number of intervening VP chunks slightly increases recall, at the cost of precision.Having constructed all instances from the test data and from a training set with the same level of partial structure, we first train the IGTree algorithm, and then let it classify the test instances.Then, for each test instance that was classified with a grammatical relation, we check whether the same verb-focuspair appears with the same relation in the GR list extracted directly from the treebank.This gives us the precision of the classifier.Checking the treebank list versus the classified list yields We have already seen from the example that the level of structure in the input text can influence the composition of the instances.We are interested in the effects of different sorts of partial structure in the input data on the classification performance of the final classifier.Therefore, we ran a series of experiments.The classification task was always that of finding grammatical relations to verbs and performance was always measured by precision and recall on those relations (the test set contained 45825 relations).The amount of structure in the input data varied.Table 4 shows the results of the experiments.In the first experiment, only POS tagged input is used.Then, NP chunks are added.Other sorts of chunks are inserted at each subsequent step.Finally, the adverbial function labels are added.We can see that the more structure we add, the better precision and recall of the grammatical relations get: precision increases from 60.7% to 74.8%, recall from 41.3% to 67.9%.This in spite of the fact that the added information is not always correct, because it was predicted for the test material on the basis of the training material by the classifiers described in Section 3.1.As we have seen in Table 1, especially ADJP and ADVP chunks and adverbial function labels did not have very high precision and recall.There are three ways how two cascaded modules can interact.• The first module can add information on which the later module can (partially) base its decisions.This is the case between the adverbial functions finder and the relations finder.The former adds an extra informative feature to the instances of the latter (Feature 16 in Table 3).Cf. column two of Table 4.• The first module can restrict the number of decisions to be made by the second one.This is the case in the combination of the chunking steps and the relations finder.Without the chunker, the relations finder would have to decide for every word, whether it is the head of a constituent that bears a relation to the verb.With the churlker., the relations finder has to make this decision for fewer words, namely only for those which are the last word in a chunk resp. the preposition of a PP chunk.Practically, this reduction of the number of decisions (which translates into a reduction of instances) as can be seen in the third column of Table 4.• The first module can reduce the number of elements used for the instances by counting one chunk as just one context element.We can see the effect in the feature that indicates the distance in elements between the focus and the verb.The more chunks are used, the smaller the average absolute distance (see column four Table 4).All three effects interact in the cascade we describe.The PP chunker reduces the number of decisions for the relations finder (instead of one instance for the preposition and one for the NP chunk, we get only one instance for the PP chunk-). introduces an extra feature (Feature 12 in Table 3), and changes the context (instead of a preposition and an NP, context may now be one PP).As we already noted above, precision and recall are monotonically increasing when adding more structure.However, we note large differences, such as NP chunks which increase Fs_i by more than 10%, and VP chunks which add another 6.8%, whereas ADVPs and ADJPs yield hardly any improvement.This may partially be explained by the fact that these chunks are less frequent than the former two.Preps, on the other hand, while hardly reducing the average distance or the number of instances, improve 1,3-1 by nearly 1%.PPs yield another 1.1%.What may come as a surprise is that adverbial functions again increase F,3,1 by nearly 2%, despite the fact that F1 for this ADVFTJNC assignment step was not very high.This result shows that cascaded modules need not be perfect to be useful.Up to now, we only looked at the overall results.Table 4 also shows individual Fp_1 values for four selected common grammatical relations: subject NP, (in)direct object NP, locative PP adjunct and temporal PP adjunct.Note that the steps have different effects on the different relations: Adding NPs increases Fp=i by 11.3% for subjects resp.16.2% for objects, but only 3.9% resp.3.7% for locatives and temporals.Adverbial functions are more important for the two adjuncts (+6.3% resp.+15%) than for the two complements (+0.2% resp.+0.7%).Argamon et al. (1998) report F13=1 for subject and object identification of respectively 86.5% and 83.0%, compared to 81.8% and 81.0% in this paper.Note however that Argamon et al. (1998) do not identify the head of subjects, subjects in embedded clauses, or subjects and objects related to the verb only through a trace, which makes their task easier.For a detailed comparison of the two methods on the same task see (Daelemans et al., 1999a).That paper also shows that the chunking method proposed here performs about as well as other methods, and that the influence of tagging errors on (NP) chunking is less than 1%.To study the effect of the errors in the lower modules other than the tagger, we used &quot;perfect&quot; test data in a last experiment, i.e. data annotated with partial information taken directly from the treebank.The results are shown in Table 5.We see that later modules suffer from errors of earlier modules (as could be expected): Fp_1 of PP chunking is 92% but could have previous modules in the cascade) vs. on &quot;perfect&quot; input (enriched with partial treebank annotation).For PPs, this means perfect POS tags and chunk labels/boundaries, for ADVFUNC additionally perfect PP chunks, for GR assignment also perfect ADVFUNC labels. been 97.9% if all previous chunks would have been correct (+5.9%).For adverbial functions, the difference is 3.5%.For grammatical relation assignment, the last module in the cascade, the difference is, not surprisingly, the largest: 7.9% for chunks only, 12.3% for chunks and ADVFUNCs.The latter percentage shows what could maximally be gained by further improving the chunker and ADVFUNCs finder.On realistic data, a realistic ADVFUNCs finder improves CR assigment by 1.9%.On perfect data, a perfect ADVFUNCs finder increases performance by 6.3%.In this paper we studied cascaded grammatical relations assignment.We showed that even the use of imperfect modules improves the overall result of the cascade.In future research we plan to also train our classifiers on imperfectly chunked material.This enables the classifier to better cope with systematic errors in train and test material.We expect that especially an improvement of the adverbial function assignment will lead to better OR assignment.Finally, since cascading proved effective for GR. assignment we intend to study the effect of cascading different types of XP chunkers on chunking performance.We might e.g. first find ADJP chunks, then use that chunker's output as additional input for the NP chunker, then use the combined output as input to the VP chunker and so on.Other chunker orderings are possible, too.Likewise, it might be better to find different grammatical relations subsequently, instead of simultaneously.

The Ups and Downs of Preposition Error Detection in ESL WritingIn this paper we describe a methodologyfor detecting preposition errors in the writ ing of non-native English speakers.Our system performs at 84% precision andclose to 19% recall on a large set of stu dent essays.In addition, we address the problem of annotation and evaluation inthis domain by showing how current ap proaches of using only one rater can skew system evaluation.We present a sampling approach to circumvent some of the issuesthat complicate evaluation of error detec tion systems.The long-term goal of our work is to develop asystem which detects errors in grammar and us age so that appropriate feedback can be given to non-native English writers, a large and growing segment of the world?s population.Estimates arethat in China alone as many as 300 million people are currently studying English as a second lan guage (ESL).Usage errors involving prepositions are among the most common types seen in thewriting of non-native English speakers.For ex ample, (Izumi et al, 2003) reported error rates for English prepositions that were as high as 10% ina Japanese learner corpus.Errors can involve incorrect selection (?we arrived to the station?), ex traneous use (?he went to outside?), and omission (?we are fond null beer?).What is responsiblefor making preposition usage so difficult for non native speakers?c ? 2008.Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.At least part of the difficulty seems to be due tothe great variety of linguistic functions that prepositions serve.When a preposition marks the argument of a predicate, such as a verb, an adjective, or a noun, preposition selection is con strained by the argument role that it marks, thenoun which fills that role, and the particular predi cate.Many English verbs also display alternations (Levin, 1993) in which an argument is sometimes marked by a preposition and sometimes not (e.g., ?They loaded the wagon with hay?/ ?They loaded hay on the wagon?).When prepositions introduceadjuncts, such as those of time or manner, selec tion is constrained by the object of the preposition (?at length?, ?in time?, ?with haste?).Finally, the selection of a preposition for a given context also depends upon the intended meaning of the writer (?we sat at the beach?, ?on the beach?, ?near the beach?, ?by the beach?).With so many sources of variation in Englishpreposition usage, we wondered if the task of se lecting a preposition for a given context might prove challenging even for native speakers.To investigate this possibility, we randomly selected200 sentences from Microsoft?s Encarta Encyclopedia, and, in each sentence, we replaced a ran domly selected preposition with a blank line.We then asked two native English speakers to perform a cloze task by filling in the blank with the best preposition, given the context provided by the rest of the sentence.Our results showed only about75% agreement between the two raters, and be tween each of our raters and Encarta.The presence of so much variability in prepo sition function and usage makes the task of thelearner a daunting one.It also poses special chal lenges for developing and evaluating an NLP error detection system.This paper addresses both the 865 development and evaluation of such a system.First, we describe a machine learning system that detects preposition errors in essays of ESL writers.To date there have been relatively few attempts to address preposition error detection,though the sister task of detecting determiner errors has been the focus of more research.Our system performs comparably with other leading sys tems.We extend our previous work (Chodorow etal., 2007) by experimenting with combination fea tures, as well as features derived from the Google N-Gram corpus and Comlex (Grishman et al, 1994).Second, we discuss drawbacks in current meth ods of annotating ESL data and evaluating errordetection systems, which are not limited to prepo sition errors.While the need for annotation by multiple raters has been well established in NLP tasks (Carletta, 1996), most previous work in error detection has surprisingly relied on only one raterto either create an annotated corpus of learner errors, or to check the system?s output.Some grammatical errors, such as number disagreement be tween subject and verb, no doubt show very highreliability, but others, such as usage errors involv ing prepositions or determiners are likely to be much less reliable.Our results show that relyingon one rater for system evaluation can be problem atic, and we provide a sampling approach which can facilitate using multiple raters for this task.In the next section, we describe a system that automatically detects errors involving incorrect preposition selection (?We arrived to the station?)and extraneous preposition usage (?He went to outside?).In sections 3 and 4, we discuss theproblem of relying on only one rater for exhaus tive annotation and show how multiple raters can be used more efficiently with a sampling approach.Finally, in section 5 we present an analysis of com mon preposition errors that non-native speakers make.2.1 Model.We have used a Maximum Entropy (ME) classi fier (Ratnaparkhi, 1998) to build a model of correctpreposition usage for 34 common English prepo sitions.The classifier was trained on 7 million preposition contexts extracted from parts of the MetaMetrics Lexile corpus that contain textbooks and other materials for high school students.Each context was represented by 25 features consisting of the words and part-of-speech (POS) tags found in a local window of +/- two positions around the preposition, plus the head verb of the preceding verb phrase (PV), the head noun of the precedingnoun phrase (PN), and the head noun of the following noun phrase (FH), among others.In analyzing the contexts, we used only tagging and heuris tic phrase-chunking, rather than parsing, so as to avoid problems that a parser might encounter with ill-formed non-native text 1 . In test mode, the clas-.sifier was given the context in which a preposition occurred, and it returned a probability for each of the 34 prepositions.2.2 Other Components.While the ME classifier constitutes the core of thesystem, it is only one of several processing com ponents that refines or blocks the system?s output.Since the goal of an error detection system is to provide diagnostic feedback to a student, typically a system?s output is heavily constrained so that it minimizes false positives (i.e., the system tries toavoid saying a writer?s preposition is used incor rectly when it is actually right), and thus does not mislead the writer.Pre-Processing Filter: A pre-processing pro gram skips over preposition contexts that contain spelling errors.Classifier performance is poor in such cases because the classifier was trained on well-edited text, i.e., without misspelled words.Inthe context of a diagnostic feedback and assess ment tool for writers, a spell checker would first highlight the spelling errors and ask the writer tocorrect them before the system analyzed the prepo sitions.Post-Processing Filter: After the ME clas sifier has output a probability for each of the 34prepositions but before the system has made its fi nal decision, a series of rule-based post-processingfilters block what would otherwise be false posi tives that occur in specific contexts.The first filter prevents the classifier from marking as an error acase where the classifier?s most probable preposi tion is an antonym of what the writer wrote, such as ?with/without?and ?from/to?.In these cases, resolution is dependent on the intent of the writerand thus is outside the scope of information cap 1 For an example of a common ungrammatical sentence from our corpus, consider: ?In consion, for some reasons,museums, particuraly known travel place, get on many peo ple.?866 tured by the current feature set.Another problem for the classifier involves differentiating between certain adjuncts and arguments.For example, in the sentence ?They described a part for a kid?, thesystem?s top choices were of and to.The benefac tive adjunct introduced by for is difficult for theclassifier to learn, perhaps because it so freely occurs in many locations within a sentence.A post processing filter prevents the system from marking as an error a prepositional phrase that begins with for and has an object headed by a human noun (a WordNet hyponym of person or group).Extraneous Use Filter: To cover extraneous use errors, we developed two rule-based filters: 1) Plural Quantifier Constructions, to handle casessuch as ?some of people?and 2) Repeated Prepo sitions, where the writer accidentally repeated the same preposition two or more times, such as ?canfind friends with with?.We found that extrane ous use errors usually constituted up to 18% of all preposition errors, and our extraneous use filters handle a quarter of that 18%.Thresholding: The final step for the preposi tion error detection system is a set of thresholds that allows the system to skip cases that are likely to result in false positives.One of these is wherethe top-ranked preposition and the writer?s prepo sition differ by less than a pre-specified amount.This was also meant to avoid flagging cases where the system?s preposition has a score only slightly higher than the writer?s preposition score, such as: ?My sister usually gets home around 3:00?(writer: around = 0.49, system: by = 0.51).In these cases, the system?s and the writer?s prepositions both fit the context, and it would be inappropriate to claimthe writer?s preposition was used incorrectly.Another system threshold requires that the probability of the writer?s preposition be lower than a pre specified value in order for it to be flagged as anerror.The thresholds were set so as to strongly fa vor precision over recall due to the high number offalse positives that may arise if there is no thresh olding.This is a tactic also used for determiner selection in (Nagata et al, 2006) and (Han et al, 2006).Both thresholds were empirically set on a development corpus.2.3 Combination Features.ME is an attractive choice of machine learning al gorithm for a problem as complex as preposition error detection, in no small part because of theavailability of ME implementations that can han dle many millions of training events and features.However, one disadvantage of ME is that it does not automatically model the interactions amongfeatures as some other approaches do, such as sup port vector machines (Jurafsky and Martin, 2008).To overcome this, we have experimented with aug menting our original feature set with ?combinationfeatures?which represent richer contextual struc ture in the form of syntactic patterns.Table 1 (first column) illustrates the four com bination features used for the example context ?take our place in the line?.The p denotes a preposition, so N-p-N denotes a syntactic context where the preposition is preceded and followed by a noun phrase.We use the preceding noun phrase (PN) and following head (FH) from the original feature set for the N-p-N feature.Column 3 shows one instantiation of combination features:Combo:word.For the N-p-N feature, the corresponding Combo:word instantiation is ?place line?since ?place?is the PN and ?line?is theFH.We also experimented with using combinations of POS tags (Combo:tag) and word+tag com binations (Combo:word+tag).So for the example, the Combo:tag N-p-N feature would be ?NN-NN?, and the Combo:word+tag N-p-N feature would beplace NN+line NN (see the fourth column of Ta ble 1).The intuition with the Combo:tag features is that the Combo:word features have the potentialto be sparse, and these capture more general pat terns of usage.We also experimented with other features such as augmenting the model with verb-preposition preferences derived from Comlex (Grishman et al, 1994), and querying the Google Terabyte N-gramcorpus with the same patterns used in the combina tion features.The Comlex-based features did not improve the model, and though the Google N-gram corpus represents much more information than our7 million event model, its inclusion improved per formance only marginally.2.4 Evaluation.In our initial evaluation of the system we col lected a corpus of 8,269 preposition contexts,error-annotated by two raters using the scheme de scribed in Section 3 to serve as a gold standard.In this study, we focus on two of the three types of preposition errors: using the incorrect preposition and using an extraneous preposition.We compared 867 Class Components Combo:word Features Combo:tag Features p-N FH line NN N-p-N PN-FH place-line NN-NN V-p-N PV-PN take-line VB-NN V-N-p-N PV-PN-FH take-place-line VB-NN-NN Table 1: Feature Examples for take our place in the line different models: the baseline model of 25 features and baseline with combination features added.Theprecision and recall for the top performing models are shown in Table 2.These results do not in clude the extraneous use filter; this filter generally increased precision by as much as 2% and recall by as much as 5%.Evaluation Metrics In the tasks of determiner and preposition selection in well-formed, nativetexts (such as (Knight and Chander, 1994), (Min nen et al, 2000), (Turner and Charniak, 2007) and (Gamon et al, 2008)), the evaluation metric most commonly used is accuracy.In these tasks, one compares the system?s output on a determiner or preposition to the gold standard of what the writeroriginally wrote.However, in the tasks of deter miner and preposition error detection, precision and recall are better metrics to use because oneis only concerned with a subset of the preposi tions (or determiners), those used incorrectly, as opposed to all of them in the selection task.In essence, accuracy has the problem of distorting system performance.Results The baseline system (described in(Chodorow et al, 2007)) performed at 79.8% precision and 11.7% recall.Next we tested the differ ent combination models: word, tag, word+tag, andall three.Surprisingly, three of the four combina tion models: tag, word+tag, all, did not improve performance of the system when added to the model, but using just the +Combo:word features improved recall by 1%.We use the +Combo:word model to test our sampling approach in section 4.As a final test, we tuned our training corpus of 7 million events by removing any contexts with unknown or misspelled words, and then retrained the model.This ?purge?resulted in a removal of nearly 200,000 training events.With this new training corpus, the +Combo:tag feature showed the biggest improvement over the baseline, withan improvement in both precision (+2.3%) and re call (+2.4%) to 82.1% and 14.1% respectively (last line of Table 2.While this improvement may seemsmall, it is in part due to the difficulty of the prob lem, but also the high baseline system score that was established in our prior work (Chodorow et al., 2007).It should be noted that with the inclusion of the extraneous use filter, performance of the +Combo:tag rose to 84% precision and close to 19% recall.Model Precision Recall Baseline 79.8% 11.7% +Combo:word 79.8% 12.8% +Combo:tag (with purge) 82.1% 14.1%Table 2: Best System Results on Incorrect Selec tion Task 2.5 Related Work.Currently there are only a handful of approachesthat tackle the problem of preposition error detec tion in English learner texts.(Gamon et al, 2008)used a language model and decision trees to de tect preposition and determiner errors in the CLEC corpus of learner essays.Their system performs at 79% precision (which is on par with our system),however recall figures are not presented thus making comparison difficult.In addition, their eval uation differs from ours in that they also include errors of omission, and their work focuses on the top twelve most frequent prepositions, while ours has greater coverage with the top 34.(Izumi etal., 2003) and (Izumi et al, 2004) used an ME ap proach to classify different grammatical errors in transcripts of Japanese interviews.They do not present performance of prepositions specifically, but overall performance for the 13 error types they target reached 25% precision and 7% recall.(Eeg-Olofsson and Knuttson, 2003) created a rule based approach to detecting preposition errors in Swedish language learners (unlike the approaches presented here, which focus on English languagelearners), and their system performed at 25% ac curacy.(Lee and Seneff, 2006) used a language model to tackle the novel problem of prepositionselection in a dialogue corpus.While their perfor mance results are quite high, 88% precision and 868 78% recall, it should be noted that their evaluation was on a small corpus with a highly constraineddomain, and focused on a limited number of prepo sitions, thus making direct comparison with our approach difficult.Although our recall figures may seem low, es pecially when compared to other NLP tasks such as parsing and anaphora resolution, this is really a reflection of how difficult the task is. For example, in the problem of preposition selection in native text, a baseline using the most frequent preposition(of) results in precision and recall of 26%.In addi tion, the cloze tests presented earlier indicate thateven in well-formed text, agreement between na tive speakers on preposition selection is only 75%.In texts written by non-native speakers, rater dis agreement increases, as will be shown in the next section.While developing an error detection system forprepositions is certainly challenging, given the re sults from our work and others, evaluation also poses a major challenge.To date, single human annotation has typically been the gold standard for grammatical error detection, such as in the work of (Izumi et al, 2004), (Han et al, 2006), (Nagata et al, 2006), (Eeg-Olofsson and Knuttson, 2003) 2 .Another method for evaluation is verification ((Ga mon et al, 2008), where a human rater checks over a system?s output.The drawbacks of this approach are: 1.every time the system is changed, a rater is needed to re-check the output, and 2.it is very hard to estimate recall.What these two evaluation methods have in common is that they side-step the issue of annotator reliability.In this section, we show how relying on only onerater can be problematic for difficult error detec tion tasks, and in section 4, we propose a method(?the sampling approach?)for efficiently evaluat ing a system that does not require the amount ofeffort needed in the standard approach to annota tion.3.1 Annotation.To create a gold-standard corpus of error annotations for system evaluation, and also to deter mine whether multiple raters are better than one, 2(Eeg-Olofsson and Knuttson, 2003) had a small evaluation on 40 preposition contexts and it is unclear whether mul tiple annotators were used.we trained two native English speakers with prior NLP annotation experience to annotate prepositionerrors in ESL text.The training was very extensive: both raters were trained on 2000 preposition contexts and the annotation manual was it eratively refined as necessary.To summarize the procedure, the two raters were shown sentences randomly selected from student essays with each preposition highlighted in the sentence.They marked each context (?2-word window around thepreposition, plus the commanding verb) for gram mar and spelling errors, and then judged whether the writer used an incorrect preposition, a correct preposition, or an extraneous preposition.Finally, the raters suggested prepositions that would best fit the context, even if there were no error (some contexts can license multiple prepositions).3.2 Reliability.Each rater judged approximately 18,000 prepo sitions contexts, with 18 sets of 100 contextsjudged by both raters for purposes of comput ing kappa.Despite the rigorous training regimen, kappa ranged from 0.411 to 0.786, with an overall combined value of 0.630.Of the prepositions that Rater 1 judged to be errors, Rater 2 judged 30.2% to be acceptable.Conversely, of the prepositions Rater 2 judged to be erroneous, Rater 1 found 38.1% acceptable.The kappa of 0.630 shows the difficulty of this task and also shows how two highly trained raters can produce very different judgments.Details on our annotation and human judgment experiments can be found in (Tetreault and Chodorow, 2008).Variability in raters?judgments translates to variability of system evaluation.For instance, in our previous work (Chodorow et al, 2007), wefound that when our system?s output was com pared to judgments of two different raters, therewas a 10% difference in precision and a 5% differ ence in recall.These differences are problematicwhen evaluating a system, as they highlight the potential to substantially over- or under-estimate per formance.The results from the previous section motivate theneed for a more refined evaluation.They sug gest that for certain error annotation tasks, such as preposition usage, it may not be appropriate to use only one rater and that if one uses multiple raters 869for error annotation, there is the possibility of cre ating an adjudicated set, or at least calculating the variability of the system?s performance.However,annotation with multiple raters has its own disadvantages as it is much more expensive and time consuming.Even using one rater to produce a sizeable evaluation corpus of preposition errors is extremely costly.For example, if we assume that500 prepositions can be annotated in 4 hours us ing our annotation scheme, and that the base rate for preposition errors is 10%, then it would take atleast 80 hours for a rater to find and mark 1000 er rors.In this section, we propose a more efficient annotation approach to circumvent this problem.4.1 Methodology.Figure 1: Sampling Approach ExampleThe sampling procedure outlined here is inspired by the one described in (Chodorow and Lea cock, 2000) for the task of evaluating the usage of nouns, verbs and adjectives.The central idea is to skew the annotation corpus so that it contains a greater proportion of errors.Here are the steps in the procedure: 1.Process a test corpus of sentences so that each.preposition in the corpus is labeled ?OK? or ?Error?by the system.2.Divide the processed corpus into two sub-.corpora, one consisting of the system?s ?OK? prepositions and the other of the system?s ?Error?prepositions.For the hypotheticaldata in Figure 1, the ?OK? sub-corpus con tains 90% of the prepositions, and the ?Error?sub-corpus contains the remaining 10%.3.Randomly sample cases from each sub-.corpus and combine the samples into an an notation set that is given to a ?blind?human rater.We generally use a higher sampling rate for the ?Error?sub-corpus because we want to ?enrich?the annotation set with a larger proportion of errors than is found in the test corpus as a whole.In Figure 1, 75% of the ?Error?sub-corpus is sampled while only 16% of the ?OK? sub-corpus is sampled.4.For each case that the human rater judges to.be an error, check to see which sub-corpus itcame from.If it came from the ?OK? sub corpus, then the case is a Miss (an error that the system failed to detect).If it came from the ?Error?sub-corpus, then the case is a Hit (an error that the system detected).If the rater judges a case to be a correct usage and it came from the ?Error?sub-corpus, then it is a False Positive (FP).the sample from the ?Error?sub-corpus.Forthe hypothetical data in Figure 1, these val ues are 600/750 = 0.80 for Hits, and 150/750 = 0.20 for FPs.Calculate the proportion ofMisses in the sample from the ?OK? sub corpus.For the hypothetical data, this is 450/1500 = 0.30 for Misses.6.The values computed in step 5 are conditional.proportions based on the sub-corpora.To calculate the overall proportions in the test cor pus, it is necessary to multiply each value by the relative size of its sub-corpus.This is shown in Table 3, where the proportion ofHits in the ?Error?sub-corpus (0.80) is multiplied by the relative size of the ?Error?sub corpus (0.10) to produce an overall Hit rate (0.08).Overall rates for FPs and Misses are calculated in a similar manner.7.Using the values from step 6, calculate Preci-.sion (Hits/(Hits + FP)) and Recall (Hits/(Hits + Misses)).These are shown in the last two rows of Table 3.Estimated Overall Rates Sample Proportion * Sub-Corpus Proportion Hits 0.80 * 0.10 = 0.08 FP 0.20 * 0.10 = 0.02 Misses 0.30 * 0.90 = 0.27 Precision 0.08/(0.08 + 0.02) = 0.80 Recall 0.08/(0.08 + 0.27) = 0.23 Table 3: Sampling Calculations (Hypothetical) 870 This method is similar in spirit to active learning ((Dagan and Engelson, 1995) and (Engelson and Dagan, 1996)), which has been used to iteratively build up an annotated corpus, but it differs fromactive learning applications in that there are no it erative loops between the system and the human annotator(s).In addition, while our methodology is used for evaluating a system, active learning is commonly used for training a system.4.2 Application.Next, we tested whether our proposed sampling approach provides good estimates of a sys tem?s performance.For this task, we used the +Combo:word model to separate a large corpusof student essays into the ?Error?and ?OK? sub corpora.The original corpus totaled over 22,000 prepositions which would normally take several weeks for two raters to double annotate and thenadjudicate.After the two sub-corpora were propor tionally sampled, this resulted in an annotation set of 752 preposition contexts (requiring roughly 6 hours for annotation), which is substantially more manageable than the full corpus.We had both raters work together to make judgments for each preposition.It is important to note that while these are notthe exact same essays used in the previous evalua tion of 8,269 preposition contexts, they come from the same pool of student essays and were on the same topics.Given these strong similarities, we feel that one can compare scores between the two approaches.The precision and recall scores forboth approaches are shown in Table 4 and are ex tremely similar, thus suggesting that the samplingapproach can be used as an alternative to exhaus tive annotation.Precision Recall Standard Approach 80% 12% Sampling Approach 79% 14% Table 4: Sampling Results It is important with the sampling approach to use appropriate sample sizes when drawing from the sub-corpora, because the accuracy of the estimatesof hits and misses will depend upon the propor tion of errors in each sub-corpus as well as on the sample sizes.The OK sub-corpus is expected to have even fewer errors than the overall base rate, so it is especially important to have a relativelylarge sample from this sub-corpus.The compari son study described above used an OK sub-corpussample that was twice as large as the Error subcorpus sample (about 500 contexts vs. 250 con texts).In short, the sampling approach is intended to alleviate the burden on annotators when faced with the task of having to rate several thousand errors of a particular type in order to produce a sizeable error corpus.On the other hand, one advantage that exhaustive annotation has over the sampling method is that it makes possible the comparison of multiple systems.With the sampling approach, one would have to resample and annotate for each system, thus multiplying the work needed.One aspect of automatic error detection that usu ally is under-reported is an analysis of the errors that learners typically make.The obvious benefit of this analysis is that it can focus development of the system.From our annotated set of preposition errors, we found that the most common prepositions that learners used incorrectly were in (21.4%), to (20.8%) and of (16.6%).The top ten prepositions accounted for 93.8% of all preposition errors in our learner corpus.Next, we ranked the common preposition ?con fusions?, the common mistakes made for each preposition.The top ten most common confusions are listed in Table 5, where null refers to cases where no preposition is licensed (the writer usedan extraneous preposition).The most common of fenses were actually extraneous errors (see Table5): using to and of when no preposition was li censed accounted for 16.8% of all errors.It is interesting to note that the most common usage errors by learners overwhelmingly involved the ten most frequently occurring prepositions in native text.This suggests that our effort to handle the 34 most frequently occurring prepositions maybe overextended and that a system that is specifically trained and refined on the top ten preposi tions may provide better diagnostic feedback to a learner.This paper has two contributions to the field of error detection in non-native writing.First, we discussed a system that detects preposition errors with high precison (up to 84%) and is competitive 871 Writer?s Prep.Rater?s Prep.Frequency to null 9.5% of null 7.3% in at 7.1% to for 4.6% in null 3.2% of for 3.1% in on 3.1% of in 2.9% at in 2.7% for to 2.5% Table 5: Common Preposition Confusions with other leading methods.We used an ME approach augmented with combination features and a series of thresholds.This system is currently incorporated in the Criterion writing evaluationservice.Second, we showed that the standard ap proach to evaluating NLP error detection systems (comparing a system?s output with a gold-standard annotation) can greatly skew system results when the annotation is done by only one rater.However, one reason why a single rater is commonly used is that building a corpus of learner errors can be extremely costly and time consuming.To address this efficiency issue, we presented a sampling approach that produces results comparable to exhaustive annotation.This makes using multiple raters possible since less time is required to assess the system?s performance.While the work presented here has focused on prepositions, the arguments against using only one rater, and for using a sampling approach generalize to other error types, such as determiners and collocations.Acknowledgements We would first like to thank our two annotators Sarah Ohls and Waverly VanWinkle for their hours of hard work.We wouldalso like to acknowledge the three anonymous reviewers and Derrick Higgins for their helpful com ments and feedback.
A Monolingual Tree-based Translation Model for Sentence SimplificationIn this paper, we consider sentence sim plification as a special form of translation with the complex sentence as the source and the simple sentence as the target.We propose a Tree-based Simplification Model (TSM), which, to our knowledge, is the first statistical simplification model covering splitting, dropping, reorderingand substitution integrally.We also de scribe an efficient method to train our model with a large-scale parallel dataset obtained from the Wikipedia and Simple Wikipedia.The evaluation shows that our model achieves better readability scores than a set of baseline systems.Sentence simplification transforms long and dif ficult sentences into shorter and more readable ones.This helps humans read texts more easilyand faster.Reading assistance is thus an important application of sentence simplification, espe cially for people with reading disabilities (Carrollet al, 1999; Inui et al, 2003), low-literacy read ers (Watanabe et al, 2009), or non-native speakers (Siddharthan, 2002).Not only human readers but also NLP applications can benefit from sentence simplification.The original motivation for sentence simplification is using it as a preprocessor to facili tate parsing or translation tasks (Chandrasekar et al., 1996).Complex sentences are considered as stumbling blocks for such systems.More recently,sentence simplification has also been shown help ful for summarization (Knight and Marcu, 2000), ? This work has been supported by the Emmy Noether Program of the German Research Foundation (DFG) underthe grant No.GU 798/3-1, and by the Volkswagen Founda tion as part of the Lichtenberg-Professorship Program under the grant No.I/82806.sentence fusion (Filippova and Strube, 2008b), se mantic role labeling (Vickrey and Koller, 2008), question generation (Heilman and Smith, 2009), paraphrase generation (Zhao et al, 2009) and biomedical information extraction (Jonnalagadda and Gonzalez, 2009).At sentence level, reading difficulty stems either from lexical or syntactic complexity.Sen tence simplification can therefore be classifiedinto two types: lexical simplification and syntac tic simplification (Carroll et al, 1999).These two types of simplification can be further implemented by a set of simplification operations.Splitting, dropping, reordering, and substitution are widely accepted as important simplification operations.The splitting operation splits a long sentence intoseveral shorter sentences to decrease the complex ity of the long sentence.The dropping operation further removes unimportant parts of a sentence to make it more concise.The reordering operationinterchanges the order of the split sentences (Sid dharthan, 2006) or parts in a sentence (Watanabeet al, 2009).Finally, the substitution operation re places difficult phrases or words with their simpler synonyms.In most cases, different simplification operations happen simultaneously.It is therefore nec essary to consider the simplification process as a combination of different operations and treatthem as a whole.However, most of the existing models only consider one of these operations.Siddharthan (2006) and Petersen and Ostendorf (2007) focus on sentence splitting, while sen tence compression systems (Filippova and Strube, 2008a) mainly use the dropping operation.As faras lexical simplification is concerned, word substitution is usually done by selecting simpler syn onyms from Wordnet based on word frequency (Carroll et al, 1999).In this paper, we propose a sentence simplifica tion model by tree transformation which is based 1353 on techniques from statistical machine translation (SMT) (Yamada and Knight, 2001; Yamada andKnight, 2002; Graehl et al, 2008).Our model in tegrally covers splitting, dropping, reordering and phrase/word substitution.The parameters of ourmodel can be efficiently learned from complex simple parallel datasets.The transformation froma complex sentence to a simple sentence is con ducted by applying a sequence of simplification operations.An expectation maximization (EM) algorithm is used to iteratively train our model.We also propose a method based on monolingualword mapping which speeds up the training pro cess significantly.Finally, a decoder is designed to generate the simplified sentences using a greedy strategy and integrates language models.In order to train our model, we further com pile a large-scale complex-simple parallel dataset(PWKP) from Simple English Wikipedia1 and En glish Wikipedia2, as such datasets are rare.We organize the remainder of the paper as follows: Section 2 describes the PWKP dataset.Sec tion 3 presents our TSM model.Sections 4 and 5 are devoted to training and decoding, respectively.Section 6 details the evaluation.The conclusions follow in the final section.We collected a paired dataset from the English Wikipedia and Simple English Wikipedia.The targeted audience of Simple Wikipedia includes?children and adults who are learning English lan guage?.The authors are requested to ?use easy words and short sentences?to compose articles.We processed the dataset as follows: Article Pairing 65,133 articles from SimpleWikipedia3 and Wikipedia4 were paired by fol lowing the ?language link?using the dump filesin Wikimedia.5 Administration articles were fur ther removed.Plain Text Extraction We use JWPL (Zesch etal., 2008) to extract plain texts from Wikipedia ar ticles by removing specific Wiki tags.Pre-processing including sentence boundary detection and tokenization with the Stanford 1http://simple.wikipedia.org 2http://en.wikipedia.org 3As of Aug 17th, 2009 4As of Aug 22nd, 2009 5http://download.wikimedia.org Parser package (Klein and Manning, 2003), and lemmatization with the TreeTagger (Schmid, 1994).Monolingual Sentence Alignment As we need a parallel dataset algned at the sentence level,we further applied monolingual sentence align ment on the article pairs.In order to achieve the best sentence alignment on our dataset, we tested three similarity measures: (i) sentence-level TF*IDF (Nelken and Shieber, 2006), (ii) word overlap (Barzilay and Elhadad, 2003) and (iii)word-based maximum edit distance (MED) (Lev enshtein, 1966) with costs of insertion, deletionand substitution set to 1.To evaluate their perfor mance we manually annotated 120 sentence pairs from the article pairs.Tab.1 reports the precision and recall of these three measures.We manually adjusted the similarity threshold to obtain a recallvalue as close as possible to 55.8% which was pre viously adopted by Nelken and Shieber (2006).Similarity Precision Recall TF*IDF 91.3% 55.4% Word Overlap 50.5% 55.1% MED 13.9% 54.7% Table 1: Monolingual Sentence Alignment The results in Tab.1 show that sentence-levelTF*IDF clearly outperforms the other two mea sures, which is consistent with the results reported by Nelken and Shieber (2006).We henceforth chose sentence-level TF*IDF to align our dataset.As shown in Tab.2, PWKP contains more than 108k sentence pairs.The sentences from Wikipedia and Simple Wikipedia are considered as ?complex?and ?simple?respectively.Both the average sentence length and average token length in Simple Wikipedia are shorter than those inWikipedia, which is in compliance with the pur pose of Simple Wikipedia.Avg.Sen. Len Avg.Tok.Len #Sen.Pairscomplex simple complex simple 25.01 20.87 5.06 4.89 108,016 Table 2: Statistics for the PWKP datasetIn order to account for sentence splitting, we al low 1 to n sentence alignment to map one complexsentence to several simple sentences.We first per form 1 to 1 mapping with sentence-level TF*IDF and then combine the pairs with the same complex sentence and adjacent simple sentences.We apply the following simplification operations to the parse tree of a complex sentence: splitting, 1354dropping, reordering and substitution.In this sec tion, we use a running example to illustrate thisprocess.c is the complex sentence to be simpli fied in our example.Fig.1 shows the parse tree of c (we skip the POS level).c: August was the sixth month in the ancient Ro man calendar which started in 735BC.NP VP S August was NPinsixththe SBAR NP NP PP WHNP S VP started PP in 735BC ancient calendar whichthe Roman month Figure 1: Parse Tree of c 3.1 Splitting.The first operation is sentence splitting, which wefurther decompose into two subtasks: (i) segmen tation, which decides where and whether to split a sentence and (ii) completion, which makes the new split sentences complete.First, we decide where we can split a sentence.In our model, the splitting point is judged by the syntactic constituent of the split boundary word in the complex sentence.The decision whether a sentence should be split is based on the length of the complex sentence.The features used in the segmentation step are shown in Tab.3.Word Constituent iLength isSplit Prob.?which?SBAR 1 true 0.0016 ?which?SBAR 1 false 0.9984 ?which?SBAR 2 true 0.0835 ?which?SBAR 2 false 0.9165 Table 3: Segmentation Feature Table (SFT) Actually, we do not use the direct constituent of a word in the parse tree.In our example, the directconstituent of the word ?which?is ?WHNP?.In stead, we use Alg.1 to calculate the constituentof a word.Alg.1 returns ?SBAR?as the adjusted constituent for ?which?.Moreover, di rectly using the length of the complex sentenceis affected by the data sparseness problem.In stead, we use iLength as the feature which is calculated as iLength = ceiling( comLengthavgSimLength), where comLength is the length of the complex sentence and avgSimLength is the average length of simple sentences in the training dataset.The ?Prob.?column shows the probabilities obtained after training on our dataset.Algorithm 1 adjustConstituent(word, tree) constituent?word.father; father ? constituent.father; while father 6= NULL AND constituent is the most left child of father do constituent?father; father ? father.father; end while return constituent; In our model, one complex sentence can be split into two or more sentences.Since many splitting operations are possible, we need to select the mostlikely one.The probability of a segmentation op eration is calculated as: P (seg|c) = ? w:c SFT (w|c) (1) where w is a word in the complex sentence c and SFT (w|c) is the probability of the word w in the Segmentation Feature Table (SFT); Fig.2 shows a possible segmentation result of our example.NP VP S August was NPinsixththe SBAR NP NP PP WHNP S VP started PP in 735BC ancient calendar which the Roman month Figure 2: Segmentation The second step is completion.In this step, we try to make the split sentences complete and grammatical.In our example, to make the second sentence ?which started in 735BC?complete and grammatical we should first drop the border word ?which?and then copy the dependent NP ?the ancient Roman calendar?to the left of ?started?to obtain the complete sentence ?the ancient Ro man calendar started in 735BC?.In our model, whether the border word should be dropped or retained depends on two features of the border word: the direct constituent of the word and the word itself, as shown in Tab.4.Const.Word isDropped Prob.WHNP which True 1.0 WHNP which False Prob.Min Table 4: Border Drop Feature Table (BDFT) In order to copy the necessary parts to complete the new sentences, we must decide which parts should be copied and where to put these parts in the new sentences.In our model, this is judged by two features: the dependency relation and theconstituent.We use the Stanford Parser for parsing the dependencies.In our example, the de 1355pendency relation between ?calendar?in the com plex sentence and the verb ?started?in the secondsplit sentence is ?gov nsubj?.6 The direct constituent of ?started?is ?VP?and the word ?calen dar?should be put on the ?left?of ?started?, see Tab.5.Dep.Const.isCopied Pos.Prob.gov nsubj VP(VBD) True left 0.9000 gov nsubj VP(VBD) True right 0.0994 gov nsubj VP(VBD) False - 0.0006 Table 5: Copy Feature Table (CFT) For dependent NPs, we copy the whole NP phrase rather than only the head noun.7 In ourexample, we copy the whole NP phrase ?the an cient Roman calendar?to the new position rather than only the word ?calendar?.The probability of a completion operation can be calculated as P (com|seg) = Y bw:s BDFT (bw|s) Y w:s Y dep:w CFT (dep).where s are the split sentences, bw is a border word in s, w is a word in s, dep is a dependency of w which is out of the scope of s. Fig.3 shows the most likely result of the completion operation for our example.NP VP pt1 August was NPinsixththe NP NP PPpt2 VP started PP in 735BC ancient calendarthe RomanNP ancient calendarthe Roman month Figure 3: Completion 3.2 Dropping and Reordering.We first apply dropping and then reordering to each non-terminal node in the parse tree from topto bottom.We use the same features for both drop ping and reordering: the node?s direct constituent and its children?s constituents pattern, see Tab.6 and Tab.7.Constituent Children Drop Prob.NP DT JJ NNP NN 1101 7.66E-4 NP DT JJ NNP NN 0001 1.26E-7 Table 6: Dropping Feature Table (DFT) 6With Stanford Parser, ?which?is a referent of ?calender?and the nsubj of ?started?.?calender?thus can be considered to be the nsubj of ?started?with ?started?as the governor.7The copied NP phrase can be further simplified in the following steps.Constituent Children Reorder Prob.NP DT JJ NN 012 0.8303 NP DT JJ NN 210 0.0039 Table 7: Reordering Feature Table (RFT)The bits ?1?and ?0?in the ?Drop?column indicate whether the corresponding constituent is re tained or dropped.The number in the ?Reorder?column represents the new order for the children.The probabilities of the dropping and reordering operations can be calculated as Equ.2 and Equ.3.P (dp|node) = DFT (node) (2) P (ro|node) = RFT (node) (3) In our example, one of the possible results is dropping the NNP ?Roman?, as shown in Fig.4.NP VP pt1 August was NPinsixththe NP NP PPpt2 VP started PP in 735BC ancient calendartheNP ancient calendarthe month Figure 4: Dropping & Reordering 3.3 Substitution.3.3.1 Word SubstitutionWord substitution only happens on the termi nal nodes of the parse tree.In our model, the conditioning features include the original word and the substitution.The substitution for a word can be another word or a multi-word expression(see Tab.8).The probability of a word substitu tion operation can be calculated as P (sub|w) = SubFT (Substitution|Origin).Origin Substitution Prob.ancient ancient 0.963 ancient old 0.0183 ancient than transport 1.83E-102 old ancient 0.005 Table 8: Substitution Feature Table (SubFT) 3.3.2 Phrase SubstitutionPhrase substitution happens on the non terminal nodes and uses the same conditioningfeatures as word substitution.The ?Origin?con sists of the leaves of the subtree rooted at the node.When we apply phrase substitution on anon-terminal node, then any simplification operation (including dropping, reordering and substitu tion) cannot happen on its descendants any more 1356 because when a node has been replaced then its descendants are no longer existing.Therefore, for each non-terminal node we must decide whether a substitution should take place at this node or at itsdescendants.We perform substitution for a non terminal node if the following constraint is met: Max(SubFT (?|node)) ? Y ch:node Max(SubFT (?|ch)).where ch is a child of the node.canbe any substitution in the SubFT.The proba bility of the phrase substitution is calculated as P (sub|node) = SubFT (Substitution|Origin).Fig.5 shows one of the possible substitution re sults for our example where ?ancient?is replaced by ?old?.NP VP pt1 August was NPinsixththe NP NP PPpt2 VP started PP in 735BC old calendartheNP old calendarthe month Figure 5: Substitution As a result of all the simplification operations, we obtain the following two sentences: s1 = Str(pt1)=?August was the sixth month in the old calendar.?and s2 = Str(pt2)=?The old calendar started in 735BC.?3.4 The Probabilistic Model.Our model can be formalized as a direct translation model from complex to simple P (s|c) multi plied by a language model P (s) as shown in Equ.4.s = argmax s P (s|c)P (s) (4) We combine the parts described in the previous sections to get the direct translation model: P (s|c) = ? ?:Str(?(c))=s (P (seg|c)P (com|seg) (5) ? node P (dp|node)P (ro|node)P (sub|node) ? w (sub|w)).where ? is a sequence of simplification operationsand Str(?(c)) corresponds to the leaves of a simplified tree.There can be many sequences of op erations that result in the same simplified sentence and we sum up all of their probabilities.In this section, we describe how we train the prob abilities in the tables.Following the work of Yamada and Knight (2001), we train our model by maximizing P (s|c) over the training corpuswith the EM algorithm described in Alg.2, us ing a constructed graph structure.We develop the Training Tree (Fig.6) to calculate P (s|c).P (s|c) is equal to the inside probability of the root in theTraining Tree.Alg.3 and Alg.4 are used to calculate the inside and outside probabilities.We re fer readers to Yamada and Knight (2001) for more details.Algorithm 2 EM Training (dataset)Initialize all probability tables using the uniform distribu tion; for several iterations do reset al cnt = 0; for each sentence pair < c, s > in dataset do tt = buildTrainingTree(< c, s >); calcInsideProb(tt); calcOutsideProb(tt); update cnt for each conditioning feature in each node of tt: cnt = cnt + node.insideProb ? node.outsideProb/root.insideProb; end for updateProbability(); end for root sp sp_res1 sp_res2 dp ro mp mp_res1 mp_res2 sub mp mp_res subsub dp ro mp_res root sp sp_res sp_res dp ro ro_res ro_res sub ro_res subsub dp ro ro_res sub_res sub_res sub_res Figure 6: Training Tree (Left) and Decoding Tree (Right) We illustrate the construction of the training tree with our running example.There are two kinds of nodes in the training tree: data nodes in rectangles and operation nodes in circles.Data nodes contain data and operation nodes execute operations.The training is a supervised learning 1357 process with the parse tree of c as input and the two strings s1 and s2 as the desired output.root stores the parse tree of c and also s1 and s2.sp, ro, mp and sub are splitting, reordering, mapping and substitution operations.sp res and mp res store the results of sp and mp.In our example, sp splits the parse tree into two parse trees pt1 and pt2 (Fig.3).sp res1 contains pt1 and s1.sp res2 contains pt2 and s2.Then dp, ro and mp are iteratively applied to each non-terminal node at each level of pt1 and pt2 from top to down.This process continues until the terminal nodesare reached or is stopped by a sub node.The function of mp operation is similar to the word mapping operation in the string-based machine trans lation.It maps substrings in the complex sentence which are dominated by the children of the current node to proper substrings in the simple sentences.Speeding Up The example above is only oneof the possible paths.We try all of the promis ing paths in training.Promising paths are thepaths which are likely to succeed in transform ing the parse tree of c into s1 and s2.We select the promising candidates using monolingual word mapping as shown in Fig.7.In this example,only the word ?which?can be a promising can didate for splitting.We can select the promisingcandidates for the dropping, reordering and map ping operations similarly.With this improvement, we can train on the PWKP dataset within 1 hour excluding the parsing time taken by the Stanford Parser.We initialize the probabilities with the uniform distribution.The binary features, such as SFT and BDFT, are assigned the initial value of 0.5.For DFT and RFT, the initial probability is 1N!, whereN is the number of the children.CFT is initial ized as 0.25.SubFT is initialized as 1.0 for anysubstitution at the first iteration.After each itera tion, the updateProbability function recalculatesthese probabilities based on the cnt for each fea ture.Algorithm 3 calcInsideProb (TrainingTree tt) for each node from level = N to root of tt do if node is a sub node then node.insideProb = P (sub|node); else if node is a mp OR sp node then node.insideProb =Qchild child.insideProb;else node.insideProb =Pchild child.insideProb;end if end for Algorithm 4 calcOutsideProb (TrainingTree tt) for each node from root to level = N of tt do if node is the root then node.outsideProb = 1.0; else if node is a sp res OR mp res node then {COMMENT: father are the fathers of the current node, sibling are the children of father excluding the current node} node.outsideProb = P father father.outsideProb ?Qsibling sibling.insideProb;else if node is a mp node then node.outsideProb = father.outsideProb ? 1.0; else if node is a sp, ro, dp or sub node then node.outsideProb = father.outsideProb ? P (sp or ro or dp or sub|node); end if end for August was the sixth in the ancient Roman calendar statedwhich in 735BC August was the sixth in the old Roman calendar stated in 735BCThe old calendar.Complex sentence Simple sentences month month Figure 7: Monolingual Word MappingFor decoding, we construct the decoding tree(Fig.6) similarly to the construction of the training tree.The decoding tree does not have mp op erations and there can be more than one sub nodes attached to a single ro res.The root contains the parse tree of the complex sentence.Due to space limitations, we cannot provide all the details of the decoder.We calculate the inside probability and out side probability for each node in the decoding tree.When we simplify a complex sentence, we start from the root and greedily select the branchwith the highest outside probability.For the sub stitution operation, we also integrate a trigram language model to make the generated sentences more fluent.We train the language model with SRILM (Stolcke, 2002).All the articles from the Simple Wikipedia are used as the training corpus, amounting to about 54 MB.Our evaluation dataset consists of 100 complex sentences and 131 parallel simple sentences from PWKP.They have not been used for training.Four baseline systems are compared in our eval uation.The first is Moses which is a state of the art SMT system widely used as a baseline in MT community.Obviously, the purpose of Mosesis cross-lingual translation rather than monolin 1358 gual simplification.The goal of our comparison is therefore to assess how well a standard SMT system may perform simplification when fed with a proper training dataset.We train Moses with the same part of PWKP as our model.The secondbaseline system is a sentence compression sys tem (Filippova and Strube, 2008a) whose demo system is available online.8 As the compressionsystem can only perform dropping, we further ex tend it to our third and fourth baseline systems, in order to make a reasonable comparison.In our third baseline system, we substitute the words in the output of the compression system with their simpler synonyms.This is done by looking up the synonyms in Wordnet and selecting the mostfrequent synonym for replacement.The word fre quency is counted using the articles from Simple Wikipedia.The fourth system performs sentence splitting on the output of the third system.This is simply done by splitting the sentences at ?and?,?or?, ?but?, ?which?, ?who?and ?that?, and dis carding the border words.In total, there are 5systems in our evaluation: Moses, the MT system; C, the compression system; CS, the compression+substitution system; CSS, the compres sion+substitution+split system; TSM, our model.We also provide evaluation measures for the sen tences in the evaluation dataset: CW: complexsentences from Normal Wikipedia and SW: par allel simple sentences from Simple Wikipedia.6.1 Basic Statistics and Examples.The first three columns in Tab.9 present the ba sic statistics for the evaluation sentences and theoutput of the five systems.tokenLen is the aver age length of tokens which may roughly reflect the lexical difficulty.TSM achieves an average token length which is the same as the Simple Wikipedia (SW).senLen is the average number of tokens inone sentence, which may roughly reflect the syn tactic complexity.Both TSM and CSS produce shorter sentences than SW.Moses is very close to CW.#sen gives the number of sentences.Moses, C and CS cannot split sentences and thus produce about the same number of sentences as available in CW.Here are two example results obtained with our TSM system.Example 1.CW: ?Genetic engineering has ex panded the genes available to breeders to utilize in creating desired germlines for new crops.?SW: 8http://212.126.215.106/compression/?New plants were created with genetic engineer ing.?TSM: ?Engineering has expanded the genes available to breeders to use in making germlines for new crops.?Example 2.CW: ?An umbrella term is a word thatprovides a superset or grouping of related con cepts, also called a hypernym.?SW: ?An umbrellaterm is a word that provides a superset or group ing of related concepts.?TSM: ?An umbrella term is a word.A word provides a superset of related concepts, called a hypernym.?In the first example, both substitution and dropping happen.TSM replaces ?utilize?and ?cre ating?with ?use?and ?making?.?Genetic?isdropped.In the second example, the complex sen tence is split and ?also?is dropped.6.2 Translation Assessment.In this part of the evaluation, we use traditional measures used for evaluating MT systems.Tab.9 shows the BLEU and NIST scores.We use ?mteval-v11b.pl?9 as the evaluation tool.CWand SW are used respectively as source and ref erence sentences.TSM obtains a very high BLEU score (0.38) but not as high as Moses (0.55).However, the original complex sentences (CW) from Normal Wikipedia get a rather high BLEU (0.50), when compared to the simple sentences.We also find that most of the sentences generated by Moses are exactly the same as those in CW:this shows that Moses only performs few modi fications to the original complex sentences.This is confirmed by MT evaluation measures: if we set CW as both source and reference, the BLEU score obtained by Moses is 0.78.TSM gets 0.55 in the same setting which is significantly smaller than Moses and demonstrates that TSM is able to generate simplifications with a greater amount of variation from the original sentence.As shown inthe ?#Same?column of Tab.9, 25 sentences generated by Moses are exactly identical to the com plex sentences, while the number for TSM is 2 which is closer to SW.It is however not clear how well BLEU and NIST discriminate simplification systems.As discussed in Jurafsky and Martin (2008), ?BLEU does poorly at comparing systems with radically different architectures and is most appropriate when evaluating incremental changes with similar architectures.?In our case, TSM andCSS can be considered as having similar architec tures as both of them can do splitting, dropping 9http://www.statmt.org/moses/ 1359 TokLen SenLen #Sen BLEU NIST #Same Flesch Lix(Grade) OOV% PPL CW 4.95 27.81 100 0.50 6.89 100 49.1 53.0 (10) 52.9 384 SW 4.76 17.86 131 1.00 10.98 3 60.4 (PE) 44.1 (8) 50.7 179 Moses 4.81 26.08 100 0.55 7.47 25 54.8 48.1 (9) 52.0 363 C 4.98 18.02 103 0.28 5.37 1 56.2 45.9 (8) 51.7 481 CS 4.90 18.11 103 0.19 4.51 0 59.1 45.1 (8) 49.5 616 CSS 4.98 10.20 182 0.18 4.42 0 65.5 (PE) 38.3 (6) 53.4 581 TSM 4.76 13.57 180 0.38 6.21 2 67.4 (PE) 36.7 (5) 50.8 353 Table 9: Evaluation and substitution.But Moses mostly cannot split and drop.We may conclude that TSM and Moses have different architectures and BLEU or NIST isnot suitable for comparing them.Here is an exam ple to illustrate this: (CW): ?Almost as soon as heleaves, Annius and the guard Publius arrive to es cort Vitellia to Titus, who has now chosen her as his empress.?(SW): ?Almost as soon as he leaves,Annius and the guard Publius arrive to take Vitellia to Titus, who has now chosen her as his empress.?(Moses): The same as (SW).(TSM): ?An nius and the guard Publius arrive to take Vitellia to Titus.Titus has now chosen her as his empress.?In this example, Moses generates an exactly iden tical sentence to SW, thus the BLUE and NIST scores of Moses is the highest.TSM simplifies the complex sentence by dropping, splitting and substitution, which results in two sentences that are quite different from the SW sentence and thus gets lower BLUE and NIST scores.Nevertheless, the sentences generated by TSM seem better than Moses in terms of simplification.6.3 Readability Assessment.Intuitively, readability scores should be suitable metrics for simplification systems.We use the Linux ?style?command to calculate the Fleschand Lix readability scores.The results are pre sented in Tab.9.?PE?in the Flesch column standsfor ?Plain English?and the ?Grade?in Lix repre sents the school year.TSM achieves significantly better scores than Moses which has the best BLEUscore.This implies that good monolingual trans lation is not necessarily good simplification.OOVis the percentage of words that are not in the Ba sic English BE850 list.10 TSM is ranked as the second best system for this criterion.The perplexity (PPL) is a score of text probability measured by a language model and normal ized by the number of words in the text (Equ.6).10http://simple.wikipedia.org/wiki/ Wikipedia:Basic_English_alphabetical_ wordlistPPL can be used to measure how tight the language model fits the text.Language models constitute an important feature for assessing readabil ity (Schwarm and Ostendorf, 2005).We train a trigram LM using the simple sentences in PWKP and calculate the PPL with SRILM.TSM gets the best PPL score.From this table, we can conclude that TSM achieves better overall readability than the baseline systems.PPL(text) = P (w1w2...wN )?1 N (6)There are still some important issues to be con sidered in future.Based on our observations, the current model performs well for word substitution and segmentation.But the completion of the new sentences is still problematic.For example, we copy the dependent NP to the new sentences.This may break the coherence between sentences.Abetter solution would be to use a pronoun to replace the NP.Sometimes, excessive droppings oc cur, e.g., ?older?and ?twin?are dropped in ?She has an older brother and a twin brother...?.This results in a problematic sentence: ?She has anbrother and a brother...?.There are also some er rors which stem from the dependency parser.InExample 2, ?An umbrella term?should be a dependency of ?called?.But the parser returns ?su perset?as the dependency.In the future, we will investigate more sophisticated features and rules to enhance TSM.In this paper, we presented a novel large-scale par allel dataset PWKP for sentence simplification.We proposed TSM, a tree-based translation model for sentence simplification which covers splitting, dropping, reordering and word/phrase substitution integrally for the first time.We also described anefficient training method with speeding up tech niques for TSM.The evaluation shows that TSM can achieve better overall readability scores than a set of baseline systems.1360
Enhanced Sentiment Learning Using Twitter Hashtags and SmileysAutomated identification of diverse sen timent types can be beneficial for manyNLP systems such as review summariza tion and public media analysis.In some ofthese systems there is an option of assign ing a sentiment value to a single sentence or a very short text.In this paper we propose a supervised sentiment classification framework whichis based on data from Twitter, a popu lar microblogging service.By utilizing50 Twitter tags and 15 smileys as sen timent labels, this framework avoids theneed for labor intensive manual annotation, allowing identification and classifi cation of diverse sentiment types of shorttexts.We evaluate the contribution of different feature types for sentiment classification and show that our framework successfully identifies sentiment types of untagged sentences.The quality of the senti ment identification was also confirmed byhuman judges.We also explore dependencies and overlap between different sen timent types represented by smileys and Twitter hashtags.A huge amount of social media including news,forums, product reviews and blogs contain nu merous sentiment-based sentences.Sentiment is defined as ?a personal belief or judgment that ?* Both authors equally contributed to this paper.is not founded on proof or certainty?1.Senti ment expressions may describe the mood of thewriter (happy/sad/bored/grateful/...)or the opin ion of the writer towards some specific entity (X is great/I hate X, etc.).Automated identification of diverse sentimenttypes can be beneficial for many NLP systems such as review summarization systems, dia logue systems and public media analysis systems.Sometimes it is directly requested by the user toobtain articles or sentences with a certain senti ment value (e.g Give me all positive reviews of product X/ Show me articles which explain why movie X is boring).In some other cases obtaining sentiment value can greatly enhance information extraction tasks like review summarization.Whilethe majority of existing sentiment extraction sys tems focus on polarity identification (e.g., positive vs. negative reviews) or extraction of a handful of pre-specified mood labels, there are many useful and relatively unexplored sentiment types.Sentiment extraction systems usually require an extensive set of manually supplied sentiment words or a handcrafted sentiment-specific dataset.With the recent popularity of article tagging, some social media types like blogs allow users to add sentiment tags to articles.This allows to use blogsas a large user-labeled dataset for sentiment learning and identification.However, the set of sentiment tags in most blog platforms is somewhat re stricted.Moreover, the assigned tag applies to the whole blog post while a finer grained sentiment extraction is needed (McDonald et al, 2007).With the recent popularity of the Twitter micro blogging service, a huge amount of frequently 1WordNet 2.1 definitions.241self-standing short textual sentences (tweets) became openly available for the research community.Many of these tweets contain a wide vari ety of user-defined hashtags.Some of these tagsare sentiment tags which assign one or more senti ment values to a tweet.In this paper we propose away to utilize such tagged Twitter data for classi fication of a wide variety of sentiment types from text.We utilize 50 Twitter tags and 15 smileys assentiment labels which allow us to build a classifier for dozens of sentiment types for short tex tual sentences.In our study we use four different feature types (punctuation, words, n-grams and patterns) for sentiment classification and evaluate the contribution of each feature type for this task.We show that our framework successfully identi fies sentiment types of the untagged tweets.We confirm the quality of our algorithm using human judges.We also explore the dependencies and overlap between different sentiment types represented by smileys and Twitter tags.Section 2 describes related work.Section 3 details classification features and the algorithm, while Section 4 describes the dataset and labels.Automated and manual evaluation protocols and results are presented in Section 5, followed by a short discussion.Sentiment analysis tasks typically combine twodifferent tasks: (1) Identifying sentiment expres sions, and (2) determining the polarity (sometimes called valence) of the expressed sentiment.These tasks are closely related as the purpose of most works is to determine whether a sentence bears a positive or a negative (implicit or explicit) opinion about the target of the sentiment.Several works (Wiebe, 2000; Turney, 2002;Riloff, 2003; Whitelaw et al, 2005) use lexical re sources and decide whether a sentence expressesa sentiment by the presence of lexical items (sen timent words).Others combine additional feature types for this decision (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Wilson et al, 2005; Bloom et al, 2007; McDonald et al, 2007; Titov and McDonald, 2008a; Melville et al, 2009).It was suggested that sentiment words may havedifferent senses (Esuli and Sebastiani, 2006; Andreevskaia and Bergler, 2006; Wiebe and Mihal cea, 2006), thus word sense disambiguation can improve sentiment analysis systems (Akkaya et al., 2009).All works mentioned above identifyevaluative sentiment expressions and their polar ity.Another line of works aims at identifying abroader range of sentiment classes expressing various emotions such as happiness, sadness, boredom, fear, and gratitude, regardless (or in addi tion to) positive or negative evaluations.Mihalcea and Liu (2006) derive lists of words and phrases with happiness factor from a corpus of blog posts, where each post is annotated by the blogger with a mood label.Balog et al (2006) use the mood annotation of blog posts coupled with news datain order to discover the events that drive the dom inant moods expressed in blogs.Mishne (2005) used an ontology of over 100 moods assigned to blog posts to classify blog texts according tomoods.While (Mishne, 2005) classifies a blog entry (post), (Mihalcea and Liu, 2006) assign a hap piness factor to specific words and expressions.Mishne used a much broader range of moods.Strapparava and Mihalcea (2008) classify blogposts and news headlines to six sentiment cate gories.While most of the works on sentiment analysis focus on full text, some works address senti ment analysis in the phrasal and sentence level, see (Yu and Hatzivassiloglou, 2003; Wilson et al,2005; McDonald et al, 2007; Titov and McDon ald, 2008a; Titov and McDonald, 2008b; Wilson et al, 2009; Tsur et al, 2010) among others.Only a few studies analyze the sentiment and polarity of tweets targeted at major brands.Jansenet al (2009) used a commercial sentiment analyzer as well as a manually labeled corpus.Davi dov et al (2010) analyze the use of the #sarcasmhashtag and its contribution to automatic recognition of sarcastic tweets.To the best of our knowledge, there are no works employing Twitter hashtags to learn a wide range of emotions and the re lations between the different emotions.242Below we propose a set of classification featuresand present the algorithm for sentiment classifica tion.3.1 Classification features.We utilize four basic feature types for sentimentclassification: single word features, n-gram fea tures, pattern features and punctuation features.For the classification, all feature types are com bined into a single feature vector.3.1.1 Word-based and n-gram-based features Each word appearing in a sentence serves as a binary feature with weight equal to the inverted count of this word in the Twitter corpus.We also took each consecutive word sequence containing2?5 words as a binary n-gram feature using a similar weighting strategy.Thus n-gram features al ways have a higher weight than features of their component words, and rare words have a higher weight than common words.Words or n-gramsappearing in less than 0.5% of the training set sen tences do not constitute a feature.ASCII smileys and other punctuation sequences containing two or more consecutive punctuation symbols were used as single-word features.Word features alsoinclude the substituted meta-words for URLs, ref erences and hashtags (see Subsection 4.1).3.1.2 Pattern-based featuresOur main feature type is based on surface pat terns.For automated extraction of patterns, we followed the pattern definitions given in (Davidov and Rappoport, 2006).We classified words into high-frequency words (HFWs) and content words (CWs).A word whose corpus frequency is more (less) than FH (FC) is considered to be a HFW(CW).We estimate word frequency from the train ing set rather than from an external corpus.Unlike (Davidov and Rappoport, 2006), we consider allsingle punctuation characters or consecutive se quences of punctuation characters as HFWs.We also consider URL, REF, and HASHTAG tags as HFWs for pattern extraction.We define a pattern as an ordered sequence of high frequency words and slots for content words.Following (Davidov and Rappoport, 2008), the FH and FC thresholds were set to 1000 words per million (upper bound for FC) and 100 words per million (lower bound for FH )2.The patterns allow 2?6 HFWs and 1?5 slots forCWs.To avoid collection of patterns which capture only a part of a meaningful multiword ex pression, we require patterns to start and to end with a HFW.Thus a minimal pattern is of the form [HFW] [CW slot] [HFW].For each sentenceit is possible to generate dozens of different pat terns that may overlap.As with words and n-gram features, we do not treat as features any patterns which appear in less than 0.5% of the training set sentences.Since each feature vector is based on a singlesentence (tweet), we would like to allow approximate pattern matching for enhancement of learn ing flexibility.The value of a pattern feature is estimated according the one of the following four scenarios3: ? ????????????????????????1 count(p) : Exact match ? all the pattern components appear in the sentence in correct order without any additional words.count(p) : Sparse match ? same as exact match but additional non-matching words can be inserted between pattern components.??n N?count(p) : Incomplete match ? only n > 1 of N pattern components appear in the sentence, while some non-matching words can be inserted in-between.At least one of the appearing components should be a HFW.0 : No match ? nothing or only a single pattern component appears in the sentence.0 ? ?1 and 0 ? ?1 are parameters we use to assign reduced scores for imperfect matches.Since the patterns we use are relatively long, ex act matches are uncommon, and taking advantageof partial matches allows us to significantly re duce the sparsity of the feature vectors.We used ? = ? = 0.1 in all experiments.This pattern based framework was proven effi cient for sarcasm detection in (Tsur et al, 2010; 2Note that the FH and FC bounds allow overlap between some HFWs and CWs.See (Davidov and Rappoport, 2008) for a short discussion.3As with word and n-gram features, the maximal featureweight of a pattern p is defined as the inverse count of a pat tern in the complete Twitter corpus.243 Davidov et al, 2010).3.1.3 Efficiency of feature selection Since we avoid selection of textual features which have a training set frequency below 0.5%, we perform feature selection incrementally, on each stage using the frequencies of the features obtained during the previous stages.Thus first we estimate the frequencies of single words in the training set, then we only consider creationof n-grams from single words with sufficient frequency, finally we only consider patterns composed from sufficiently frequent words and n grams.3.1.4 Punctuation-based features In addition to pattern-based features we used the following generic features: (1) Sentence length in words, (2) Number of ?!?characters in the sentence, (3) Number of ???characters in the sentence, (4) Number of quotes in the sentence, and (5) Number of capitalized/all capitals wordsin the sentence.All these features were normal ized by dividing them by the (maximal observed value times averaged maximal value of the other feature groups), thus the maximal weight of each of these features is equal to the averaged weight of a single pattern/word/n-gram feature.3.2 Classification algorithm.In order to assign a sentiment label to new exam ples in the test set we use a k-nearest neighbors(kNN)-like strategy.We construct a feature vec tor for each example in the training and the test set.We would like to assign a sentiment class toeach example in the test set.For each feature vec tor V in the test set, we compute the Euclidean distance to each of the matching vectors in the training set, where matching vectors are defined as ones which share at least one pattern/n-gram/word feature with v.Let ti, i = 1 . . .k be the k vectors with low est Euclidean distance to v4 with assigned labels Li, i = 1 . . .k. We calculate the mean distance d(ti, v) for this set of vectors and drop from the set up to five outliers for which the distance was more then twice the mean distance.The label assigned 4We used k = 10 for all experiments.to v is the label of the majority of the remaining vectors.If a similar number of remaining vectors have different labels, we assigned to the test vector the most frequent of these labels according to their frequency in the dataset.If there are no matching vectors found for v, we assigned the default ?no sentiment?label since there is significantly more non-sentiment sentences than sentiment sentences in Twitter.In our experiments we used an extensive Twit ter data collection as training and testing sets.In our training sets we utilize sentiment hashtags andsmileys as classification labels.Below we de scribe this dataset in detail.4.1 Twitter dataset.We have used a Twitter dataset generously pro vided to us by Brendan O?Connor.This dataset includes over 475 million tweets comprising roughly 15% of all public, non-?low quality?tweets created from May 2009 to Jan 2010.Tweets are short sentences limited to 140 UTF 8 characters.All non-English tweets and tweets which contain less than 5 proper English words5 were removed from the dataset.Apart of simple text, tweets may contain URLaddresses, references to other Twitter users (ap pear as @<user>) or a content tags (also called hashtags) assigned by the tweeter (#<tag>)which we use as labels for our supervised clas sification framework.Two examples of typical tweets are: ?#ipad #sucks and 6,510 people agree.See more on Ipad sucks page: http://j.mp/4OiYyg??, and ?Pay nomind to those who talk behind ur back, it sim ply means that u?re 2 steps ahead.#ihatequotes?.Note that in the first example the hashtagged words are a grammatical part of the sentence (itbecomes meaningless without them) while #ihate qoutes of the second example is a mere sentiment label and not part of the sentence.Also note that hashtags can be composed of multiple words (with no spaces).5Identification of proper English words was based on an available WN-based English dictionary 244 Category # of tags % agreement Strong sentiment 52 87 Likely sentiment 70 66 Context-dependent 110 61 Focused 45 75 No sentiment 3564 99 Table 1: Annotation results (2 judges) for the 3852 mostfrequent tweeter tags.The second column displays the av erage number of tags, and the last column shows % of tags annotated similarly by two judges.During preprocessing, we have replaced URL links, hashtags and references by URL/REF/TAG meta-words.This substitution obviously had some effect on the pattern recognition phase (see Section 3.1.2), however, our algorithm is robust enough to overcome this distortion.4.2 Hashtag-based sentiment labels.The Twitter dataset contains above 2.5 million dif ferent user-defined hashtags.Many tweets include more than a single tag and 3852 ?frequent?tags appear in more than 1000 different tweets.Two human judges manually annotated these frequenttags into five different categories: 1 ? strong sen timent (e.g #sucks in the example above), 2 ?most likely sentiment (e.g., #notcute), 3 ? contextdependent sentiment (e.g., #shoutsout), 4 ? fo cused sentiment (e.g., #tmobilesucks where the target of the sentiment is part of the hashtag), and 5 ? no sentiment (e.g. #obama).Table 1 shows annotation results and the percentage of similarly assigned values for each category.We selected 50 hashtags annotated ?1?or ?2?by both judges.For each of these tags we automatically sampled 1000 tweets resulting in 50000 la beled tweets.We avoided sampling tweets which include more than one of the sampled hashtags.As a no-sentiment dataset we randomly sampled 10000 tweets with no hashtags/smileys from thewhole dataset assuming that such a random sam ple is unlikely to contain a significant amount of sentiment sentences.4.3 Smiley-based sentiment labels.While there exist many ?official?lists of possibleASCII smileys, most of these smileys are infrequent or not commonly accepted and used as sen timent indicators by online communities.We used the Amazon Mechanical Turk (AMT) service in order to obtain a list of the most commonly used and unambiguous ASCII smileys.We asked each of ten AMT human subjects to provide at least 6 commonly used ASCII mood-indicating smileystogether with one or more single-word descrip tions of the smiley-related mood state.From the obtained list of smileys we selected a subset of 15 smileys which were (1) provided by at least threehuman subjects, (2) described by at least two human subject using the same single-word descrip tion, and (3) appear at least 1000 times in our Twitter dataset.We then sampled 1000 tweets foreach of these smileys, using these smileys as sentiment tags in the sentiment classification frame work described in the previous section.The purpose of our evaluation was to learn how well our framework can identify and distinguishbetween sentiment types defined by tags or smileys and to test if our framework can be successfully used to identify sentiment types in new un tagged sentences.5.1 Evaluation using cross-validation.In the first experiment we evaluated the consistency and quality of sentiment classification us ing cross-validation over the training set.Fullyautomated evaluation allowed us to test the performance of our algorithm under several dif ferent feature settings: Pn+W-M-Pt-, Pn+W+M-Pt-, Pn+W+M+Pt-, Pn-W-M-Pt+ and FULL, where +/?stands for utilization/omission of the followingfeature types: Pn:punctuation, W:Word, M:n grams (M stands for ?multi?), Pt:patterns.FULL stands for utilization of all feature types.In this experimental setting, the training set was divided to 10 parts and a 10-fold cross validation test is executed.Each time, we use 9 parts as thelabeled training data for feature selection and con struction of labeled vectors and the remaining part is used as a test set.The process was repeated tentimes.To avoid utilization of labels as strong fea tures in the test set, we removed all instances of involved label hashtags/smileys from the tweets used as the test set.245 Setup Smileys Hashtags random 0.06 0.02 Pn+W-M-Pt- 0.16 0.06 Pn+W+M-Pt- 0.25 0.15 Pn+W+M+Pt- 0.29 0.18 Pn-W-M-Pt+ 0.5 0.26 FULL 0.64 0.31 Table 2: Multi-class classification results for smileys andhashtags.The table shows averaged harmonic f-score for 10 fold cross validation.51 (16) sentiment classes were used for hashtags (smileys).Multi-class classification.Under multi-class classification we attempt to assign a single label (51 labels in case of hashtags and 16 labels in case of smileys) to each of vectors in the test set.Note that the random baseline for this task is 0.02 (0.06)for hashtags (smileys).Table 2 shows the perfor mance of our framework for these tasks.Results are significantly above the random baseline and definitely nontrivial considering theequal class sizes in the test set.While still relatively low (0.31 for hashtags and 0.64 for smi leys), we observe much better performance forsmileys which is expected due to the lower num ber of sentiment types.The relatively low performance of hashtags can be explained by ambiguity of the hashtags andsome overlap of sentiments.Examination of clas sified sentences reveals that many of them can be reasonably assigned to more than one of the available hashtags or smileys.Thus a tweet ?I?mreading stuff that I DON?T understand again!ha haha...wth am I doing?may reasonably matchtags #sarcasm, #damn, #haha, #lol, #humor, #an gry etc. Close examination of the incorrectly classified examples also reveals that substantialamount of tweets utilize hashtags to explicitly in dicate the specific hashtagged sentiment, in these cases that no sentiment value could be perceived by readers unless indicated explicitly, e.g. ?De Blob game review posted on our blog.#fun?.Obviously, our framework fails to process such cases and captures noise since no sentiment datais present in the processed text labeled with a spe cific sentiment label.Binary classification.In the binary classification experiments, we classified a sentence as either appropriate for a particular tag or as not bear Hashtags Avg #hate #jealous #cute #outrageous Pn+W-M-Pt- 0.57 0.6 0.55 0.63 0.53 Pn+W+M-Pt- 0.64 0.64 0.67 0.66 0.6 Pn+W+M+Pt- 0.69 0.66 0.67 0.69 0.64 Pn-W-M-Pt+ 0.73 0.75 0.7 0.69 0.69 FULL 0.8 0.83 0.76 0.71 0.78 Smileys Avg :) ; ) X( : d Pn+W-M-Pt- 0.64 0.66 0.67 0.56 0.65 Pn+W+M-Pt- 0.7 0.73 0.72 0.64 0.69 Pn+W+M+Pt- 0.7 0.74 0.75 0.66 0.69 Pn-W-M-Pt+ 0.75 0.78 0.75 0.68 0.72 FULL 0.86 0.87 0.9 0.74 0.81Table 3: Binary classification results for smileys and hashtags.Avg column shows averaged harmonic f-score for 10fold cross validation over all 50(15) sentiment hashtags (smi leys).ing any sentiment6.For each of the 50 (15) labelsfor hashtags (smileys) we have performed a bi nary classification when providing as training/testsets only positive examples of the specific senti ment label together with non-sentiment examples.Table 3 shows averaged results for this case and specific results for selected tags.We can see thatour framework successfully identifies diverse sentiment types.Obviously the results are much bet ter than those of multi-class classification, and the observed > 0.8 precision confirms the usefulnessof the proposed framework for sentiment classifi cation of a variety of different sentiment types.We can see that even for binary classification settings, classification of smiley-labeled sentencesis a substantially easier task compared to classifi cation of hashtag-labeled tweets.Comparing the contributed performance of different feature typeswe can see that punctuation, word and pattern features, each provide a substantial boost for classi fication quality while we observe only a marginalboost when adding n-grams as classification features.We can also see that pattern features contribute the performance more than all other fea tures together.5.2 Evaluation with human judges.In the second set of experiments we evaluated our framework on a test set of unseen and untaggedtweets (thus tweets that were not part of the train 6Note that this is a useful application in itself, as a filter that extracts sentiment sentences from a corpus for further focused study/processing.246 ing data), comparing its output to tags assigned by human judges.We applied our framework with its FULL setting, learning the sentiment tags fromthe training set for hashtags and smileys (sepa rately) and executed the framework on the reduced Tweeter dataset (without untagged data) allowingit to identify at least five sentences for each senti ment class.In order to make the evaluation harsher, we re moved all tweets containing at least one of the relevant classification hashtags (or smileys).For each of the resulting 250 sentences for hashtags,and 75 sentences for smileys we generated an ?as signment task?.Each task presents a human judgewith a sentence and a list of ten possible hash tags.One tag from this list was provided by ouralgorithm, 8 other tags were sampled from the re maining 49 (14) available sentiment tags, and the tenth tag is from the list of frequent non-sentiment tags (e.g. travel or obama).The human judge was requested to select the 0-2 most appropriate tags from the list.Allowing assignment of multiple tags conforms to the observation that even short sentences may express several different sentimenttypes and to the observation that some of the selected sentiment tags might express similar senti ment types.We used the Amazon Mechanical Turk service to present the tasks to English-speaking subjects.Each subject was given 50 tasks for Twitter hash tags or 25 questions for smileys.To ensure the quality of assignments, we added to each test fivemanually selected, clearly sentiment bearing, as signment tasks from the tagged Twitter sentences used in the training set.Each set was presented to four subjects.If a human subject failed to provide the intended ?correct?answer to at least two of the control set questions we reject him/her from the calculation.In our evaluation the algorithmis considered to be correct if one of the tags se lected by a human judge was also selected by thealgorithm.Table 4 shows results for human judge ment classification.The agreement score for this task was ? = 0.41 (we consider agreement when at least one of two selected items are shared).Table 4 shows that the majority of tags selectedby humans matched those selected by the algo rithm.Precision of smiley tags is substantially Setup % Correct % No sentiment Control Smileys 84% 6% 92% Hashtags 77% 10% 90%Table 4: Results of human evaluation.The second col umn indicates percentage of sentences where judges find noappropriate tags from the list.The third column shows per formance on the control set.Hashtags #happy #sad #crazy # bored#sad 0.67 - - #crazy 0.67 0.25 - #bored 0.05 0.42 0.35 #fun 1.21 0.06 1.17 0.43 Smileys :) ; ) : ( X(; ) 3.35 - - : ( 3.12 0.53 - X( 1.74 0.47 2.18 : S 1.74 0.42 1.4 0.15 Table 5: Percentage of co-appearance of tags in tweeter corpus.higher than of hashtag labels, due to the lessernumber of possible smileys and the lesser ambi guity of smileys in comparison to hashtags.5.3 Exploration of feature dependencies.Our algorithm assigns a single sentiment type for each tweet.However, as discussed above, some sentiment types overlap (e.g., #awesome and #amazing).Many sentences may express several types of sentiment (e.g., #fun and #scary in ?Oh My God http://goo.gl/fb/K2N5z #entertainment #fun #pictures #photography #scary #teaparty?).We would like to estimate such inter-sentiment dependencies and overlap automatically from the labeled data.We use two different methods for overlap estimation: tag co-occurrence and feature overlap.5.3.1 Tag co-occurrenceMany tweets contain more than a single hashtag or a single smiley type.As mentioned, we ex clude such tweets from the training set to reduce ambiguity.However such tag co-appearances canbe used for sentiment overlap estimation.We cal culated the relative co-occurrence frequencies of some hashtags and smileys.Table 5 shows some of the observed co-appearance ratios.As expected some of the observed tags frequently co-appear with other similar tags.247 Hashtags #happy #sad #crazy # bored#sad 12.8 - - #crazy 14.2 3.5 - #bored 2.4 11.1 2.1 #fun 19.6 2.1 15 4.4 Smileys :) ; ) : ( X(; ) 35.9 - - : ( 31.9 10.5 - X( 8.1 10.2 36 : S 10.5 12.6 21.6 6.1 Table 6: Percentage of shared features in feature vectors for different tags.Interestingly, it appears that a relatively high ratio of co-appearance of tags is with opposite meanings (e.g., ?#ilove eating but #ihate feeling fat lol?or ?happy days of training going to end in a few days #sad #happy?).This is possibly due to frequently expressed contrast sentiment types in the same sentence ? a fascinating phenomenareflecting the great complexity of the human emo tional state (and expression).5.3.2 Feature overlapIn our framework we have created a set of fea ture vectors for each of the Twitter sentiment tags.Comparison of shared features in feature vector sets allows us to estimate dependencies betweendifferent sentiment types even when direct tag cooccurrence data is very sparse.A feature is considered to be shared between two different senti ment labels if for both sentiment labels there is at least a single example in the training set whichhas a positive value of this feature.In order to automatically analyze such dependencies we calcu late the percentage of sharedWord/n-gram/Pattern features between different sentiment labels.Table 6 shows the observed feature overlap values for selected sentiment tags.We observe the trend of results obtained by comparison of shared feature vectors is similar to those obtained by means of label co-occurrence, although the numbers of the shared features arehigher.These results, demonstrating the patternbased similarity of conflicting, sometimes contradicting, emotions are interesting from a psycho logical and cognitive perspective.We presented a framework which allows an au tomatic identification and classification of various sentiment types in short text fragments which isbased on Twitter data.Our framework is a su pervised classification one which utilizes Twitterhashtags and smileys as training labels.The substantial coverage and size of the processed Twitter data allowed us to identify dozens of senti ment types without any labor-intensive manuallylabeled training sets or pre-provided sentiment specific features or sentiment words.We evaluated diverse feature types for senti ment extraction including punctuation, patterns,words and n-grams, confirming that each feature type contributes to the sentiment classifica tion framework.We also proposed two different methods which allow an automatic identification of sentiment type overlap and inter-dependencies.In the future these methods can be used for automated clustering of sentiment types and senti ment dependency rules.While hashtag labels arespecific to Twitter data, the obtained feature vectors are not heavily Twitter-specific and in the fu ture we would like to explore the applicability ofTwitter data for sentiment multi-class identifica tion and classification in other domains.
Anaphora For Everyone: Pronominal Anaphora Resolution Without A ParserWe present an algorithm for anaphora res- olutkm which is a modified and extended version of that developed by (Lappin and Leass,/994).In contrast to that work, our al- gorithm does not require in-depth, full, syn..tactic parsing of text.Instead, with minimal compromise in output quality, the modifica- tions enable the resolution process to work from tile output of a part of speech tag- ge~; enriched only with annotations of gram- matica\] functkm of lexical items in the in- put text stream.Evaluation of the results of our in-tplementation demonstrates that ac- curate anaphora resolution can be realized within natural anguage processing fl'ame- works which do not--~,)r cannot- employ ro- bust and rcqiable parsing components.(l,appin and Leass, 1994) describe an algorithm for pronominal anaphora resolution with high rate of cor- rect analyses.While one of the strong points of this algorithm is that it operates primarily on syntactic in- formation ahme, this also turns out to be a limiting factor for its wide use: current state-of-the-art of prac- tically applicable parsing technology still falls short of robust and reliable delivery of syntactic analysis of real texts to the level of detail and precision that the filters a nd constraints described by I ,appin and l ,eass assume.We are particularly interested in a class of text pro- cessing applications, capable of delivery of content analysis to a depth inw~lving non-trivial amount of discourse processing, including anaphora resolution.The operational context prohibits us from making any assumptions concerning domain, style, and genre of input; as a result, we have developed a text processing framework which builds its capabilities entirely on the basis of a considerably shallower linguistic analysis of the input stream, thus trading off depth of base level analysis for breadth of cown:age.In this paper, we present work on modifying the lmp- pin/Leass algorithm in a way which enables it to work off a flat morpho-syntactic analysis of the sentences of a text, while retaining a degree of quality and accuracy in pronorainal anaphora resolution comparable to that reported in (Lappin and l,eass, 1994).The modifica- tions discussed below make the algorithm available to a wide range of text processing frameworks, which, due to the lack of full syntactic parsing capability, nor- really would have been unable to use this high preci- sion anap hora resolution tool.The work is additionally important, we feel, as it shows that informatkm about the content and logical structure of a text, in princi-.pie a core requirement for higher level semantic and discourse processes, can be effectively approximated by the right mix of constituent analysis and inferences about functional relations.The base level linguistic analysis for actaphora resolu- tion is the output of a part of speech tagger, augmented with syntactic function annotatkms for each input to.ken; this kind of analysis is generated by the mor- pbosyntactic tagging system described in (Voutilainen et al, 1992), (Karlsson et al, 1995) (hencehvth 1,1NC:- ~;olq').In addition to extremely high levels of accuracy in recall and precision of tag assignment ((VoutiJainen et al, 1992) report 99.77?/,, overall recall and 95.54% overall preciskm, over a variety of text genres, and in comparison with other state-of-the-art tagging sys- tems), the primary motivation for adopting this system is the requirement todevelop a robust ext processor- with anaphora resolution being just one of its discourse analysis functkms capable of reliably handling arbi- trary kinds of input.The tagger provides a very simple analysis of the structure of the text: for each lexical item in each sen- tence, it provides a set of values which indicate the morphological, lexical, grammatical nd syntactic fea- tures of the item in tile context in which it appears.In addition, the modified algorithm we present requh:es annota tion of the input text stream by a simple position-- identification function which associates an integer with each token in a text sequentially (we will refer to a to- ken's integer value as its oJ~et).As an example, given the text "For 1995 the company set up its headquar- ters in Hall \] l, the newest and most presti-.gious of CeBIT's 23 hal Is." tile anaphora resolutkm algorithm would be presented with the h}llowing analysis tream.Note, in particu-.lar, the grammatical function information (e.g., @SUl~J, O)q.FMAINV) and the integer values (e.g., "offt 39") asso- cia ted with each token."For /o f f139" "for" PREP @ADVL "1995/o f f140 ....1995" NUM CARD @<P " the/o f f l41" "the" DET CENTRAL ART SG/PL @DN> "company/o f f142" "company" N NOM SG/PL @SUBJ "set/off143" "set" V PAST VF IN @+FMAINV "up/of f144" "up" ADV ADVL @ADVL " i t s /o f f145 .... it" PRON GEN SG3 @GN> "headquar ters /o f f146 .... headquar ters" N NOM SG/PL @OBJ " in /o f f147 .... in" PREP @<NOM @ADVL "Ha l l /o f f148 .... hal l" N NOM SG @NN> " l l /o f f149" "Ii" NUM CARD @<P "$ , /o f f l50 ....," PUNCT " the/o f f l51" "the" DET CENTRAL ART SG/PL @DN> "newest /o f f152 .... new" A SUP @PCOMPL-O "and/of f153 .... and" CC @CC "most /o f f154" "much" ADV SUP @AD-A> "pres t ig ious /o f f155 .... p res t ig ious" A ABS @<P "of /o f f156 .... of" PREP @<NOM-OF "CeBIT ' s /o f f157" "cebit" N GEN SG @GN> "23/0f f158 ....23" NUM CARD @QN> "ha l l s /o f f159 .... hal l" N NOM PL @<P "$ . /o f f160 ....PUNCT 2.1 Data collection.Although LINGSOFT does not provide specific infor- mation about constituent structure, partial constituen- cy-specifically, identification of sequences of tokens as phrasal units--can be inferred from the analysis by running the tagged text through a set of filters, which are stated as regular expressions over metatokens such as the ones illustrated above.For the purposes of anaphora resolution, the pri- mary data set consists of a complete listing of all noun phrases, reduced to modifier-head sequences.This data set is obtained by means of a phrasal grammar whose patterns characterize the composition of a noun phrase (NP) in terms of possible token sequences.The output of NP identification is a set of token/feature matrix/offset sequences, where offset value is deter- mined by the offset of the first token in the sequence.The offset indicates the position of the NP in the text, and so provides crucial information about precedence relations.A secondary data set consists of observations about the syntactic ontexts in which the NPs identified by the phrasal grammar appear.These observations are derived using a set of patterns designed to detect nom- inal sequences in two subordinate syntactic environ- ments: containment in an adverbial adjunct and con- tainment in an NP (i.e., containment in a prepositional or clausal complement of a noun, or containment in a relative clause).This is accomplished by running a set of patterns which identify NPs that occur locally to ad- verbs, relative pronouns, and noun-preposition r noun- complementizer sequences over the tagged text in con- junction with the basic NP patterns described above.Because the syntactic,patterns are stated as regular ex- pressions, misanalyses are inevitable.In practice, how- ever, the extent o which incorrect analyses of syntactic context affect the overall accuracy of the algorithm is not large; we will return to a discussion of this point in section 4.A third set of patterns identifies and tags occurrences of "expletive" it.These patterns target occurrences of the pronoun it in certain contexts, e.g., as the subject of members of a specific set of verbs (seem, appear, etc.), or as the subject of adjectives with clausal complements.Once the extraction procedures are complete and the results unified, a set of discourse referents--abstract ob- jects which represent the participants inthe discourse-- is generated from the set of NP observations.A particu- larly convenient implementation f discourse referents is to represent them as objects in the Common Lisp Object System, with slots which encode the following information parameters (where ADJUNCT and EMBED indicate whether a discourse referent was observed in either of the two syntactic ontexts discussed above): TEXT: text form TYPE: referential type (e.g., REF, PRO, RFLX) AGR: person, number, gender GFUN: grammatical function ADJUNCT: T o r NIL EMBED: T o r NIL POS: text position Note that each discourse referent contains information about itself and the context in which it appears, but the only information about its relation to other dis- course referents is in the form of precedence r lations (as determined by text position).The absence of explicit information about configurational relations marks the crucial difference between our algorithm and the Lap- pin/Leass algorithm.(Lappin and Leass, 1994) use configurational information in two ways: as a factor in the determination of the salience of a discourse refer- ent (discussed below), and as input to a set of disjoint reference filters.Our implementation seeks to perform exactly the same tasks by inferring hierarchical rela- tions from a less rich base.The modifications and assumptions required to accomplish this goal will be highlighted in the following discussion.2.2 Anaphora resolution.Once the representation f the text has been recast as a set of discourse referents (ordered by offset value), it is sent to the anaphora resolution algorithm proper.The basic logic of the algorithm parallels that of the Lap- pin/Leass algorithm.The interpretation procedure in- volves moving through the text sentence by sentence and interpreting the discourse referents in each sen- tence from left to right.There are two possible in- terpretations of a discourse referent: either it is taken to introduce a new participant in the discourse, or it is taken to refer to a previously interpreted iscourse referent.Coreference is determined by first eliminating from consideration those discourse referents to which an anaphoric expression cannot possibly refer, then se- lecting the optimal antecedent from the candidates that remain, where optimality is determined by a salience measure.In order to present the details of anaphora resolution, we define below our notions--and implementations-- of coreference and salience.2.2.1 Coreference As in the Lappin and Leass algorithm, the anaphor- antecedent relation is established between two dis- course referents (cf.(Helm, 1982), (Kamp, 1981)), @hile the more general notion of coreference is represented in terms of equivalence classes of anaphorically re- lated discourse referents, which we will refer to as "COREF classes".Thus, the problem of interpreting an anaphoric expression boils down to the problem of es- tablishing an anaphoric link between the anaphor and some previously interpreted iscourse referent (pos- sibly another anaphor); a consequence of establishing 114 this link is that the anaphor becomes a member of the COREF class already associated with its antecedent.In our implementation, COREF classes are repre- sented as objects in the Common Lisp Object System which contain information about the COREF class as a whole, including canonical form (typically deter- mined by the discourse referent which introduces the class), membership, and, most importantly, salience (discussed below).1 The connection between a dis- course referent and its COREF class is mediated through the COREF object as follows: every discourse referent includes an information parameter which is a pointer to a COREF object; discourse referents which have been determined to be coreferential share the same COREF value (and so literally point to the same object).Imple- menting coreference in this way provides a means of getting from any discourse referent in a COREF class to information about the class as a whole.2.2.2 Salience The information parameter of a COREF object most cru- cial to anaphora resolution is its salience, which is de- termined by the status of the members of the COREF class it re.presents with respect to 10 contextual, gram- matical, and syntactic onstraints.Following (Lappin and Leass, 1994), we will refer to these constraints as "salience factors".Individual salience factors are asso- ciated with numerical values; the overall salience, or "salience weight" of a COREF is the sum of the values of the salience factors that are satisfied by some member of the COREF class (note that values may be satisfied at most once by each member of the class).The salience factors used by our algorithm are defined below with their values.Our salience factors mirror those used by (Lappin and Leass, 1994), with the exception of Poss-s, discussed below, and CNTX-S, which is sensitive to the context in which a discourse referent appears, where a context is a topically coherent segment of text, as deter- mined by a text-segmentation algorithm which follows (Hearst, 1994).SENT-S: 100 iff in the current sentence CNTX-S: 50 iff in the current context SUBJ-S: 80 iff GFUN = subject EXST-S: 70 iff in an existential construction POSS-S: 65 iff GFUN = possessive ACC-S: 50 iff GFUN = direct object DAT-S: 40 iff GFUN = indirect object OBLQ-S: 30 iff the complement of a preposition HEAD-S: 80 iff EMBED = NIL ARG-S: 50 iff ADJUNCT = NIL Note that the values of salience factors are arbitrary; what is crucial, as pointed out by (Lappin and Leass, 1994), is the relational structure imposed on the factors by these values.The relative ranking of the factors is justified both linguistically, as a reflection of the role of the functional hierarchy in determining anaphoric relations (cf.(Keenan and Comrie, 1977)), as well as by experimental results--both Lappin and Leass' and our own.For all factors except CNTX-S and POSS-S, we adopt the values derived from a series of experiments described in (Lappin and Leass, 1994) which used dif- ferent settings to determine the relative importance of 1The implementation of aCOREF object needs to be aware of po- tenlial circularities, thus a COREF does not actually contain its member discourse r ferents, but rather alisting of their offsets, each factor as a function of the overall success of the algorithm.Our values for CNTX-S and POSS-S were de- termined using similar tests.An important feature of our implementation of salience, following that of Lappin and Leass, is that it is variable: the salience of a COREF class decreases and increases according to the frequency of reference to the class.When an anaphoric link is established between a pronoun and a previously introduced iscourse refer- ent, the pronoun is added to the COREF class associated with the discourse referent, its COREF value is set to the COREF value of the antecedent (i.e., to the COREF ob- ject which represents he class), and the salience of the COREF object is recalculated according to how the new member satisfies the set of salience factors.This final step raises the overall salience of the COREF, since the new member will minimally satisfy SENT-S and CNTX-S.Salience is not stable, however: in order to realisti- cally represent the local prominence of discourse ref- erents in a text, a decay function is built into the algo- rithm, so that salience weight decreases over time.If new members are not added, the salience weight of a COREF eventually reduces to zero.The consequence of this variability in salience is that a very general heuris- tic for anaphora resolution is established: resolve a pronoun to the most salient candidate antecedent.2.2.3 Interpretation As noted above, in terms of overall strategy, the resolu- tion procedure follows that of Lappin and Leass.The first step in interpreting the discourse referents in a new sentence isto decrease the salience weights of the COREF classes that have already been established by a factor of two.Next, the algorithm locates all non-anaphoric dis- course referents in the sentence under consideration, generates a new COREF class for each one, and calcu- lates its salience weight according to how the discourse referent satisfies the set of salience factors.The second step involves the interpretation f lexical anaphors (reflexives and reciprocals).A list of candi- date antecedent-anaphor pairs is generated for every lexical anaphor, based on the hypothesis that a lexical anaphor must refer to a coargument.In the absence of configurational information, coarguments are iden- tified using grammatical function information (as de- termined by LINGSOFT) and precedence relations.A reflexive can have one of three possible grammatical function values: direct object, indirect object, or oblique.In the first case, the closest preceding discourse referent with grammatical function value subject is identified as a possible antecedent.In the latter cases, both the clos- est preceding subject and the closest preceding direct object hat is not separated from the anaphor by a sub- ject are identified as possible antecedents.If more than one possible antecedent is located for a lexical anaphor, the one with the highest salience weight is determined to be the actual antecedent.Once an antecedent has been located, the anaphor is added to the COREF class associated with the antecedent, and the salience of the COREF class is recalculatec~ accordingly.The final step is the interpretation f pronouns.The basic resolution heuristic, as noted above, is quite sim- ple: generate a set of candidate antecedents, then es- tablish coreference with the candidate which has the greatest salience weight (in the event of a tie, the clos- est candidateis chosen).In order to generate the candi- date set, however, those discourse referents with which 115 a pronoun cannot refer must be eliminated from consid- eration.This is accomplished by running the overall candidate pool (the set of interpreted iscourse ref- erents whose salience values exceed an arbitrarily set threshold) through two sets of filters: a set of morpho- logical agreement filters, which eliminate from consid- eration any discourse referent which disagrees in per- son, numbeb or gender with the pronoun, and a set of disjoint reference filters.The determination f disjoint reference represents a significant point of divergence between our algorithm and the Lappin/Leass algorithm, because, as is well known, configurational relations play a prominent role in determining which constituents in a sentence a pro- noun may refer to.Three conditions are of particular relevance to the anaphora resolution algorithm: Condition \]: A pronoun cannot corefer with a coargument.Condition 2: A pronoun cannot corefer with a nonpronominal constituent which it both commands and precedes.Condition 3: A pronoun cannot corefer with a constituent which contains it.In the absence of configurafional information, our al- gorithm relies on inferences from grammatical func- tion and precedence todetermine disjoint reference.In practice, even without accurate information about con- stituent structure, the syntactic filters described below are extremely accurate (see the discussion of this point in section 4).Condition i is implemented bylocating all discourse referents with GFUN value direct object, indirect object, or oblique which follow a pronoun with GFUN value subject or direct object, as long as no subject intervenes (the hypothesis being that a subject indicates the beginning of the next clause).Discourse referents which satisfy these conditions are identified as disjoint.Condition 2 is implemented by locating for ev- ery non-adjunct and non-embedded pronoun the set of non-pronominal discourse referents in its sentence which follow it, and eliminating these as potential an- tecedents.In effect, the command relation is inferred from precedence and the information provided by the syntactic patterns: an argument which is neither con- tained in an adjunct nor embedded in another nominal commands those expressions which it precedes.Condition 3 makes use of the observation that a dis- course referent contains every object o its right with a non-nil EMBED value.The algorithm identifies as dis- joint a discourse referent and every pronoun which fol- lows it and has a non-nil EMBED value, until a discourse referent with EMBED value NIL is located (marking the end of the containment domain).Condiditon 3 also rules out coreference between a genitive pronoun and the NP it modifies.After the morphological nd syntactic filters have been applied, the set of discourse referents that remain constitute the set of candidate antecedents for the pro- noun.The candidate set is subjected to a final evalu- ation procedure which performs two functions: it de- creases the salience of candidates which the pronoun precedes (cataphora is penalized), and it increases the sa li ence of candida tes which satisfy either a locality or a parallelism condition (described below), both of which apply to intrasentential c ndidates.The h)cality heuristic isdesigned to negate the effects of subordinationwhen both candidate and anaphor ap- pear in the same subordinate context, the assumption being that the prominence of a candidate should be de- termined with respect o the position of the anaphor.This is a point of difference between our algorithm and the one described in (Lappin and Leass, 1994).The salience of a candidate which is determined tobe in the same subordinate context as a pronoun (determined as a function of precedence r lations and EMBED and ADJUNCT values) is temporarily increased to the level it would have were the candidate not in the subordi- nate context; the level is returned to normal after the anaphor is resolved.The parallelism heuristic rewards candidates which are such that the pair consisting of the GFUN values of candidate and anaphor are identical to GFUN values of a previously identified anaphor-antecedent pair.This parallelism heuristic differs from a similar one used by the Lappin/Leass algorithm, which rewards candi- dates whose grammatical function is identical to that of an anaphor.Once the generation and evaluation of the candidate set is complete, the candidates are ranked according to salience weight, and the candidate with the high- est salience weight is determined tobe the antecedent of the pronoun under consideration.In the event of a tie, the candidate which most immediately precedes the anaphor is selected as the antededent (where prece- dence is determined by comparing offset values).The COREF value of the pronoun is set to that of the an- tecedent, adding it to the the antecedent's COREF class, and the salience of the class is recalculated accordingly.The larger context from which the sample analysis in the beginning of Section 2 was taken is as follows: "...while Apple and its PowerPC partners claimed some prime real estate on the show floor, Apple's most interesting offerings de- buted behind the scenes.Gone was the nar- row corner booth that Apple shoehorned its products into last year.For 1995 the com- pany set up its headquarters in Hall 11, the newest and most prestigious of CeNT's 23 halls."The anaphora resolution algorithm generates the fol- lowing analysis for the first italicized pronoun.For each candidate, ~ the annotation i square brackets in- dicates its offset value, and the number to the right indicates its salience weight at the point of interpreta- tkm of the pronoun.ANA: its \[@off/\]33\] CND: Apple \[@of 1/131\] 432 Apple \[/aol f/10\] \] 352 its \[@off/\].03\] 352 App\]e's \[@offf/\] I 5\] 1352 prilne real estat(!\[@off/\]08\] 165 show f loor \ [ (aof f /1 \ ]2 l \]55 year \[@o~f/137 I 310/3 The candidate set illustrates several important points.First, the equality in salience weights of the candi- dates at offsets 101, 103, and 115 is a consequence of 2Note that our syntactic filters are quite capable of discarding a number of configurationally inappropriate antecedents, which appear to satisfy the precedence r lation.116 the fact that these discourse referents are members of the same COP, Et ~' class.Their unification into a single class indicates both successful anaphora resolution (of the pronoun at offset 103), as well as the operation of higherqevel discourse processing designed to identify all references to a particular COREF class, not just the anaphoric ones (cf.(Kennedy and Boguraev, :1996)).The higher salience of the optimal candidate--which ix also a member of this COREF class--shows the effect of the locality heuristic described in section 2.2.3.Both the pronoun and the candidate appear in the same sub- ordinate context (within a relative clause); as a result the salience of the candidate (but not of the class to which it bekmgs) is temporarily boosted to negate the effect of subordinatkm.An abbreviated candidate set for the second itali- cized pronoun is given below: ANA: i t s {61of f /145\ ] CND: company \[(,)ot I / 142 \] :H,0 App l e ((,!of 17/ 13 / \] 192 it:~:; {(aof I / I 3 ~ \] 192 This set is interesting because it illustrates the promi- nent role of SENT-S in controlling salience: company ix correctly identified as the antecedent of the pronotm, despite the frequency of mention of members of the COREF class containing Apple and its, because it occurs in the same sentence as the anaphor.Of course, this ex- ample also indicates the need fl~r additional heuristics designed to connect company with Apple, since these discourse referents clearly make reference to the same object.We are currentlyworking towards this goal; see (Kennedy and Boguraev, \]996) for discussion.'l'he following text segment illust rates the resolution of in tersen ten tia l a napho ra."Sun's prototype lntemet access device uses a 1-10-Mhz MicroSPARCprocesso~; and is diskless.Its dimensions are 5.5 inches x 9 inches x 2inches."ANA: \]its \[\[aol f /347\ ] CNI): IAlte~:ileL access devic() \[(,!o~\[/33\[i\] 180 M i c KOf;PARCI)rOC e!s sot \[(4oEI /34\] \] 16!i ~;un 's \[<4o1 f /3 : t3 I \ [40 The first sentence in this fl'agment introduces three dis- course referents bearing different grammatical func- tions, none of which appear in subordinate contexts.Since the sentence in which the anaphor occurs does not contain any candidates (the discourse referent in- troduced by dimensions ix eliminated from considera- tion by both the morphok)gical nct disjoint reference filters), only those from the previous entence are con- sidered (each is compatible with the morphological requirements of the anaphor).These are ranked ac- cording to salience weight, where the crucial factor is grammatical function value.The result of the ranking is that Internet access device--the candidate which satis- fies the highest-weighted salience facto1, SUBl-S--is the optimal candidate, and so correctly identified as the an tecedentQuantitative evaluation shows the anaphora resolution algorithm described here to run at a rate of 75'70 accu- racy.The data set on which the evaluatkm was based consisted of 27 texts, taken from a random selection of genres, including press releases, product annotmce- meats, news stories, magazine articles, and other doc- uments existing as World Wide Web pages.Within these texts, we counted 3(16 third person anaphoric pro- nouns; of these, 231l were correctly resolved to the dis- course referent identified as the antecedent by the first author.3 This rate of accuracy is clearly comparable to that of the Lappin/Leass algorithm, which (Lappin and Leass, \] 994) report as 85?/,,.Several observations about he results and the com- parison with (lmppin and I,eass, 1994) are in order.First, and most obviously, some deterioratkm in qual- ity is to be expected, given the relatively impoverished linguistic base we start with.Second, it is important to note that this is not just a matter of simple comparison.The results in (l.appin and Leass, 1994) describe the output of the procedttre applied to a singh,' text genre: computer manuals.Ar- guably, this is an example of a particularly well be- haved text; in any case, it is not clear how the figure would be normalized over a wide range of text types, some of them not completely 'clean', as is the case with our data.Third, close analysis of the most common types of error our algorithm currently makes reveals two spe- cific configurations in the input which confuse the pro- cedure and contribute to the error rate: gender mis- match (35% of errors) and certain long range contextttal (stylistic) phenomena, best exemplified by text contain- ing quoted passages in-line (14% of errors).Implementing a gender (dis-)agreement fil er is not technically complex; as noted above, the current algo- rithrn contains one.The persistence of gender mis- matches in the output simply reflects the lack of a con- sistent gender slot in the I,\[NGSOFT tagger output.Aug- menting the algorithm with a lexical database which includes more detailed gender information will result in improved accuracy.Ensuring proper interpretatkm of anaphors both within and outside of quoted text requires, in effect, a method of evaluating quoted speech separately from its surrotmdingcnntext.Al hough acomplex problem, we feel that this is possible, given that our input data stream embodies a richer notkm of position and con- text, as a resu\[t of an independent text segmentation procedure adapted from (\[ learst, 1994) (and discussed above in section 2.2.2).What is worth noting is the small number of errors which can be directly attributed to the absence of con- figurational inh~rmation.Of the 75 misinterpreted pro- nouns, only 2 inw~lved a failure to establish configu- ratkmally determined disjoint reference (both of these inw~lved Condition 3), and only an additional several errors could be tmambiguously traced to a failure to correctly identify the syntactic ontext in which a dis~ course referent appeared (as determined by a misfireof the salience factors ensitive to syntactic context, I lEAD- S and ARC:S).Overall, these considerations lead to two conchl-.sions.First, with the incorporation of more explicit morphological nd contextual information, it should 3The set of 306 "anaphoric" pronouns excluded 30 occurrences of "expletive" it not identified by the expletive patterns (prhnari ly occurrences in object position), as well as 6 occurrences of it which referred to a VP or propositional constituent.We are currently mfinin g the existing expletive patterns for improved accuracy.117 be possible to increase the overall quality of our out- put, bringing it much closer in line with Lappin and Leass' results.Again, straight comparison would not be trivial, as e.g. quoted text passages are not a natural part of computer manuals, and are, on the other hand, an extremely common occurrence in the types of text we are dealing with.Second, and most importantly, the absence of ex- plicit configurational information does not result in a substantial degradation i the accuracy of an anaphora resolution algorithm that is otherwise similar to that described in (Lappin and Leass, 1994).Lappin and Leass' algorithm for pronominal anaphora resolution is capable of high accuracy, but requires in- depth, full, syntactic parsing of text.The modifications of that algorithm that we have developed make it avail- able to a larger set of text processing frameworks, as we assume a considerably 'poorer' analysis ubstrate.While adaptations to the input format and interpreta- tion procedures have necessarily addressed the issues of coping with a less rich level of linguistic analysis, there is only a small compromise in the quality of the results.Our evaluation indicates that the problems with the current implementation donot stem from the absence of a parse, but rather from factors which can be addressed within the constraints imposed by the shallow base analysis.The overall success of the algo- rithm is important, then, not only for the immediate utility of the particular modifications, but also because the strategy we have developed for circumventing the need for full syntactic analysis is applicable to other in- terpretation tasks which, like the problem of anaphora resolution, lie in the space of higher level semantic and discourse analysis.
HMM-Based Word Alignment In Statistical TranslationIn this paper, we describe a new model for word alignment in statistical trans- lation and present experimental results.The idea of the model is to make the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions.To achieve this goal, the approach us- es a first-order Hidden Markov model (HMM) for the word alignment problem as they are used successfully in speech recognition for the time alignment prob- lem.The difference to the time align- ment HMM is that there is no monotony constraint for the possible word order- ings.We describe the details of the mod- el and test the model on several bilingual corpora.In this paper, we address the problem of word alignments for a bilingual corpus.In the recent years, there have been a number of papers con- sidering this or similar problems: (Brown et al, 1990), (Dagan et al, 1993), (Kay et al, 1993), (Fung et al, 1993).In our approach, we use a first-order Hidden Markov model (HMM) (aelinek, 1976), which is similar, but not identical to those used in speech recognition.The key component of this approach is to make the alignment probabilities dependent not on the absolute position of the word align- ment, but on its relative position; i.e. we consider the differences in the index of the word positions rather than the index itself.The organization of the paper is as follows.After reviewing the statistical approach to ma- chine translation, we first describe the convention- al model (mixture model).We then present our first-order HMM approach in lull detail.Finally we present some experimental results and compare our model with the conventional model.The goal is the translation of a text given in some language F into a target language E. For conve- nience, we choose for the following exposition as language pair French and English, i.e. we are giv- en a French string f~ = fx ...fj...fJ, which is to be translated into an English string e / = el...ei...cl. Among all possible English strings, we will choose the one with the highest probability which is given by Bayes' decision rule: a{ = argmax{P,.(c{lAa)} q = argmax {Pr(ejt) . l ' r ( f  le\[)} el ~ Pr(e{) is the language model of the target lan- guage, whereas Pr(fJle{) is the string translation model.The argmax operation denotes the search problem.In this paper, we address the problem of introducing structures into the probabilistic de- pendencies in order to model the string translation probability Pr(f~ le{).A key issne in modeling the string translation probability Pr(J'~le I) is the question of how we define the correspondence b tween the words of the English sentence and the words of the French sentence.In typical cases, we can assume a sort of pairwise dependence by considering all word pairs (fj, ei) for a given sentence pair I.-/1\[~'J', elqlj' We fur- ther constrain this model by assigning each French word to exactly one English word.Models describ- ing these types of dependencies are referred to as alignment models.In this section, we describe two models for word alignrnent in detail: ,.a mixture-based alignment model, which was introduced in (Brown et al, 1990); ? an HMM-based alignment model.In this paper, we address the question of how to define specific models for the alignment probabil- ities.The notational convention will be as fol- lows.We use the symbol Pr(.)to denote general 836 probability distributions with (nearly) no Sl)eeitic asSUml)tions.In contrast, for modcl-t)ased prol)-- ability distributions, we use the generic symbol v(.).3.1 Al ignment w i th M ix ture D is t r i |mt ion.Here, we describe the mixture-based alignment model in a fornmlation which is different fronl the original formulation ill (Brown el, a\[., 1990).We will ,is(: this model as reference tbr the IIMM- based alignments to lie 1)resented later.The model is based on a decomposition of the joint probability \[br ,l'~ into a product over the probabilities for each word J): a j=l wheFe~ fo\[' norll-la\]iz;i, t on 17(~/SOllS~ the 8elltC\]\[ce length probability p(J\] l) has been included.The next step now is to assutne a sort O\['l,airwise inter- act, ion between tim French word f j an(l each, F,n- glish word ci, i = 1, ... l . These dep('ndencies are captured in the lbrm of a rnixtnre distritmtion: 1 p(J)le{) = ~_.p(i, fjlc I) i=1 I = ~_~p(ilj, l).p(fjle~) i=1 Putting everything together, we have the following mixture-based ntodel: J l r,'(fi!l~I) = p(JIO ' H ~_~ \[~,(ilJ, l).~,(j)led\] (1) j= l i=t with the following ingredients: ? sentence length prob~d)ility: P(J l l); ? mixture alignment probability: p( i l j , I); ? translation probM)ility: p(f\[e).Assuming a tmifornl ~flignment prol)ability 1 .p(ilj, 1) = 7 we arrive at the lh'st model proposed t)y (Brown et al, 1990).This model will be referred to as IB M 1 model.To train the translation probabilities p(J'fc), we use a bilingual (;orpus consisting of sentence pairs \[:/ ';4"1 : ', . , s Using the ,,laxin,ul , like- lihood criterion, we ol)tain the following iterative L a equation (Brown et al, 1990): / ) ( f ie) = ~ - will, $' A(f,e) = ~ 2 ~5(f,J).~) }~ a(e,e~.~) For unilbrm alignment probabilities, it can be shown (Brown et al, 1990), that there is only one optinnnn and therefore the I,',M algorithm (Baum, 1!)72) always tinds the global optimum.For mixture alignment model with nonunilbrm alignment probabilities (subsequently referred to as IBM2 model), there ~tre to() many alignrnent parameters Pill j , I) to be estimated for smMl co l pora.Therefore, a specific model tbr tile Mign- ment in:obabilities i used: r ( i - j~- ) (~) p( i l j , 1) = l . I E i ' : l "( it --" J J-) This model assumes that the position distance rel- ative to the diagonal ine of the (j, i) plane is the dominating factor (see Fig.1).'lb train this mod- el, we use the ,naximutn likelihood criterion in the so-called ulaximmn al)proximation, i.e. the likeli- hood criterion covers only tile most lik(-.ly align: inch, rather than the set of all alignm(,nts: d P,'(f(I,:I) ~ I I ~"IU HO, ~)v(J} I,:~)\] (a) j=l In training, this criterion amounts to a sequence of iterations, each of which consists of two steps: * posi l ion al ignmcnl: (riven the model parame- ters, deLerlniim the mosL likely position align- \]lient.paramc, lcr cst imal ion: Given the position alignment, i.e. goiug along the alignment paths for all sentence pairs, perform maxi- tnulu likelihood estimation of the model pa- rameters; for model-De(' distributions, these estimates result in relative frequencies.l)ue to the natnre of tile nfixture tnod(:l, there is no interaction between d jacent word positions.Theretbre, the optimal position i for each posi- tion j can be determined in(lependently of the neighbouring positions.Thus l.he resulting train- ing procedure is straightforward.a.2 Al ignment w i th HMM We now propose all HMM-based alignment model.'\['he motivation is that typicMly we have a strong localization effect in aligning the words in parallel texts (for language pairs fi:om \]ndoeuropean lan- guages): the words are not distrilmted arbitrarily over the senteuce \])ositions, but tend to form clus- ters.Fig.1 illustrates this effect for the language pair German- 15'nglish.Each word of the German sentence is assigned to a word of the English sentence.The alignments have a strong tendency to preserve the local neigh- borhood when going from the one langnage to the other language.In mm,y cases, although not al~ ways, there is an even stronger restriction: the differeuce in the position index is smMler than 3.837 DAYS BOTH ON EIGHT AT IT MAKE CAN WE IF THINK I WELL + + + + + + + + +j~ + + + + + + + + + ~J ~+ + +++++++/+?+.- . + + + + + + +/+ + + + + + + + + + ~x~ + + + + + + + + + +/+ D + + + + + ++ + + ~ + + + + + + + + + + _~ + + + + + + + + + +~ + + ++ ++++ + +jg + + + + + + + + + +~ +++ + + + + + + + g + + ++ + ++ + + + + z aa Figure 1: Word alignment for a German- English sentence pair.To describe these word-by-word aligmnents, we introduce the mapping j ---+ aj, which assigns a word f j in position j to a word el in position { = aj.The concept of these alignments i similar to the ones introduced by (Brown et al, 1990), but we wilt use another type of dependence in the probability distributions.Looking at such align- ments produced by a hmnan expert, it is evident that the mathematical model should try to cap- ture the strong dependence of aj on the previous aligmnent.Therefore the probability of alignment aj for position j should have a dependence on the previous alignment aj _ 1 : p(a j ia j_ l , i ) , where we have inchided the conditioning on the total length \[ of the English sentence for normal- ization reasons.A sinfilar approach as been cho- sen by (Da.gan et al, 1993).Thus the problem formulation is similar to that of the time align- ment problem in speech recognition, where the so-called IIidden Markov models have been suc- cessfully used for a long time (Jelinek, 1976).Us- ing the same basic principles, we can rewrite the probability by introducing the 'hidden' alignments af := al. . .aj . . .aa for a sentence pair If,a; e{\]: Pr(f~al es) = ~_,Vr(fal, aT\[ eI't, a7 ,1 = ~ 1-IP"(k,"stfT-',"{ -*,e/) a I j=l So far there has been no basic restriction of the approach.We now assume a first-order depen- dence on the alignments aj only: Vr(fj,aslf{ -~, J-* a I , e l ) where, in addition, we have assmned that tile translation probability del)ends only oil aj and not oil aj-:l. Putting everything together, we have the ibllowing llMM-based model: a Pr(f:i'le{) = ~ I-I \[p(ajlaj - ' , l).p(Y)lea,)\] (4) af J=, with the following ingredients: ? IlMM alignment probability: p(i\]i', I) or p(a j la j _ l , I ) ; ? translation probabflity: p(f\]e).In addition, we assume that the t{MM align- ment probabilities p(i\[i', \[) depend only on the jump width (i - i').Using a set of non-negative parameters {s ( i - i')}, we can write the IIMM alignment probabilities in the form: 4 i - i') (5) p(ili', i ) = E ' s(1 - i') 1=1 This form ensures that for each word position i', i' = 1, ..., I, the ItMM alignment probabilities satisfy the normMization constraint.Note the similarity between Equations (2) and (5).The mixtm;e model can be interpreted as a zeroth-order model in contrast to the first-order tlMM model.As with the IBM2 model, we use again the max- imum approximation: J Pr(fiSle~) "~ max\]--\[ \[p(asl<*j-1, z)p(fj l<~,)\] (6) a ' / .ll.j,,, j= l In th is case, the task o f f ind ing the opt ima l alignment is more involved than in the case of the mixture model (lBM2).Thereibre, we have to re- sort to dynainic programming for which we have the following typical reeursion formula: Q(i, j ) = p(f j lel) ,nvax \[p(ili', 1) . Q(i', j - 1)\] i =l , . , , I Here, Q(i, j ) is a sort of partial probability as in time alignment for speech recognition (Jelinek, 197@.The models were tested on several tasks: ? the Avalanche Bulletins published by the Swiss Federal Institute for Snow and Avalanche Research (SHSAR) in Davos, Switzerland and made awtilable by the Eu- p "q I ropean Corpus Initiative (I,CI/MCI, 1994); ? the Verbmobil Corpus consisting of sponta- neously spoken dialogs in the domain of ap- pointment scheduling (Wahlster, 1993); 838 ,, the EuTrans C, orpus which contains tyl)ical phrases from the tourists and t.ravel docnain.(EuTrans, 1996).' l 'able \] gives the details on the size of tit<; cor- pora a, ud t;\]t<'it' vocal>ulary.It shottld I>e noted that in a.ll thes(; three ca.ses the ratio el' vocal)t,- \]ary size a.ml numl)er of running words is not very faw)rable.Tall)le, I: (,orpol :L (,o~pt s l,angua.ge Words Voc.Size AvalancJte \] A\[ \ [ ra i l s Verlmlobil Frolt ch (~('~ l lal l Spanish I,;nglish ( le 11 an English 62849 ,\]4805 --1:77@- 15888 150279 25,\] 27 1993 2265 2008 t 63(} dO 17 2`\]/13 For several years 1)et;weeu 83 and !)2, the Avalanche Bulletins are awdlabte for I>oth Get- ntan and I!'ren(;\]l. The following is a tyl)ical sen-- t<;nce t>air fS;onl the <;or:IreS: Bei zu('.rst recht holnm, Sl)~i.tev tM'eren 'l'em- l)eraJ, uren sind vou Samsta.g his 1)ienstag tno f gett auf <l<'~t; All>ennor(ls<'.ite un</ am All>en-.ha.uptkanml oberhalb 2000 m 60 his 80 cm Neuschnee gel'aJlen.l)ar des temp&'atures d' abord dlevdes, puis plus basses, 60 h 8(1 cm de neige sent tombs de samedi h. mardi matin sur le versant herd el; la eft're des Alpes au-dessus de 2000 l\[1.An exa,nq)le fi'om the Vet%mobil corpus is given in Figure 1.l,;ach of the three COrlJora.were ttsed to train 1)oth al ignnmnt models, the mixture-I>ased al ignment model in Eq.(1) and the llMM-base<l a.lignntent mod('l in Eq.(d).ltere, we will consider the ex- p<'.rimenta.l tesl;s on tit<'.Avalanche corpus in more detail.The traii, ing procedure consiste(l of the following steps: ? , Init ial ization training: IBMI model trahted for t0 iterations of the i';M algorithm.,, l{,efinement traiuiug: The translation pcoba- 1)ilities Dotn the initialization training wet'(; use+d to initialize both the IBM2 model and the I I M M-based nligntnent mo<t<'+l IBM2 Model: 5 iteratious using Lit(" max- i lnum a.I)proximatiolt (Eq+(3)) I IMM Model: 5 iterations usiug l le max-.imum al)l)roximation (Fq.(6)) 'l'h(, resulting perl>h:'~xity (inverse g<~olu(;l.ric av- era,ge of the likelihoods) for the dilferent lno(lels ave given iu tim Tal>\[es 2 and 3 for the Awdanehe <:<)rims.In adclitiou t;o the total i>erl>lexity, whi<'.h is the' globa.l opt imizat ion criterion, the tables al- so show the perplexities of the translation prob- abilities and of the al ignment probabil it ies.The last line in Table 2 gives the perplexity measures wh(m a.lJplying the rtlaxilnun| approximat ion and COml>uting the perph'~xity in t;\]lis approximation.These values are equal to the ones after initializing the IBM2 and HMM models, as they should be.From Ta,ble 3, we can see.that the mixture align- ment gives slightly better perplexity values for the translation l)roba.1)ilities, whereas the I IMM mod- el produces a smaller perplexity for the al ignment l>rohal)ilities.In the calculatiot, of the, perplexi- ties, th<' seld;en(;e length probal)ility was not in= eluded.Tahle 2: IBM I: Translation, a, l igmnent and total pert)h'~xil.y as a. fimction of' the iteration.Iteration Tra,nslatiotl.Alignrnent Total 0 1 2 9 10 99.36 3.72 2.67 t.87 1.86 20.07 20.07 20.07 20.07 20.07 1994.00 7/1.57 53.62 37.55 37.36 Max.3.88 20.07 77.!)5 'l'able 3: '1 rans\] ~+tion, aligmn en t and totaJ perplex- ity as a function of the itcra.tion for the IBM2 (A) and the I IMM model (13) Iter.Tratmlat;i(m A 0 l 2 3 ,\] 5 1~ 0 1 3 4 5 A ligniN.elJ t 3.88- 20.07 3.17 10.82 3.25 10.15 3.22 10.10 3.20 \] 0.06 3.18 10.05 3.88 20.07 3.37 7.99 3.46 6.17 ;{./17 5.90 "Ld6 5.85 3.`\]5 5.8,\] ' l 'otal 77.95 34.27 33.03 32.48 32.18 32.00 77.95 26.98 2t.36 20.48 20.2/1 20.18 Anoth<2r inl;crc:sting question is whether the IIMM alignntent model helps in finding good and sharply fo('usscd word+to-word (-orres\]Jondences.As an (;xamf,1o, Table 4 gives a COmlm+rison of the translatioJ~ probabil it ies p(f l e) bctweett the mixture and the IIMM alignnw+nt model For the (,e, u +l word Alpensiidhang.The counts of the words a.re given in brackets.The, re is virLually no ,:lilfc~rc~nce between the translation l.al>les for the two nn)dels (1BM2 and I IMM).But+ itt general, the tl M M model seems to giw'.slightly better re- suits in the cases of (;, ttna t COml+olmd words like Alpcus'iidha'n,(I vcrsant sud des Alpes which re- quire \['u,tction words in the trattslation.839 Table 4: Alpens/idhang.IBM1 Alpes (684) 0.171 des (1968) 0.035 le (1419) 0.039 sud (416) 0.427 sur (769) 0.040 versant (431) 0.284 IBM2 Alpes (684) 0.276 sud (41.6) 0.371 versant (431) 0.356 HMM Alpes (684) 0.284 des (1968) 0.028 sud (416) 0.354 versant (431) 0.333 This is a result of the smoother position align- ments produced by the HMM model.A pro- nounced example is given in Figure 2.'She prob- lem of the absolute position alignment can he demonstrated at the positions (a) and (c): both Schneebretlgefahr und Schneeverfrachtungen have a high probability on neige.The IBM2 models chooses the position near the diagonal, as this is the one with the higher probability.Again, Schneebrettgefahr generates de which explains the wrong alignment near the diagonal in (c).However, this strength of the HMM model can also be a weakness as in the case of est developpe ist ... entstanden (see (b) in Figure 2.The required two large jumps are correctly found by the mixture model, but not by the HMM mod- el.These cases suggest an extention to the HMM model.In general, there are only a small number of big jumps in the position alignments in a given sentence pair.Therefore a model could be useful that distinguishes between local and big jumps.The models have also been tested on the Verb- mobil Translation Corpus as well as on a small Corpus used in the EuTrans project.The sen- tences in the EuTrans corpus are in general short phrases with simple grammatical structures.However, the training corpus is very small and the produced alignments are generally of poor quality.There is no marked difference for the two align- ment models.Table 5: Perplexity results (b) Verbmobil Corpus.for (a) EuTrans and Model Iter.Transl.Align.Total IBM1 10 2.610 6.233 16.267 IBM2 5 2.443 4.003 9.781 HMM 5 2.461 3.934 9.686 IBM1 10 4.373 10.674 46.672 IBM2 5 4.696 6.538 30.706 ItMM 5 4.859 5.452 26.495 The Verbmobil Corpus consists of spontaneous- ly spoken dialogs in the domain of appointment scheduling.The assumption that every word in the source language is aligned to a word in the target language breaks down for many sentence pairs, resulting in poor alignment.This in turn affects the quality of the translation probabilities.Several extensions to the current IIMM based model could be used to tackle these problems: * The results presented here did not use the concept of the empty word.For the HMM- based model this, however, requires a second- order rather than a first-order model.We could allow for multi-word phrases in.both languages.In addition to the absolute or relative align- ment positions, the alignment probabilities can be assumed to depend on part of speech tags or on the words themselves.(confer model 4 in (Brown et al, 1990)).5 Conclusion.In this paper, we have presented an itMM-based approach for rnodelling word aligmnents in par- allel texts.The characteristic feature of this ap- proach is to make the alignment probabilities ex- plicitly dependent on the alignment position of the previous word.We have tested the model suc- cessfully on real data.The HMM-based approach produces translation probabilities comparable to the mixture alignment model.When looking at the position alignments those generated by the ItMM model are in general much smoother.This could be especially helpful for languages uch as German, where compound words are matched to several words in the source language.On the oth- er hand, large jumps due to different word order- ings in the two languages are successfully modeled.We are presently studying and testing a nmltilevel HMM model that allows only a small number of large jumps.The ultimate test of the different alignment and translation models can only be car- ried out in the framework of a fully operational translation system.6 Acknowledgement.This research was partly supported by the (\]er- man Federal Ministery of Education, Science, t{e- search and Technology under the Contract Num- ber 01 IV 601 A (Verbmobil) and under the Esprit Research Project 20268 'EuTrans).
The CoNLL 2007 Shared Task on Dependency ParsingThe Conference on Computational Natural Language Learning features a shared task, inwhich participants train and test their learn ing systems on the same data sets.In 2007, as in 2006, the shared task has been devoted to dependency parsing, this year with both a multilingual track and a domain adaptation track.In this paper, we define the tasks of the different tracks and describe how the data sets were created from existing treebanks for ten languages.In addition, we characterize the different approaches of the participating systems, report the test results, and provide a first analysis of these results.Previous shared tasks of the Conference on Compu tational Natural Language Learning (CoNLL) havebeen devoted to chunking (1999, 2000), clause iden tification (2001), named entity recognition (2002, 2003), and semantic role labeling (2004, 2005).In 2006 the shared task was multilingual dependency parsing, where participants had to train a single parser on data from thirteen different languages, which enabled a comparison not only of parsing and learning methods, but also of the performance that can be achieved for different languages (Buchholz and Marsi, 2006).In dependency-based syntactic parsing, the task is to derive a syntactic structure for an input sentence by identifying the syntactic head of each word in the sentence.This defines a dependency graph, where the nodes are the words of the input sentence and the arcs are the binary relations from head to dependent.Often, but not always, it is assumed that all words except one have a syntactic head, which means that the graph will be a tree with the single independent word as the root.In labeled dependency parsing, we additionally require the parser to assign a specific type (or label) to each dependency relation holding between a head word and a dependent word.In this year?s shared task, we continue to explore data-driven methods for multilingual dependencyparsing, but we add a new dimension by also intro ducing the problem of domain adaptation.The way this was done was by having two separate tracks: a multilingual track using essentially the same setup as last year, but with partly different languages, and a domain adaptation track, where the task was to usemachine learning to adapt a parser for a single lan guage to a new domain.In total, test results weresubmitted for twenty-three systems in the multilin gual track, and ten systems in the domain adaptationtrack (six of which also participated in the multilingual track).Not everyone submitted papers describ ing their system, and some papers describe more than one system (or the same system in both tracks), which explains why there are only (!)twenty-one papers in the proceedings.In this paper, we provide task definitions for the two tracks (section 2), describe data sets extracted from available treebanks (section 3), report results for all systems in both tracks (section 4), give an overview of approaches used (section 5), provide a first analysis of the results (section 6), and conclude with some future directions (section 7).915In this section, we provide the task definitions that were used in the two tracks of the CoNLL 2007 Shard Task, the multilingual track and the domain adaptation track, together with some background and motivation for the design choices made.First of all, we give a brief description of the data format and evaluation metrics, which were common to the two tracks.2.1 Data Format and Evaluation Metrics.The data sets derived from the original treebanks (section 3) were in the same column-based format as for the 2006 shared task (Buchholz and Marsi, 2006).In this format, sentences are separated by ablank line; a sentence consists of one or more to kens, each one starting on a new line; and a token consists of the following ten fields, separated by a single tab character: 1.ID: Token counter, starting at 1 for each new.sentence.2.FORM: Word form or punctuation symbol..underscore if not available.4.CPOSTAG: Coarse-grained part-of-speech tag,.where the tagset depends on the language.5.POSTAG: Fine-grained part-of-speech tag,.where the tagset depends on the language, or identical to the coarse-grained part-of-speech tag if not available.6.FEATS: Unordered set of syntactic and/or mor-.phological features (depending on the particu lar language), separated by a vertical bar (|), or an underscore if not available.7.HEAD: Head of the current token, which is. either a value of ID or zero (0).Note that, depending on the original treebank annotation, there may be multiple tokens with HEAD=0.8. DEPREL: Dependency relation to the HEAD..The set of dependency relations depends on the particular language.Note that, dependingon the original treebank annotation, the dependency relation when HEAD=0 may be mean ingful or simply ROOT.9.PHEAD: Projective head of current token,.which is either a value of ID or zero (0), or an underscore if not available.10.PDEPREL: Dependency relation to the.PHEAD, or an underscore if not available.The PHEAD and PDEPREL were not used at all in this year?s data sets (i.e., they always contained underscores) but were maintained for compatibilitywith last year?s data sets.This means that, in prac tice, the first six columns can be considered as input to the parser, while the HEAD and DEPREL fields are the output to be produced by the parser.Labeled training sets contained all ten columns; blind test sets only contained the first six columns; and gold standard test sets (released only after the end of the test period) again contained all ten columns.All data files were encoded in UTF-8.The official evaluation metric in both tracks wasthe labeled attachment score (LAS), i.e., the per centage of tokens for which a system has predicted the correct HEAD and DEPREL, but results were also reported for unlabeled attachment score (UAS), i.e., the percentage of tokens with correct HEAD, and the label accuracy (LA), i.e., the percentage oftokens with correct DEPREL.One important difference compared to the 2006 shared task is that all to kens were counted as ?scoring tokens?, including inparticular all punctuation tokens.The official eval uation script, eval07.pl, is available from the shared task website.1 2.2 Multilingual Track.The multilingual track of the shared task was organized in the same way as the 2006 task, with an notated training and test data from a wide range of languages to be processed with one and the same parsing system.This system must therefore be able to learn from training data, to generalize to unseentest data, and to handle multiple languages, possibly by adjusting a number of hyper-parameters.Par ticipants in the multilingual track were expected to submit parsing results for all languages involved.1http://depparse.uvt.nl/depparse-wiki/SoftwarePage 916 One of the claimed advantages of dependency parsing, as opposed to parsing based on constituent analysis, is that it extends naturally to languages with free or flexible word order.This explains the interest in recent years for multilingual evaluation of dependency parsers.Even before the 2006 shared task, the parsers of Collins (1997) and Charniak (2000), originally developed for English, had been adapted for dependency parsing of Czech, and theparsing methodology proposed by Kudo and Mat sumoto (2002) and Yamada and Matsumoto (2003) had been evaluated on both Japanese and English.The parser of McDonald and Pereira (2006) had been applied to English, Czech and Danish, and theparser of Nivre et al (2007) to ten different languages.But by far the largest evaluation of mul tilingual dependency parsing systems so far was the2006 shared task, where nineteen systems were eval uated on data from thirteen languages (Buchholz and Marsi, 2006).One of the conclusions from the 2006 shared task was that parsing accuracy differed greatly between languages and that a deeper analysis of the factors involved in this variation was an important problem for future research.In order to provide an extended empirical foundation for such research, we tried to select the languages and data sets for this year?s task based on the following desiderata:?The selection of languages should be typolog ically varied and include both new languages and old languages (compared to 2006).The creation of the data sets should involve as little conversion as possible from the original treebank annotation, meaning that preference should be given to treebanks with dependency annotation.The training data sets should include at least 50,000 tokens and at most 500,000 tokens.2 The final selection included data from Arabic, Basque, Catalan, Chinese, Czech, English, Greek, Hungarian, Italian, and Turkish.The treebanks from 2The reason for having an upper bound on the training set size was the fact that, in 2006, some participants could not train on all the data for some languages because of time limitations.Similar considerations also led to the decision to have a smaller number of languages this year (ten, as opposed to thirteen).which the data sets were extracted are described in section 3.2.3 Domain Adaptation Track.One well known characteristic of data-driven pars ing systems is that they typically perform muchworse on data that does not come from the training domain (Gildea, 2001).Due to the large over head in annotating text with deep syntactic parse trees, the need to adapt parsers from domains withplentiful resources (e.g., news) to domains with little resources is an important problem.This prob lem is commonly referred to as domain adaptation, where the goal is to adapt annotated resources from a source domain to a target domain of interest.Almost all prior work on domain adaptation as sumes one of two scenarios.In the first scenario, there are limited annotated resources available in the target domain, and many studies have shown thatthis may lead to substantial improvements.This includes the work of Roark and Bacchiani (2003), Flo rian et al (2004), Chelba and Acero (2004), Daume?and Marcu (2006), and Titov and Henderson (2006).Of these, Roark and Bacchiani (2003) and Titov and Henderson (2006) deal specifically with syntactic parsing.The second scenario assumes that there are no annotated resources in the target domain.This is a more realistic situation and is considerably more difficult.Recent work by McClosky et al (2006)and Blitzer et al (2006) have shown that the exis tence of a large unlabeled corpus in the new domain can be leveraged in adaptation.For this shared-task,we are assuming the latter setting ? no annotated re sources in the target domain.Obtaining adequate annotated syntactic resourcesfor multiple languages is already a challenging prob lem, which is only exacerbated when these resources must be drawn from multiple and diverse domains.As a result, the only language that could be feasibly tested in the domain adaptation track was English.The setup for the domain adaptation track was asfollows.Participants were provided with a large an notated corpus from the source domain, in this case sentences from the Wall Street Journal.Participants were also provided with data from three different target domains: biomedical abstracts (developmentdata), chemical abstracts (test data 1), and parent child dialogues (test data 2).Additionally, a large 917unlabeled corpus for each data set (training, devel opment, test) was provided.The goal of the task was to use the annotated source data, plus any unlabeled data, to produce a parser that is accurate for each of the test sets from the target domains.3 Participants could submit systems in either the ?open?or ?closed?class (or both).The closed classrequires a system to use only those resources provided as part of the shared task.The open class al lows a system to use additional resources provided those resources are not drawn from the same domain as the development or test sets.An example might be a part-of-speech tagger trained on the entire PennTreebank and not just the subset provided as train ing data, or a parser that has been hand-crafted or trained on a different training set.In this section, we describe the treebanks used in the shared task and give relevant information about the data sets created from them.3.1 Multilingual Track.Arabic The analytical syntactic annotation of the Prague Arabic Dependency Treebank (PADT) (Hajic?et al, 2004) can be considered a pure dependency annotation.The conversion, done by Otakar Smrz, from the original format to the column-based format described in section 2.1 was therefore relatively straightforward, although not all the information in the original annotation could be transfered to the new format.PADT was one of the treebanks used in the 2006 shared task but then only contained about 54,000 tokens.Since then, the size of the treebank has more than doubled, with around 112,000 tokens.In addition, the morphological annotation has been made more informative.It is also worth noting that the parsing units in this treebank are in many cases larger than conventional sentences, which partly explains the high average number of tokens per ?sentence?(Buchholz and Marsi, 2006).3Note that annotated development data for the target domainwas only provided for the development domain, biomedical abstracts.For the two test domains, chemical abstracts and parentchild dialogues, the only annotated data sets were the gold stan dard test sets, released only after test runs had been submitted.Basque For Basque, we used the 3LB Basquetreebank (Aduriz et al, 2003).At present, the tree bank consists of approximately 3,700 sentences, 334of which were used as test data.The treebank com prises literary and newspaper texts.It is annotated in a dependency format and was converted to the CoNLL format by a team led by Koldo Gojenola.Catalan The Catalan section of the CESS-ECESyntactically and Semantically Annotated Cor pora (Mart??et al, 2007) is annotated with, among other things, constituent structure and grammatical functions.A head percolation table was used for automatically converting the constituent trees into dependency trees.The original data only contains functions related to the verb, and a function tablewas used for deriving the remaining syntactic func tions.The conversion was performed by a team led by Llu??s Ma`rquez and Anto`nia Mart??.Chinese The Chinese data are taken from theSinica treebank (Chen et al, 2003), which contains both syntactic functions and semantic func tions.The syntactic head was used in the conversion to the CoNLL format, carried out by Yu-Ming Hsieh and the organizers of the 2006 shared task, and thesyntactic functions were used wherever it was pos sible.The training data used is basically the sameas for the 2006 shared task, except for a few correc tions, but the test data is new for this year?s shared task.It is worth noting that the parsing units in this treebank are sometimes smaller than conventionalsentence units, which partly explains the low aver age number of tokens per ?sentence?(Buchholz and Marsi, 2006).Czech The analytical syntactic annotation of the Prague Dependency Treebank (PDT) (Bo?hmova?et al., 2003) is a pure dependency annotation, just as for PADT.It was also used in the shared task 2006, but there are two important changes compared tolast year.First, version 2.0 of PDT was used in stead of version 1.0, and a conversion script wascreated by Zdenek Zabokrtsky, using the new XML based format of PDT 2.0.Secondly, due to the upper bound on training set size, only sections 1?3 of PDT constitute the training data, which amounts to some 450,000 tokens.The test data is a small subset of the development test set of PDT.918English For English we used the Wall Street Jour nal section of the Penn Treebank (Marcus et al,1993).In particular, we used sections 2-11 for training and a subset of section 23 for testing.As a pre processing stage we removed many functions tagsfrom the non-terminals in the phrase structure repre sentation to make the representations more uniformwith out-of-domain test sets for the domain adapta tion track (see section 3.2).The resulting data set was then converted to dependency structures using the procedure described in Johansson and Nugues (2007a).This work was done by Ryan McDonald.Greek The Greek Dependency Treebank(GDT) (Prokopidis et al, 2005) adopts a de pendency structure annotation very similar to those of PDT and PADT, which means that the conversionby Prokopis Prokopidis was relatively straightfor ward.GDT is one of the smallest treebanks in this year?s shared task (about 65,000 tokens) and contains sentences of Modern Greek.Just like PDT and PADT, the treebank contains more than one level of annotation, but we only used the analytical level of GDT.Hungarian For the Hungarian data, the Szegedtreebank (Csendes et al, 2005) was used.The tree bank is based on texts from six different genres, ranging from legal newspaper texts to fiction.Theoriginal annotation scheme is constituent-based, fol lowing generative principles.It was converted into dependencies by Zo?ltan Alexin based on heuristics.Italian The data set used for Italian is a subsetof the balanced section of the Italian Syntactic Semantic Treebank (ISST) (Montemagni et al,2003) and consists of texts from the newspaper Cor riere della Sera and from periodicals.A team led by Giuseppe Attardi, Simonetta Montemagni, and Maria Simi converted the annotation to the CoNLLformat, using information from two different anno tation levels, the constituent structure level and the dependency structure level.Turkish For Turkish we used the METU-Sabanc?Turkish Treebank (Oflazer et al, 2003), which was also used in the 2006 shared task.A new test set of about 9,000 tokens was provided by Gu?ls?en Eryig?it (Eryig?it, 2007), who also handled the conversion to the CoNLL format, which means that we could use all the approximately 65,000 tokens of the originaltreebank for training.The rich morphology of Turkish requires the basic tokens in parsing to be inflec tional groups (IGs) rather than words.IGs of a single word are connected to each other deterministically using dependency links labeled DERIV, referred to as word-internal dependencies in the following, and the FORM and the LEMMA fields may be empty (they contain underscore characters in the data files).Sentences do not necessarily have a unique root; most internal punctuation and a few foreign words also have HEAD=0.3.2 Domain Adaptation Track.As mentioned previously, the source data is drawn from a corpus of news, specifically the Wall Street Journal section of the Penn Treebank (Marcus et al,1993).This data set is identical to the English train ing set from the multilingual track (see section 3.1).For the target domains we used three different labeled data sets.The first two were annotated as part of the PennBioIE project (Kulick et al, 2004) and consist of sentences drawn from either biomedical or chemical research abstracts.Like the source WSJ corpus, this data is annotated using thePenn Treebank phrase structure scheme.To con vert these sets to dependency structures we used the same procedure as before (Johansson and Nugues,2007a).Additional care was taken to remove sen tences that contained non-WSJ part-of-speech tagsor non-terminals (e.g., HYPH part-of-speech tag in dicating a hyphen).Furthermore, the annotation scheme for gaps and traces was made consistent with the Penn Treebank wherever possible.As already mentioned, the biomedical data set was distributed as a development set for the training phase, while the chemical data set was only used for final testing.The third target data set was taken from theCHILDES database (MacWhinney, 2000), in partic ular the EVE corpus (Brown, 1973), which has beenannotated with dependency structures.Unfortu nately the dependency labels of the CHILDES datawere inconsistent with those of the WSJ, biomedi cal and chemical data sets, and we therefore opted to only evaluate unlabeled accuracy for this data set.Furthermore, there was an inconsistency in how main and auxiliary verbs were annotated for this data set relative to others.As a result of this, submitting 919 Multilingual Domain adaptation Ar Ba Ca Ch Cz En Gr Hu It Tu PCHEM CHILDES Language family Sem.Isol.Rom.Sin.Sla.Ger.Hel.F.-U.Rom.Tur.Ger.Annotation d d c+f c+f d c+f d c+f c+f d c+f d Training data Development data Tokens (k) 112 51 431 337 432 447 65 132 71 65 5 Sentences (k) 2.9 3.2 15.0 57.0 25.4 18.6 2.7 6.0 3.1 5.6 0.2 Tokens/sentence 38.3 15.8 28.8 5.9 17.0 24.0 24.2 21.8 22.9 11.6 25.1 LEMMA Yes Yes Yes No Yes No Yes Yes Yes Yes No No.CPOSTAG 15 25 17 13 12 31 18 16 14 14 25 No.POSTAG 21 64 54 294 59 45 38 43 28 31 37 No.FEATS 21 359 33 0 71 0 31 50 21 78 0 No.DEPREL 29 35 42 69 46 20 46 49 22 25 18 No.DEPREL H=0 18 17 1 1 8 1 22 1 1 1 1 % HEAD=0 8.7 9.7 3.5 16.9 11.6 4.2 8.3 4.6 5.4 12.8 4.0 % HEAD left 79.2 44.5 60.0 24.7 46.9 49.0 44.8 27.4 65.0 3.8 50.0 % HEAD right 12.1 45.8 36.5 58.4 41.5 46.9 46.9 68.0 29.6 83.4 46.0 HEAD=0/sentence 3.3 1.5 1.0 1.0 2.0 1.0 2.0 1.0 1.2 1.5 1.0 % Non-proj.arcs 0.4 2.9 0.1 0.0 1.9 0.3 1.1 2.9 0.5 5.5 0.4 % Non-proj.sent.10.1 26.2 2.9 0.0 23.2 6.7 20.3 26.4 7.4 33.3 8.0 Punc.attached S S A S S A S A A S A DEPRELS for punc.10 13 6 29 16 13 15 1 10 12 8 Test data PCHEM CHILDES Tokens 5124 5390 5016 5161 4724 5003 4804 7344 5096 4513 5001 4999 Sentences 131 334 167 690 286 214 197 390 249 300 195 666 Tokens/sentence 39.1 16.1 30.0 7.5 16.5 23.4 24.4 18.8 20.5 15.0 25.6 12.9 % New words 12.44 24.98 4.35 9.70 12.58 3.13 12.43 26.10 15.07 36.29 31.33 6.10 % New lemmas 2.82 11.13 3.36 n/a 5.28 n/a 5.82 14.80 8.24 9.95 n/a n/a Table 1: Characteristics of the data sets for the 10 languages of the multilingual track and the development set and the two test sets of the domain adaptation track.920results for the CHILDES data was considered op tional.Like the chemical data set, this data set was only used for final testing.Finally, a large corpus of unlabeled in-domaindata was provided for each data set and made avail able for training.This data was drawn from theWSJ, PubMed.com (specific to biomedical and chemical research literature), and the CHILDES data base.The data was tokenized to be as consistent as pos sible with the WSJ training set.3.3 Overview.Table 1 describes the characteristics of the data sets.For the multilingual track, we provide statistics over the training and test sets; for the domain adaptationtrack, the statistics were extracted from the develop ment set.Following last year?s shared task practice (Buchholz and Marsi, 2006), we use the following definition of projectivity: An arc (i, j) is projective iff all nodes occurring between i and j are dominated by i (where dominates is the transitive closure of the arc relation).In the table, the languages are abbreviated to their first two letters.Language families are: Semitic, Isolate, Romance, Sino-Tibetan, Slavic, Germanic, Hellenic, Finno-Ugric, and Turkic.The type of the original annotation is either constituents plus (some)functions (c+f) or dependencies (d).For the train ing data, the number of words and sentences are given in multiples of thousands, and the averagelength of a sentence in words (including punctuation tokens).The following rows contain information about whether lemmas are available, the num ber of coarse- and fine-grained part-of-speech tags, the number of feature components, and the number of dependency labels.Then information is given on how many different dependency labels can co-occurwith HEAD=0, the percentage of HEAD=0 depen dencies, and the percentage of heads preceding (left) or succeeding (right) a token (giving an indication of whether a language is predominantly head-initial or head-final).This is followed by the average numberof HEAD=0 dependencies per sentence and the per centage of non-projective arcs and sentences.The last two rows show whether punctuation tokens are attached as dependents of other tokens (A=Always,S=Sometimes) and specify the number of depen dency labels that exist for punctuation tokens.Note that punctuation is defined as any token belonging to the UTF-8 category of punctuation.This means, for example, that any token having an underscore in the FORM field (which happens for word-internal IGs in Turkish) is also counted as punctuation here.For the test sets, the number of words and sen tences as well as the ratio of words per sentence are listed, followed by the percentage of new words and lemmas (if applicable).For the domain adaptation sets, the percentage of new words is computed with regard to the training set (Penn Treebank).As already stated in the introduction, test runs weresubmitted for twenty-three systems in the multilin gual track, and ten systems in the domain adaptationtrack (six of which also participated in the multilin gual track).In the result tables below, systems are identified by the last name of the teammember listed first when test runs were uploaded for evaluation.In general, this name is also the first author of a paper describing the system in the proceedings, but there are a few exceptions and complications.First of all, for four out of twenty-seven systems, no paper was submitted to the proceedings.This is the case for the systems of Jia, Maes et al, Nash, and Zeman, which is indicated by the fact that these names appear initalics in all result tables.Secondly, two teams sub mitted two systems each, which are described in a single paper by each team.Thus, the systems called ?Nilsson?and ?Hall, J.?are both described in Hall et al.(2007a), while the systems called ?Duan (1)?and ?Duan (2)?are both described in Duan et al (2007).Finally, please pay attention to the fact that there are two teams, where the first author?s last name is Hall.Therefore, we use ?Hall, J.?and ?Hall, K.?, to disambiguate between the teams involving Johan Hall (Hall et al, 2007a) and Keith Hall (Hall et al, 2007b), respectively.Tables 2 and 3 give the scores for the multilingual track in the CoNLL 2007 shared task.The Averagecolumn contains the average score for all ten lan guages, which determines the ranking in this track.Table 4 presents the results for the domain adapta tion track, where the ranking is determined based on the PCHEM results only, since the CHILDES data set was optional.Note also that there are no labeled 921 Team Average Arabic Basque Catalan Chinese Czech English Greek Hungarian Italian Turkish Nilsson 80.32(1) 76.52(1) 76.94(1) 88.70(1) 75.82(15) 77.98(3) 88.11(5) 74.65(2) 80.27(1) 84.40(1) 79.79(2) Nakagawa 80.29(2) 75.08(2) 72.56(7) 87.90(3) 83.84(2) 80.19(1) 88.41(3) 76.31(1) 76.74(8) 83.61(3) 78.22(5) Titov 79.90(3) 74.12(6) 75.49(3) 87.40(6) 82.14(7) 77.94(4) 88.39(4) 73.52(10) 77.94(4) 82.26(6) 79.81(1) Sagae 79.90(4) 74.71(4) 74.64(6) 88.16(2) 84.69(1) 74.83(8) 89.01(2) 73.58(8) 79.53(2) 83.91(2) 75.91(10) Hall, J. 79.80(5)* 74.75(3) 74.99(5) 87.74(4) 83.51(3) 77.22(6) 85.81(12) 74.21(6) 78.09(3) 82.48(5) 79.24(3) Carreras 79.09(6)* 70.20(11) 75.75(2) 87.60(5) 80.86(10) 78.60(2) 89.61(1) 73.56(9) 75.42(9) 83.46(4) 75.85(11) Attardi 78.27(7) 72.66(8) 69.48(12) 86.86(7) 81.50(8) 77.37(5) 85.85(10) 73.92(7) 76.81(7) 81.34(8) 76.87(7) Chen 78.06(8) 74.65(5) 72.39(8) 86.66(8) 81.24(9) 73.69(10) 83.81(13) 74.42(3) 75.34(10) 82.04(7) 76.31(9) Duan (1) 77.70(9)* 69.91(13) 71.26(9) 84.95(10) 82.58(6) 75.34(7) 85.83(11) 74.29(4) 77.06(5) 80.75(9) 75.03(12) Hall, K. 76.91(10)* 73.40(7) 69.81(11) 82.38(14) 82.77(4) 72.27(12) 81.93(15) 74.21(5) 74.20(11) 80.69(10) 77.42(6) Schiehlen 76.18(11) 70.08(12) 66.77(14) 85.75(9) 80.04(11) 73.86(9) 86.21(9) 72.29(12) 73.90(12) 80.46(11) 72.48(15) Johansson 75.78(12)* 71.76(9) 75.08(4) 83.33(12) 76.30(14) 70.98(13) 80.29(17) 72.77(11) 71.31(13) 77.55(14) 78.46(4) Mannem 74.54(13)* 71.55(10) 65.64(15) 84.47(11) 73.76(17) 70.68(14) 81.55(16) 71.69(13) 70.94(14) 78.67(13) 76.42(8) Wu 73.02(14)* 66.16(14) 70.71(10) 81.44(15) 74.69(16) 66.72(16) 79.49(18) 70.63(14) 69.08(15) 78.79(12) 72.52(14) Nguyen 72.53(15)* 63.58(16) 58.18(17) 83.23(13) 79.77(12) 72.54(11) 86.73(6) 70.42(15) 68.12(17) 75.06(16) 67.63(17) Maes 70.66(16)* 65.12(15) 69.05(13) 79.21(16) 70.97(18) 67.38(15) 69.68(21) 68.59(16) 68.93(16) 73.63(18) 74.03(13) Canisius 66.99(17)* 59.13(18) 63.17(16) 75.44(17) 70.45(19) 56.14(17) 77.27(19) 60.35(18) 64.31(19) 75.57(15) 68.09(16) Jia 63.00(18)* 63.37(17) 57.61(18) 23.35(20) 76.36(13) 54.95(18) 82.93(14) 65.45(17) 66.61(18) 74.65(17) 64.68(18) Zeman 54.87(19) 46.06(20) 50.61(20) 62.94(19) 54.49(20) 50.21(20) 53.59(22) 55.29(19) 55.24(20) 62.13(19) 58.10(19) Marinov 54.55(20)* 54.00(19) 51.24(19) 69.42(18) 49.87(21) 53.47(19) 52.11(23) 54.33(20) 44.47(21) 59.75(20) 56.88(20) Duan (2) 24.62(21)* 82.64(5) 86.69(7) 76.89(6) Nash 8.65(22)* 86.49(8) Shimizu 7.20(23) 72.02(20) Table 2: Labeled attachment score (LAS) for the multilingual track in the CoNLL 2007 shared task.Teams are denoted by the last name of their first member, with italics indicating that there is no corresponding paper in the proceedings.The number in parentheses next to each score gives the rank.A star next to a score in the Average column indicates a statistically significant difference with the next lower rank.Team Average Arabic Basque Catalan Chinese Czech English Greek Hungarian Italian Turkish Nakagawa 86.55(1)* 86.09(1) 81.04(5) 92.86(4) 88.88(2) 86.28(1) 90.13(2) 84.08(1) 82.49(3) 87.91(1) 85.77(3) Nilsson 85.71(2) 85.81(2) 82.84(1) 93.12(3) 84.52(12) 83.59(4) 88.93(5) 81.22(4) 83.55(1) 87.77(2) 85.77(2) Titov 85.62(3) 83.18(7) 81.93(2) 93.40(1) 87.91(4) 84.19(3) 89.73(4) 81.20(5) 82.18(4) 86.26(6) 86.22(1) Sagae 85.29(4)* 84.04(4) 81.19(3) 93.34(2) 88.94(1) 81.27(8) 89.87(3) 80.37(11) 83.51(2) 87.68(3) 82.72(9) Carreras 84.79(5) 81.48(10) 81.11(4) 92.46(5) 86.20(9) 85.16(2) 90.63(1) 81.37(3) 79.92(9) 87.19(4) 82.41(10) Hall, J. 84.74(6)* 84.21(3) 80.61(6) 92.20(6) 87.60(5) 82.35(6) 86.77(12) 80.66(9) 81.71(6) 86.26(5) 85.04(5) Attardi 83.96(7)* 82.53(8) 76.88(11) 91.41(7) 86.73(8) 83.40(5) 86.99(10) 80.75(8) 81.81(5) 85.54(8) 83.56(7) Chen 83.22(8) 83.49(5) 78.65(8) 90.87(8) 85.91(10) 80.14(11) 84.91(13) 81.16(6) 79.25(11) 85.91(7) 81.92(12) Hall, K. 83.08(9) 83.45(6) 78.55(9) 87.80(15) 87.91(3) 78.47(12) 83.21(15) 82.04(2) 79.34(10) 84.81(9) 85.18(4) Duan (1) 82.77(10) 79.04(13) 77.59(10) 89.71(12) 86.88(7) 80.82(10) 86.97(11) 80.77(7) 80.66(7) 84.20(11) 81.03(13) Schiehlen 82.42(11)* 81.07(11) 73.30(14) 90.79(10) 85.45(11) 81.73(7) 88.91(6) 80.47(10) 78.61(12) 84.54(10) 79.33(15) Johansson 81.13(12)* 80.91(12) 80.43(7) 88.34(13) 81.30(15) 77.39(13) 81.43(18) 79.58(12) 75.53(15) 81.55(15) 84.80(6) Mannem 80.30(13) 81.56(9) 72.88(15) 89.81(11) 78.84(17) 77.20(14) 82.81(16) 78.89(13) 75.39(16) 82.91(12) 82.74(8) Nguyen 80.00(14)* 73.46(18) 69.15(18) 88.12(14) 84.05(13) 80.91(9) 88.01(7) 77.56(15) 78.13(13) 80.40(16) 80.19(14) Jia 78.46(15) 74.20(17) 70.24(16) 90.83(9) 83.39(14) 70.41(18) 84.37(14) 75.65(16) 77.19(14) 82.36(14) 75.96(17) Wu 78.44(16)* 77.05(14) 75.77(12) 85.85(16) 79.71(16) 73.07(16) 81.69(17) 78.12(14) 72.39(18) 82.57(13) 78.15(16) Maes 76.60(17)* 75.47(16) 75.27(13) 84.35(17) 76.57(18) 74.03(15) 71.62(21) 75.19(17) 72.93(17) 78.32(18) 82.21(11) Canisius 74.83(18)* 76.89(15) 70.17(17) 81.64(18) 74.81(19) 72.12(17) 78.23(19) 72.46(18) 67.80(19) 79.08(17) 75.14(18) Zeman 62.02(19)* 58.55(20) 57.42(20) 68.50(20) 62.93(20) 59.19(20) 58.33(22) 62.89(19) 59.78(20) 68.27(19) 64.30(19) Marinov 60.83(20)* 64.27(19) 58.55(19) 74.22(19) 56.09(21) 59.57(19) 54.33(23) 61.18(20) 50.39(21) 65.52(20) 64.13(20) Duan (2) 25.53(21)* 86.94(6) 87.87(8) 80.53(8) Nash 8.77(22)* 87.71(9) Shimizu 7.79(23) 77.91(20) Table 3: Unlabeled attachment scores (UAS) for the multilingual track in the CoNLL 2007 shared task.Teams are denoted by the last name of their first member, with italics indicating that there is no correspond ing paper in the proceedings.The number in parentheses next to each score gives the rank.A star next to a score in the Average column indicates a statistically significant difference with the next lower rank.922 LAS UAS Team PCHEM-c PCHEM-o PCHEM-c PCHEM-o CHILDES-c CHILDES-o Sagae 81.06(1) 83.42(1) Attardi 80.40(2) 83.08(3) 58.67(3) Dredze 80.22(3) 83.38(2) 61.37(1) Nguyen 79.50(4)* 82.04(4)* Jia 76.48(5)* 78.92(5)* 57.43(5) Bick 71.81(6)* 78.48(1)* 74.71(6)* 81.62(1)* 58.07(4) 62.49(1) Shimizu 64.15(7)* 63.49(2) 71.25(7)* 70.01(2)* Zeman 50.61(8) 54.57(8) 58.89(2) Schneider 63.01(3)* 66.53(3)* 60.27(2) Watson 55.47(4) 62.79(4) 45.61(3) Wu 52.89(6) Table 4: Labeled (LAS) and unlabeled (UAS) attachment scores for the closed (-c) and open (-o) classes of the domain adaptation track in the CoNLL 2007 shared task.Teams are denoted by the last name of their first member, with italics indicating that there is no corresponding paper in the proceedings.The number in parentheses next to each score gives the rank.A star next to a score in the PCHEM columns indicates a statistically significant difference with the next lower rank.attachment scores for the CHILDES data set, for reasons explained in section 3.2.The number in paren theses next to each score gives the rank.A star next to a score indicates that the difference with the nextlower rank is significant at the 5% level using a z test for proportions.A more complete presentation of the results, including the significance results for all the tasks and their p-values, can be found on the shared task website.4 Looking first at the results in the multilingual track, we note that there are a number of systems performing at almost the same level at the top of the ranking.For the average labeled attachment score, the difference between the top score (Nilsson) andthe fifth score (Hall, J.)is no more than half a percentage point, and there are generally very few significant differences among the five or six best sys tems, regardless of whether we consider labeled or unlabeled attachment score.For the closed class of the domain adaptation track, we see a very similar pattern, with the top system (Sagae) being followed very closely by two other systems.For the open class, the results are more spread out, but then thereare very few results in this class.It is also worth not ing that the top scores in the closed class, somewhat unexpectedly, are higher than the top scores in the 4http://nextens.uvt.nl/depparse-wiki/AllScores open class.But before we proceed to a more detailed analysis of the results (section 6), we will make an attempt to characterize the approaches represented by the different systems.In this section we give an overview of the models, inference methods, and learning methods used in theparticipating systems.For obvious reasons the dis cussion is limited to systems that are described bya paper in the proceedings.But instead of describ ing the systems one by one, we focus on the basic methodological building blocks that are often foundin several systems although in different combina tions.For descriptions of the individual systems, we refer to the respective papers in the proceedings.Section 5.1 is devoted to system architectures.We then describe the two main paradigms for learning and inference, in this year?s shared task as well as in last year?s, which we call transition-based parsers (section 5.2) and graph-based parsers (section 5.3), adopting the terminology of McDonald and Nivre (2007).5 Finally, we give an overview of the domain adaptation methods that were used (section 5.4).5This distinction roughly corresponds to the distinction made by Buchholz and Marsi (2006) between ?stepwise?and ?all-pairs?approaches.923 5.1 Architectures.Most systems perform some amount of pre- andpost-processing, making the actual parsing compo nent part of a sequential workflow of varying lengthand complexity.For example, most transition based parsers can only build projective dependencygraphs.For languages with non-projective depen dencies, graphs therefore need to be projectivized for training and deprojectivized for testing (Hall et al., 2007a; Johansson and Nugues, 2007b; Titov and Henderson, 2007).Instead of assigning HEAD and DEPREL in a single step, some systems use a two-stage approach for attaching and labeling dependencies (Chen et al, 2007; Dredze et al, 2007).In the first step unlabeled dependencies are generated, in the second step these are labeled.This is particularly helpful for factored parsing models, in which label decisions cannot be easily conditioned on larger parts of the structure due to the increased complexity of inference.Onesystem (Hall et al, 2007b) extends this two-stage ap proach to a three-stage architecture where the parser and labeler generate an n-best list of parses which in turn is reranked.6 In ensemble-based systems several base parsers provide parsing decisions, which are added together for a combined score for each potential dependencyarc.The tree that maximizes the sum of these com bined scores is taken as the final output parse.This technique is used by Sagae and Tsujii (2007) and in the Nilsson system (Hall et al, 2007a).It is worthnoting that both these systems combine transition based base parsers with a graph-based method for parser combination, as first described by Sagae and Lavie (2006).Data-driven grammar-based parsers, such as Bick (2007), Schneider et al (2007), and Watson andBriscoe (2007), need pre- and post-processing in order to map the dependency graphs provided as train ing data to a format compatible with the grammar used, and vice versa.5.2 Transition-Based Parsers.Transition-based parsers build dependency graphs by performing sequences of actions, or transitions.Both learning and inference is conceptualized in 6They also flip the order of the labeler and the reranker.terms of predicting the correct transition based onthe current parser state and/or history.We can fur ther subclassify parsers with respect to the model (or transition system) they adopt, the inference method they use, and the learning method they employ.5.2.1 Models The most common model for transition-based parsers is one inspired by shift-reduce parsing, where a parser state contains a stack of partially processed tokens and a queue of remaining input tokens, and where transitions add dependency arcs and perform stack and queue operations.This type of model is used by the majority of transition-based parsers (Attardi et al, 2007; Duan et al, 2007; Hallet al, 2007a; Johansson and Nugues, 2007b; Man nem, 2007; Titov and Henderson, 2007; Wu et al, 2007).Sometimes it is combined with an explicit probability model for transition sequences, which may be conditional (Duan et al, 2007) or generative (Titov and Henderson, 2007).An alternative model is based on the list-based parsing algorithm described by Covington (2001),which iterates over the input tokens in a sequen tial manner and evaluates for each preceding token whether it can be linked to the current token or not.This model is used by Marinov (2007) and in com ponent parsers of the Nilsson ensemble system (Hall et al, 2007a).Finally, two systems use models based on LR parsing (Sagae and Tsujii, 2007; Watson and Briscoe, 2007).5.2.2 InferenceThe most common inference technique in transition based dependency parsing is greedy deterministic search, guided by a classifier for predicting the next transition given the current parser state and history,processing the tokens of the sentence in sequen tial left-to-right order7 (Hall et al, 2007a; Mannem, 2007; Marinov, 2007; Wu et al, 2007).Optionally multiple passes over the input are conducted until no tokens are left unattached (Attardi et al, 2007).As an alternative to deterministic parsing, several parsers use probabilistic models and maintain a heap or beam of partial transition sequences in order to pick the most probable one at the end of the sentence 7For diversity in parser ensembles, right-to-left parsers are also used.924 (Duan et al, 2007; Johansson and Nugues, 2007b; Sagae and Tsujii, 2007; Titov and Henderson, 2007).One system uses as part of their parsing pipeline a ?neighbor-parser?that attaches adjacent words and a ?root-parser?that identifies the root word(s) of asentence (Wu et al, 2007).In the case of grammar based parsers, a classifier is used to disambiguate in cases where the grammar leaves some ambiguity (Schneider et al, 2007; Watson and Briscoe, 2007) 5.2.3 Learning Transition-based parsers either maintain a classifierthat predicts the next transition or a global proba bilistic model that scores a complete parse.To train these classifiers and probabilitistic models several approaches were used: SVMs (Duan et al, 2007; Hall et al, 2007a; Sagae and Tsujii, 2007), modified finite Newton SVMs (Wu et al, 2007), maximum entropy models (Sagae and Tsujii, 2007), multiclassaveraged perceptron (Attardi et al, 2007) and max imum likelihood estimation (Watson and Briscoe, 2007).In order to calculate a global score or probabil ity for a transition sequence, two systems used a Markov chain approach (Duan et al, 2007; Sagae and Tsujii, 2007).Here probabilities from the output of a classifier are multiplied over the whole sequence of actions.This results in a locally normalized model.Two other entries used MIRA (Mannem,2007) or online passive-aggressive learning (Johansson and Nugues, 2007b) to train a globally normalized model.Titov and Henderson (2007) used an in cremental sigmoid Bayesian network to model the probability of a transition sequence and estimated model parameters using neural network learning.5.3 Graph-Based Parsers.While transition-based parsers use training data to learn a process for deriving dependency graphs, graph-based parsers learn a model of what it meansto be a good dependency graph given an input sen tence.They define a scoring or probability function over the set of possible parses.At learning timethey estimate parameters of this function; at pars ing time they search for the graph that maximizes this function.These parsers mainly differ in the type and structure of the scoring function (model),the search algorithm that finds the best parse (inference), and the method to estimate the function?s pa rameters (learning).5.3.1 Models The simplest type of model is based on a sum oflocal attachment scores, which themselves are cal culated based on the dot product of a weight vector and a feature representation of the attachment.Thistype of scoring function is often referred to as a first order model.8 Several systems participating in this year?s shared task used first-order models (Schiehlen and Spranger, 2007; Nguyen et al, 2007; Shimizu and Nakagawa, 2007; Hall et al, 2007b).Canisius and Tjong Kim Sang (2007) cast the same type ofarc-based factorization as a weighted constraint sat isfaction problem.Carreras (2007) extends the first-order model to incorporate a sum over scores for pairs of adjacent arcs in the tree, yielding a second-order model.In contrast to previous work where this was constrained to sibling relations of the dependent (McDonald and Pereira, 2006), here head-grandchild relations can be taken into account.In all of the above cases the scoring function isdecomposed into functions that score local proper ties (arcs, pairs of adjacent arcs) of the graph.By contrast, the model of Nakagawa (2007) considersglobal properties of the graph that can take multi ple arcs into account, such as multiple siblings and children of a node.5.3.2 Inference Searching for the highest scoring graph (usually atree) in a model depends on the factorization cho sen and whether we are looking for projective ornon-projective trees.Maximum spanning tree algorithms can be used for finding the highest scor ing non-projective tree in a first-order model (Hall et al, 2007b; Nguyen et al, 2007; Canisius and Tjong Kim Sang, 2007; Shimizu and Nakagawa,2007), while Eisner?s dynamic programming algorithm solves the problem for a first-order factoriza tion in the projective case (Schiehlen and Spranger,2007).Carreras (2007) employs his own exten sion of Eisner?s algorithm for the case of projectivetrees and second-order models that include head grandparent relations.8It is also known as an edge-factored model.925 The methods presented above are mostly efficient and always exact.However, for models that takeglobal properties of the tree into account, they can not be applied.Instead Nakagawa (2007) uses Gibbssampling to obtain marginal probabilities of arcs be ing included in the tree using his global model and then applies a maximum spanning tree algorithm to maximize the sum of the logs of these marginals and return a valid cycle-free parse.5.3.3 Learning Most of the graph-based parsers were trained usingan online inference-based method such as passive aggressive learning (Nguyen et al, 2007; Schiehlen and Spranger, 2007), averaged perceptron (Carreras, 2007), or MIRA (Shimizu and Nakagawa, 2007), while some systems instead used methods based on maximum conditional likelihood (Nakagawa, 2007; Hall et al, 2007b).5.4 Domain Adaptation.5.4.1 Feature-Based ApproachesOne way of adapting a learner to a new domain without using any unlabeled data is to only include fea tures that are expected to transfer well (Dredze et al., 2007).In structural correspondence learning a transformation from features in the source domain to features of the target domain is learnt (Shimizu and Nakagawa, 2007).The original source features along with their transformed versions are then used to train a discriminative parser.5.4.2 Ensemble-Based Approaches Dredze et al (2007) trained a diverse set of parsers in order to improve cross-domain performance byincorporating their predictions as features for an other classifier.Similarly, two parsers trained with different learners and search directions were used in the co-learning approach of Sagae and Tsujii (2007).Unlabeled target data was processed with both parsers.Sentences that both parsers agreed on were then added to the original training data.This combined data set served as training data for one of the original parsers to produce the final system.In a similar fashion, Watson and Briscoe (2007) used a variant of self-training to make use of the unlabeled target data.5.4.3 Other Approaches Attardi et al (2007) learnt tree revision rules for the target domain by first parsing unlabeled target data using a strong parser; this data was then combined with labeled source data; a weak parser was applied to this new dataset; finally tree correction rules are collected based on the mistakes of the weak parser with respect to the gold data and the output of the strong parser.Another technique used was to filter sentences of the out-of-domain corpus based on their similarity to the target domain, as predicted by a classifier (Dredze et al, 2007).Only if a sentence was judged similar to target domain sentences was it included in the training set.Bick (2007) used a hybrid approach, where a data driven parser trained on the labeled training data was given access to the output of a Constraint Grammar parser for English run on the same data.Finally,Schneider et al (2007) learnt collocations and rela tional nouns from the unlabeled target data and used these in their parsing algorithm.Having discussed the major approaches taken in the two tracks of the shared task, we will now return tothe test results.For the multilingual track, we com pare results across data sets and across systems, and report results from a parser combination experiment involving all the participating systems (section 6.1).For the domain adaptation track, we sum up the most important findings from the test results (section 6.2).6.1 Multilingual Track.6.1.1 Across Data Sets The average LAS over all systems varies from 68.07 for Basque to 80.95 for English.Top scores varyfrom 76.31 for Greek to 89.61 for English.In gen eral, there is a good correlation between the top scores and the average scores.For Greek, Italian, and Turkish, the top score is closer to the average score than the average distance, while for Czech, the distance is higher.The languages that produced themost stable results in terms of system ranks with re spect to LAS are Hungarian and Italian.For UAS, Catalan also falls into this group.The language that 926 Setup Arabic Chinese Czech Turkish 2006 without punctuation 66.9 90.0 80.2 65.7 2007 without punctuation 75.5 84.9 80.0 71.6 2006 with punctuation 67.0 90.0 80.2 73.8 2007 with punctuation 76.5 84.7 80.2 79.8 Table 5: A comparison of the LAS top scores from 2006 and 2007.Official scoring conditions in boldface.For Turkish, scores with punctuation also include word-internal dependencies.produced the most unstable results with respect to LAS is Turkish.In comparison to last year?s languages, the lan guages involved in the multilingual track this year can be more easily separated into three classes with respect to top scores: ? Low (76.31?76.94): Arabic, Basque, Greek ? Medium (79.19?80.21): Czech, Hungarian, Turkish ? High (84.40?89.61): Catalan, Chinese, English, Italian It is interesting to see that the classes are more easilydefinable via language characteristics than via char acteristics of the data sets.The split goes across training set size, original data format (constituentvs.dependency), sentence length, percentage of unknown words, number of dependency labels, and ra tio of (C)POSTAGS and dependency labels.The class with the highest top scores contains languages with a rather impoverished morphology.Mediumscores are reached by the two agglutinative lan guages, Hungarian and Turkish, as well as by Czech.The most difficult languages are those that combinea relatively free word order with a high degree of in flection.Based on these characteristics, one would expect to find Czech in the last class.However, theCzech training set is four times the size of the train ing set for Arabic, which is the language with the largest training set of the difficult languages.However, it would be wrong to assume that train ing set size alone is the deciding factor.A closer look at table 1 shows that while Basque and Greekin fact have small training data sets, so do Turkish and Italian.Another factor that may be associated with the above classification is the percent age of new words (PNW) in the test set.Thus, theexpectation would be that the highly inflecting lan guages have a high PNW while the languages with little morphology have a low PNW.But again, thereis no direct correspondence.Arabic, Basque, Cata lan, English, and Greek agree with this assumption: Catalan and English have the smallest PNW, and Arabic, Basque, and Greek have a high PNW.But the PNW for Italian is higher than for Arabic and Greek, and this is also true for the percentage of new lemmas.Additionally, the highest PNW can be found in Hungarian and Turkish, which reach higherscores than Arabic, Basque, and Greek.These con siderations suggest that highly inflected languages with (relatively) free word order need more training data, a hypothesis that will have to be investigated further.There are four languages which were included inthe shared tasks on multilingual dependency parsing both at CoNLL 2006 and at CoNLL 2007: Arabic, Chinese, Czech, and Turkish.For all four lan guages, the same treebanks were used, which allows a comparison of the results.However, in some cases the size of the training set changed, and at least one treebank, Turkish, underwent a thorough correction phase.Table 5 shows the top scores for LAS.Since the official scores excluded punctuation in 2006 but includes it in 2007, we give results both with and without punctuation for both years.For Arabic and Turkish, we see a great improve ment of approximately 9 and 6 percentage points.For Arabic, the number of tokens in the training set doubled, and the morphological annotation was made more informative.The combined effect ofthese changes can probably account for the substan tial improvement in parsing accuracy.For Turkish, the training set grew in size as well, although only by600 sentences, but part of the improvement for Turkish may also be due to continuing efforts in error cor 927 rection and consistency checking.We see that the choice to include punctuation or not makes a large difference for the Turkish scores, since non-final IGs of a word are counted as punctuation (because they have the underscore character as their FORM value), which means that word-internal dependency links are included if punctuation is included.9 However, regardless of whether we compare scores with or without punctuation, we see a genuine improvement of approximately 6 percentage points.For Chinese, the same training set was used.Therefore, the drop from last year?s top score to thisyear?s is surprising.However, last year?s top scor ing system for Chinese (Riedel et al, 2006), which did not participate this year, had a score that wasmore than 3 percentage points higher than the sec ond best system for Chinese.Thus, if we comparethis year?s results to the second best system, the dif ference is approximately 2 percentage points.This final difference may be attributed to the properties of the test sets.While last year?s test set was taken from the treebank, this year?s test set contains texts from other sources.The selection of the textual basis also significantly changed average sentence length: The Chinese training set has an average sentence lengthof 5.9.Last year?s test set alo had an average sen tence length of 5.9.However, this year, the average sentence length is 7.5 tokens, which is a significant increase.Longer sentences are typically harder to parse due to the increased likelihood of ambiguous constructions.Finally, we note that the performance for Czech is almost exactly the same as last year, despite the fact that the size of the training set has been reduced to approximately one third of last year?s training set.It is likely that this in fact represents a relative im provement compared to last year?s results.6.1.2 Across Systems The LAS over all languages ranges from 80.32 to54.55.The comparison of the system ranks averaged over all languages with the ranks for single lan 9The decision to include word-internal dependencies in thisway can be debated on the grounds that they can be parsed de terministically.On the other hand, they typically correspond toregular dependencies captured by function words in other lan guages, which are often easy to parse as well.It is thereforeunclear whether scores are more inflated by including word internal dependencies or deflated by excluding them.guages show considerably more variation than last year?s systems.Buchholz and Marsi (2006) report that ?[f]or most parsers, their ranking differs at most a few places from their overall ranking?.This year,for all of the ten best performing systems with re spect to LAS, there is at least one language for which their rank is at least 5 places different from theiroverall rank.The most extreme case is the top per forming Nilsson system (Hall et al, 2007a), which reached rank 1 for five languages and rank 2 fortwo more languages.Their only outlier is for Chi nese, where the system occupies rank 14, with a LAS approximately 9 percentage points below the top scoring system for Chinese (Sagae and Tsujii, 2007).However, Hall et al (2007a) point out that the official results for Chinese contained a bug, and the true performance of their system was actuallymuch higher.The greatest improvement of a system with respect to its average rank occurs for En glish, for which the system by Nguyen et al (2007) improved from the average rank 15 to rank 6.Twomore outliers can be observed in the system of Jo hansson and Nugues (2007b), which improves from its average rank 12 to rank 4 for Basque and Turkish.The authors attribute this high performance to their parser?s good performance on small training sets.However, this hypothesis is contradicted by their re sults for Greek and Italian, the other two languages with small training sets.For these two languages, the system?s rank is very close to its average rank.6.1.3 An Experiment in System Combination Having the outputs of many diverse dependencyparsers for standard data sets opens up the interest ing possibility of parser combination.To combine the outputs of each parser we used the method of Sagae and Lavie (2006).This technique assigns to each possible labeled dependency a weight that isequal to the number of systems that included the de pendency in their output.This can be viewed as an arc-based voting scheme.Using these weightsit is possible to search the space of possible depen dency trees using directed maximum spanning tree algorithms (McDonald et al, 2005).The maximum spanning tree in this case is equal to the tree that on average contains the labeled dependencies that most systems voted for.It is worth noting that variants of this scheme were used in two of the participating 928 5 10 15 20Number of Systems 80 82 84 86 88 Accu racy Unlabeled AccuracyLabeled Accuracy Figure 1: System Combination systems, the Nilsson system (Hall et al, 2007a) and the system of Sagae and Tsujii (2007).Figure 1 plots the labeled and unlabeled accuracies when combining an increasing number of sys tems.The data used in the plot was the output of allcompeting systems for every language in the multilingual track.The plot was constructed by sorting the systems based on their average labeled accuracy scores over all languages, and then incremen tally adding each system in descending order.10 We can see that both labeled and unlabeled accuracy are significantly increased, even when just the top three systems are included.Accuracy begins to degrade gracefully after about ten different parsers have been added.Furthermore, the accuracy never falls below the performance of the top three systems.6.2 Domain Adaptation Track.For this task, the results are rather surprising.A lookat the LAS and UAS for the chemical research ab stracts shows that there are four closed systems that outperform the best scoring open system.The best system (Sagae and Tsujii, 2007) reaches an LAS of 81.06 (in comparison to their LAS of 89.01 for theEnglish data set in the multilingual track).Consider ing that approximately one third of the words of the chemical test set are new, the results are noteworthy.The next surprise is to be found in the relatively low UAS for the CHILDES data.At a first glance, this data set has all the characteristics of an easy 10The reason that there is no data point for two parsers is that the simple voting scheme adopted only makes sense with at least three parsers voting.set; the average sentence is short (12.9 words), and the percentage of new words is also small (6.10%).Despite these characteristics, the top UAS reaches 62.49 and is thus more than 10 percentage points below the top UAS for the chemical data set.One major reason for this is that auxiliary and main verb dependencies are annotated differently in the CHILDES data than in the WSJ training set.As aresult of this discrepancy, participants were not re quired to submit results for the CHILDES data.The best performing system on the CHILDES corpus is an open system (Bick, 2007), but the distance tothe top closed system is approximately 1 percent age point.In this domain, it seems more feasible touse general language resources than for the chemi cal domain.However, the results prove that the extra effort may be unnecessary.Two years of dependency parsing in the CoNLL shared task has brought an enormous boost to thedevelopment of dependency parsers for multiple lan guages (and to some extent for multiple domains).But even though nineteen languages have been covered by almost as many different parsing and learn ing approaches, we still have only vague ideas about the strengths and weaknesses of different methodsfor languages with different typological characteris tics.Increasing our knowledge of the multi-causal relationship between language structure, annotation scheme, and parsing and learning methods probablyremains the most important direction for future re search in this area.The outputs of all systems for alldata sets from the two shared tasks are freely avail able for research and constitute a potential gold mine for comparative error analysis across languages and systems.For domain adaptation we have barely scratched the surface so far.But overcoming the bottleneckof limited annotated resources for specialized do mains will be as important for the deployment of human language technology as being able to handle multiple languages in the future.One result fromthe domain adaptation track that may seem surprising at first is the fact that closed class systems outperformed open class systems on the chemical ab stracts.However, it seems that the major problem in 929 adapting pre-existing parsers to the new domain was not the domain as such but the mapping from the native output of the parser to the kind of annotationprovided in the shared task data sets.Thus, find ing ways of reusing already invested development efforts by adapting the outputs of existing systemsto new requirements, without substantial loss in ac curacy, seems to be another line of research that may be worth pursuing.AcknowledgmentsFirst and foremost, we want to thank all the peo ple and organizations that generously provided us with treebank data and helped us prepare the data sets and without whom the shared task would have been literally impossible: Otakar Smrz, CharlesUniversity, and the LDC (Arabic); Maxux Aranzabe, Kepa Bengoetxea, Larraitz Uria, Koldo Gojenola, and the University of the Basque Coun try (Basque); Ma.Anto`nia Mart??Anton??n, Llu??s Ma`rquez, Manuel Bertran, Mariona Taule?, DifdaMonterde, Eli Comelles, and CLiC-UB (Cata lan); Shih-Min Li, Keh-Jiann Chen, Yu-Ming Hsieh, and Academia Sinica (Chinese); Jan Hajic?, Zdenek Zabokrtsky, Charles University, and the LDC (Czech); Brian MacWhinney, Eric Davis, the CHILDES project, the Penn BioIE project, and the LDC (English); Prokopis Prokopidis and ILSP(Greek); Csirik Ja?nos and Zolta?n Alexin (Hun garian); Giuseppe Attardi, Simonetta Montemagni, Maria Simi, Isidoro Barraco, Patrizia Topi, Kiril Ribarov, Alessandro Lenci, Nicoletta Calzolari, ILC, and ELRA (Italian); Gu?ls?en Eryig?it, Kemal Oflazer, and Ruket C?ak?c?(Turkish).Secondly, we want to thank the organizers of last year?s shared task, Sabine Buchholz, Amit Dubey, Erwin Marsi, and Yuval Krymolowski, who solved all the really hard problems for us and answered all our questions, as well as our colleagues who helped review papers: Jason Baldridge, Sabine Buchholz,James Clarke, Gu?ls?en Eryig?it, Kilian Evang, Ju lia Hockenmaier, Yuval Krymolowski, Erwin Marsi, Bea?ta Megyesi, Yannick Versley, and Alexander Yeh.Special thanks to Bertjan Busser and Erwin Marsi for help with the CoNLL shared task website and many other things, and to Richard Johansson for letting us use his conversion tool for English.Thirdly, we want to thank the program chairs for EMNLP-CoNLL 2007, Jason Eisner and Taku Kudo, the publications chair, Eric Ringger, the SIGNLL officers, Antal van den Bosch, Hwee Tou Ng, and Erik Tjong Kim Sang, and members of the LDC staff, Tony Castelletto and Ilya Ahtaridis, for great cooperation and support.Finally, we want to thank the following people,who in different ways assisted us in the organi zation of the CoNLL 2007 shared task: Giuseppe Attardi, Eckhard Bick, Matthias Buch-Kromann,Xavier Carreras, Tomaz Erjavec, Svetoslav Mari nov, Wolfgang Menzel, Xue Nianwen, Gertjan van Noord, Petya Osenova, Florian Schiel, Kiril Simov, Zdenka Uresova, and Heike Zinsmeister.
Dependency Parsing by Belief PropagationWe formulate dependency parsing as a graphical model with the novel ingredient of global constraints.We show how to apply loopy belief propagation (BP), a simple and tool for and inference.As a parsing algorithm, BP is both asymptotically and empirically efficient.Even with second-order features or latent variables, which would make exact parsing considerslower or NP-hard, BP needs only with a small constant factor.Furthermore, such features significantly improve parse accuracy over exact first-order methods.Incorporating additional features would increase the runtime additively rather than multiplicatively.Computational linguists worry constantly about runtime.Sometimes we oversimplify our models, trading linguistic nuance for fast dynamic programming.Alternatively, we write down a better but intractable model and then use approximations.The CL community has often approximated using heavy pruning or reranking, but is beginning to adopt other methods from the machine learning community, such as Gibbs sampling, rejection sampling, and certain variational approximations.We propose borrowing a different approximation technique from machine learning, namely, loopy belief propagation (BP).In this paper, we show that BP can be used to train and decode complex parsing models.Our approach calls a simpler parser as a subroutine, so it still exploits the useful, well-studied combinatorial structure of the parsing problem.1We wish to make a dependency parse’s score depend on higher-order features, which consider arbitrary interactions among two or more edges in the parse (and perhaps also other latent variables such as part-of-speech tags or edge labels).Such features can help accuracy—as we show.Alas, they raise the polynomial runtime of projective parsing, and render non-projective parsing NP-hard.Hence we seek approximations.We will show how BP’s “message-passing” discipline offers a principled way for higher-order features to incrementally adjust the numerical edge weights that are fed to a fast first-order parser.Thus the first-order parser is influenced by higher-order interactions among edges—but not asymptotically slowed down by considering the interactions itself.BP’s behavior in our setup can be understood intuitively as follows.Inasmuch as the first-order parser finds that edge e is probable, the higher-order features will kick in and discourage other edges e' to the extent that they prefer not to coexist with e.2 Thus, the next call to the first-order parser assigns lower probabilities to parses that contain these e'.(The method is approximate because a first-order parser must equally penalize all parses containing e', even those that do not in fact contain e.) This behavior is somewhat similar to parser stacking (Nivre and McDonald, 2008; Martins et al., 2008), in which a first-order parser derives some of its input features from the full 1-best output of another parser.In our method, a first-order parser derives such input features from its own previous full output (but probabilistic output rather than just 1best).This circular process is iterated to convergence.Our method also permits the parse to interact cheaply with other variables.Thus first-order parsing, part-of-speech tagging, and other tasks on a common input could mutually influence one another.Our method and its numerical details emerge naturally as an instance of the well-studied loopy BP algorithm, suggesting several potential future improvements to accuracy (Yedidia et al., 2004; Braunstein et al., 2005) and efficiency (Sutton and McCallum, 2007).Loopy BP has occasionally been used before in NLP, with good results, to handle non-local features (Sutton and McCallum, 2004) or joint decoding (Sutton et al., 2004).However, our application to parsing requires an innovation to BP that we explain in §5—a global constraint to enforce that the parse is a tree.The tractability of some such global constraints points the way toward applying BP to other computationally intensive NLP problems, such as syntax-based alignment of parallel text.To apply BP, we must formulate dependency parsing as a search for an optimal assignment to the variables of a graphical model.We encode a parse using the following variables: Sentence.The n-word input sentence W is fully observed (not a lattice).Let W = W0W1 · · · Wn, where W0 is always the special symbol ROOT.Tags.If desired, the variables T = T1T2 · · · Tn may specify tags on the n words, drawn from some tagset T (e.g., parts of speech).These variables are needed iff the tags are to be inferred jointly with the parse.Links.The O(n2) boolean variables {Lij : 0 < i < n,1 < j < n, i =� j} correspond to the possible links in the dependency parse.3 Lij = true is interpreted as meaning that there exists a dependency link from parent i —* child j.4 Link roles, etc.It would be straightforward to add other variables, such as a binary variable Lij that is true iff there is a link i � j labeled with role r (e.g., AGENT, PATIENT, TEMPORAL ADJUNCT).We wish to define a probability distribution over all configurations, i.e., all joint assignments A to these variables.Our distribution is simply an undirected graphical model, or Markov random field (MRF):5 specified by the collection of factors Fm : A H R[ 'O.Each factor is a function that consults only a subset of A.We say that the factor has degree d if it depends on the values of d variables in A, and that it is unary, binary, ternary, or global if d is respectively 1, 2, 3, or unbounded (grows with n).A factor function Fm(A) may also depend freely on the observed variables—the input sentence W and a known (learned) parameter vector 0.For notational simplicity, we suppress these extra arguments when writing and drawing factor functions, and when computing their degree.In this treatment, these observed variables are not specified by A, but instead are absorbed into the very definition of Fm.In defining a factor Fm, we often define the circumstances under which it fires.These are the only circumstances that allow Fm(A) =� 1.When Fm does not fire, Fm(A) = 1 and does not affect the product in equation (1).A hard factor Fm fires only on parses A that violate some specified condition.It has value 0 on those parses, acting as a hard constraint to rule them out.TREE.A hard global constraint on all the Lij variables at once.It requires that exactly n of these variables be true, and that the corresponding links form a directed tree rooted at position 0.PTREE.This stronger version of TREE requires further that the tree be projective.That is, it prohibits Lij and LH from both being true if i —* j crosses k —* E. (These links are said to cross if one of k, E is strictly between i and j while the other is strictly outside that range.)EXACTLY1.A family of O(n) hard global constraints, indexed by 1 < j < n. EXACTLY1j requires that j have exactly one parent, i.e., exactly one of the Lij variables must be true.Note that EXACTLY1 is implied by TREE or PTREE.ATMOST1.A weaker version.ATMOST1j requires j to have one or zero parents.NAND.A family of hard binary constraints.NANDij,kt requires that Lij and Lkt may not both be true.We will be interested in certain subfamilies.NOT2.Shorthand for the family of O(n3) binary constraints {NANDij,kj}.These are collectively equivalent to ATMOST1, but expressed via a larger number of simpler constraints, which can make the BP approximation less effective (footnote 30).NO2CYCLE.Shorthand for the family of O(n2) binary constraints {NANDij,ji}.A soft factor Fm acts as a soft constraint that prefers some parses to others.In our experiments, it is always a log-linear function returning positive values: where θ is a learned, finite collection of weights and f is a corresponding collection of feature functions, some of which are used by Fm.(Note that fh is permitted to consult the observed input W. It also sees which factor Fm it is scoring, to support reuse of a single feature function fh and its weight θh by unboundedly many factors in a model.)LINK.A family of unary soft factors that judge the links in a parse A individually.LINKij fires iff Lij = true, and then its value depends on (i, j), W, and θ.Our experiments use the same features as McDonald et al. (2005).A first-order (or “edge-factored”) parsing model (McDonald et al., 2005) contains only LINK factors, along with a global TREE or PTREE factor.Though there are O(n2) link factors (one per Lij), only n of them fire on any particular parse, since the global factor ensures that exactly n are true.We’ll consider various higher-order soft factors: PAIR.The binary factor PAIRij,kt fires with some value iff Lij and Lkt are both true.Thus, it penalizes or rewards a pair of links for being simultaneously present.This is a soft version of NAND.GRAND.Shorthand for the family of O(n3) binary factors {PAIRij,jk}, which evaluate grandparentparent-child configurations, i —* j —* k. For example, whether preposition j attaches to verb i might depend on its object k. In non-projective parsing, we might prefer (but not require) that a parent and child be on the same side of the grandparent.SIB.Shorthand for the family of O(n3) binary factors {PAIRij,ik}, which judge whether two children of the same parent are compatible.E.g., a given verb may not like to have two noun children both to its left.6 The children do not need to be adjacent.CHILDSEQ.A family of O(n) global factors.CHILDSEQi scores i’s sequence of children; hence it consults all variables of the form Lij.The scoring follows the parametrization of a weighted split head-automaton grammar (Eisner and Satta, 1999).If 5 has children 2, 7, 9 under A, then CHILDSEQi is a product of subfactors of the form PAIR5#,57, PAIR57,59, PAIR59,5# (right child sequence) and PAIR5#,52, PAIR52,5# (left child sequence).NOCROSS.A family of O(n2) global constraints.If the parent-to-j link crosses the parent-to-` link, then NOCROSSjt fires with a value that depends only on j and `.(If j and ` do not each have exactly one parent, NOCROSSjt fires with value 0; i.e., it incorporates EXACTLY1j and EXACTLY1t.)7 TAGi is a unary factor that evaluates whether Ti’s value is consistent with W (especially Wi).TAGLINKij is a ternary version of the LINKij factor whose value depends on Lij, Ti and Tj (i.e., its feature functions consult the tag variables to decide whether a link is likely).One could similarly enrich the other features above to depend on tags and/or link roles; TAGLINK is just an illustrative example.TRIGRAM is a global factor that evaluates the tag sequence T according to a trigram model.It is a product of subfactors, each of which scores a trigram of adjacent tags Ti_2, Ti_1, Ti, possibly also considering the word sequence W (as in CRFs).MacKay (2003, chapters 16 and 26) provides an excellent introduction to belief propagation, a generalization of the forward-backward algorithm that is deeply studied in the graphical models literature (Yedidia et al., 2004, for example).We briefly sketch the method in terms of our parsing task.The basic BP idea is simple.Variable L34 maintains a distribution over values true and false—a “belief”—that is periodically recalculated based on the current distributions at other variables.8 Readers familiar with Gibbs sampling can regard this as a kind of deterministic approximation.In Gibbs sampling, L34’s value is periodically resampled based on the current values of other variables.Loopy BP works not with random samples but their expectations.Hence it is approximate but tends to converge much faster than Gibbs sampling will mix.It is convenient to visualize an undirected factor graph (Fig.1), in which each factor is connected to the variables it depends on.Many factors may connect to—and hence influence—a given variable such as L34.If X is a variable or a factor, N(X) denotes its set of neighbors.Given an input sentence W and a parameter vector θ, the collection of factors Fm defines a probability distribution (1).The parser should determine the values of the individual variables.In other words, we would like to marginalize equation (1) to obtain the distribution p(L34) over L34 = true vs. false, the distribution p(T4) over tags, etc.If the factor graph is acyclic, then BP computes these marginal distributions exactly.Given 8Or, more precisely—this is the tricky part—based on versions of those other distributions that do not factor in L34’s reciprocal influence on them.This prevents (e.g.)L34 and T3 from mutually reinforcing each other’s existing beliefs. an HMM, for example, BP reduces to the forwardbackward algorithm.BP’s estimates of these distributions are called beliefs about the variables.BP also computes beliefs about the factors, which are useful in learning θ (see §7).E.g., if the model includes the factor TAGLINKij, which is connected to variables Lij, Ti, Tj, then BP will estimate the marginal joint distribution p(Lij, Ti, Tj) over (boolean, tag, tag) triples.When the factor graph has loops, BP’s beliefs are usually not the true marginals of equation (1) (which are in general intractable to compute).Indeed, BP’s beliefs may not be the true marginals of any distribution p(A) over assignments, i.e., they may be globally inconsistent.All BP does is to incrementally adjust the beliefs till they are at least locally consistent: e.g., the beliefs at factors TAGLINKij and TAGLINKik must both imply9 the same belief about variable Ti, their common neighbor.This iterated negotiation among the factors is handled by message passing along the edges of the factor graph.A message to or from a variable is a (possibly unnormalized) probability distribution over the values of that variable.The variable V sends a message to factor F, saying “My other neighboring factors G jointly suggest that I have posterior distribution qV ,F (assuming that they are sending me independent evidence).” Meanwhile, factor F sends messages to V , saying, “Based on my factor function and the messages received from my other neighboring variables U about their values (and assuming that those messages are independent), I suggest you have posterior distribution rF,V over your values.” To be more precise, BP at each iteration k (until convergence) updates two kinds of messages: from factors to variables.Each message is a probability distribution over values v of V , normalized by a scaling constant n. Alternatively, messages may be left as unnormalized distributions, choosing n =� 1 only as needed to prevent over- or underflow.Messages are initialized to uniform distributions.Whenever we wish, we may compute the beliefs at V and F: These beliefs do not truly characterize the expected behavior of Gibbs sampling (§4.1), since the products in (5)–(6) make conditional independence assumptions that are valid only if the factor graph is acyclic.Furthermore, on cyclic (“loopy”) graphs, BP might only converge to a local optimum (Weiss and Freedman, 2001), or it might not converge at all.Still, BP often leads to good, fast approximations.One iteration of standard BP simply updates all the messages as in equations (3)–(4): one message per edge of the factor graph.Therefore, adding new factors to the model increases the runtime per iteration additively, by increasing the number of messages to update.We believe this is a compelling advantage over dynamic programming—in which new factors usually increase the runtime and space multiplicatively by exploding the number of distinct items.10 But how long does updating each message take?The runtime of summing over all assignments EA in 10For example, with unknown tags T, a model with PTREE+TAGLINK will take only O(n3 + n2g2) time for BP, compared to O(n3g2) time for dynamic programming (Eisner & Satta 1999).Adding TRIGRAM, which is string-local rather than tree-local, will increase this only to O(n3 + n2g2 + ng3), compared to O(n3g6) for dynamic programming.Even more dramatic, adding the SIB family of O(n3) PAIRij,ik factors will add only O(n3) to the runtime of BP (Table 1).By contrast, the runtime of dynamic programming becomes exponential, because each item must record its headword’s full set of current children. equation (4) may appear prohibitive.Crucially, however, F(A) only depends on the values in A of F’s its neighboring variables N(F).So this sum is proportional to a sum over restricted assignments to just those variables.11 For example, computing a message from TAGLINKij —* Ti only requires iterating over all (boolean, tag, tag) triples.12 The runtime to update that message is therefore O(2 · |T  |· |T |).The above may be tolerable for a ternary factor.But how about global factors?EXACTLY1j has n neighboring boolean variables: surely we cannot iterate over all 2n assignments to these!TREE is even worse, with 2O(n2) assignments to consider.We will give specialized algorithms for handling these summations more efficiently.A historical note is in order.Traditional constraint satisfaction corresponds to the special case of (1) where all factors Fm are hard constraints (with values in {0,1}).In that case, loopy BP reduces to an algorithm for generalized arc consistency (Mackworth, 1977; Bessi`ere and R´egin, 1997; Dechter, 2003), and updating a factor’s outgoing messages is known as constraint propagation.R´egin (1994) famously introduced an efficient propagator for a global constraint, ALLDIFFERENT, by adapting combinatorial bipartite matching algorithms.In the same spirit, we will demonstrate efficient propagators for our global constraints, e.g. by adapting combinatorial algorithms for weighted parsing.We are unaware of any previous work on global factors in sum-product BP, although for max-product BP,13 Duchi et al. (2007) independently showed that a global 1-to-1 alignment constraint—a kind of weighted ALLDIFFERENT—permits an efficient propagator based on weighted bipartite matching.Table 1 shows our asymptotic runtimes for all factors in §§3.3–3.4.Remember that if several of these factors are included, the total runtime is additive.14 Propagating the local factors is straightforward (§5.1).We now explain how to handle the global factors.Our main trick is to work backwards from marginal beliefs.Let F be a factor and V be one of its neighboring variables.At any time, F has a marginal belief about V (see footnote 9), A s.t.A[V]=v a sum over (6)’s products of incoming messages.By the definition of rF→V in (4), and distributivity, we can also express the marginal belief (7) as a pointwise product of outgoing and incoming messages15 up to a constant.If we can quickly sum up the marginal belief (7), then (8) says we can divide out each particular incoming message q��) V →F to obtain its corresponding outgoing message r���1) 14We may ignore the cost of propagators at the variables.Each outgoing message from a variable can be computed in time proportional to its size, which may be amortized against the cost of generating the corresponding incoming message.15E.g., the familiar product of forward and backward messages that is used to extract posterior marginals from an HMM.Note that the marginal belief and both messages are unnormalized distributions over values v of V .F and k are clear from context below, so we simplify the notation so that (7)–(8) become TRIGRAM must sum over assignments to the tag sequence T. The belief (6) in a given assignment is a product of trigram scores (which play the role of transition weights) and incoming messages qTj (playing the role of emission weights).The marginal belief (7) needed above, b(Ti = t), is found by summing over assignments where Ti = t. All marginal beliefs are computed together in O(ng3) total time by the forward-backward algorithm.16 EXACTLY1j is a sparse hard constraint.Even though there are 2n assignments to its n neighboring variables {Lij}, the factor function returns 1 on only n assignments and 0 on the rest.In fact, for a given i, b(Lij = true) in (7) is defined by (6) to have exactly one non-zero summand, in which A puts Lij = true and all other Ligj = false.We compute the marginal beliefs for all i together in O(n) total time: TREE and PTREE must sum over assignments to the O(n2) neighboring variables {Lij}.There are now exponentially many non-zero summands, those in which A corresponds to a valid tree.Nonetheless, 16Which is itself an exact BP algorithm, but on a different graph—a junction tree formed from the graph of TRIGRAM subfactors.Each variable in the junction tree is a bigram.If we had simply replaced the global TRIGRAM factor with its subfactors in the full factor graph, we would have had to resort to Generalized BP (Yedidia et al., 2004) to obtain the same exact results.17But taking it = 1 gives the same results, up to a constant.18As a matter of implementation, this odds ratio qL,, can be used to represent the incoming message qL., everywhere. we can follow the same approach as for EXACTLY1.Steps 1 and 4 are modified to iterate over all i, j such that Lij is a variable.In step 3, the partition function PA b(A) is now 7r times the total weight of all trees, where the weight of a given tree is the product of the gLij values of its n edges.In step 2, the marginal belief b(Lij = true) is now 7r times the total weight of all trees having edge i → j.We perform these combinatorial sums by calling a first-order parsing algorithm, with edge weights qij.Thus, as outlined in §2, a first-order parser is called each time we propagate through the global TREE or PTREE constraint, using edge weights that include the first-order LINK factors but also multiply in any current messages from higher-order factors.The parsing algorithm simultaneously computes the partition function b(), and all O(n2) marginal beliefs b(Lij = true).For PTREE (projective), it is the inside-outside version of a dynamic programming algorithm (Eisner, 1996).For TREE (nonprojective), Koo et al. (2007) and Smith and Smith (2007) show how to employ the matrix-tree theorem.In both cases, the total time is O(n3).19 NOCROSSj` must sum over assignments to O(n) neighboring variables {Lij} and {Lk`}.The nonzero summands are assignments where j and E each have exactly one parent.At step 1, 7r def = Qi qLij(false) · Qk qLke(false).At step 2, the marginal belief b(Lij = true) sums over the n nonzero assignments containing i → j.It is 7r · gLij · Pk �qLke · PAIRij,k`, where PAIRij,k` is xj` if i → j crosses k → E and is 1 otherwise. xj` is some factor value defined by equation (2) to penalize or reward the crossing.Steps 3–4 are just as in EXACTLY1j.The question is how to compute b(Lij = true) for each i in only O(1) time,20 so that we can propagate each of the O(n2) NOCROSSj` in O(n) time.This is why we allowed xj` to depend only on j, E. We can rewrite the sum b(Lij = true) as crossing k noncrossing k 19A dynamic algorithm could incrementally update the outgoing messages if only a few incoming messages have changed (as in asynchronous BP).In the case of TREE, dynamic matrix inverse allows us to update any row or column (i.e., messages from all parents or children of a given word) and find the new inverse in O(n2) time (Sherman and Morrison, 1950).20Symmetrically, we compute b(Lke = true) for each k. To find this in O(1) time, we precompute for each E an array of partial sums Q`[s, t] def = Ps<k<t �qLke.Since Q`[s, t] = Q`[s, t−1]+�qLte, we can compute each entry in O(1) time.The total precomputation time over all E, s, t is then O(n3), with the array Q` shared across all factors NOCROSSjq.The crossing sum is respectively Q`[0, i−1]+Q`[j+1, n], Q`[i+ 1, j − 1], or 0 according to whether E ∈ (i, j), E ∈� [i, j], or E = i.21 The non-crossing sum is Q`[0, n] minus the crossing sum.CHILDSEQi , like TRIGRAM, is propagated by a forward-backward algorithm.In this case, the algorithm is easiest to describe by replacing CHILDSEQi in the factor graph by a collection of local subfactors, which pass messages in the ordinary way.22 Roughly speaking,23 at each j ∈ [1, n], we introduce a new variable Cij—a hidden state whose value is the position of i’s previous child, if any (so 0 ≤ Cij < j).So the ternary subfactor on (Cij, Lij, Ci,j+1) has value 1 if Lij = false and Ci,j+1 = Ci,j; a sibling-bigram score (PAIRiCij,iCi,j+1) if Lij = true and Ci,j+1 = j; and 0 otherwise.The sparsity of this factor, which is 0 almost everywhere, is what gives CHILDSEQi a total runtime of O(n2) rather than O(n3).It is equivalent to forward-backward on an HMM with n observations (the Lij) and n states per observation (the Cj), with a deterministic (thus sparse) transition function.BP computes local beliefs, e.g. the conditional probability that a link Lij is present.But if we wish to output a single well-formed dependency tree, we need to find a single assignment to all the {Lij} that satisfies the TREE (or PTREE) constraint.Our final belief about the TREE factor is a distribution over such assignments, in which a tree’s probability is proportional to the probability of its edge weights gLij (incoming messages).We could simply return the mode of this distribution (found by using a 1-best first-order parser) or the k-best trees, or take samples.21There are no NOCROSSje factors with f = j.22We still treat CHILDSEQi as a global factor and compute all its correct outgoing messages on a single BP iteration, via serial forward and backward sweeps through the subfactors.Handling the subfactors in parallel, (3)–(4), would need O(n) iterations.23Ignoring the treatment of boundary symbols “#” (see §3.4).In our experiments, we actually take the edge weights to be not the messages �qLij from the links, def �bLij = log bLij(true)/bLij(false)).These are passed into a fast algorithm for maximum spanning tree (Tarjan, 1977) or maximum projective spanning tree (Eisner, 1996).This procedure is equivalent to minimum Bayes risk (MBR) parsing (Goodman, 1996) with a dependency accuracy loss function.Notice that the above decoding approaches do not enforce any hard constraints other than TREE in the final output.In addition, they only recover values of the Lij variables.They marginalize over other variables such as tags and link roles.This solves the problem of “nuisance” variables (which merely fragment probability mass among refinements of a parse).On the other hand, it may be undesirable for variables whose values we desire to recover.24Our training method also uses beliefs computed by BP, but at the factors.We choose the weight vector 0 by maximizing the log-probability of training data 24An alternative is to attempt to find the most probable (“MAP”) assignment to all variables—using the max-product algorithm (footnote 13) or one of its recent variants.The estimated marginal beliefs become “max marginals,” which assess the 1-best assignment consistent with each value of the variable.We can indeed build max-product propagators for our global constraints.PTREE still propagates in O(n3) time: simply change the first-order parser’s semiring (Goodman, 1999) to use max instead of sum.TREE requires O(n4) time: it seems that the O(n2) max marginals must be computed separately, each requiring a separate call to an O(n2) maximum spanning tree algorithm (Tarjan, 1977).If max-product BP converges, we may simply output each variable’s favorite value (according to its belief), if unique.However, max-product BP tends to be unstable on loopy graphs, and we may not wish to wait for full convergence in any case.A more robust technique for extracting an assignment is to mimic Viterbi decoding, and “follow backpointers” of the max-product computation along some spanning subtree of the factor graph.A slower but potentially more stable alternative is deterministic annealing.Replace each factor Fm(A) with Fm(A)1/T , where T > 0 is a temperature.As T --+ 0 (“quenches”), the distribution (1) retains the same mode (the MAP assignment), but becomes more sharply peaked at the mode, and sum-product BP approaches max-product BP.Deterministic annealing runs sum-product BP while gradually reducing T toward 0 as it iterates.By starting at a high T and reducing T slowly, it often manages in practice to find a good local optimum.We may then extract an assignment just as we do for max-product. under equation (1), regularizing only by early stopping.If all variables are observed in training, this objective function is convex (as for any log-linear model).The difficult step in computing the gradient of our objective is finding Vθ log Z, where Z in equation (1) is the normalizing constant (partition function) that sums over all assignments A.(Recall that Z, like each Fm, depends implicitly on W and 0.)As usual for log-linear models, Since VθFm(A) only depends on the assignment A’s values for variables that are connected to Fm in the factor graph, its expectation under p(A) depends only on the marginalization of p(A) to those variables jointly.Fortunately, BP provides an estimate of that marginal distribution, namely, its belief about the factor Fm, given W and 0 (§4.2).25 Note that the hard constraints do not depend on 0 at all; so their summands in equation (10) will be 0.We employ stochastic gradient descent (Bottou, 2003), since this does not require us to compute the objective function itself but only to (approximately) estimate its gradient as explained above.Alternatively, given any of the MAP decoding procedures from §6, we could use an error-driven learning method such as the perceptron or MIRA.26We asked: (1) For projective parsing, where higherorder factors have traditionally been incorporated into slow but exact dynamic programming (DP), what are the comparative speed and quality of the BP approximation?(2) How helpful are such higherorder factors—particularly for non-projective parsing, where BP is needed to make them tractable?(3) Do our global constraints (e.g., TREE) contribute to the goodness of BP’s approximation?We built a first-order projective parser—one that uses only factors PTREE and LINK—and then compared the cost of incorporating second-order factors, GRAND and CHILDSEQ, by BP versus DP.28 Under DP, the first-order runtime of O(n3) is increased to O(n4) with GRAND, and to O(n5) when we add CHILDSEQ as well.BP keeps runtime down to O(n3)—although with a higher constant factor, since it takes several rounds to converge, and since it computes more than just the best parse.29 Figures 2–3 compare the empirical runtimes for various input sentence lengths.With only the GRAND factor, exact DP can still find the Viterbi parse (though not the MBR parse29) faster than ten iterations of the asymptotically better BP (Fig.2), at least for sentences with n < 75.However, once we add the CHILDSEQ factor, BP is always faster— dramatically so for longer sentences (Fig.3).More complex models would widen BP’s advantage.Fig.4 shows the tradeoff between runtime and search error of BP in the former case (GRAND only).To determine BP’s search error at finding the MBR parse, we measured its dependency accuracy not against the gold standard, but against the optimal MBR parse under the model, which DP is able to find.After 10 iterations, the overall macro-averaged search error compared to O(n4) DP MBR is 0.4%; compared to O(n5) (not shown), 2.4%.More BP iterations may help accuracy.In future work, we plan to compare BP’s speed-accuracy curve on more complex projective models with the speed-accuracy curve of pruned or reranked DP.The BP approximation can be used to improve the accuracy of non-projective parsing by adding higher-order features.These would be NP-hard to incorporate exactly; DP cannot be used.We used BP with a non-projective TREE factor to train conditional log-linear parsing models of two highly non-projective languages, Danish and Dutch, as well as slightly non-projective English (§8.1).In all three languages, the first-order non-projective parser greatly overpredicts the number of crossing links.We thus added NOCROSS factors, as well as GRAND and CHILDSEQ as before.All of these significantly improve the first-order baseline, though not necessarily cumulatively (Table 2).Finally, Table 2 compares loopy BP to a previously proposed “hill-climbing” method for approximate inference in non-projective parsing McDonald and Pereira (2006).Hill-climbing decodes our richest non-projective model by finding the best projective parse under that model—using slow, higherorder DP—and then greedily modifies words’ parents until the parse score (1) stops improving. with TREE, decoding it with weaker constraints is asymptotically faster (except for NOT2) but usually harmful.(Parenthetical numbers show that the harm is compounded if the weaker constraints are used in training as well; even though this matches training to test conditions, it may suffer more from BP’s approximate gradients.)Decoding the TREE model with the even stronger PTREE constraint can actually be helpful for a more projective language.All results use 5 iterations of BP.BP for non-projective languages is much faster and more accurate than the hill-climbing method.Also, hill-climbing only produces an (approximate) 1-best parse, but BP also obtains (approximate) marginals of the distribution over all parses.Given the BP architecture, do we even need the hard TREE constraint?Or would it suffice for more local hard constraints to negotiate locally via BP?We investigated this for non-projective first-order parsing.Table 3 shows that global constraints are indeed important, and that it is essential to use TREE during training.At test time, the weaker but still global EXACTLY1 may suffice (followed by MBR decoding to eliminate cycles), for total time O(n2).Table 3 includes NOT2, which takes O(n3) time, merely to demonstrate how the BP approximation becomes more accurate for training and decoding when we join the simple NOT2 constraints into more global ATMOST1 constraints.This does not change the distribution (1), but makes BP enforce stronger local consistency requirements at the factors, relying less on independence assumptions.In general, one can get better BP approximations by replacing a group of factors F,,t(A) with their product.30 The above experiments concern gold-standard 30In the limit, one could replace the product (1) with a single all-purpose factor; then BP would be exact—but slow.(In constraint satisfaction, joining constraints similarly makes arc consistency slower but better at eliminating impossible values.) accuracy under a given first-order, non-projective model.Flipping all three of these parameters for Danish, we confirmed the pattern by instead measuring search error under a higher-order, projective model (PTREE+LINK+GRAND), when PTREE was weakened during decoding.Compared to the MBR parse under that model, the search errors from decoding with weaker hard constraints were 2.2% for NOT2, 2.1% for EXACTLY1, 1.7% for EXACTLY1 + NO2CYCLE, and 0.0% for PTREE.Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable.For projective parsing, it is significantly faster than exact dynamic programming, at the cost of small amounts of search error, We are interested in extending these ideas to phrase-structure and lattice parsing, and in trying other higher-order features, such as those used in parse reranking (Charniak and Johnson, 2005; Huang, 2008) and history-based parsing (Nivre and McDonald, 2008).We could also introduce new variables, e.g., nonterminal refinements (Matsuzaki et al., 2005), or secondary links Mid (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc.(Sleator and Temperley, 1993; Buch-Kromann, 2006).Other parsing-like problems that could be attacked with BP appear in syntax-based machine translation.Decoding is very expensive with a synchronous grammar composed with an n-gram language model (Chiang, 2007)—but our footnote 10 suggests that BP might incorporate a language model rapidly.String alignment with synchronous grammars is quite expensive even for simple synchronous formalisms like ITG (Wu, 1997)—but Duchi et al. (2007) show how to incorporate bipartite matching into max-product BP.Finally, we can take advantage of improvements to BP proposed in the context of other applications.For example, instead of updating all messages in parallel at every iteration, it is empirically faster to serialize updates using a priority queue (Elidan et al., 2006; Sutton and McCallum, 2007).31 31These methods need alteration to handle our global propagators, which do update all their outgoing messages at once.
Online Large-Margin Training of Syntactic and Structural Translation FeaturesMinimum-error-rate training (MERT) is a bottleneck for current development in statistical machine translation because it is limited in the number of weights it can reliably optimize.Building on the work of Watanabe et al., we explore the use of the MIRA algorithm of Crammer et al. as an alternative to MERT.We first show that by parallel processing and exploiting more of the parse forest, we can obtain results using MIRA that match or surpass MERT in terms of both translation quality and computational cost.We then test the method on two classes of features that address deficiencies in the Hiero hierarchical phrasebased model: first, we simultaneously train a large number of Marton and Resnik’s soft syntactic constraints, and, second, we introduce a novel structural distortion model.In both cases we obtain significant improvements in translation performance.Optimizing them in combination, for a total of 56 feature weights, improve performance by 2.6 a subset of the NIST 2006 Arabic-English evaluation data.Since its introduction by Och (2003), minimum error rate training (MERT) has been widely adopted for training statistical machine translation (MT) systems.However, MERT is limited in the number of feature weights that it can optimize reliably, with folk estimates of the limit ranging from 15 to 30 features.One recent example of this limitation is a series of experiments by Marton and Resnik (2008), in which they added syntactic features to Hiero (Chiang, 2005; Chiang, 2007), which ordinarily uses no linguistically motivated syntactic information.Each of their new features rewards or punishes a derivation depending on how similar or dissimilar it is to a syntactic parse of the input sentence.They found that in order to obtain the greatest improvement, these features had to be specialized for particular syntactic categories and weighted independently.Not being able to optimize them all at once using MERT, they resorted to running MERT many times in order to test different combinations of features.But it would have been preferable to use a training method that can optimize the features all at once.There has been much work on improving MERT’s performance (Duh and Kirchoff, 2008; Smith and Eisner, 2006; Cer et al., 2008), or on replacing MERT wholesale (Turian et al., 2007; Blunsom et al., 2008).This paper continues a line of research on online discriminative training (Tillmann and Zhang, 2006; Liang et al., 2006; Arun and Koehn, 2007), extending that of Watanabe et al. (2007), who use the Margin Infused Relaxed Algorithm (MIRA) due to Crammer et al.(2003; 2006).Our guiding principle is practicality: like Watanabe et al., we train on a small tuning set comparable in size to that used by MERT, but by parallel processing and exploiting more of the parse forest, we obtain results using MIRA that match or surpass MERT in terms of both translation quality and computational cost on a large-scale translation task.Taking this further, we test MIRA on two classes of features that make use of syntactic information and hierarchical structure.First, we generalize Marton and Resnik’s (2008) soft syntactic constraints by training all of them simultaneously; and, second, we introduce a novel structural distortion model.We obtain significant improvements in both cases, and further large improvements when the two feature sets are combined.The paper proceeds as follows.We describe our training algorithm in section 2; our generalization of Marton and Resnik’s soft syntactic constraints in section 3; our novel structural distortion features in section 4; and experimental results in section 5.The translation model is a standard linear model (Och and Ney, 2002), which we train using MIRA (Crammer and Singer, 2003; Crammer et al., 2006), following Watanabe et al. (2007).We describe the basic algorithm first and then progressively refine it.Let e, by abuse of notation, stand for both output strings and their derivations.We represent the feature vector for derivation e as h(e).Initialize the feature weights w. Then, repeatedly: passes through the training data are made, we only average the weight vectors from the last pass.)The technique of averaging was introduced in the context of perceptrons as an approximation to taking a vote among all the models traversed during training, and has been shown to work well in practice (Freund and Schapire, 1999; Collins, 2002).We follow McDonald et al. (2005) in applying this technique to MIRA.Note that the objective (1) is not the same as that used by Watanabe et al. ; rather, it is the same as that used by Crammer and Singer (2003) and related to that of Taskar et al.(2005).We solve this optimization problem using a variant of sequential minimal optimization (Platt, 1998): for each i, initialize αij = C for a single value of j such that eij = e∗i , and initialize αij = 0 for all other values of j.Then, repeatedly choose a sentence i and a pair of hypotheses j, j0, and let where where we set C = 0.01.The first term means that we want w0 to be close to w, and second term (the generalized hinge loss) means that we want w0 to score e∗i higher than each eij by a margin at least as wide as the loss `ij.When training is finished, the weight vectors from all iterations are averaged together.(If multiple where the function clip[x,y](z) gives the closest number to z in the interval [x, y].Assuming BLEU as the evaluation criterion, the loss `ij of ei j relative to e∗i should be related somehow to the difference between their BLEU scores.However, BLEU was not designed to be used on individual sentences; in general, the highest-BLEU translation of a sentence depends on what the other sentences in the test set are.Sentence-level approximations to BLEU exist (Lin and Och, 2004; Liang et al., 2006), but we found it most effective to perform BLEU computations in the context of a set O of previously-translated sentences, following Watanabe et al. (2007).However, we don’t try to accumulate translations for the entire dataset, but simply maintain an exponentially-weighted moving average of previous translations.More precisely: For an input sentence f, let e be some hypothesis translation and let {rk} be the set of reference translations for f. Let c(e; {rk}), or simply c(e) for short, be the vector of the following counts: |e|, the effective reference length mink |rk|, and, for 1 ≤ n ≤ 4, the number of n-grams in e, and the number of n-gram matches between e and {rk}.These counts are sufficient to calculate a BLEU score, which we write as BLEU(c(e)).The pseudo-document O is an exponentially-weighted moving average of these vectors.That is, for each training sentence, let eˆ be the 1-best translation; after processing the sentence, we update O, and its input length Of: We can then calculate the BLEU score of hypotheses e in the context of O.But the larger O is, the smaller the impact the current sentence will have on the BLEU score.To correct for this, and to bring the loss function roughly into the same range as typical margins, we scale the BLEU score by the size of the input: which we also simply write as B(e).Finally, the loss function is defined to be: We now describe the selection of e∗.We know of three approaches in previous work.The first is to force the decoder to output the reference sentence exactly, and select the derivation with the highest model score, which Liang et al. (2006) call bold updating.The second uses the decoder to search for the highest-BLEU translation (Tillmann and Zhang, 2006), which Arun and Koehn (2007) call max-BLEU updating.Liang et al. and Arun and Koehn experiment with these methods and both opt for a third method, which Liang et al. call local updating: generate an n-best list of translations and select the highest-BLEU translation from it.The intuition is that due to noise in the training data or reference translations, a high-BLEU translation may actually use peculiar rules which it would be undesirable to encourage the model to use.Hence, in local updating, the search for the highest-BLEU translation is limited to the n translations with the highest model score, where n must be determined experimentally.Here, we introduce a new oracle-translation selection method, formulating the intuition behind local updating as an optimization problem: Instead of choosing the highest-BLEU translation from an n-best list, we choose the translation that maximizes a combination of (approximate) BLEU and the model.We can also interpret (10) in the following way: we want e∗ to be the max-BLEU translation, but we also want to minimize (1).So we balance these two criteria against each other: where (B(e) − h(e) · w) is that part of (1) that depends on e∗, and µ is a parameter that controls how much we are willing to allow some translations to have higher BLEU than e∗ if we can better minimize (1).Setting µ = 0 would reduce to max-BLEU updating; setting µ = ∞ would never update w at all.Setting µ = 0.5 reduces to equation (10).Figure 1 shows the 10-best unique translations for a single input sentence according to equation (11) under various settings of µ.The points at far right are the translations that are scored highest according to the model.The p = 0 points in the upper-left corner are typical of oracle translations that would be selected under the max-BLEU policy: they indeed have a very high BLEU score, but are far removed from the translations preferred by the model; thus they would cause violent updates to w. Local updating would select the topmost point labeled p = 1.Our scheme would select one of the p = 0.5 points, which have BLEU scores almost as high as the max-BLEU translations, yet are not very far from the translations preferred by the model.What is the set {eij} of translation hypotheses?Ideally we would let it be the set of all possible translations, and let the objective function (1) take all of them into account.This is the approach taken by Taskar et al. (2004), but their approach assumes that the loss function can be decomposed into local loss functions.Since our loss function cannot be so decomposed, we select: The last case is what Crammer et al. (2006) call max-loss updating (where “loss” refers to the generalized hinge loss) and Taskar et al.(2005) call lossaugmented inference.The rationale here is that since the objective (1) tries to minimize maxj(l'ij − Ahij · w'), we should include the translations that have the highest (l'ij − Ahij · w) in order to approximate the effect of using the whole forest.See Figure 1 again for an illustration of the hypotheses selected for a single sentence.The maxBLEU points in the upper left are not included (and would have no effect even if they were included).The p = oo points in the lower-right are the negative examples: they are poor translations that are scored too high by the model, and the learning algorithm attempts to shift them to the left.To perform the forest rescoring, we need to use several approximations, since an exact search for BLEU-optimal translations is NP-hard (Leusch et al., 2008).For every derivation e in the forest, we calculate a vector c(e) of counts as in Section 2.2 except using unclipped counts of n-gram matches (Dreyer et al., 2007), that is, the number of matches for an ngram can be greater than the number of occurrences of the n-gram in any reference translation.This can be done efficiently by calculating c for every hyperedge (rule application) in the forest: We keep track of n-grams using the same scheme used to incorporate an n-gram language model into the decoder (Wu, 1996; Chiang, 2007).To find the best derivation in the forest, we traverse it bottom-up as usual, and for every set of alternative subtranslations, we select the one with the highest score.But here a rough approximation lurks, because we need to calculate B on the nodes of the forest, but B does not have the optimal substructure property, i.e., the optimal score of a parent node cannot necessarily be calculated from the optimal scores of its children.Nevertheless, we find that this rescoring method is good enough for generating high-BLEU oracle translations and low-BLEU negative examples.One convenient property of MERT is that it is embarrassingly parallel: we decode the entire tuning set sending different sentences to different processors, and during optimization of feature weights, different random restarts can be sent to different processors.In order to make MIRA comparable in efficiency to MERT, we must parallelize it.But with an online learning algorithm, parallelization requires a little more coordination.We run MIRA on each processor simultaneously, with each maintaining its own weight vector.A master process distributes different sentences from the tuning set to each of the processors; when each processor finishes decoding a sentence, it transmits the resulting hypotheses, with their losses, to all the other processors and receives any hypotheses waiting from other processors.Those hypotheses were generated from different weight vectors, but can still provide useful information.The sets of hypotheses thus collected are then processed as one batch.When the whole training process is finished, we simply average all the weight vectors from all the processors.Having described our training algorithm, which includes several practical improvements to Watanabe et al.’s usage of MIRA, we proceed in the remainder of the paper to demonstrate the utility of the our training algorithm on models with large numbers of structurally sensitive features.The first features we explore are based on a line of research introduced by Chiang (2005) and improved on by Marton and Resnik (2008).A hierarchical phrase-based translation model is based on synchronous context-free grammar, but does not normally use any syntactic information derived from linguistic knowledge or treebank data: it uses translation rules that span any string of words in the input sentence, without regard for parser-defined syntactic constituency boundaries.Chiang (2005) experimented with a constituency feature that rewarded rules whose source language side exactly spans a syntactic constituent according to the output of an external source-language parser.This feature can be viewed as a soft syntactic constraint: it biases the model toward translations that respect syntactic structure, but does not force it to use them.However, this more syntactically aware model, when tested in Chinese-English translation, did not improve translation performance.Recently, Marton and Resnik (2008) revisited the idea of constituency features, and succeeded in showing that finer-grained soft syntactic constraints yield substantial improvements in BLEU score for both Chinese-English and Arabic-English translation.In addition to adding separate features for different syntactic nonterminals, they introduced a new type of constraint that penalizes rules when the source language side crosses the boundaries of a source syntactic constituent, as opposed to simply rewarding rules when they are consistent with the source-language parse tree.Marton and Resnik optimized their features’ weights using MERT.But since MERT does not scale well to large numbers of feature weights, they were forced to test individual features and manually selected feature combinations each in a separate model.Although they showed gains in translation performance for several such models, many larger, potentially better feature combinations remained unexplored.Moreover, the best-performing feature subset was different for the two language pairs, suggesting that this labor-intensive feature selection process would have to be repeated for each new language pair.Here, we use MIRA to optimize Marton and Resnik’s finer-grained single-category features all at once.We define below two sets of features, a coarsegrained class that combines several constituency categories, and a fine-grained class that puts different categories into different features.Both kinds of features were used by Marton and Resnik, but only a few at a time.Crucially, our training algorithm provides the ability to train all the fine-grained features, a total of 34 feature weights, simultaneously.Coarse-grained features As the basis for coarsegrained syntactic features, we selected the following nonterminal labels based on their frequency in the tuning data, whether they frequently cover a span of more than one word, and whether they represent linguistically relevant constituents: NP, PP, S, VP, SBAR, ADJP, ADVP, and QP.We define two new features, one which fires when a rule’s source side span in the input sentence matches any of the above-mentioned labels in the input parse, and another which fires when a rule’s source side span crosses a boundary of one of these labels (e.g., its source side span only partially covers the words in a VP subtree, and it also covers some or all or the words outside the VP subtree).These two features are equivalent to Marton and Resnik’s XP= and XP' feature combinations, respectively.Fine-grained features We selected the following nonterminal labels that appear more than 100 times in the tuning data: NP, PP, S, VP, SBAR, ADJP, WHNP, PRT, ADVP, PRN, and QP.The labels that were excluded were parts of speech, nonconstituent labels like FRAG, or labels that occurred only two or three times.For each of these labels X, we added a separate feature that fires when a rule’s source side span in the input sentence matches X, and a second feature that fires when a span crosses a boundary of X.These features are similar to Marton and Resnik’s X= and X+, except that our set includes features for WHNP, PRT, and PRN.In addition to parser-based syntactic constraints, which were introduced in prior work, we introduce a completely new set of features aimed at improving the modeling of reordering within Hiero.Again, the feature definition gives rise to a larger number of features than one would expect to train successfully using MERT.In a phrase-based model, reordering is performed both within phrase pairs and by the phrasereordering model.Both mechanisms are able to learn that longer-distance reorderings are more costly than shorter-distance reorderings: phrase pairs, because phrases that involve more extreme reorderings will (presumably) have a lower count in the data, and phrase reordering, because models are usually explicitly dependent on distance.By contrast, in a hierarchical model, all reordering is performed by a single mechanism, the rules of the grammar.In some cases, the model will be able to learn a preference for shorter-distance reorderings, as in a phrase-based system, but in the case of a word being reordered across a nonterminal, or two nonterminals being reordered, there is no dependence in the model on the size of the nonterminal or nonterminals involved in reordering.So, for example, if we have rules we might expect that rule (12) is more common in general, but that rule (13) becomes more and more rare as X1 gets larger.The default Hiero features have no way to learn this.To address this defect, we can classify every nonterminal pair occurring on the right-hand side of each grammar rule as “reordered” or “not reordered”, that is, whether it intersects any other word alignment link or nonterminal pair (see Figure 2).We then define coarse- and fine-grained versions of the structural distortion model.Coarse-grained features Let R be a binaryvalued random variable that indicates whether a nonterminal occurrence is reordered, and let S be an integer-valued random variable that indicates how many source words are spanned by the nonterminal occurrence.We can estimate P(R  |S) via relativefrequency estimation from the rules as they are extracted from the parallel text, and incorporate this probability as a new feature of the model.Fine-grained features A difficulty with the coarse-grained reordering features is that the grammar extraction process finds overlapping rules in the training data and might not give a sensible probability estimate; moreover, reordering statistics from the training data might not carry over perfectly into the translation task (in particular, the training data may have some very freely-reordering translations that one might want to avoid replicating in translation).As an alternative, we introduce a fine-grained version of our distortion model that can be trained directly in the translation task as follows: define a separate binary feature for each value of (R, S), where R is as above and S E J*, 1, ... , 9, >_101 and * means any size.For example, if a nonterminal with span 11 has its contents reordered, then the features (true, >_10) and (true, *) would both fire.Grouping all sizes of 10 or more into a single feature is designed to avoid overfitting.Again, using MIRA makes it practical to train with the full fine-grained feature set—coincidentally also a total of 34 features.We now describe our experiments to test MIRA and our features, the soft-syntactic constraints and the structural distortion features, on an Arabic-English translation task.It is worth noting that this experimentation is on a larger scale than Watanabe et al.’s (2007), and considerably larger than Marton and Resnik’s (2008).The baseline model was Hiero with the following baseline features (Chiang, 2005; Chiang, 2007): The probability features are base-100 logprobabilities.The rules were extracted from all the allowable parallel text from the NIST 2008 evaluation (152+175 million words of Arabic+English), aligned by IBM Model 4 using GIZA++ (union of both directions).Hierarchical rules were extracted from the most in-domain corpora (4.2+5.4 million words) and phrases were extracted from the remainder.We trained the coarse-grained distortion model on 10,000 sentences of the training data.Two language models were trained, one on data similar to the English side of the parallel text and one on 2 billion words of English.Both were 5gram models with modified Kneser-Ney smoothing, lossily compressed using a perfect-hashing scheme similar to that of Talbot and Brants (2008) but using minimal perfect hashing (Botelho et al., 2005).We partitioned the documents of the NIST 2004 (newswire) and 2005 Arabic-English evaluation data into a tuning set (1178 sentences) and a development set (1298 sentences).The test data was the NIST 2006 Arabic-English evaluation data (NIST part, newswire and newsgroups, 1529 sentences).To obtain syntactic parses for this data, we tokenized it according to the Arabic Treebank standard using AMIRA (Diab et al., 2004), parsed it with the Stanford parser (Klein and Manning, 2003), and then forced the trees back into the MT system’s tokenization.1 We ran both MERT and MIRA on the tuning set using 20 parallel processors.We stopped MERT when the score on the tuning set stopped increasing, as is common practice, and for MIRA, we used the development set to decide when to stop training.2 In our runs, MERT took an average of 9 passes through the tuning set and MIRA took an average of 8 passes.(For comparison, Watanabe et al. report decoding their tuning data of 663 sentences 80 times.)Table 1 shows the results of our experiments with the training methods and features described above.All significance testing was performed against the first line (MERT baseline) using paired bootstrap resampling (Koehn, 2004).First of all, we find that MIRA is competitive with MERT when both use the baseline feature set.Indeed, the MIRA system scores significantly higher on the test set; but if we break the test set down by genre, we see that the MIRA system does slightly worse on newswire and better on newsgroups.(This is largely attributable to the fact that the MIRA translations tend to be longer than the MERT translations, and the newsgroup references are also relatively longer than the newswire references.)When we add more features to the model, the two training methods diverge more sharply.When training with MERT, the coarse-grained pair of syntax features yields a small improvement, but the finegrained syntax features do not yield any further improvement.By contrast, when the fine-grained features are trained using MIRA, they yield substantial improvements.We observe similar behavior for the structural distortion features: MERT is not able to take advantage of the finer-grained features, but MIRA is.Finally, using MIRA to combine both classes of features, 56 in all, produces the largest improvement, 2.6 BLEU points over the MERT baseline on the full test set.We also tested some of the differences between our training method and Watanabe et al.’s (2007); the results are shown in Table 2.Compared with local updating (line 2), our method of selecting the oracle translation and negative examples does better by 0.5 BLEU points on the development data.Using lossaugmented inference to add negative examples to local updating (line 3) does not appear to help.Nevertheless, the negative examples are important: for if Setting Dev full 53.6 local updating, no LAI local updating, LAI p = 0.5 oracle, no LAI no sharing of updates 53.1−− we use our method for selecting the oracle translation without the additional negative examples (line 4), the algorithm fails, generating very long translations and unable to find a weight setting to shorten them.It appears, then, that the additional negative examples enable the algorithm to reliably learn from the enhanced oracle translations.Finally, we compared our parallelization method against a simpler method in which all processors learn independently and their weight vectors are all averaged together (line 5).We see that sharing information among the processors makes a significant difference.In this paper, we have brought together two existing lines of work: the training method of Watanabe et al. (2007), and the models of Chiang (2005) and Marton and Resnik (2008).Watanabe et al.’s work showed that large-margin training with MIRA can be made feasible for state-of-the-art MT systems by using a manageable tuning set; we have demonstrated that parallel processing and exploiting more of the parse forest improves MIRA’s performance and that, even using the same set of features, MIRA’s performance compares favorably to MERT in terms of both translation quality and computational cost.Marton and Resnik (2008) showed that it is possible to improve translation in a data-driven framework by incorporating source-side syntactic analysis in the form of soft syntactic constraints.This work joins a growing body of work demonstrating the utility of syntactic information in statistical MT.In the area of source-side syntax, recent research has continued to improve tree-to-string translation models, soften the constraints of the input tree in various ways (Mi et al., 2008; Zhang et al., 2008), and extend phrase-based translation with sourceside soft syntactic constraints (Cherry, 2008).All this work shows strong promise, but Marton and Resnik’s soft syntactic constraint approach is particularly appealing because it can be used unobtrusively with any hierarchically-structured translation model.Here, we have shown that using MIRA to weight all the constraints at once removes the crucial drawback of the approach, the problem of feature selection.Finally, we have introduced novel structural distortion features to fill a notable gap in the hierarchical phrase-based approach.By capturing how reordering depends on constituent length, these features improve translation quality significantly.In sum, we have shown that removing the bottleneck of MERT opens the door to many possibilities for better translation.Thanks to Michael Bloodgood for performing initial simulations of parallelized perceptron training.Thanks also to John DeNero, Kevin Knight, Daniel Marcu, and Fei Sha for valuable discussions and suggestions.This research was supported in part by DARPA contract HR0011-06-C-0022 under subcontract to BBN Technologies and HR0011-06-02-001 under subcontract to IBM.
Bayesian Unsupervised Topic SegmentationThis paper describes a novel Bayesian approach to unsupervised topic segmentation.Unsupervised systems for this task are driven the tendency of wellformed segments to induce a compact and consistent lexical distribution.We show that lexical cohesion can be placed in a Bayesian context by modeling the words in each topic segment as draws from a multinomial language model associated with the segment; maximizing the observation likelihood in such a model yields a lexically-cohesive segmentation.This contrasts with previous approaches, which relied on hand-crafted cohesion metrics.The Bayesian framework provides a principled way to incorporate additional features such as cue phrases, a powerful indicator of discourse structure that has not been previously used in unsupervised segmentation systems.Our model yields consistent improvements over an array of state-of-the-art systems on both text and speech datasets.We also show that both an entropy-based analysis and a well-known previous technique can be deTopic segmentation is one of the fundamental problems in discourse analysis, where the task is to divide a text into a linear sequence of topicallycoherent segments.Hearst’s TEXTTILING (1994) introduced the idea that unsupervised segmentation can be driven by lexical cohesion, as high-quality segmentations feature homogeneous lexical distributions within each topic segment.Lexical cohesion has provided the inspiration for several successful systems (e.g., Utiyama and Isahara, 2001; Galley et al.2003; Malioutov and Barzilay, 2006), and is currently the dominant approach to unsupervised topic segmentation.But despite the effectiveness of lexical cohesion for unsupervised topic segmentation, it is clear that there are other important indicators that are ignored by the current generation of unsupervised systems.For example, consider cue phrases, which are explicit discourse markers such as “now” or “however” (Grosz and Sidner, 1986; Hirschberg and Litman, 1993; Knott, 1996).Cue phrases have been shown to be a useful feature for supervised topic segmentation (Passonneau and Litman, 1993; Galley et al., 2003), but cannot be incorporated by current unsupervised models.One reason for this is that existing unsupervised methods use arbitrary, hand-crafted metrics for quantifying lexical cohesion, such as weighted cosine similarity (Hearst, 1994; Malioutov and Barzilay, 2006).Without supervision, it is not possible to combine such metrics with additional sources of information.Moreover, such hand-crafted metrics may not generalize well across multiple datasets, and often include parameters which must be tuned on development sets (Malioutov and Barzilay, 2006; Galley et al., 2003).In this paper, we situate lexical cohesion in a Bayesian framework, allowing other sources of information to be incorporated without the need for labeled data.We formalize lexical cohesion in a generative model in which the text for each segment is produced by a distinct lexical distribution.Lexically-consistent segments are favored by this model because probability mass is conserved for a narrow subset of words.Thus, lexical cohesion arises naturally through the generative process, and other sources of information – such as cue words – can easily be incorporated as emissions from the segment boundaries.More formally, we treat the words in each sentence as draws from a language model associated with the topic segment.This is related to topicmodeling methods such as latent Dirichlet allocation (LDA; Blei et al. 2003), but here the induced topics are tied to a linear discourse structure.This property enables a dynamic programming solution to find the exact maximum-likelihood segmentation.We consider two approaches to handling the language models: estimating them explicitly, and integrating them out, using the Dirichlet Compound Multinomial distribution (also known as the multivariate Polya distribution).We model cue phrases as generated from a separate multinomial that is shared across all topics and documents in the dataset; a high-likelihood model will obtain a compact set of cue phrases.The addition of cue phrases renders our dynamic programming-based inference inapplicable, so we design a sampling-based inference technique.This algorithm can learn in a completely unsupervised fashion, but it also provides a principled mechanism to improve search through the addition of declarative linguistic knowledge.This is achieved by biasing the selection of samples towards boundaries with known cue phrases; this does not change the underlying probabilistic model, but guides search in the direction of linguistically-plausible segmentations.We evaluate our algorithm on corpora of spoken and written language, including the benchmark ICSI meeting dataset (Janin et al., 2003) and a new textual corpus constructed from the contents of a medical textbook.In both cases our model achieves performance surpassing multiple state-of-the-art baselines.Moreover, we demonstrate that the addition of cue phrases can further improve segmentation performance over cohesion-based methods.In addition to the practical advantages demonstrated by these experimental results, our model reveals interesting theoretical properties.Other researchers have observed relationships between discourse structure and entropy (e.g., Genzel and Charniak, 2002).We show that in a special case of our model, the segmentation objective is equal to a weighted sum of the negative entropies for each topic segment.This finding demonstrates that a relationship between discourse segmentation and entropy is a natural consequence of modeling topic structure in a generative Bayesian framework.In addition, we show that the benchmark segmentation system of Utiyama and Isahara (2001) can be viewed as another special case of our Bayesian model.Existing unsupervised cohesion-based approaches can be characterized in terms of the metric used to quantify cohesion and the search technique.Galley et al. (2003) characterize cohesion in terms of lexical chains – repetitions of a given lexical item over some fixed-length window of sentences.In their unsupervised model, inference is performed by selecting segmentation points at the local maxima of the cohesion function.Malioutov and Barzilay (2006) optimize a normalized minimum-cut criteria based on a variation of the cosine similarity between sentences.Most similar to our work is the approach of Utiyama and Isahara (2001), who search for segmentations with compact language models; as shown in Section 3.1.1, this can be viewed as a special case of our model.Both of these last two systems use dynamic programming to search the space of segmentations.An alternative Bayesian approach to segmentation was proposed by Purver et al. (2006).They assume a set of documents that is characterized by some number of hidden topics that are shared across multiple documents.They then build a linear segmentation by adding a switching variable to indicate whether the topic distribution for each sentence is identical to that of its predecessor.Unlike Purver et al., we do not assume a dataset in which topics are shared across multiple documents; indeed, our model can be applied to single documents individually.Additionally, the inference procedure of Purver et al. requires sampling multiple layers of hidden variables.In contrast, our inference procedure leverages the nature of linear segmentation to search only in the space of segmentation points.The relationship between discourse structure and cue phrases has been studied extensively; for an early example of computational work on this topic, see (Grosz, 1977).Passonneau and Litman (1993) were the first to investigate the relationship between cue phrases and linear segmentation.More recently, cue phrases have been applied to topic segmentation in the supervised setting.In a supervised system that is distinct from the unsupervised model described above, Galley et al. (2003) automatically identify candidate cue phrases by mining labeled data for words that are especially likely to appear at segment boundaries; the presence of cue phrases is then used as a feature in a rule-based classifier for linear topic segmentation.Elsner and Charniak (2008) specify a list of cue phrases by hand; the cue phrases are used as a feature in a maximum-entropy classifier for conversation disentanglement.Unlike these approaches, we identify candidate cue phrases automatically from unlabeled data and incorporate them in the topic segmentation task without supervision.The core idea of lexical cohesion is that topicallycoherent segments demonstrate compact and consistent lexical distributions (Halliday and Hasan, 1976).Lexical cohesion can be placed in a probabilistic context by modeling the words in each topic segment as draws from a multinomial language model associated with the segment.Formally, if sentence t is in segment j, then the bag of words xt is drawn from the multinomial language model θj.This is similar in spirit to hidden topic models such as latent Dirichlet allocation (Blei et al., 2003), but rather than assigning a hidden topic to each word, we constrain the topics to yield a linear segmentation of the document.We will assume that topic breaks occur at sentence boundaries, and write zt to indicate the topic assignment for sentence t. The observation likelihood is, where X is the set of all T sentences, z is the vector of segment assignments for each sentence, and Θ is the set of all K language models.2 A linear segmentation is ensured by the additional constraint that zt must be equal to either zt−1 (the previous sentence’s segment) or zt−1 + 1 (the next segment).To obtain a high likelihood, the language models associated with each segment should concentrate their probability mass on a compact subset of words.Language models that spread their probability mass over a broad set of words will induce a lower likelihood.This is consistent with the principle of lexical cohesion.Thus far, we have described a segmentation in terms of two parameters: the segment indices z, and the set of language models Θ.For the task of segmenting documents, we are interested only in the segment indices, and would prefer not to have to search in the space of language models as well.We consider two alternatives: taking point estimates of the language models (Section 3.1), and analytically marginalizing them out (Section 3.2).One way to handle the language models is to choose a single point estimate for each set of segmentation points z.Suppose that each language model is drawn from a symmetric Dirichlet prior: θj — Dir(θ0).Let nj be a vector in which each element is the sum of the lexical counts over all the sentences in segment j: nj,i = E{t:zt=j} mt,i, where mt,i is the count of word i in sentence t. Assuming that each xt — θj, then the posterior distribution for θj is Dirichlet with vector parameter nj +θ0 (Bernardo and Smith, 2000).The expected value of this distribution is the multinomial distribution ˆθj, where, In this equation, W indicates the number of words in the vocabulary.Having obtained an estimate for the language model ˆθj, the observed data likelihood for segment j is a product over each sentence in the segment, 2Our experiments will assume that the number of topics K is known.This is common practice for this task, as the desired number of segments may be determined by the user (Malioutov and Barzilay, 2006).By viewing the likelihood as a product over all terms in the vocabulary, we observe interesting connections with prior work on segmentation and information theory.In this section, we explain how our model generalizes the well-known method of Utiyama and Isahara (2001; hereafter U&I).As in our work, Utiyama and Isahara propose a probabilistic framework based on maximizing the compactness of the language models induced for each segment.Their likelihood equation is identical to our equations 3-5.They then define the language models for each segment as �Bj,i = nj,iW1 , without rigorous justifiW+Ei nj,i cation.This form is equivalent to Laplacian smoothing (Manning and Sch¨utze, 1999), and is a special case of our equation 2, with B0 = 1.Thus, the language models in U&I can be viewed as the expectation of the posterior distribution p(Bj|{xt : zt = j}, B0), in the special case that B0 = 1.Our approach generalizes U&I and provides a Bayesian justification for the language models that they apply.The remainder of the paper further extends this work by marginalizing out the language model, and by adding cue phrases.We empirically demonstrate that these extensions substantially improve performance.Our model also has a connection to entropy, and situates entropy-based segmentation within a Bayesian framework.Equation 1 defines the objective function as a product across sentences; using equations 3-5 we can decompose this across segments instead.Working in logarithms, The last line substitutes in the logarithm of equation 5.Setting B0 = 0 and rearranging equation 2, we obtain nj,i = Nj�Bj,i, with Nj = PW i nj,i, the total number of words in segment j.Substituting this into equation 6, we obtain where H(Bj) is the negative entropy of the multinomial Bj.Thus, with B0 = 0, the log conditional probability in equation 6 is optimized by a segmentation that minimizes the weighted sum of entropies per segment, where the weights are equal to the segment lengths.This result suggests intriguing connections with prior work on the relationship between entropy and discourse structure (e.g., Genzel and Charniak, 2002; Sporleder and Lapata, 2006).The previous subsection uses point estimates of the language models to reveal connections to entropy and prior work on segmentation.However, point estimates are theoretically unsatisfying from a Bayesian perspective, and better performance may be obtained by marginalizing over all possible laneach segment, so the overall likelihood for the pointestimate version also decomposes across segments.Any objective function that can be decomposed into a product across segments can be maximized using dynamic programming.We define B(t) as the value of the objective function for the optimal segmentation up to sentence t. The contribution to the objective function from a single segment between sentences t' and t is written, b(t', t) = p({xt, ... xt}|zt-...t = j) where pdcm refers to the Dirichlet compound multinomial distribution (DCM), also known as the multivariate Polya distribution (Johnson et al., 1997).The DCM distribution expresses the expectation over all multinomial language models, when conditioning on the Dirichlet prior θ0.When θ0 is a symmetric Dirichlet prior, where nj,i is the count of word i in segment j, and Nj = PWi nj,i, the total number of words in the segment.The symbol F refers to the Gamma function, an extension of the factorial function to real numbers.Using the DCM distribution, we can compute the data likelihood for each segment from the lexical counts over the entire segment.The overall observation likelihood is a product across the likelihoods for each segment.The optimal segmentation maximizes the joint probability, p(X, z|θ0) = p(X|z, θ0)p(z).We assume that p(z) is a uniform distribution over valid segmentations, and assigns no probability mass to invalid segmentations.The data likelihood is defined for point estimate language models in equation 5 and for marginalized language models in equation 7.Note that equation 7 is written as a product over segments.The point estimates for the language models depend only on the counts within The maximum value of the objective function is then given by the recurrence relation, B(t) = maxt,<t B(t')b(t'+1, t), with the base case B(0) = 1.These values can be stored in a table of size T (equal to the number of sentences); this admits a dynamic program that performs inference in polynomial time.3 If the number of segments is specified in advance, the dynamic program is slightly more complex, with a table of size TK.The Dirichlet compound multinomial integrates over language models, but we must still set the prior θ0.We can re-estimate this prior based on the observed data by interleaving gradient-based search in a Viterbi expectation-maximization framework (Gauvain and Lee, 1994).In the E-step, we estimate a segmentation z� of the dataset, as described in Section 3.3.In the M-step, we maximize p(θ0|X, z) ∝ p(X|θ0, z)p(θ0).Assuming a non-informative hyperprior p(θ0), we maximize the likelihood in Equation 7 across all documents.The maximization is performed using a gradient-based search; the gradients are dervied by Minka (2003).This procedure is iterated until convergence or a maximum of twenty iterations.One of the key advantages of a Bayesian framework for topic segmentation is that it permits the principled combination of multiple data sources, even without labeled data.We are especially interested in cue phrases, which are explicit markers for discourse structure, such as “now” or “first” (Grosz and Sidner, 1986; Hirschberg and Litman, 1993; Knott, 1996).Cue phrases have previously been used in supervised topic segmentation (e.g., Galley et al. 2003); we show how they can be used in an unsupervised setting.The previous section modeled lexical cohesion by treating the bag of words in each sentence as a series of draws from a multinomial language model indexed by the topic segment.To incorporate cue phrases, this generative model is modified to reflect the idea that some of the text will be topic-specific, but other terms will be topic-neutral cue phrases that express discourse structure.This idea is implemented by drawing the text at each topic boundary from a special language model φ, which is shared across all topics and all documents in the dataset.For sentences that are not at segment boundaries, the likelihood is as before: p(xt|z, o, φ) = Q i∈xt θzt,i.For sentences that immediately follow segment boundaries, we draw the first ` words from φ instead.Writing x�`) t for the ` cue words in xt, and Rt for the remaining words, the likelihood for a segment-initial sentence is, We draw φ from a symmetric Dirichlet prior φ0.Following prior work (Galley et al., 2003; Litman and Passonneau, 1995), we consider only the first word of each sentence as a potential cue phrase; thus, we set ` = 1 in all experiments.To estimate or marginalize the language models o and φ, it is necessary to maintain lexical counts for each segment and for the segment boundaries.The counts for φ are summed across every segment in the entire dataset, so shifting a boundary will affect the probability of every segment, not only the adjacent segments as before.Thus, the factorization that enabled dynamic programming inference in Section 3.3 is no longer applicable.Instead, we must resort to approximate inference.Sampling-based inference is frequently used in related Bayesian models.Such approaches build a stationary Markov chain by repeatedly sampling among the hidden variables in the model.The most commonly-used sampling-based technique is Gibbs sampling, which iteratively samples from the conditional distribution of each hidden variable (Bishop, 2006).However, Gibbs sampling is slow to converge to a stationary distribution when the hidden variables are tightly coupled.This is the case in linear topic segmentation, due to the constraint that zt E {zt−1, zt−1 + 11 (see Section 3).For this reason, we apply the more general Metropolis-Hastings algorithm, which permits sampling arbitrary transformations of the latent variables.In our framework, such transformations correspond to moves through the space of possible segmentations.A new segmentation z0 is drawn from the previous hypothesized segmentation z based on a proposal distribution q(z0|z).4 The probability of accepting a proposed transformation depends on the ratio of the joint probabilities and a correction term for asymmetries in the proposal distribution: The Metropolis-Hastings algorithm guarantees that by accepting samples at this ratio, our sampling procedure will converge to the stationary distribution for the hidden variables z.When cue phrases are included, the observation likelihood is written: As in Section 3.2, we can marginalize over the language models.We obtain a product of DCM distributions: one for each segment, and one for all cue phrases in the dataset.Metropolis-Hastings requires a proposal distribution to sample new configurations.The proposal distri4Because the cue phrase language model 0 is used across the entire dataset, transformations affect the likelihood of all documents in the corpus.For clarity, our exposition will focus on the single-document case. bution does not affect the underlying probabilistic model – Metropolis-Hastings will converge to the same underlying distribution for any non-degenerate proposal.However, a well-chosen proposal distribution can substantially speed convergence.Our basic proposal distribution selects an existing segmentation point with uniform probability, and considers a set of local moves.The proposal is constructed so that no probability mass is allocated to moves that change the order of segment boundaries, or merge two segments; one consequence of this restriction is that moves cannot add or remove segments.5 We set the proposal distribution to decrease exponentially with the move distance, thus favoring incremental transformations to the segmentation.More formally, let d(z —* z') > 0 equal the distance that the selected segmentation point is moved when we transform the segmentation from z to z'.We can write the proposal distribution q(z'  |z) a c(z —* z')d(z —* z')A, where A < 0 sets the rate of exponential decay and c is an indicator function enforcing the constraint that the moves do not reach or cross existing segmentation points.6 We can also incorporate declarative linguistic knowledge by biasing the proposal distribution in favor of moves that place boundaries near known cue phrase markers.We multiply the unnormalized chance of proposing a move to location z —* z' by a term equal to one plus the number of candidate cue phrases in the segment-initial sentences in the new configuration z', written num-cue(z').Formally, qling(z'  |z') a (1 + num-cue(z'))q(z'  |z).We use a list of cue phrases identified by Hirschberg and Litman (1993).We evaluate our model with both the basic and linguistically-enhanced proposal distributions.As in section 3.4, we set the priors 00 and 00 using gradient-based search.In this case, we perform gradient-based optimization after epochs of 1000 max-move, where max-move is the maximum move-length, set to 5 in our experiments.These parameters affect the rate of convergence but are unrelated to the underlying probability model.In the limit of enough samples, all nonpathological settings will yield the same segmentation results.Metropolis-Hasting steps.Interleaving samplingbased inference with direct optimization of parameters can be considered a form of Monte Carlo Expectation-Maximization (MCEM; Wei and Tanner, 1990).Corpora We evaluate our approach on corpora from two different domains: transcribed meetings and written text.For multi-speaker meetings, we use the ICSI corpus of meeting transcripts (Janin et al., 2003), which is becoming a standard for speech segmentation (e.g., Galley et al. 2003; Purver et al.2006).This dataset includes transcripts of 75 multi-party meetings, of which 25 are annotated for segment boundaries.For text, we introduce a dataset in which each document is a chapter selected from a medical textbook (Walker et al., 1990).7 The task is to divide each chapter into the sections indicated by the author.This dataset contains 227 chapters, with 1136 sections (an average of 5.00 per chapter).Each chapter contains an average of 140 sentences, giving an average of 28 sentences per segment.Metrics All experiments are evaluated in terms of the commonly-used Pk (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002) scores.Both metrics pass a window through the document, and assess whether the sentences on the edges of the window are properly segmented with respect to each other.WindowDiff is stricter in that it requires that the number of intervening segments between the two sentences be identical in the hypothesized and the reference segmentations, while Pk only asks whether the two sentences are in the same segment or not.Pk and WindowDiff are penalties, so lower values indicate better segmentations.We use the evaluation source code provided by Malioutov and Barzilay (2006).System configuration We evaluate our Bayesian approach both with and without cue phrases.Without cue phrases, we use the dynamic programming inference described in section 3.3.This system is referred to as BAYESSEG in Table 1.When adding cue phrases, we use the Metropolis-Hastings model described in 4.1.Both basic and linguisticallymotivated proposal distributions are evaluated (see Section 4.2); these are referred to as BAYESSEGCUE and BAYESSEG-CUE-PROP in the table.For the sampling-based systems, results are averaged over five runs.The initial configuration is obtained from the dynamic programming inference, and then 100,000 sampling iterations are performed.The final segmentation is obtained by annealing the last 25,000 iterations to a temperature of zero.The use of annealing to obtain a maximum a posteriori (MAP) configuration from sampling-based inference is common (e.g., Finkel 2005; Goldwater 2007).The total running time of our system is on the order of three minutes per document.Due to memory constraints, we divide the textbook dataset into ten parts, and perform inference in each part separately.We may achieve better results by performing inference over the entire dataset simultaneously, due to pooling counts for cue phrases across all documents.Baselines We compare against three competitive alternative systems from the literature: U&I (Utiyama and Isahara, 2001); LCSEG (Galley et al., 2003); MCS (Malioutov and Barzilay, 2006).All three systems are described in the related work (Section 2).In all cases, we use the publicly available executables provided by the authors.Parameter settings For LCSEG, we use the parameter values specified in the paper (Galley et al., 2003).MCS requires parameter settings to be tuned on a development set.Our corpora do not include development sets, so tuning was performed using the lecture transcript corpus described by Malioutov and Barzilay (2006).Our system does not require parameter tuning; priors are re-estimated as described in Sections 3.4 and 4.3.U&I requires no parameter tuning, and is used “out of the box.” In all experiments, we assume that the number of desired segments is provided.Preprocessing Standard preprocessing techniques are applied to the text for all comparisons.The Porter (1980) stemming algorithm is applied to group equivalent lexical items.A set of stop-words is also removed, using the same list originally employed by several competitive systems (Choi, 2000; ter performance.BAYESSEG is the cohesion-only Bayesian system with marginalized language models.BAYESSEG-CUE is the Bayesian system with cue phrases.BAYESSEG-CUE-PROP adds the linguisticallymotivated proposal distribution.Utiyama and Isahara, 2001; Malioutov and Barzilay, 2006).Table 1 presents the performance results for three instantiations of our Bayesian framework and three competitive alternative systems.As shown in the table, the Bayesian models achieve the best results on both metrics for both corpora.On the medical textbook corpus, the Bayesian systems achieve a raw performance gain of 2-3% with respect to all baselines on both metrics.On the ICSI meeting corpus, the Bayesian systems perform 4-5% better than the best baseline on the Pk metric, and achieve smaller improvement on the WindowDiff metric.The results on the meeting corpus also compare favorably with the topic-modeling method of Purver et al. (2006), who report a Pk of .289 and a WindowDiff of .329.Another observation from Table 1 is that the contribution of cue phrases depends on the dataset.Cue phrases improve performance on the meeting corpus, but not on the textbook corpus.The effectiveness of cue phrases as a feature depends on whether the writer or speaker uses them consistently.At the same time, the addition of cue phrases prevents the use of exact inference techniques, which may explain the decline in results for the meetings dataset.To investigate the quality of the cue phrases that our model extracts, we list its top ten cue phrases for each dataset in Table 2.Cue phrases are ranked by their chi-squared value, which is computed based on the number of occurrences for each word at the beginning of a hypothesized segment, as compared to the expectation.For cue phrases listed in bold, the chi-squared value is statistically significant at the level of p < .01, indicating that the frequency with which the cue phrase appears at the beginning of segments is unlikely to be a chance phenomenon.As shown in the left column of the table, our model has identified several strong cue phrases from the meeting dataset which appear to be linguistically plausible.Galley et al. (2003) performed a similar chi-squared analysis, but used the true segment boundaries in the labeled data; this can be thought of as a sort of ground truth.Four of the ten cue phrases identified by our system overlap with their analysis; these are indicated with asterisks.In contrast to our model’s success at extracting cue phrases from the meeting dataset, only very common words are selected for the textbook dataset.This may help to explain why cue phrases improve performance for meeting transcripts, but not for the textbook.This paper presents a novel Bayesian approach to unsupervised topic segmentation.Our algorithm is capable of incorporating both lexical cohesion and cue phrase features in a principled manner, and outperforms state-of-the-art baselines on text and transcribed speech corpora.We have developed exact and sampling-based inference techniques, both of which search only over the space of segmentations and marginalize out the associated language models.Finally, we have shown that our model provides a theoretical framework with connections to information theory, while also generalizing and justifying prior work.In the future, we hope to explore the use of similar Bayesian techniques for hierarchical segmentation, and to incorporate additional features such as prosody and speaker change information.The authors acknowledge the support of the National Science Foundation (CAREER grant IIS0448168) and the Microsoft Research Faculty Fellowship.Thanks to Aaron Adler, S. R. K. Branavan, Harr Chen, Michael Collins, Randall Davis, Dan Roy, David Sontag and the anonymous reviewers for helpful comments and suggestions.We also thank Michel Galley, Igor Malioutov, and Masao Utiyama for making their topic segmentation code publically available.Any opinions, findings, and conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the views of the NSF.
First- and Second-Order Expectation Semirings with Applications to Minimum-Risk Training on Translation ForestsMany statistical translation models can be regarded as weighted logical deduction.Under this paradigm, we use weights from the expectation semiring (Eisner, 2002), to compute first-order statistics (e.g., the expected hypothesis length or feature counts) over packed forests of translations (lattices or hypergraphs).We then introduce novel semiring, which computes second-order statistics (e.g., the variance of the hypothesis length or the gradient of entropy).This second-order semiring is essential for many interesting training paradigms such as minimum risk, deterministic annealing, active learning, and semi-supervised learning, where gradient descent optimization requires computing the gradient of entropy or risk.We use these semirings in an open-source machine translation toolkit, enabling minimum-risk training a benefit of up to 1.0A hypergraph or “packed forest” (Gallo et al., 1993; Klein and Manning, 2004; Huang and Chiang, 2005) is a compact data structure that uses structure-sharing to represent exponentially many trees in polynomial space.A weighted hypergraph also defines a probability or other weight for each tree, and can be used to represent the hypothesis space considered (for a given input) by a monolingual parser or a tree-based translation system, e.g., tree to string (Quirk et al., 2005; Liu et al., 2006), string to tree (Galley et al., 2006), tree to tree (Eisner, 2003), or string to string with latent tree structures (Chiang, 2007).Given a hypergraph, we are often interested in computing some quantities over it using dynamic programming algorithms.For example, we may want to run the Viterbi algorithm to find the most probable derivation tree in the hypergraph, or the k most probable trees.Semiring-weighted logic programming is a general framework to specify these algorithms (Pereira and Warren, 1983; Shieber et al., 1994; Goodman, 1999; Eisner et al., 2005; Lopez, 2009).Goodman (1999) describes many useful semirings (e.g., Viterbi, inside, and Viterbin-best).While most of these semirings are used in “testing” (i.e., decoding), we are mainly interested in the semirings that are useful for “training” (i.e., parameter estimation).The expectation semiring (Eisner, 2002), originally proposed for finite-state machines, is one such “training” semiring, and can be used to compute feature expectations for the Estep of the EM algorithm, or gradients of the likelihood function for gradient descent.In this paper, we apply the expectation semiring (Eisner, 2002) to a hypergraph (or packed forest) rather than just a lattice.We then propose a novel second-order expectation semiring, nicknamed the “variance semiring.” The original first-order expectation semiring allows us to efficiently compute a vector of firstorder statistics (expectations; first derivatives) on the set of paths in a lattice or the set of trees in a hypergraph.The second-order expectation semiring additionally computes a matrix of secondorder statistics (expectations of products; second derivatives (Hessian); derivatives of expectations).We present details on how to compute many interesting quantities over the hypergraph using the expectation and variance semirings.These quantities include expected hypothesis length, feature expectation, entropy, cross-entropy, KullbackLeibler divergence, Bayes risk, variance of hypothesis length, gradient of entropy and Bayes risk, covariance and Hessian matrix, and so on.The variance semiring is essential for many interesting training paradigms such as deterministic annealing (Rose, 1998), minimum risk (Smith and Eisner, 2006), active and semi-supervised learning (Grandvalet and Bengio, 2004; Jiao et al., 2006).In these settings, we must compute the gradient of entropy or risk.The semirings can also be used for second-order gradient optimization algorithms.We implement the expectation and variance semirings in Joshua (Li et al., 2009a), and demonstrate their practical benefit by using minimumrisk training to improve Hiero (Chiang, 2007).We use a specific tree-based system called Hiero (Chiang, 2007) as an example, although the discussion is general for any systems that use a hypergraph to represent the hypothesis space.In Hiero, a synchronous context-free grammar (SCFG) is extracted from automatically wordaligned corpora.An illustrative grammar rule for Chinese-to-English translation is where the Chinese word In, means of, and the alignment, encoded via subscripts on the nonterminals, causes the two phrases around In, to be reordered around of in the translation.Given a source sentence, Hiero uses a CKY parser to generate a hypergraph, encoding many derivation trees along with the translation strings.Formally, a hypergraph is a pair (V, E), where V is a set of nodes (vertices) and E is a set of hyperedges, with each hyperedge connecting a set of antecedent nodes to a single consequent node.1 In parsing parlance, a node corresponds to an item in the chart (which specifies aligned spans of input and output together with a nonterminal label).The root node corresponds to the goal item.A hyperedge represents an SCFG rule that has been “instantiated” at a particular position, so that the nonterminals on the right and left sides have been replaced by particular antecedent and consequent items; this corresponds to storage of backpointers in the chart.We write T(e) to denote the set of antecedent nodes of a hyperedge e. We write I(v) for the hypergraph, a trigram language model is integrated.Rectangles represent items, where each item is identified by the non-terminal symbol, source span, and left- and right-side language model states.An item has one or more incoming hyperedges.A hyperedge consists of a rule, and a pointer to an antecedent item for each non-terminal symbol in the rule. set of incoming hyperedges of node v (i.e., hyperedges of which v is the consequent), which represent different ways of deriving v. Figure 1 shows a simple Hiero-style hypergraph.The hypergraph encodes four different derivation trees that share some of the same items.By exploiting this sharing, a hypergraph can compactly represent exponentially many trees.We observe that any finite-state automaton can also be encoded as a hypergraph (in which every hyperedge is an ordinary edge that connects a single antecedent to a consequent).Thus, the methods of this paper apply directly to the simpler case of hypothesis lattices as well.We assume a hypergraph HG, which compactly encodes many derivation trees d E D. Given HG, we wish to extract the best derivations—or other aggregate properties of the forest of derivations.Semiring parsing (Goodman, 1999) is a general framework to describe such algorithms.To define a particular algorithm, we choose a semiring K and specify a “weight” ke E K for each hyperedge e. The desired aggregate result then emerges as the total weight of all derivations in the hypergraph.For example, to simply count derivations, one can assign every hyperedge weight 1 in the semiring of ordinary integers; then each derivation also has weight 1, and their total weight is the number of derivations.We write K = (K, ®, ®, 0,1) for a semiring with elements K, additive operation ®, multiplicative operation ⊗, additive identity 0, and multiplicative identity 1.The ⊗ operation is used to obtain the weight of each derivation d by multiplying the weights of its component hyperedges e, that is, kd = ®eEd ke.The ⊕ operation is used to sum over all derivations d in the hypergraph to obtain the total weight of the hypergraph HG, which is � �eEd ke.2 Figure 2 shows how to dED compute the total weight of an acyclic hypergraph HG.3 In general, the total weight is a sum over exponentially many derivations d. But Figure 2 sums over these derivations in time only linear on the size of the hypergraph.Its correctness relies on axiomatic properties of the semiring: namely, ⊕ is associative and commutative with identity 0, ⊗ is associative with two-sided identity 1, and ⊗ distributes over ⊕ from both sides.The distributive property is what makes Figure 2 work.The other properties are necessary to ensure that The algorithm in Figure 2 is general and can be applied with any semiring (e.g., Viterbi).Below, we present our novel semirings.We now introduce the computational problems of this paper and the semirings we use to solve them.We are given a function p : D → R>0, which decomposes multiplicatively over component hyperedges e of a derivation d ∈ D: that is, p(d) def = eEd pe.In practice, p(d) will specify a probability distribution over the derivations in the hyper2Eisner (2002) uses closed semirings that are also equipped with a Kleene closure operator *.For example, in the real semiring (Il2, +, x, 0, 1), we define p* = (1 − p)−1 (= 1 + p + p2 + ...) for |p |< 1 and is undefined otherwise.The closure operator enables exact summation over the infinitely many paths in a cyclic FSM, or trees in a hypergraph with non-branching cycles, without the need to iterate around cycles to numerical convergence.For completeness, we specify the closure operator for our semirings, satisfying the axioms k* = 1 ® k ® k* = 1 ® k* ® k, but we do not use it in our experiments since our hypergraphs are acyclic.3We assume that HG has already been built by deductive inference (Shieber et al., 1994).But in practice, the nodes’ inside weights ,3(v) are usually accumulated as the hypergraph is being built, so that pruning heuristics can consult them.4Actually, the notation ®eEd ke assumes that ® is commutative as well, as does the notation “for u E T(e)” in our algorithms; neither specifies a loop order.One could however use a non-commutative semiring by ordering each hyperedge’s antecedents and specifying that a derivation’s weight is the product of the weights of its hyperedges when visited in prefix order.Tables 1–2 will not assume any commutativity. graph.It is often convenient to permit this probability distribution to be unnormalized, i.e., one may have to divide it through by some Z to get a proper distribution that sums to 1.We are also given two functions of interest r, s : D → R, each of which decomposes additively over its component hyperedges e: that is, r(d) def = EeEd re, and s(d) def = EeEd se.We are now interested in computing the following quantities on the hypergraph HG: Note that r/Z, s/Z, and t/Z are expectations under p of r(d), s(d), and r(d)s(d), respectively.More formally, the probabilistic interpretation is that D is a discrete sample space (consisting gorithm is a more efficient alternative to Figure 2 for computing the total weight (k, x) of the hypergraph, especially if the xe are vectors.First, at lines 2–3, the inside and outside algorithms are run using only the ke weights, obtaining only k (without x) but also obtaining all inside and outside weights ,Q, α ∈ K as a side effect.Then the second component x� of the total weight is accumulated in lines 5–11 as a linear combination of all the xe values, namely x� = Ee kexe, where ke is computed at lines 8–10 using α and ,Q weights.The linear coefficient ke is the “exclusive weight” for hyperedge e, meaning that the product keke is the total weight in K of all derivations d ∈ D that include e. of all derivations in the hypergraph), p is a measure over this space, and r, s : D — R are random variables.Then r/Z and s/Z give the expectations of these random variables, and t/Z gives the expectation of their product t = rs, so that t/Z − (r/Z)(s/Z) gives their covariance.Example 1: r(d) is the length of the translation corresponding to derivation d (arranged by setting re to the number of target-side terminal words in the SCFG rule associated with e).Then r/Z is the expected hypothesis length.Example 2: r(d) evaluates the loss of d compared to a reference translation, using some additively decomposable loss function.Then r/Z is the risk (expected loss), which is useful in minimum-risk training.Example 3: r(d) is the number of times that a certain feature fires on d. Then r/Z is the expected feature count, which is useful in maximum-likelihood training.We will generalize later in Section 4 to allow r(d) to be a vector of features.Example 4: Suppose r(d) and s(d) are identical and both compute hypothesis length.Then the second-order statistic t/Z is the second moment of the length distribution, so the variance of hypothesis length can be found as t/Z − (r/Z)2.We will use the semiring parsing framework to compute the quantities (1)–(4).Although each is a sum over exponentially many derivations, we will compute it in O(JHGJ) time using Figure 2.In the simplest case, let K = (R, +, x, 0, 1), and define ke = pe for each hyperedge e. Then the algorithm of Figure 2 reduces to the classical inside algorithm (Baker, 1979) and computes Z.Next suppose K is the expectation semiring (Eisner, 2002), shown in Table 1.Define ke = (pe, pere).Then Figure 2 will return (Z, r).Finally, suppose K is our novel second-order expectation semiring, which we introduce in Table 2.Define ke = (pe, pere, pese, perese).Then the algorithm of Figure 2 returns (Z, r, s, t).Note that, to compute t, one cannot simply construct a first-order expectation semiring by defining t(d) def = r(d)s(d) because t(d), unlike r(d) and s(d), is not additively decomposable over the hyperedges in d.5 Also, when r(d) and s(d) are identical, the second-order expectation semiring allows us to compute variance as t/Z − (r/Z)2, which is why we may call our second-order expectation semiring the variance semiring.To prove our claim about the first-order expectation semiring, we first observe that the definitions in Table 1 satisfy the semiring axioms.The reader can easily check these axioms (as well as the closure axioms in footnote 2).With a valid semiring, we then simply observe that Figure 2 returns the total weight ®dED ®eEd ke = ®dED (p(d), p(d)r(d)) = (Z, r).It is easy to verify the second equality from the definitions of ®, Z, and r. The first equality requires proving that ®eEd ke = (p(d), p(d)r(d)) from the definitions of ®, ke, p(d), and r(d).The main intuition is that ® can be used to build up (p(d), p(d)r(d)) inductively from the ke: if d decomposes into two disjoint subderivations d1, d2, then (p(d), p(d)r(d)) = (p(d1)p(d2),p(d1)p(d2)(r(d1) + r(d2))) = (p(d1),p(d1)r(d1)) ® (p(d2),p(d2)r(d2)).The base cases are where d is a single hyperedge e, in which case (p(d), p(d)r(d)) = ke (thanks to our choice of ke), and where d is empty, in which case 5However, in a more tricky way, the second-order expectation semiring can be constructed using the first-order expectation semiring, as will be seen in Section 4.3. ing is a pair (p, r).The second and third rows define the operations between two elements (p1, r1) and (p2, r2), and the last two rows define the identities.Note that the multiplicative identity 1 has an r component of 0.(= saeea) is stored as a pair (sa, `a) where sa and `a are the sign bit of a and natural logarithm of |a|, respectively.This table shows the operations between two values a = sa2ea and b = sb2eb, assuming `a ≥ `b.Note: log(1 + x) (where The proof for the second-order expectation semiring is similar.In particular, one mainly needs to show that ®e∈d ke = (p(d), p(d)r(d), p(d)s(d), p(d)r(d)s(d)).In Tables 1–2, we do not discuss how to store p, r, s, and t. If p is a probability, it often suffers from the underflow problem. r, s, and t may suffer from both underflow and overflow problems, depending on their scales.To address these, we could represent p in the log domain as usual.However, r, s, and t can be positive or negative, and we cannot directly take the log of a negative number.Therefore, we represent real numbers as ordered pairs.Specifically, to represent a = sae`a, we store (sa, fa), where the sa ∈ {+, −} is the sign bit of a and the floatingpoint number fa is the natural logarithm of |a|.6 Table 3 shows the “·” and “+”operations.6An alternative that avoids log and exp is to store a = fa2ea as (fa, ea), where fa is a floating-point number and ea is a sufficiently wide integer.E.g., combining a 32-bit fa with a 32-bit ea will in effect extend fa’s 8-bit internal exponent to 32 bits by adding ea to it.This gives much more dynamic range than the 11-bit exponent of a 64-bit doubleprecision floating-point number, if vastly less than in Table 3.In this section, we generalize beyond the above case where p, r, s are R-valued.In general, p may be an element of some other semiring, and r and s may be vectors or other algebraic objects.When r and s are vectors, especially highdimensional vectors, the basic “inside algorithm” of Figure 2 will be slow.We will show how to speed it up with an “inside-outside algorithm.” In general, for P, R, 5, T, we can define the first-order expectation semiring EP,R = (P × R, ⊕, ⊗, 0,1) and the second-order expectation semiring EP,R,S,T = (P × R × 5 × T, ⊕, ⊗, 0, 1), using the definitions from Tables 1–2.But do those definitions remain meaningful, and do they continue to satisfy the semiring axioms?Indeed they do when P = R, R = Rn, 5 = Rm, T = Rn×m, with rs defined as the outer product rsT (a matrix) where sT is the transpose of s. In this way, the second-order semiring EP,R,S,T lets us take expectations of vectors and outer products of vectors.So we can find means and covariances of any number of linearly decomposable quantities (e.g., feature counts) defined on the hypergraph.We will consider some other choices in Sections 4.3–4.4 below.Thus, for generality, we conclude this section by stating the precise technical conditions needed to construct EP,R and EP,R,S,T: As a matter of notation, note that above and in Tables 1–2, we overload “+” to denote any of the addition operations within P, R, 5, T; overload “0” to denote their respective additive identities; and overload concatenation to denote any of the multiplication operations within or between P, R, S, T. “1” refers to the multiplicative identity of P. We continue to use distinguished symbols ®, ®, 0,1 for the operations and identities in our “main semiring of interest,” EP,R or EP,R,S,T .To compute equations (1)–(4) in this more general setting, we must still require multiplicative or additive decomposability, defining p(d) def = HeEd pe, r(d) def EeEd re, s(d) def EeEd se as before.But the H and E operators here now denote appropriate operations within P, R, and S respectively (rather than the usual operations within R).Under the first-order expectation semiring ER,R-, the inside algorithm of Figure 2 will return (Z, r) where r is a vector of n feature expectations.However, Eisner (2002, section 5) observes that this is inefficient when n is large.Why?The inside algorithm takes the trouble to compute an inside weight β(v) E R x Rn for each node v in the hypergraph (or lattice).The second component of β(v) is a presumably dense vector of all features that fire in all subderivations rooted at node v. Moreover, as β(v) is computed in lines 3–8, that vector is built up (via the ® and ® operations of Table 1) as a linear combination of other dense vectors (the second components of the various β(u)).These vector operations can be slow.A much more efficient approach (usually) is the traditional inside-outside algorithm (Baker, 1979).7 Figure 4 generalizes the inside-outside algorithm to work with any expectation semiring EK,X.8 We are given a hypergraph HG whose edges have weights (ke, xe) in this semiring (so now ke E K denotes only part of the edge weight, not all of it).INSIDE-OUTSIDE(HG, K, X) finds ®dED ®eEd (ke, xe), which has the form ( ˆk, ˆx).But, INSIDE(HG, EK,X) could accomplish the same thing.So what makes the inside-outside algorithm more efficient?It turns out that xˆ can be found quickly as a single linear combination Ee kexe of just the feature vectors xe that appear on individual hyperedges—typically a sum of very sparse vectors!And the linear coefficients ke, as well as ˆk, are computed entirely within the cheap semiring K. They are based on β and α values obtained by first running INSIDE(HG, K) and OUTSIDE(HG, K), which use only the ke part of the weights and ignore the more expensive xe.It is noteworthy that the expectation semiring is not used at all by Figure 4.Although the return value (ˆk, ˆx) is in the expectation semiring, it is built up not by ® and ® but rather by computing kˆ and xˆ separately.One might therefore wonder why the expectation semiring and its operations are still needed.One reason is that the input to Figure 4 consists of hyperedge weights (ke, xe) in the expectation semiring—and these weights may well have been constructed using ® and ®.For example, Eisner (2002) uses finite-state operations such as composition, which do combine weights entirely within the expectation semiring before their result is passed to the forward-backward algorithm.A second reason is that when we work with a second-order expectation semiring in Section 4.4 below, the ˆk, β, and α values in Figure 4 will turn out to be elements of a first-order expectation semiring, and they must still be constructed by first-order ® and ®, via calls to Figures 2–3.Why does inside-outside work?Whereas the inside algorithm computes ®dED ®eEd in any semiring, the inside-outside algorithm exploits the special structure of an expectation semiring.By that semiring’s definitions of ® and � ( PdED QeEd ke, PdED PeEd(Qe0Ed,e0�e ke0)xe).The first component (giving ˆk) is found by calling the inside algorithm on just the ke part of the weights.The second component (giving ˆx) can be rearranged into Pe Pd: eEd(Q7e''Ed,e0�e ke0)xe = Pe kexe, where ke def Pd: eEd (l le0Ed,e0�e ke0) is found from Q, α.The application described at the start of this subsection is the classical inside-outside algorithm.Here (ke,xe) def = (pe, pere), and the algorithm returns (ˆk, ˆx) = (Z, r).In fact, that We now observe that the second-order expectation semiring EP,R,S,T can be obtained indirectly by nesting one first-order expectation semiring inside another!First “lift” P to obtain the first-order expectation semiring K def = EP,R.Then lift this a second time to obtain the “nested” first-order expectation semiring EK,X = E(EP,R),(SxT), where we equip Xdef = 5 x T with the operations (s1, t1) + (s2, t2) def = (s1 + s2, t1 + t2) and (p, r)(s, t) def = (ps, pt + rs).The resulting first-order expectation semiring has elements of the form ((p, r), (s, t)).Table 4 shows that it is indeed isomorphic to EP,R,S,T, with corresponding elements (p, r, s, t).This construction of the second-order semiring as a first-order semiring is a useful bit of abstract algebra, because it means that known properties of first-order semirings will also apply to secondorder ones.First of all, we are immediately guaranteed that the second-order semiring satisfies the semiring axioms.Second, we can directly apply the inside-outside algorithm there, as we now see.Given a hypergraph weighted by a second-order expectation semiring EP,R,S,T.By recasting this as the first-order expectation semiring EK,X where K = EP,R and X = (5 x T), we can again apply INSIDE-OUTSIDE(HG, K, X) to find the total weight of all derivations.For example, to speed up Section 3.2, we may define (ke, xe) = ((pe,pere), (pese,perese)) for each hyperedge e. Then the inside-outside algorithm of Figure 4 will compute (ˆk, ˆx) = ((Z, r), (s, t)), more quickly than the inside algorithm of Figure 2 computed (Z, r, s, t).Figure 4 in this case will run the inside and outside algorithms in the semiring EP,R, so that ke, ˆk, α, Q, and ke will now be elements of P x R (not just elements of P as in the first-order case).Finally it finds xˆ = Pe kexe, where xe E 5 x T.9 This is a particularly effective speedup over the inside algorithm when R consists of scalars (or small vectors) whereas 5, T are sparse highdimensional vectors.We will see exactly this case in our experiments, where our weights (p, r, s, t) denote (probability, risk, gradient of probability, gradient of risk), or (probability, entropy, gradient of probability, gradient of entropy).In Sections 3.2 and 4.1, we saw how our semirings helped find the sum Z of all p(d), and compute expectations r, s, t of r(d), s(d), and r(d)s(d).It turns out that these semirings can also compute first- and second-order partial derivatives of all the above results, with respect to a parameter vector B E Rm.That is, we ask how they are affected when B changes slightly from its current value.The elementary values pe, re, se are now assumed to implicitly be functions of B.Case 1: Recall that Z def = Pd p(d) is computed by INSIDE(HG, R) if each hyperedge e has weight pe.“Lift” this weight to (pe, Vpe), where Vpe E Rm is a gradient vector.Now (Z, VZ) will be returned by INSIDE(HG, ER,Rm)— or, more efficiently, by INSIDE-OUTSIDE(HG, R, Rm).Case 2: To differentiate a second time, “lift” the above weights again to obtain ((pe, Vpe), V(pe, Vpe)) ((pe,Vpe), (Vpe, V2pe)), where V2pe E Rmxm is the Hessian matrix of second-order mixed partial derivatives.These weights are in a second-order expectation semiring.10 Now 9Figure 4 was already proved generally correct in Section 4.2.To understand more specifically how (s, t) gets computed, observe in analogy to the end of Section 4.2 that 10Modulo the trivial isomorphism from ((p, r), (s, t)) to (p, r, s, t) (see Section 4.3), the intended semiring both here and in Case 3 is the one that was defined at the start of Section 4.1, in which r, s are vectors and their product is defined = (Z, VZ, VZ, V2Z) will be returned by INSIDE(HG, ER,R—,R—,R—x—), or more efficiently by INSIDE-OUTSIDE(HG, ER,R—, Rm x Rm×m).Case 3: Our experiments will need to find expectations and their partial derivatives.Recall that (Z, r) is computed by INSIDE(HG, ER,Rn) when the edge weights are (pe, pere) with re E Rn.Lift these weights to ((pe,pere),V(pe,pere)) = swapping the second and third components of the 4-tuple and transposing the matrix in the fourth component.Algebraically, this changes nothing because and are isomorphic, thanks to symmetries in Table 2.This method computes the expectation of the gradient rather than the gradient of the expectation—they are equal. relied on the fact that this relationship still holds even when the scalars Z, are replaced by more complex objects that we wish to differentiate.Our discussion below sticks to the scalar case for simplicity, but would generalize fairly straightforwardly. seem wonderful and mysterious.We now show in two distinct ways why this follows from our setup of Section 3.1.At the end, we derive as a special case the well-known relationship between gradients and expectations in log-linear models.From Expectations to Gradients One perspective is that our fundamentally finds expectations.Thus, we must be finding VZ by formulating it as a certain expectation r. Specifto be rsT, a matrix.However, when using this semiring to compute second derivatives (Case 2) or covariances, one may exploit the invariant that r = s, e.g., to avoid storing s and to pere Vpe pere. def where pe =exp(re· models, that V log Z = (VZ)/Z = ¯r/Z, the vector of feature expectations (Lau et al., 1993).Given a hypergraph HG whose hyperedges e are annotated with values pe.Recall from Section 3.1 that this defines a probability distribution over all derivations d in the hypergraph, namely p(d)/Z where p(d) def = 11eEd pe.In Section 3, we show how to compute the expected hypothesis length or expected feature counts, using the algorithm of Figure 2 with a first-order expectation semiring ER,R.In general, given hyperedge weights (pe, pere), the algorithm computes (Z, r) and thus r/Z, the expectation of r(d) def = EeEd re.We now show how to compute a few other quantities by choosing re appropriately.Entropy on a Hypergraph The entropy of the distribution of derivations in a hypergraph14 is where the first term Zq can be computed using the inside algorithm with hyperedge weights qe, and the numerator and denominator of the second term using an expectation semiring with hydef peredge weights (pe, pere) with re = log qe.The KL divergence to p from q can be computed as KL(p II q) = H(p, q) − H(p).Expected Loss (Risk) Given a reference sentence y*, the expected loss (i.e., Bayes risk) of the hypotheses in the hypergraph is defined as, where Y(d) is the target yield of d and L(y, y*) is the loss of the hypothesis y with respect to the reference y*.The popular machine translation metric, BLEU (Papineni et al., 2001), is not additively decomposable, and thus we are not able to compute the expected loss for it.Tromble et al. (2008) develop the following loss function, of which a linear approximation to BLEU is a special case, provided that we define re = log pe (so that r(d) = EeEd re = log p(d)).Of course, we can compute (Z, r) as explained in Section 3.2.Cross-Entropy and KL Divergence We may be interested in computing the cross-entropy or KL divergence between two distributions p and q.For example, in variational decoding for machine translation (Li et al., 2009b), p is a distribution represented by a hypergraph, while q, represented by a finite state automaton, is an approximation to p. The cross entropy between p and q is defined as 14Unfortunately, it is intractable to compute the entropy of the distribution over strings (each string’s probability is a sum over several derivations).But Li et al. (2009b, section 5.4) do estimate the gap between derivational and string entropies. where w is an n-gram type, N is a set of n-gram types with n E [1, 4], #w(y) is the number of occurrence of the n-gram w in y, δw(y*) is an indicator to check if y* contains at least one occurrence of w, and θn is the weight indicating the relative importance of an n-gram match.If the hypergraph is already annotated with n-gram (n > 4) language model states, this loss function is additively def decomposable.Using re = Le where Le is the loss for a hyperedge e, we compute the expected loss, With second-order expectation semirings, we can compute from a hypergraph the expectation and variance of hypothesis length; the feature expectation vector and covariance matrix; the Hessian (matrix of second derivatives) of Z; and the gradients of entropy and expected loss.The computations should be clear from earlier discussion.Below we compute gradient of entropy or Bayes risk.Gradient of Entropy or Risk It is easy to see that the gradient of entropy (5) is We may compute (Z, r, VZ, Vr) as explained in Case 3 of Section 5 by using defdef ke = (pe,pere,Vpe, (Vpe)re + peVre) = (pe,pe log pe, Vpe, (1 + log pe)Vpe), where Vpe depends on the particular parameterization of the model (see Section 7.1 for an example).Similarly, the gradient of risk of (9) is We may compute (Z, r, VZ, Vr) using ke = (pe, peLe, Vpe, LeVpe).We now show how we improve the training of a Hiero MT model by optimizing an objective function that includes entropy and risk.Our objective function could be computed with a first-order expectation semiring, but computing it along with its gradient requires a second-order one.We assume a globally normalized linear model for its simplicity.Each derivation d is scored by where 4b(d) E RI is a vector of features of d. We then define the unnormalized distribution p(d) as where the scale factor γ adjusts how sharply the distribution favors the highest-scoring hypotheses.Adjusting θ or γ changes the distribution p. Minimum error rate training (MERT) (Och, 2003) tries to tune θ to minimize the BLEU loss of a decoder that chooses the most probable output according to p. (γ has no effect.)MERT’s specialized linesearch addresses the problem that this objective function is piecewise constant, but it does not scale to a large number of parameters.Smith and Eisner (2006) instead propose a differentiable objective that can be optimized by gradient descent: the Bayes risk R(p) of (7).This is the expected loss if one were (hypothetically) to use a randomized decoder, which chooses a hypothesis d in proportion to its probability p(d).If entropy H(p) is large (e.g., small γ), the Bayes risk is smooth and has few local minima.Thus, Smith and Eisner (2006) try to avoid local minima by starting with large H(p) and decreasing it gradually during optimization.This is called deterministic annealing (Rose, 1998).As H(p) —* 0 (e.g., large γ), the Bayes risk does approach the MERT objective (i.e. minimizing 1-best error).The objective is minimize R(p) − T · H(p) (14) where the “temperature” T starts high and is explicitly decreased as optimization proceeds.Solving (14) for a given T requires computing the entropy H(p) and risk R(p) and their gradients with respect to θ and γ. Smith and Eisner (2006) followed MERT in constraining their decoder to only an n-best list, so for them, computing these quantities did not involve dynamic programming.We compare those methods to training on a hypergraph containing exponentially many hypotheses.In this condition, we need our new secondorder semiring methods and must also approximate BLEU (during training only) by an additively decomposable loss (Tromble et al., 2008).15 Our algorithms require that p(d) of (13) is multiplicatively decomposable.It suffices to define 4b(d) def = Ee∈d 4be, so that all features are local to individual hyperedges; the vector 4be indicates which features fire on hyperedge e. Then score(d) of (12) is additively decomposable: We can then set pe = exp(γ · scoree), and Vpe = γpe4b(e), and use the algorithms described in Section 6 to compute H(p) and R(p) and their gradients with respect to θ and γ.16 15Pauls et al. (2009) concurrently developed a method to maximize the expected n-gram counts on a hypergraph using gradient descent.Their objective is similar to the minimum risk objective (though without annealing), and their gradient descent optimization involves in algorithms in computing expected feature/n-gram counts as well as expected products of features and n-gram counts, which can be viewed as instances of our general algorithms with first- and second-order semirings.They focused on tuning only a small number (i.e. nine) of features as in a regular MERT setting, while our experiments involve both a small and a large number of features.16It is easy to verify that the gradient of a function f (e.g. entropy or risk) with respect to γ can be written as a weighted sum of gradients with respect to the feature weights θi, i.e.We built a translation model on a corpus for IWSLT 2005 Chinese-to-English translation task (Eck and Hori, 2005), which consists of 40k pairs of sentences.We used a 5-gram language model with modified Kneser-Ney smoothing, trained on the bitext’s English using SRILM (Stolcke, 2002).We first investigate how minimum-risk training (MR), with and without deterministic annealing (DA), performs compared to regular MERT.MR without DA just fixes T = 0 and γ = 1 in (14).All MR or MR+DA uses an approximated BLEU (Tromble et al., 2008) (for training only), while MERT uses the exact corpus BLEU in training.The first five rows in Table 5 present the results by tuning the weights offive features (θ ∈ R5).We observe that MR or MR+DA performs worse than MERT on the dev set.This may be mainly because MR or MR+DA uses an approximated BLEU while MERT doesn’t.On the test set, MR or MR+DA on an n-best list is comparable to MERT.But our new approach, MR or MR+DA on a hypergraph, does consistently better (statistically significant) than MERT, despite approximating BLEU.17 Did DA help?For both n-best and hypergraph, MR+DA did obtain a better BLEU score than plain MR on the dev set.18 This shows that DA helps with the local minimum problem, as hoped.However, DA’s improvement on the dev set did not transfer to the test set.MR (with or without DA) is scalable to tune a large number of features, while MERT is not.To achieve competitive performance, we adopt a forest reranking approach (Li and Khudanpur, 2009; Huang, 2008).Specifically, our training has two stages.In the first stage, we train a baseline system as usual.We also find the optimal feature weights for the five features mentioned before, using the method of MR+DA operating on a hypergraph.In the second stage, we generate a hypergraph for each sentence in the training data (which consists of about 40k sentence pairs), using the baseline training scenarios.In the “small” model, five features (i.e., one for the language model, three for the translation model, and one for word penalty) are tuned.In the “large” model, 21k additional unigram and bigram features are used. system.In this stage, we add 21k additional unigram and bigram target-side language model features (cf.Li and Khudanpur (2008)).For example, a specific bigram “the cat” can be a feature.Note that the total score by the baseline system is also a feature in the second-stage model.With these features and the 40k hypergraphs, we run the MR training to obtain the optimal weights.During test time, a similar procedure is followed.For a given test sentence, the baseline system first generates a hypergraph, and then the hypergraph is reranked by the second-stage model.The last row in Table 5 reports the BLEU scores.Clearly, adding more features improves (statistically significant) the case with only five features.We plan to incorporate more informative features described by Chiang et al. (2009).19We presented first-order expectation semirings and inside-outside computation in more detail than (Eisner, 2002), and developed extensions to higher-order expectation semirings.This enables efficient computation of many interesting quantities over the exponentially many derivations encoded in a hypergraph: second derivatives (Hessians), expectations of products (covariances), and expectations such as risk and entropy along with their derivatives.To our knowledge, algorithms for these problems have not been presented before.Our approach is theoretically elegant, like other work in this vein (Goodman, 1999; Lopez, 2009; Gimpel and Smith, 2009).We used it practically to enable a new form of minimum-risk training that improved Chinese-English MT by 1.0 BLEU point.Our implementation will be released within the open-source MT toolkit Joshua (Li et al., 2009a).
Divide and Conquer: Crowdsourcing the Creation of Cross-Lingual Textual Entailment CorporaWe address the creation of cross-lingual textual entailment corpora by means of crowdsourcing.Our goal is to define a cheap and replicable data collection methodology that minimizes the manual work done by expert annotators, without resorting to preprocessing tools or already annotated monolingual datasets.In line with recent works emphasizing the need of large-scale annotation efforts for textual entailment, our work aims to: the scarcity of data available to train evaluate systems, and the recourse to crowdsourcing as an effective way to reduce the costs of data collection without sacrificing quality.We show that a complex data creation task, for which even experts usually feature low agreement scores, can be effectively decomposed into simple subtasks assigned to non-expert annotators.The resulting dataset, obtained from a pipeline of different jobs routed to Amazon Mechanical Turk, contains more than 1,600 aligned pairs for each combination of texts-hypotheses in English, Italian and German.Cross-lingual Textual Entailment (CLTE) has been recently proposed by (Mehdad et al., 2010; Mehdad et al., 2011) as an extension of Textual Entailment (Dagan and Glickman, 2004).The task consists of deciding, given a text (T) and an hypothesis (H) in different languages, if the meaning of H can be inferred from the meaning of T. As in other NLP applications, both for monolingual and cross-lingual TE,the availability of large quantities of annotated data is an enabling factor for systems development and evaluation.Until now, however, the scarcity of such data on the one hand, and the costs of creating new datasets of reasonable size on the other, have represented a bottleneck for a steady advancement of the state of the art.In the last few years, monolingual TE corpora for English and other European languages have been created and distributed in the framework of several evaluation campaigns, including the RTE Challenge1, the Answer Validation Exercise at CLEF2, and the Textual Entailment task at EVALITA3.Despite the differences in the design of the tasks, all the released datasets were collected through similar procedures, always involving expensive manual work done by expert annotators.Moreover, in the data creation process, large amounts of hand-crafted T-H pairs often have to be discarded in order to retain only those featuring full agreement, in terms of the assigned entailment judgements, among multiple annotators.The amount of discarded pairs is usually high, contributing to increase the costs of creating textual entailment datasets4.The issues related to the shortage of datasets and the high costs for their creation are more evident in the CLTE scenario, where: i) the only dataset currently available is an English-Spanish corpus obtained by translating the RTE-3 corpus (Negri and Mehdad, 2010), and ii) the application of the standard methods adopted to build RTE pairs requires proficiency in multiple languages, thus significantly increasing the costs of the data creation process.To address these issues, in this paper we devise a cost-effective methodology to create cross-lingual textual entailment corpora.In particular, we focus on the following problems: (1) Is it possible to collect T-H pairs minimizing the intervention of expert annotators?To address this question, we explore the feasibility of crowdsourcing the corpus creation process.As a contribution beyond the few works on TE/CLTE data acquisition, we define an effective methodology that: i) does not involve experts in the most complex (and costly) stages of the process, ii) does not require preprocessing tools, and iii) does not rely on the availability of already annotated RTE corpora. to non-experts, difficult to accomplish, and not suitable for the application of the quality-check mechanisms provided by current crowdsourcing services.Our “divide and conquer” solution represents the first attempt to address a complex task involving content generation and labelling through the definition of a cheap and reliable pipeline of simple tasks which are easy to define, accomplish, and control. guages.Moreover, since the core monolingual tasks of the process are carried out by manipulating English texts, we are able to address the very large community of English speaking workers, with a considerable reduction of costs and execution time.Finally, as a by-product of our method, the acquired pairs are fully aligned for all language combinations, thus enabling meaningful comparisons between scenarios of different complexity (monolingual TE, and CLTE between close or distant languages).We believe that, in the same spirit of recent works promoting large-scale annotation efforts around entailment corpora (Sammons et al., 2010; Bentivogli et al., 2010), the proposed approach and the resulting dataset5 will contribute to meeting the strong need for resources to develop and evaluate novel solutions for textual entailment.Crowdsourcing services, such as Amazon Mechanical Turk6 (MTurk) and CrowdFlower7, have been recently used with success for a variety of NLP applications (Callison-Burch and Dredze, 2010).The idea is that the acquisition and annotation of large amounts of data needed to train and evaluate NLP tools can be carried out in a cost-effective manner by defining simple Human Intelligence Tasks (HITs) routed to a crowd of non-expert workers (aka “Turkers”) hired through on-line marketplaces.As regards textual entailment, the first work exploring the use of crowdsourcing services for data annotation is described in (Snow et al., 2008), which shows high agreement between non-expert annotations of the RTE-1 dataset and existing gold standard labels assigned by expert labellers.Focusing on the actual generation of monolingual entailment pairs, (Wang and Callison-Burch, 2010) experiments the use of MTurk to collect facts and counter facts related to texts extracted from an existing RTE corpus annotated with named entities.Taking a step beyond the task of annotating existing datasets, and showing the feasibility of involving non-experts also in the generation of TE pairs, this approach is more relevant to our objectives.However, at least two major differences with our work have to be remarked.First, they still use available RTE data to obtain a monolingual TE corpus, whereas we pursue the more ambitious goal of generating from scratch aligned CLTE corpora for different language combinations.To this aim, we do not resort to already annotated data, nor languagespecific preprocessing tools.Second, their approach involves qualitative analysis of the collected data only a posteriori, after manual removal of invalid and trivial generated hypotheses.In contrast, our approach integrates quality control mechanisms at all stages of the data collection/annotation process, thus minimizing the recourse to experts to check the quality of the collected material.Related research in the CLTE direction is reported in (Negri and Mehdad, 2010), which describes the creation of an English-Spanish corpus obtained from the RTE-3 dataset by translating the English hypotheses into Spanish.Translations have been crowdsourced adopting a methodology based on translation-validation cycles, defined as separate HITs.Although simplifying the CLTE corpus creation problem, which is recast as the task of translating already available annotated data, this solution is relevant to our work for the idea of combining gold standard units and “validation HITS” as a way to control the quality of the collected data at runtime.The design of data acquisition HITs has to take into account several factors, each having a considerable impact on the difficulty of instructing the workers, the quality and quantity of the collected data, the time and overall costs of the acquisition.A major distinction has to be made between jobs requiring data annotation, and those involving content generation.In the former case, Turkers are presented with the task of labelling input data referring to a fixed set of possible values (e.g. making a choice between multiple alternatives, assigning numerical scores to rank the given data).In the latter case, Turkers are faced with creative tasks consisting in the production of textual material (e.g. writing a correct translation, or a summary of a given text).The ease of controlling the quality of the acquired data depends on the nature of the job.For annotation jobs, quality control mechanisms can be easily set up by calculating Turkers’ agreement, by applying voting schemes, or by adding hidden gold units to the data to be annotated8.In contrast, the quality of the results of content generation jobs is harder to assess, due to the fact that multiple valid results are acceptable (e.g. the same content can be expressed, translated, or summarized in different ways).In such situations the standard quality control mechanisms are not directly applicable, and the detection of errors requires either costly manual verification at the end of the acquisition process, or more complex and creative solutions integrating HITs for quality check.Most of the approaches to content generation proposed so far rely on post hoc verification to filter out undesired low-quality data (Mrozinski et al., 2008; Mihalcea and Strapparava, 2009; Wang and Callison-Burch, 2010).The few solutions integrating validation HITs address the translation of single sentences, a task that is substantially different from ours (Negri and Mehdad, 2010; Bloodgood and Callison-Burch, 2010).Compared to sentence translation, the task of creating CLTE pairs is both harder to explain without recurring to notions that are difficult to understand to non-experts (e.g.“semantic equivalence”, “unidirectional entailment”), and harder to execute without mastering these notions.To tackle these issues the “divide and conquer” approach described in the next section consists in the decomposition of a difficult content generation job into easier subtasks that are: i) self-contained and easy to explain, ii) easy to execute without any NLP expertise, and iii) suitable for the integration of a variety of runtime control mechanisms (regional qualifications, gold units, “validation HITs”) able to ensure a good quality of the collected material.8Both MTurk and CrowdFlower provide means to check workers’ reliability, and weed out untrusted ones without money waste.These include different types of qualification mechanisms, the possibility of giving work only to known trusted Turkers (only with MTurk), and the possibility of adding hidden gold standard units in the data to be annotated (offered as a built-in mechanism only by CrowdFlower). tions (e.g.L2/L2, L2/L3).The execution of the two “multilingual” stages is not strictly necessary but depends on: i) the availability of parallel sentences to start the process, and ii) the actual objectives in terms of language combinations to be covered10.As regards the first stage, in this work we started from a set of 467 English/Italian/German aligned sentences extracted from parallel documents downloaded from the Cafebabel European Magazine11.Concerning the second multilingual stage, we performed only one round of translations from English to Italian to extend the 3 combinations obtained without translations (ENG/ENG, ENG/ITA, and ENG/GER) with the new language combinations ITA/ITA, ITA/ENG, and ITA/GER.Our approach builds on a pipeline of HITs routed to MTurk’s workforce through the CrowdFlower interface.The objective is to collect aligned T-H pairs for different language combinations, reproducing an RTE-like annotation style.However, our annotation is not limited to the standard RTE framework, where only unidirectional entailment from T to H is considered.As a useful extension, we annotate any possible entailment relation between the two text fragments, including: i) bidirectional entailment (i.e. semantic equivalence between T and H), ii) unidirectional entailment from T to H, and iii) unidirectional entailment from H to T. The resulting pairs can be easily used to generate not only standard RTE datasets9, but also general-purpose collections featuring multi-directional entailment relations.We collect large amounts of CLTE pairs carrying out the most difficult part of the process (the creation of entailment-annotated pairs) at a monolingual level.Starting from a set of parallel sentences in n languages, (e.g.L1, L2, L3), n entailment corpora are created: one monolingual (L1/L1), and n-1 crosslingual (L1/L2, and L1/L3).The monolingual corpus is obtained by modifying the sentences only in one language (L1).Original and modified sentences are then paired and annotated to form an entailment dataset for L1.The CLTE corpora are obtained by combining the modified sentences in L1 with the original sentences in L2 and L3, and projecting to the multilingual pairs the annotations assigned to the monolingual pairs.In principle, only two stages of the process require crowdsourcing multilingual tasks, but do not concern entailment annotations.The first one, at the beginning of the process, aims to obtain a set of parallel sentences to start with, and can be done in different ways (e.g. crowdsourcing the translation of a set of sentences).The second one, at the end of the process, consists of translating the modified L1 sentences into other languages (e.g.L2) in order to extend the corpus to cover new language combina9With the positive examples drawn from bidirectional and unidirectional entailments from T to H, and the negative ones drawn from unidirectional entailments from H to T. The main steps of our corpus creation process, depicted in Figure 1, can be summarized as follows: Step1: Sentence modification.The original English sentences (ENG) are modified through (monolingual) generation HITs asking Turkers to: i) preserve the meaning of the original sentences using different surface forms, or ii) slightly change their meaning by adding or removing content.Our assumption, in line with (Bos et al., 2009), is that 10Starting from parallel sentences in n languages, the n corpora obtained without recurring to translations can be augmented, by means of translation HITs, to create the full set of language combinations.Each round of translation adds 1 monolingual corpus, and n-1 CLTE corpora. another way to think about entailment is to consider whether one text T1 adds new information to the content of another text T: if so, then T is entailed by T1.The result of this phase is a set of texts (ENG1) that can be of three types: Step2: TE Annotation.Entailment pairs composed of the original sentences (ENG) and the modified ones (ENG1) are used as input of (monolingual) annotation HITs asking Turkers to decide which of the two texts contains more information.As a result, each ENG/ENG1 pair is annotated as an example of uni-/bidirectional entailment, and stored in the monolingual English corpus.Since the original ENG texts are aligned with the ITA and GER texts, the entailment annotations of ENG/ENG1 pairs can be projected to the other language pairs and the ITA/ENG1 and GER/ENG1 pairs are stored in the CLTE corpus.The possibility of projecting TE annotations is based on the assumption that the semantic information is mostly preserved during the translation process.This particularly holds at the denotative level (i.e. regarding the truth values of the sentence) which is crucial to semantic inference.At other levels (e.g. lexical) there might be slight semantic variations which, however, are very unlikely to play a crucial role in determining entailment relations.Step3: Translation.The modified sentences (ENG1) are translated into Italian (ITA1) through (multilingual) generation HITs reproducing the approach described in (Negri and Mehdad, 2010).As a result, three new datasets are produced by automatically projecting annotations: the monolingual ITA/ITA1, and the cross-lingual ENG/ITA1 and GER/ITA1.Since the solution adopted for sentence translation does not present novelty factors, the remainder of this paper will omit further details on it.Instead, the following sections will focus on the more challenging tasks of sentence modification and TE annotation.Sentence modification and TE annotation have been decomposed into a pipeline of simpler monolingual English sub-tasks.Such pipeline, depicted in Figure 2, involves several types of generation/annotation HITs designed to be easily understandable to nonexperts.Each HIT consists of: i) a set of instructions for a specific task (e.g. paraphrasing a text), ii) the data to be manipulated (e.g. an English sentence), and iii) a test to check workers’ reliability.To cope with the quality control issues discussed in Section 3, such tests are realized using gold standard units, either hidden in the data to be annotated (annotation HITs) or defined as test questions that workers must correctly answer (generation HITs).Moreover, regional qualifications are applied to all HITs.As a further quality check, all the annotation HITs consider Turkers’ agreement as a way to filter out low quality results (only annotations featuring agreement among 4 out of 5 workers are retained).The six HITs defined for each subtask can be described as follows: new sentence workers are asked to judge which of two given English sentences is more detailed.4b.Remove Information (generation).Modify an English text to create a more general one by removing part of its content.As a reliability test, before generating the new sentence workers are asked to judge which of two given English sentences is less detailed.cide which of two English sentences (the original ENG, and a modified ENG1) provides more information.These HITs are combined in an iterative process that alternates text generation, grammaticality check, and entailment annotation steps.As a result, for each original ENG text we obtain multiple ENG1 variants of the three types (paraphrases, more general texts, and more specific texts) and, in turn, a set of annotated monolingual (ENG/ENG1) TE pairs.As described in Section 4.1, the resulting monolingual English TE corpus (ENG/ENG1) is used to create the following mono/cross-lingual TE corpora:This section provides a quantitative and qualitative analysis of the results of our corpus creation methodology, focusing on the collected ENG-ENG1 monolingual dataset.It has to be remarked that, as an effect of the adopted methodology, all the observations and the conclusions drawn hold for the collected CLTE corpora as well.Table 1 provides some details about each step of the pipeline shown in Figure 2.For each HIT the table presents: i) the number of items (sentences, or pairs of sentences) given in input, ii) the number of items (sentences or annotations) produced as output, iii) the number of items discarded when the agreement threshold was not reached, iv) the number of entailment pairs added to the corpus, v) the time (days and hours) required by the MTurk workforce to complete the job, and vi) the cost of the job.In HIT-1 (Paraphrase) 1,414 paraphrases were collected asking three different meaning-preserving modifications of each of the 467 original sentences12.From a practical point of view, such redundancy aims to ensure a sufficient number of grammatically correct and semantically equivalent modified sentences.From a theoretical point of view, collecting many variants of a small pool of original sentences aims to create pairs featuring different entailment relations with similar superficial forms.This, in principle, should allow to obtain a dataset which requires TE systems to focus more on deeper semantic phenomena than on the surface realization of the pairs.The collected paraphrases were sent as input to HIT-2 (Grammaticality).After this validation HIT, the number of acceptable paraphrases was reduced to 1,326 (with 88 discarded sentences, corresponding to 6.22% of the total).The retained paraphrases were paired with their corresponding original sentences, and sent to HIT-3 (Bidirectional Entailment) to be judged for semantic equivalence.The pairs marked as bidirectional entailments (1,205) were divided in three groups: 25% of the pairs (301) were directly stored in the final corpus, while the ENG1 paraphrases of the remaining 75% (904) were equally distributed to the next modification steps.In both HIT-4a (Add Information) and HIT-4b (Remove information) two new modified sentences were asked for each of the 452 paraphrases received as input.The sentences collected in these generation tasks were respectively 916 and 923.The new modified sentences were sent back to HIT-2 (Grammaticality) and HIT-3 (Bidirectional Entailment).As a result 1,438 new pairs were created; out of these, 148 resulted to be bidirectional entailments and were stored in the corpus.Finally, the 1,298 entailment pairs judged as nonbidirectional in the two previously completed HIT3 (8+1,290) were given as input to HIT-5 (Unidirectional Entailment).The pairs which passed the agreement threshold were classified according to the judgement received, and stored in the corpus as unidirectional entailment pairs.The analysis of Table 1 allows to formulate some considerations.First, the percentage of discarded items confirms the effectiveness of decomposing complex generation tasks into simpler subtasks that integrate validation HITs and quality checks based on non-experts’ agreement.In fact, on average, around 9.5% of the generated items were discarded without experts’ intervention13.Second, the amount of discarded items gives evidence about the relative difficulty of each HIT.As expected, we observe lower rejection rates, corresponding to higher inter-annotator agreement, for grammaticality HITs (5.55% on average) than for more complex entailment-related tasks (12.02% on average).Looking at costs and execution time, it is hard to draw definite conclusions due to several factors that influence the progress of the crowdsourced jobs (e.g. the fluctuations of Turkers’ performances, the time of the day at which jobs are posted, the difficulty to set the optimal cost for a given HIT14).On the one hand, as expected, the more creative “Add Info” task proved to be more demanding than the “Remove Info”: even though it was paid more, 13Moreover, it is worthwhile noticing that around 20% of the collected items were automatically rejected (and not paid) due to failures on the gold standard controls created both for generation and annotation tasks.14The payment for each HIT was set on the basis of a previous feasibility study aimed at determining the best trade-off between cost and execution time.However, replicating our approach would not necessarily result in the same costs. it still took little more time to be completed.On the other hand, although the “Unidirectional Entailment” task was expected to be more difficult and thus rewarded more than the “Bidirectional Entailment” one, in the end it took notably less time to be completed.Nevertheless, the overall figures (435 USD, and about 22.5 days of MTurk work to complete the process)15 clearly demonstrate the effectiveness of the approach.Even considering the time needed for an expert to manage the pipeline (i.e. one week to prepare gold units, and to handle the I/O of each HIT), these figures show that our methodology provides a cheaper and faster way to collect entailment data in comparison with the RTE average costs reported in Section 1.As regards the amount of data collected, the resulting corpus contains 1,620 pairs with the following distribution of entailment relations: i) 449 bidirectional entailments, ii) 491 ENG→ENG1 unidirectional entailments, and iii) 680 ENG←ENG1 unidirectional entailments.It must be noted that our methodology does not lead to the creation of pairs where some information is provided in one text and not in the other, and viceversa, as Example 1 shows: ENG: New theories were emerging in the field of psychology.ENG1: New theories were rising, which announced a kind of veiled racism.These negative examples in both directions represent a natural extension of the dataset, relevant also for specific application-oriented scenarios, and their creation will be addressed in future work.Besides the achievement of our primary objectives, the adopted approach led to some interesting by-products.First, the generated corpora are perfectly suitable to produce entailment datasets similar to those used in the traditional RTE evaluation framework.In particular, considering any possible entailment relation between two text fragments, our annotation subsumes the one proposed in RTE campaigns.This allows for the cost-effective generation of RTE-like annotations from the acquired cor15Although by projecting annotations the ENG1/ITA and ENG1/GER CLTE corpora came for free, the ITA1/ITA, ITA1/ENG, and ITA1/GER combinations created by crowdsourcing translations added 45 USD and approximately 5 days to these figures. pora by combining ENG↔ENG1 and ENG→ENG1 pairs to form 940 positive examples (449+491), keeping the 680 ENG←ENG1 as negative examples.Moreover, by swapping ENG and ENG1 in the unidirectional entailment pairs, 491 additional negative examples and 680 positive examples can be easily obtained.Finally, the output of HITs 1-2-3 in Table 1 represents per se a valuable collection of 1,205 paraphrases.This suggests the great potential of crowdsourcing for paraphrase acquisition.Through manual verification of more than 50% of the corpus (900 pairs), a total number of 53 pairs (5.9%) were found incorrect.The different errors were classified as follows: Type 1: Sentence modification errors.Generation HITs are a minor source of errors, being responsible for 10 problematic pairs.These errors are either introduced by generating a false statement (Example 2), or by forming a not fully understandable, awkward, or non-natural sentence (Example 3).Type 2: TE annotation errors.The notion of containing more/less information, used in the “Unidirectional Entailment” HIT, can mostly be applied straightforwardly to the entailment definition.However, the concept of “more/less detailed”, which generally works for factual statements, in some cases is not applicable.In fact, the MTurk workers have regularly interpreted the instructions about the amount of information as concerning the quantity of concepts contained in a sentence.This is not always corresponding to the actual entailment relation between the sentences.As a consequence, 43 pairs featuring wrong entailment annotations were encountered.These errors can be classified as follows: a) 13 pairs, where the added/removed information changes the meaning of the sentence.In these cases, the modified sentence was judged more/less specific than the original one, leading to unidirectional entailment annotation.On the contrary, in terms of the standard entailment definition, the correct annotation is “no entailment” (as in Example 4, which was annotated as ENG→ENG1): These pairs were labelled as unidirectional entailments (in the example above ENG→ENG1), under the assumption that a proper name is more specific and informative than a pronoun.However, adhering to the TE definition, co-referring expressions are equivalent, and their realization does not play any role in the entailment decision.This implies that the correct entailment annotation is “bidirectional”. c) 9 pairs where the sentences are semantically equivalent, but contain a piece of information which is explicit in one sentence, and implicit in the other.In these cases, Turkers judged the sentence containing the explicit mention as more specific, and thus the pair was annotated as unidirectional entailment.In Example 6, the expression “the trigger” in ENG1 implicitly means “the click of the trigger”, making the two sentences equivalent, and the entailment bidirectional (instead of ENG→ENG1). d) 7 pairs where the information removed from or added to the sentence is not relevant to the entailment relation.In these cases, the modified sentence was judged less/more specific than the original one (and thus considered as unidirectional entailment), even though the correct judgement is “bidirectional”, as in: e) 4 pairs where the added/removed information concerns universally quantified general statements, about which the interpretation of “more/less specific” given by Turkers resulted in the wrong annotation.In Example 8, the additional information (“multicultural”) restricts the set to which it refers (“couples”) making ENG entailed by ENG1, and not vice versa as resulted from Turkers’ annotation.In light of this analysis, we conclude that the sentence modification methodology proved to be successful, as the low number of Type 1 errors shows.Considering that the most expensive phase in the creation of a TE dataset is the generation of the pairs, this is a significant achievement.Differently, the entailment assessment phase appears to be more problematic, accounting for the majority of errors.As shown by Type 2 errors, this is due to a partial misalignment between the instructions given in our HITs, and the formal definition of textual entailment.For this reason, further experimentation will explore different ways to instruct workers (e.g. asking to consider proper names and pronouns as equivalent) in order to reduce the amount of errors produced.As a final remark, considering that in the creation of a TE dataset the manual check of the annotated pairs represents a minor cost, even the involvement of experts to filter out wrong annotations would not decrease the cost-effectiveness of the proposed methodology.There is an increasing need of annotated data to develop new solutions to the Textual Entailment problem, explore new entailment-related tasks, and set up experimental frameworks targeting real-world applications.Following the recent trends promoting annotation efforts that go beyond the established RTE Challenge framework (unidirectional entailment between monolingual T-H pairs), in this paper we addressed the multilingual dimension of the problem.Our primary goal was the creation of large-scale collections of entailment pairs for different language combinations.Besides that, we considered cost effectiveness and replicability as additional requirements.To achieve our objectives, we developed a “divide and conquer” methodology based on crowdsourcing.Our approach presents several key innovations with respect to the related works on TE data acquisition.These include the decomposition of a complex content generation task in a pipeline of simpler subtasks accessible to a large crowd of non-experts, and the integration of quality control mechanisms at each stage of the process.The result of our work is the first large-scale dataset containing both monolingual and cross-lingual corpora for several combinations of texts-hypotheses in English, Italian, and German.Among the advantages of our method it is worth mentioning: i) the full alignment between the created corpora, ii) the possibility to easily extend the dataset to new languages, and iii) the feasibility of creating general-purpose corpora, featuring multi-directional entailment relations, that subsume the traditional RTE-like annotation.This work has been partially supported by the ECfunded project CoSyne (FP7-ICT-4-24853).The authors would like to thank Emanuele Pianta for the helpful discussions, and Giovanni Moretti for the valuable support in the creation of the CLTE dataset.
Named Entity Recognition in Tweets: An Experimental StudyPeople tweet more than 100 Million times daily, yielding a noisy, informal, but sometimes informative corpus of 140-character messages that mirrors the zeitgeist in an unprecedented manner.The performance of standard NLP tools is severely degraded on tweets.This paper addresses this issue by re-building the NLP pipeline beginning with part-of-speech tagging, through chunking, to recognition.Our novel doubles compared with the NER system. the redundancy inherent in tweets to achieve this performance, using LabeledLDA to exploit Freebase dictionaries as a source of distant supervision.LabeledLDA outperforms coincreasing 25% over ten common entity types.NLP tools are available at:Status Messages posted on Social Media websites such as Facebook and Twitter present a new and challenging style of text for language technology due to their noisy and informal nature.Like SMS (Kobus et al., 2008), tweets are particularly terse and difficult (See Table 1).Yet tweets provide a unique compilation of information that is more upto-date and inclusive than news articles, due to the low-barrier to tweeting, and the proliferation of mobile devices.1 The corpus of tweets already exceeds the size of the Library of Congress (Hachman, 2011) and is growing far more rapidly.Due to the volume of tweets, it is natural to consider named-entity recognition, information extraction, and text mining over tweets.Not surprisingly, the performance of “off the shelf” NLP tools, which were trained on news corpora, is weak on tweet corpora.In response, we report on a re-trained “NLP pipeline” that leverages previously-tagged out-ofdomain text, 2 tagged tweets, and unlabeled tweets to achieve more effective part-of-speech tagging, chunking, and named-entity recognition.1 The Hobbit has FINALLY started filming!I cannot wait!2 Yess!Yess!Its official Nintendo announced today that they Will release the Nintendo 3DS in north America march 27 for $250 3 Government confirms blast n nuclear plants n japan...don’t knw wht s gona happen nw... We find that classifying named entities in tweets is a difficult task for two reasons.First, tweets contain a plethora of distinctive named entity types (Companies, Products, Bands, Movies, and more).Almost all these types (except for People and Locations) are relatively infrequent, so even a large sample of manually annotated tweets will contain few training examples.Secondly, due to Twitter’s 140 character limit, tweets often lack sufficient context to determine an entity’s type without the aid of background knowledge.To address these issues we propose a distantly supervised approach which applies LabeledLDA (Ramage et al., 2009) to leverage large amounts of unlabeled data in addition to large dictionaries of entities gathered from Freebase, and combines information about an entity’s context across its mentions.We make the following contributions: LabeledLDA is applied, utilizing constraints based on an open-domain database (Freebase) as a source of supervision.This approach increases F1 score by 25% relative to co-training (Blum and Mitchell, 1998; Yarowsky, 1995) on the task of classifying named entities in Tweets.The rest of the paper is organized as follows.We successively build the NLP pipeline for Twitter feeds in Sections 2 and 3.We first present our approaches to shallow syntax – part of speech tagging (§2.1), and shallow parsing (§2.2).§2.3 describes a novel classifier that predicts the informativeness of capitalization in a tweet.All tools in §2 are used as features for named entity segmentation in §3.1.Next, we present our algorithms and evaluation for entity classification (§3.2).We describe related work in §4 and conclude in §5.We first study two fundamental NLP tasks – POS tagging and noun-phrase chunking.We also discuss a novel capitalization classifier in §2.3.The outputs of all these classifiers are used in feature generation for named entity recognition in the next section.For all experiments in this section we use a dataset of 800 randomly sampled tweets.All results (Tables 2, 4 and 5) represent 4-fold cross-validation experiments on the respective tasks.3 Part of speech tagging is applicable to a wide range of NLP tasks including named entity segmentation and information extraction.Prior experiments have suggested that POS tagging has a very strong baseline: assign each word to its most frequent tag and assign each Out of Vocabulary (OOV) word the most common POS tag.This baseline obtained a 0.9 accuracy on the Brown corpus (Charniak et al., 1993).However, the application of a similar baseline on tweets (see Table 2) obtains a much weaker 0.76, exposing the challenging nature of Twitter data.A key reason for this drop in accuracy is that Twitter contains far more OOV words than grammatical text.Many of these OOV words come from spelling variation, e.g., the use of the word “n” for “in” in Table 1 example 3.Although NNP is the most frequent tag for OOV words, only about 1/3 are NNPs.The performance of off-the-shelf news-trained POS taggers also suffers on Twitter data.The stateof-the-art Stanford POS tagger (Toutanova et al., 2003) improves on the baseline, obtaining an accuracy of 0.8.This performance is impressive given that its training data, the Penn Treebank WSJ (PTB), is so different in style from Twitter, however it is a huge drop from the 97% accuracy reported on the PTB.There are several reasons for this drop in performance.Table 3 lists common errors made by the Stanford tagger.First, due to unreliable capitalization, common nouns are often misclassified as proper nouns, and vice versa.Also, interjections and verbs are frequently misclassified as nouns.In addition to differences in vocabulary, the grammar of tweets is quite different from edited news text.For instance, tweets often start with a verb (where the subject ‘I’ is implied), as in: “watchng american dad.” To overcome these differences in style and vocabulary, we manually annotated a set of 800 tweets (16K tokens) with tags from the Penn TreeBank tag set for use as in-domain training data for our POS tagging system, T-POS.4 We add new tags for the Twitter specific phenomena: retweets, @usernames, #hashtags, and urls.Note that words in these categories can be tagged with 100% accuracy using simple regular expressions.To ensure fair comparison in Table 2, we include a postprocessing step which tags these words appropriately for all systems.To help address the issue of OOV words and lexical variations, we perform clustering to group together words which are distributionally similar (Brown et al., 1992; Turian et al., 2010).In particular, we perform hierarchical clustering using Jcluster (Goodman, 2001) on 52 million tweets; each word is uniquely represented by a bit string based on the path from the root of the resulting hierarchy to the word’s leaf.We use the Brown clusters resulting from prefixes of 4, 8, and 12 bits.These clusters are often effective in capturing lexical variations, for example, following are lexical variations on the word “tomorrow” from one cluster after filtering out other words (most of which refer to days): T-POS uses Conditional Random Fields5 (Lafferty et al., 2001), both because of their ability to model strong dependencies between adjacent POS tags, and also to make use of highly correlated features (for example a word’s identity in addition to prefixes and suffixes).Besides employing the Brown clusters computed above, we use a fairly standard set of features that include POS dictionaries, spelling and contextual features.On a 4-fold cross validation over 800 tweets, T-POS outperforms the Stanford tagger, obtaining a 26% reduction in error.In addition we include 40K tokens of annotated IRC chat data (Forsythand and Martell, 2007), which is similar in style.Like Twitter, IRC data contains many misspelled/abbreviated words, and also more pronouns, and interjections, but fewer determiners than news.Finally, we also leverage 50K POS-labeled tokens from the Penn Treebank (Marcus et al., 1994).Overall T-POS trained on 102K tokens (12K from Twitter, 40K from IRC and 50K from PTB) results in a 41% error reduction over the Stanford tagger, obtaining an accuracy of 0.883.Table 3 lists gains on some of the most common error types, for example, T-POS dramatically reduces error on interjections and verbs that are incorrectly classified as nouns by the Stanford tagger.Shallow parsing, or chunking is the task of identifying non-recursive phrases, such as noun phrases, verb phrases, and prepositional phrases in text.Accurate shallow parsing of tweets could benefit several applications such as Information Extraction and Named Entity Recognition.Off the shelf shallow parsers perform noticeably worse on tweets, motivating us again to annotate indomain training data.We annotate the same set of 800 tweets mentioned previously with tags from the CoNLL shared task (Tjong Kim Sang and Buchholz, 2000).We use the set of shallow parsing features described by Sha and Pereira (2003), in addition to the Brown clusters mentioned above.Part-of-speech tag features are extracted based on cross-validation output predicted by T-POS.For inference and learning, again we use Conditional Random Fields.We utilize 16K tokens of in-domain training data (using cross validation), in addition to 210K tokens of newswire text from the CoNLL dataset.Table 4 reports T-CHUNK’s performance at shallow parsing of tweets.We compare against the offthe shelf OpenNLP chunker6, obtaining a 22% reduction in error.A key orthographic feature for recognizing named entities is capitalization (Florian, 2002; Downey et al., 2007).Unfortunately in tweets, capitalization is much less reliable than in edited texts.In addition, there is a wide variety in the styles of capitalization.In some tweets capitalization is informative, whereas in other cases, non-entity words are capitalized simply for emphasis.Some tweets contain all lowercase words (8%), whereas others are in ALL CAPS (0.6%).To address this issue, it is helpful to incorporate information based on the entire content of the message to determine whether or not its capitalization is informative.To this end, we build a capitalization classifier, T-CAP, which predicts whether or not a tweet is informatively capitalized.Its output is used as a feature for Named Entity Recognition.We manually labeled our 800 tweet corpus as having either “informative” or “uninformative” capitalization.The criteria we use for labeling is as follows: if a tweet contains any non-entity words which are capitalized, but do not begin a sentence, or it contains any entities which are not capitalized, then its capitalization is “uninformative”, otherwise it is “informative”.For learning , we use Support Vector Machines.7 The features used include: the fraction of words in the tweet which are capitalized, the fraction which appear in a dictionary of frequently lowercase/capitalized words but are not lowercase/capitalized in the tweet, the number of times the word ‘I’ appears lowercase and whether or not the first word in the tweet is capitalized.Results comparing against the majority baseline, which predicts capitalization is always informative, are shown in Table 5.Additionally, in §3 we show that features based on our capitalization classifier improve performance at named entity segmentation.We now discuss our approach to named entity recognition on Twitter data.As with POS tagging and shallow parsing, off the shelf named-entity recognizers perform poorly on tweets.For example, applying the Stanford Named Entity Recognizer to one of the examples from Table 1 results in the following output: [Nintendo]LOC announced today that they Will release the [Nintendo]ORG 3DS in north [America]LOC march 27 for $250 The OOV word ‘Yess’ is mistaken as a named entity.In addition, although the first occurrence of ‘Nintendo’ is correctly segmented, it is misclassified, whereas the second occurrence is improperly segmented – it should be the product “Nintendo 3DS”.Finally “north America” should be segmented as a LOCATION, rather than just ‘America’.In general, news-trained Named Entity Recognizers seem to rely heavily on capitalization, which we know to be unreliable in tweets.Following Collins and Singer (1999), Downey et al. (2007) and Elsner et al.(2009), we treat classification and segmentation of named entities as separate tasks.This allows us to more easily apply techniques better suited towards each task.For example, we are able to use discriminative methods for named entity segmentation and distantly supervised approaches for classification.While it might be beneficial to jointly model segmentation and (distantly supervised) classification using a joint sequence labeling and topic model similar to that proposed by Sauper et al. (2010), we leave this for potential future work.Because most words found in tweets are not part of an entity, we need a larger annotated dataset to effectively learn a model of named entities.We therefore use a randomly sampled set of 2,400 tweets for NER.All experiments (Tables 6, 8-10) report results using 4-fold cross validation. they can refer to people or companies), we believe they could be more easily classified using features of their associated user’s profile than contextual features of the text.T-SEG models Named Entity Segmentation as a sequence-labeling task using IOB encoding for representing segmentations (each word either begins, is inside, or is outside of a named entity), and uses Conditional Random Fields for learning and inference.Again we include orthographic, contextual and dictionary features; our dictionaries included a set of type lists gathered from Freebase.In addition, we use the Brown clusters and outputs of T-POS, T-CHUNK and T-CAP in generating features.We report results at segmenting named entities in Table 6.Compared with the state-of-the-art newstrained Stanford Named Entity Recognizer (Finkel et al., 2005), T-SEG obtains a 52% increase in F1 score.Because capitalization in Twitter is less informative than news, in-domain data is needed to train models which rely less heavily on capitalization, and also are able to utilize features provided by T-CAP.We exhaustively annotated our set of 2,400 tweets (34K tokens) with named entities.8 A convention on Twitter is to refer to other users using the @ symbol followed by their unique username.We deliberately choose not to annotate @usernames as entities in our data set because they are both unambiguous, and trivial to identify with 100% accuracy using a simple regular expression, and would only serve to inflate our performance statistics.While there is ambiguity as to the type of @usernames (for example, Because Twitter contains many distinctive, and infrequent entity types, gathering sufficient training data for named entity classification is a difficult task.In any random sample of tweets, many types will only occur a few times.Moreover, due to their terse nature, individual tweets often do not contain enough context to determine the type of the entities they contain.For example, consider following tweet: KKTNY in 45min without any prior knowledge, there is not enough context to determine what type of entity “KKTNY” refers to, however by exploiting redundancy in the data (Downey et al., 2010), we can determine it is likely a reference to a television show since it often co-occurs with words such as watching and premieres in other contexts.9 In order to handle the problem of many infrequent types, we leverage large lists of entities and their types gathered from an open-domain ontology (Freebase) as a source of distant supervision, allowing use of large amounts of unlabeled data in learning.Freebase Baseline: Although Freebase has very broad coverage, simply looking up entities and their types is inadequate for classifying named entities in context (0.38 F-score, §3.2.1).For example, according to Freebase, the mention ‘China’ could refer to a country, a band, a person, or a film.This problem is very common: 35% of the entities in our data appear in more than one of our (mutually exclusive) Freebase dictionaries.Additionally, 30% of entities mentioned on Twitter do not appear in any Freebase dictionary, as they are either too new (for example a newly released videogame), or are misspelled or abbreviated (for example ‘mbp’ is often used to refer to the “mac book pro”).Distant Supervision with Topic Models: To model unlabeled entities and their possible types, we apply LabeledLDA (Ramage et al., 2009), constraining each entity’s distribution over topics based on its set of possible types according to Freebase.In contrast to previous weakly supervised approaches to Named Entity Classification, for example the CoTraining and Naive Bayes (EM) models of Collins and Singer (1999), LabeledLDA models each entity string as a mixture of types rather than using a single hidden variable to represent the type of each mention.This allows information about an entity’s distribution over types to be shared across mentions, naturally handling ambiguous entity strings whose mentions could refer to different types.Each entity string in our data is associated with a bag of words found within a context window around all of its mentions, and also within the entity itself.As in standard LDA (Blei et al., 2003), each bag of words is associated with a distribution over topics, Multinomial(Oe), and each topic is associated with a distribution over words, Multinomial(ot).In addition, there is a one-to-one mapping between topics and Freebase type dictionaries.These dictionaries constrain Oe, the distribution over topics for each entity string, based on its set of possible types, FB[e].For example, OAmazon could correspond to a distribution over two types: COMPANY, and LOCATION, whereas OApple might represent a distribution over COMPANY, and FOOD.For entities which aren’t found in any of the Freebase dictionaries, we leave their topic distributions Oe unconstrained.Note that in absence of any constraints LabeledLDA reduces to standard LDA, and a fully unsupervised setting similar to that presented by Elsner et. al.(2009).In detail, the generative process that models our data for Named Entity Classification is as follows: Generate ze,i from Mult(Oe).Generate the word we,i from Mult(o,;e,i).To infer values for the hidden variables, we apply Collapsed Gibbs sampling (Griffiths and Steyvers, 2004), where parameters are integrated out, and the ze,is are sampled directly.In making predictions, we found it beneficial to consider Otrain e as a prior distribution over types for entities which were encountered during training.In practice this sharing of information across contexts is very beneficial as there is often insufficient evidence in an isolated tweet to determine an entity’s type.For entities which weren’t encountered during training, we instead use a prior based on the distribution of types across all entities.One approach to classifying entities in context is to assume that Otrain e is fixed, and that all of the words inside the entity mention and context, w, are drawn based on a single topic, z, that is they are all drawn from Multinomial(o,;).We can then compute the posterior distribution over types in closed form with a simple application of Bayes rule: During development, however, we found that rather than making these assumptions, using Gibbs Sampling to estimate the posterior distribution over types performs slightly better.In order to make predictions, for each entity we use an informative Dirichlet prior based on Otrain e and perform 100 iterations of Gibbs Sampling holding the hidden topic variables in the training data fixed (Yao et al., 2009).Fewer iterations are needed than in training since the typeword distributions, β have already been inferred.To evaluate T-CLASS’s ability to classify entity mentions in context, we annotated the 2,400 tweets with 10 types which are both popular on Twitter, and have good coverage in Freebase: PERSON, GEO-LOCATION, COMPANY, PRODUCT, FACILITY, TV-SHOW, MOVIE, SPORTSTEAM, BAND, and OTHER.Note that these type annotations are only used for evaluation purposes, and not used during training T-CLASS, which relies only on distant supervision.In some cases, we combine multiple Freebase types to create a dictionary of entities representing a single type (for example the COMPANY dictionary contains Freebase types /business/consumer company and /business/brand).Because our approach does not rely on any manually labeled examples, it is straightforward to extend it for a different sets of types based on the needs of downstream applications.Training: To gather unlabeled data for inference, we run T-SEG, our entity segmenter (from §3.1), on 60M tweets, and keep the entities which appear 100 or more times.This results in a set of 23,651 distinct entity strings.For each entity string, we collect words occurring in a context window of 3 words from all mentions in our data, and use a vocabulary of the 100K most frequent words.We run Gibbs sampling for 1,000 iterations, using the last sample to estimate entity-type distributions Oe, in addition to type-word distributions βt.Table 7 displays the 20 entities (not found in Freebase) whose posterior distribution Oe assigns highest probability to selected types.Results: Table 8 presents the classification results of T-CLASS compared against a majority baseline which simply picks the most frequent class (PERSON), in addition to the Freebase baseline, which only makes predictions if an entity appears in exactly one dictionary (i.e., appears unambiguous).T-CLASS also outperforms a simple supervised baseline which applies a MaxEnt classifier using 4-fold cross validation over the 1,450 entities which were annotated for testing.Additionally we compare against the co-training algorithm of Collins and Singer (1999) which also leverages unlabeled data and uses our Freebase type lists; for seed rules we use the “unambiguous” Freebase entities.Our results demonstrate that T-CLASS outperforms the baselines and achieves a 25% increase in F1 score over co-training.Tables 9 and 10 present a breakdown of F1 scores by type, both collapsing types into the standard classes used in the MUC competitions (PERSON, LOCATION, ORGANIZATION), and using the 10 popular Twitter types described earlier.Entity Strings vs.Entity Mentions: DL-Cotrain and LabeledLDA use two different representations for the unlabeled data during learning.LabeledLDA groups together words across all mentions of an entity string, and infers a distribution over its possible types, whereas DL-Cotrain considers the entity mentions separately as unlabeled examples and predicts a type independently for each.In order to ensure that the difference in performance between LabeledLDA and DL-Cotrain is not simply due to this difference in representation, we compare both DL-Cotrain and LabeledLDA using both unlabeled datasets (grouping words by all mentions vs. keeping mentions separate) in Table 11.As expected, DL-Cotrain performs poorly when the unlabeled examples group mentions; this makes sense, since CoTraining uses a discriminative learning algorithm, so when trained on entities and tested on individual mentions, the performance decreases.Additionally, LabeledLDA’s performance is poorer when considering mentions as “documents”.This is likely due to the fact that there isn’t enough context to effectively learn topics when the “documents” are very short (typically fewer than 10 words).End to End System: Finally we present the end to end performance on segmentation and classification (T-NER) in Table 12.We observe that T-NER again outperforms co-training.Moreover, comparing against the Stanford Named Entity Recognizer on the 3 MUC types, T-NER doubles FI score.There has been relatively little previous work on building NLP tools for Twitter or similar text styles.Locke and Martin (2009) train a classifier to recognize named entities based on annotated Twitter data, handling the types PERSON, LOCATION, and ORGANIZATION.Developed in parallel to our work, Liu et al. (2011) investigate NER on the same 3 types, in addition to PRODUCTs and present a semisupervised approach using k-nearest neighbor.Also ing topic models (e.g.LabeledLDA) for classifying developed in parallel, Gimpell et al. (2011) build a named entities has a similar effect, in that informaPOS tagger for tweets using 20 coarse-grained tags. tion about an entity’s distribution of possible types Benson et. al.(2011) present a system which ex- is shared across its mentions. tracts artists and venues associated with musical per- 5 Conclusions formances.Recent work (Han and Baldwin, 2011; We have demonstrated that existing tools for POS Gouws et al., 2011) has proposed lexical normaliza- tagging, Chunking and Named Entity Recognition tion of tweets which may be useful as a preprocess- perform quite poorly when applied to Tweets.To ing step for the upstream tasks like POS tagging and address this challenge we have annotated tweets and NER.In addition Finin et. al.(2010) investigate built tools trained on unlabeled, in-domain and outthe use of Amazon’s Mechanical Turk for annotat- of-domain data, showing substantial improvement ing Named Entities in Twitter, Minkov et. al.(2005) over their state-of-the art news-trained counterparts, investigate person name recognizers in email, and for example, T-POS outperforms the Stanford POS Singh et. al.(2010) apply a minimally supervised Tagger, reducing error by 41%.Additionally we approach to extracting entities from text advertise- have shown the benefits of features generated from ments.T-POS and T-CHUNK in segmenting Named Entities.In contrast to previous work, we have demon- We identified named entity classification as a parstrated the utility of features based on Twitter- ticularly challenging task on Twitter.Due to their specific POS taggers and Shallow Parsers in seg- terse nature, tweets often lack enough context to menting Named Entities.In addition we take a dis- identify the types of the entities they contain.In adtantly supervised approach to Named Entity Classi- dition, a plethora of distinctive named entity types fication which exploits large dictionaries of entities are present, necessitating large amounts of training gathered from Freebase, requires no manually anno- data.To address both these issues we have presented tated data, and as a result is able to handle a larger and evaluated a distantly supervised approach based number of types than previous work.Although we on LabeledLDA, which obtains a 25% increase in F1 found manually annotated data to be very beneficial score over the co-training approach to Named Enfor named entity segmentation, we were motivated tity Classification suggested by Collins and Singer to explore approaches that don’t rely on manual la- (1999) when applied to Twitter. bels for classification due to Twitter’s wide range of Our POS tagger, Chunker Named Entity Recnamed entity types.Additionally, unlike previous ognizer are available for use by the research work on NER in informal text, our approach allows community: http://github.com/aritter/ the sharing of information across an entity’s men- twitter_nlp tions which is quite beneficial due to Twitter’s terse Acknowledgments nature.We would like to thank Stephen Soderland, Dan Previous work on Semantic Bootstrapping has Weld and Luke Zettlemoyer, in addition to the taken a weakly-supervised approach to classifying anonymous reviewers for helpful comments on a named entities based on large amounts of unla- previous draft.This research was supported in part beled text (Etzioni et al., 2005; Carlson et al., 2010; by NSF grant IIS-0803481, ONR grant N00014-11Kozareva and Hovy, 2010; Talukdar and Pereira, 1-0294, Navy STTR contract N00014-10-M-0304, a 2010; McIntosh, 2010).In contrast, rather than National Defense Science and Engineering Graduate predicting which classes an entity belongs to (e.g.(NDSEG) Fellowship 32 CFR 168a and carried out a multi-label classification task), LabeledLDA esti- at the University of Washington’s Turing Center. mates a distribution over its types, which is then useful as a prior when classifying mentions in context.In addition there has been been work on SkipChain CRFs (Sutton, 2004; Finkel et al., 2005) which enforce consistency when classifying multiple occurrences of an entity within a document.Us1532
A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency ParsingMost current dependency parsers presuppose that input words have been morphologically disambiguated using a part-of-speech tagger before parsing begins.We present a transitionbased system for joint part-of-speech tagging and labeled dependency parsing with nonprojective trees.Experimental evaluation on Chinese, Czech, English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of-theart results for all languages.Dependency-based syntactic parsing has been the focus of intense research efforts during the last decade, and the state of the art today is represented by globally normalized discriminative models that are induced using structured learning.Graphbased models parameterize the parsing problem by the structure of the dependency graph and normally use dynamic programming for inference (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Bohnet, 2010), but other inference methods have been explored especially for non-projective parsing (Riedel and Clarke, 2006; Smith and Eisner, 2008; Martins et al., 2009; Martins et al., 2010; Koo et al., 2010).Transitionbased models parameterize the problem by elementary parsing actions and typically use incremental beam search (Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Clark, 2011).Despite notable differences in model structure, graph-based and transition-based parsers both give state-of-theart accuracy with proper feature selection and optimization (Koo and Collins, 2010; Zhang and Nivre, 2011; Bohnet, 2011).It is noteworthy, however, that almost all dependency parsers presuppose that the words of an input sentence have been morphologically disambiguated using (at least) a part-of-speech tagger.This is in stark contrast to the best parsers based on PCFG models, such as the Brown parser (Charniak and Johnson, 2005) and the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), which not only can perform their own part-of-speech tagging but normally give better parsing accuracy when they are allowed to do so.This suggests that joint models for tagging and parsing might improve accuracy also in the case of dependency parsing.It has been argued that joint morphological and syntactic disambiguation is especially important for richly inflected languages, where there is considerable interaction between morphology and syntax such that neither can be fully disambiguated without considering the other.Thus, Lee et al. (2011) show that a discriminative model for joint morphological disambiguation and dependency parsing outperforms a pipeline model in experiments on Latin, Ancient Greek, Czech and Hungarian.However, Li et al. (2011) and Hatori et al.(2011) report improvements with a joint model also for Chinese, which is not a richly inflected language but is nevertheless rich in part-of-speech ambiguities.In this paper, we present a transition-based model for joint part-of-speech tagging and labeled dependency parsing with non-projective trees.Experiments show that joint modeling improves both tagging and parsing accuracy, leading to state-of-the-art accuracy for richly inflected languages like Czech and German as well as more configurational languages like Chinese and English.To our knowledge, this is the first joint system that performs labeled dependency parsing.It is also the first joint system that achieves state-of-the-art accuracy for non-projective dependency parsing.Transition-based dependency parsing was pioneered by Yamada and Matsumoto (2003) and Nivre et al. (2004), who used classifiers trained to predict individual actions of a deterministic shift-reduce parser.Recent research has shown that better accuracy can be achieved by using beam search and optimizing models on the entire sequence of decisions needed to parse a sentence instead of single actions (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Clark, 2011; Zhang and Nivre, 2011; Bohnet, 2011).In addition, a number of different transition systems have been proposed, in particular for dealing with non-projective dependencies, which were beyond the scope of early systems (Attardi, 2006; Nivre, 2007; Nivre, 2009; Titov et al., 2009).In this section, we start by defining a transition system for joint tagging and parsing based on the non-projective transition system proposed in Nivre (2009).We then show how to perform beam search and structured online learning with this model, and conclude by discussing feature representations.Given a set P of part-of-speech tags and a set D of dependency labels, a tagged dependency tree for a sentence x = w1, ... , wn is a directed tree T = (Vx, A) with labeling functions 7r and 6 such that: The set Vx of nodes is the set of positive integers up to and including n, each corresponding to the linear position of a word in the sentence, plus an extra artificial root node 0.The set A of arcs is a set of pairs (i, j), where i is the head node and j is the dependent node.The functions 7r and 6 assign a unique part-of-speech label to each node/word and a unique dependency label to each arc, respectively.This notion of dependency tree differs from the standard definition only by including part-of-speech labels as well as dependency labels (K¨ubler et al., 2009).Following Nivre (2008), we define a transition system for dependency parsing as a quadruple 5 = (C, T, cs, Ct), where A transition sequence for a sentence x in 5 is a sequence of configuration-transition pairs C0,m = In this paper, we take the set C of configurations to be the set of all 5-tuples c = (E, B, A, 7r, 6) such that E (the stack) and B (the buffer) are disjoint sublists of the nodes Vx of some sentence x, A is a set of dependency arcs over Vx, and 7r and 6 are labeling functions as defined above.We take the initial configuration for a sentence x = w1, ... , wn to be cs(x) = ([0], [1, ... , n], 11, 1, 1), where L is the function that is undefined for all arguments, and we take the set Ct of terminal configurations to be the set of all configurations of the form c = ([0], [ ], A, 7r, 6) (for any A, 7r and 6).The tagged dependency tree defined for x by c = (E, B, A, 7r, 6) is the tree (Vx, A) with labeling functions 7r and 6, which we write TREE(x, c).The set T of transitions is shown in Figure 1.The LEFT-ARCd and RIGHT-ARCd transitions both add an arc (with dependency label d) between the two nodes on top of the stack and replaces these nodes by the head node of the new arc (which is the rightmost node for LEFT-ARCd and the leftmost node for RIGHT-ARCd).The SHIFTp transition extracts the first node in the buffer, pushes it onto the stack and labels it with the part-of-speech tag p. The SWAP transition extracts the second topmost node from the stack and moves it back to the buffer, subject to the condition that the two top nodes on the stack are still in the order given by the sentence.Except for the addition of a tag parameter p to the SHIFT transition, this is equivalent to the system described in Nivre (2009), which thanks to the SWAP transition can handle arbitrary non-projective trees.The soundness and completeness results given in that paper trivially carry over to the new system.The only thing to note is that, before a terminal configuration can be reached, every word has to be pushed onto the stack in a SHIFTp transition, which ensures that every node/word in the output tree will be tagged.While early transition-based parsers generally used greedy best-first inference and locally trained classifiers, recent work has shown that higher accuracy can be obtained using beam search and global structure learning to mitigate error propagation.In particular, it seems that the globally learned models can exploit a much richer feature space than locally trained classifiers, as shown by Zhang and Nivre (2011).Since joint tagging and parsing increases the size of the search space and is likely to require novel features, we use beam search in combination with structured perceptron learning.The beam search algorithm used to derive the best parse y for a sentence x is outlined in Figure 2.In addition to the sentence x, it takes as input a weight vector w corresponding to a linear model for scoring transitions out of configurations and two prunw and beam parameters b1 and b2.The symbols h.c, h.s and h.f denote, respectively, the configuration, score and feature representation of a hypothesis h; h.c.A denotes the arc set of h.c. ing parameters b1 and b2.A parse hypothesis h is represented by a configuration h.c, a score h.s and a feature vector h.f for the transition sequence up to h.c.Hypotheses are stored in the list BEAM, which is sorted by descending scores and initialized to hold the hypothesis h0 corresponding to the initial configuration cs(x) with score 0.0 and all features set to 0.0 (lines 1–4).In the main loop (lines 5–13), a set of new hypotheses is derived and stored in the list TMP, which is finally pruned and assigned as the new value of BEAM.The main loop terminates when all hypotheses in BEAM contain terminal configurations, and the dependency tree extracted from the top scoring hypothesis is returned (lines 14–16).The set of new hypotheses is created in two nested loops (lines 7–12), where every hypothesis h in BEAM is updated using every permissible transition t for the configuration h.c.The feature representation of the new hypothesis is obtained by adding the feature vector f(t, h.c) for the current configurationtransition pair to the feature vector of the old hypothesis (line 9).Similarly, the score of the new hypothesis is the sum of the score f(t, h.c) · w of the current configuration-transition pair and the score of the old hypothesis (line 10).The feature representation/score of a complete parse y for x with transition sequence C0,m is thus the sum of the feature representations/scores of the configurationtransition pairs in C0,m: Finally, the configuration of the new hypothesis is obtained by evaluating t(h.c) (line 11).The new hypothesis is then inserted into TMP in score-sorted order (line 12).The pruning parameters b1 and b2 determine the number of hypotheses allowed in the beam and at the same time control the tradeoff between syntactic and morphological ambiguity.First, we extract the b1 highest scoring hypotheses with distinct dependency trees.Then we extract the b2 highest scoring remaining hypotheses, which will typically be tagging variants of dependency trees that are already in the beam.In this way, we prevent the beam from getting filled up with too many tagging variants of the same dependency tree, which was found to be harmful in preliminary experiments.One final thing to note about the inference algorithm is that the notion of permissibility for a transition t out of a configuration c can be used to capture not only formal constraints on transitions – such as the fact that it is impossible to perform a SHIFTp transition with an empty buffer or illegal to perform a LEFT-ARCd transition with the special root node on top of the stack – but also to filter out unlikely dependency labels or tags.Thus, in the experiments later on, we will typically constrain the parser so that SHIFTp is permissible only if p is one of the k best part-of-speech tags with a score no more than α below the score of the 1-best tag, as determined by a preprocessing tagger.We also filter out instances of LEFT-ARCd and RIGHT-ARCd, where d does not occur in the training data for the predicted part-ofspeech tag combination of the head and dependent.This procedure leads to a significant speed up.In order to learn a weight vector w from a training set {(xj, yj)1 j=1 of sentences with their tagged dependency trees, we use a variant of the structured perceptron, introduced by Collins (2002), which makes N iterations over the training data and updates the weight vector for every sentence xj where the highest scoring parse y* is different from yj.More precisely, we use the passive-aggressive update of Crammer et al. (2006): where We also use the early update strategy found beneficial for parsing in several previous studies (Collins and Roark, 2004; Zhang and Clark, 2008; Huang and Sagae, 2010), which means that, during learning, we terminate the beam search as soon as the hypothesis corresponding to the gold parse yj falls out of the beam and update with respect to the partial transition sequence constructed up to that point.Finally, we use the standard technique of averaging over all weight vectors, as originally proposed by Collins (2002).As already noted, the feature representation f(x, y) of an input sentence x with parse y decomposes into feature representations f(c, t) for the transitions t(c) needed to derive y from cs(x).Features may refer to any aspect of a configuration, as encoded in the stack E, the buffer B, the arc set A and the labelings 7r and S. In addition, we assume that each word w in the input is assigned up to k candidate part-of-speech tags 7ri(w) with corresponding scores s(7ri(w)). use Ei and Bi to denote the ith token in the stack E and buffer B, respectively, with indexing starting at 0, and we use the following functors to extract properties of a token: πi() = ith best tag; s(πi()) = score of ith best tag; π() = finally predicted tag; w() = word form; pi() = word prefix of i characters; si() = word suffix of i characters.Score differences are binned in discrete steps of 0.05.The bulk of features used in our system are taken from Zhang and Nivre (2011), although with two important differences.First of all, like Hatori et al. (2011), we have omitted all features that presuppose an arc-eager parsing order, since our transition system defines an arc-standard order.Secondly, any feature that refers to the part-of-speech tag of a word w in the buffer B will in our system refer to the topscoring tag π1(w), rather than the finally predicted tag.By contrast, for a word in the stack E, part-ofspeech features refer to the tag π(w) chosen when shifting w onto the stack (which may or may not be the same as π1(w)).In addition to the standard features for transitionbased dependency parsing, we have added features specifically to improve the tagging step in the joint model.The templates for these features, which are specified in Figure 3, all involve the ith best tag assigned to the first word of the buffer B (the next word to be shifted in a SHIFTP transition) in combination with neighboring words, word prefixes, word suffixes, score differences and tag rank.Finally, in some experiments, we make use of two additional feature sets, which we call graph features (G) and cluster features (C), respectively.Graph features are defined over the factors of a graph-based dependency parser, which was shown to improve the accuracy of a transition-based parser by Zhang and Clark (2008).However, while their features were limited to certain first- and second-order factors, we use features over second- and third-order factors as found in the parsers of Bohnet and Kuhn (2012).These features are scored as soon as the factors are completed, using a technique that is similar to what Hatori et al. (2011) call delayed features, although they use it for part-of-speech tags in the lookahead while we use it for subgraphs of the dependency tree.Cluster features, finally, are features over word clusters, as first used by Koo et al. (2008), which replace part-of-speech tag features.2 We use a hash kernel to map features to weights.It has been observed that most of the computing time in feature-rich parsers is spent retrieving the index of each feature in the weight vector (Bohnet, 2010).This is usually done via a hash table, but significant speedups can be achieved by using a hash kernel, which simply replaces table lookup by a hash function (Bloom, 1970; Shi et al., 2009; Bohnet, 2010).The price to pay for these speedups is that there may be collisions, so that different features are mapped to the same index, but this is often compensated by the fact that the lower time and memory requirements of the hash kernel enables the use of negative features, that is, features that are never seen in the training set but occur in erroneous hypotheses at training time and can therefore be helpful also at inference time.As a result, the hash kernel often improves accuracy as well as efficiency compared to traditional techniques that only make use of features that occur in gold standard parses (Bohnet, 2010).We have evaluated the model for joint tagging and dependency parsing on four typologically diverse languages: Chinese, Czech, English, and German.Most of the experiments use the CoNLL 2009 data sets with the training, development and test split used in the Shared Task (Hajiˇc et al., 2009), but for better comparison with previous work we also report results for the standard benchmark data sets for Chinese and English.For Chinese, this is the Penn Chinese Treebank 5.1 (CTB5), converted with the head-finding rules and conversion tools of Zhang and Clark (2008), and with the same split as in Zhang and Clark (2008) and Li et al. (2011).3 For English, this is the WSJ section of the Penn Treebank, converted with the head-finding rules of Yamada and Matsumoto (2003) and the labeling rules of Nivre (2006).4 In order to assign k-best part-of-speech tags and scores to words in the training set, we used a perceptron tagger with 10-fold jack-knifing.The same type of tagger was trained on the entire training set in order to supply tags for the development and test sets.The feature set of the tagger was optimized for English and German and provides state-of-theart accuracy for these two languages.The 1-best tagging accuracy for section 23 of the Penn Treebank is 97.28, which is on a par with Toutanova et al. (2003).For German, we obtain a tagging accuracy of 97.24, which is close to the 97.39 achieved by the RF-Tagger (Schmid and Laws, 2008), which to our knowledge is the best tagger for German.5 The results are not directly comparable to the RF-Tagger as it was evaluated on a different part of the Tiger Treebank and trained on a larger part of the Treebank.We could not use the larger training set as it contains the test set of the CoNLL 2009 data that we use to evaluate the joint model.For Czech, the 1best tagging accuracy is 99.11 and for Chinese 92.65 on the CoNLL 2009 test set.We trained parsers with 25 iterations and report results for the model obtained after the last iteration.For cluster features, available only for English and German, we used standard Brown clusters based on the English and German Gigaword Corpus.We restricted the vocabulary to words that occur at least 10 times, used 800 clusters, and took cluster prefixes of length 6 to define features.We report the following evaluation metrics: partof-speech accuracy (POS), unlabeled attachment score (UAS), labeled attachment score (LAS), and tagged labeled attachment score (TLAS).TLAS is a new metric defined as the percentage of words that are assigned the correct part-of-speech tag, the correct head and the correct dependency label.In line with previous work, punctuation is included in the evaluation for the CoNLL data sets but excluded for the two benchmark data sets.Table 1 presents results on the development sets of the CoNLL 2009 shared task with varying values of the two tag parameters k (number of candidates) and α (maximum score difference to 1-best tag) and beam parameters fixed at b1 = 40 and b2 = 4.We use the combined TLAS score on the development set to select the optimal settings for each language.For Chinese, we obtain the best result with 3 tags and a threshold of 0.1.6 Compared to the baseline, we observe a POS improvement of 0.60 and a LAS improvement of 0.51.For Czech, we get the best TLAS with k = 3 and α = 0.2, where POS improves by 0.06 and LAS by 0.46.For English, the best setting is k = 2 and α = 0.1 with a POS improvement of 0.17 and a LAS improvement of 0.62.For German, finally, we see the greatest improvement with k = 3 the updated scores later reported due to some improvements of the parser.Rows 3–4: Baseline (k = 1) and best settings for k and α on development set.Rows 5–6: Wider beam (b1 = 80) and added graph features (G) and cluster features (C).Second beam parameter b2 fixed at 4 in all cases. and α = 0.3, where POS improves by 0.66 and LAS by 0.86.Table 2 shows the results on the CoNLL 2009 test sets.For all languages except English, we obtain state-of-the-art results already with bi = 40 (row 4), and for all languages both tagging and parsing accuracy improve compared to the baseline (row 3).The improvement in TLAS is statistically significant with p < 0.01 for all languages (paired t-test).Row 5 shows the scores with a beam of 80 and the additional graph features.Here the LAS scores for Chinese, Czech and German are higher than the best results on the CoNLL 2009 data sets, and the score for English is highly competitive.For Chinese, we achieve 78.51 LAS, which is 1.5 percentage points higher than the reference score, while the POS score is 0.54 higher than our baseline.For Czech, we get 83.73 LAS, which is by far the highest score reported for this data set, together with state-of-the-art POS accuracy.For German, we obtain 89.05 LAS and 97.78 POS, which in both cases is substantially better than in the CoNLL shared task.We believe it is also the highest POS accuracy ever reported for a tagger/parser trained only on the Tiger Treebank.Row 6, finally, presents results with added cluster features for English and German, which results in additional improvements in all metrics.Table 3 gives the results for the Penn Treebank converted with the head-finding rules of Yamada and Matsumoto (2003) and the labeling rules of Nivre (2006).We use k = 3 and α = 0.4, which gave the best results on the development set.The UAS improves by 0.24 when we do joint tagging and parsing.The POS accuracy improves slightly by 0.12 but to a lower degree than for the English CoNLL data where we observed an improvement of 0.20.Nonetheless, the improvement in the joint TLAS score is statistically significant at p < 0.01 (paired t-test).Our joint tagger and dependency parser with graph features gives very competitive unlabeled dependency scores for English with 93.38 UAS.To the best of our knowledge, this is the highest score reported for a (transition-based) dependency parser that does not use additional information sources.By adding cluster features and widening the beam to bi = 80, we achieve 93.67 UAS.We also obtain a POS accuracy of 97.42, which is on a par with the best results obtained using semi-supervised taggers (Søgaard, 2011).Table 4 shows the results for the Chinese Penn Treebank CTB 5.1 together with related work.In experiments with the development set, we could confirm the results from the Chinese CoNLL data set and obtained the best results with the same settings (k = 3, α = 0.1).With bi = 40, UAS improves by 0.25 and POS by 0.30, and the TLAS improvement is again highly significant (p < 0.01, paired t-test).We get the highest UAS, 81.42, with a beam of 80 and added graph features, in which case POS accuracy increases from 92.81 to 93.24.Since our tagger was not optimized for Chinese, we have lower baseline results for the tagger than both Li et al. (2011) and Hatori et al.(2011) but still manage to achieve the highest reported UAS.The speed of the joint tagger and dependency parser is quite reasonable with about 0.4 seconds per sentence on the WSJ-PTB test set, given that we perform tagging and labeled parsing with a beam of 80 while incorporating the features of a third-order graph-based model.Experiments were performed on a computer with an Intel i7-3960X CPU (3.3 GHz and 6 cores).These performance values are preliminary since we are still working on the speed-up of the parser.In order to better understand the benefits of the joint model, we performed an error analysis for German parts of speech in German with F-scores for the left-handside category.ADJ* (ADJD or ADJA) = adjective; ADV = adverb; ART = determiner; APPR = preposition; NE = proper noun; NN = common noun; PRELS = relative pronoun; VVFIN = finite verb; VVINF = non-finite verb; VAFIN = finite auxiliary verb; VAINF = non-finite auxiliary verb; VVPP = participle; XY = not a word.We use α* to denote the set of categories with α as a prefix. and English, where we compared the baseline and the joint model with respect to F-scores for individual part-of-speech categories and dependency labels.For the part-of-speech categories, we found an improvement across the board for both languages, with no category having a significant decrease in F-score, but we also found some interesting patterns for categories that improved more than the average.Table 5 shows selected entries from the confusion matrix for German, where we see substantial improvements for finite and non-finite verbs, which are often morphologically ambiguous but which can be disambiguated using syntactic context.We also see improved accuracies for common and proper nouns, which are both capitalized in standard German orthography and therefore often mistagged, and for relative pronouns, which are less often confused for determiners in the joint model.Table 6 gives a similar snapshot for English, and we again see improvements for verb categories that are often morphologically ambiguous, such as past participles, which can be confused for past tense verbs, and present tense verbs in third person singular, which can be confused for nouns.We also see some improvement for the singular noun categoparts of speech in English with F-scores for the left-handside category.DT = determiner; IN = preposition or subordinating conjunction; JJ = adjective; JJR = comparative adjective; NN = singular or mass noun; NNS = plural noun; POS = possessive clitic; RB = adverb; RBR = comparative adverb; RP = particle; UH = interjection; VB = base form verb; VBD = past tense verb; VBG = gerund or present participle; VBN = past participle; VBP = present tense verb, not 3rd person singular; VBZ = present tense verb, 3rd person singular.We use α* to denote the set of categories with α as a prefix. ry and for adverbs, which are less often confused for prepositions or subordinating conjunctions thanks to the syntactic information in the joint model.For dependency labels, it is hard to extract any striking patterns and it seems that we mainly see an improvement in overall parsing accuracy thanks to less severe tagging errors.However, it is worth observing that, for both English and German, we see significant F-score improvements for the core grammatical functions subject (91.3 —* 92.1 for German, 95.6 —* 96.1 for English) and object (86.9 —* 87.9 for German, 90.2 —* 91.9 for English).Our work is most closely related to Lee et al. (2011), Li et al.(2011) and Hatori et al. (2011), who all present discriminative models for joint tagging and dependency parsing.However, all three models only perform unlabeled parsing, while our model incorporates dependency labels into the parsing process.Whereas Lee et al. (2011) and Li et al.(2011) take a graph-based approach to dependency parsing, Hatori et al. (2011) use a transition-based model similar to ours but limited to projective dependency trees.Both Li et al. (2011) and Hatori et al.(2011) only evaluate their model on Chinese, and of these only Hatori et al. (2011) report consistent improvements in both tagging and parsing accuracy.Like our system, the parser of Lee et al. (2011) can handle nonprojective trees and experimental results are presented for four languages, but their graph-based model is relatively simple and the baselines therefore well below the state of the art.We are thus the first to show consistent improvements in both tagging and (labeled) parsing accuracy across typologically diverse languages at the state-of-the-art level.Moreover, the capacity to handle non-projective dependencies, which is crucial to attain good performance on Czech and German, does not seem to hurt performance on English and Chinese, where the benchmark sets contain only projective trees.The use of beam search in transition-based dependency parsing in order to mitigate the problem of error propagation was first proposed by Johansson and Nugues (2006), although they still used a locally trained model.Globally normalized models were first explored by Titov and Henderson (2007), who were also the first to use a parameterized SHIFT transition like the one found in both Hatori et al. (2011) and our own work, although Titov and Henderson (2007) used it to define a generative model by parameterizing the SHIFT transition by an input word.Zhang and Clark (2008) was the first to combine beam search with a globally normalized discriminative model, using structured perceptron learning and the early update strategy of Collins and Roark (2004), and also explored the addition of graphbased features to a transition-based parser.This approach was further pursued in Zhang and Clark (2011) and was used by Zhang and Nivre (2011) to achieve state-of-the-art results in dependency parsing for both Chinese and English through the addition of rich non-local features.Huang and Sagae (2010) combined structured perceptron learning and beam search with the use of a graph-structured stack to allow ambiguity packing in the beam, a technique that was reused by Hatori et al. (2011).Finally, as noted in the introduction, although joint tagging and parsing is rare in dependency parsing, most state-of-the-art parsers based on PCFG models naturally incorporate part-of-speech tagging and usually achieve better parsing accuracy (albeit not always tagging accuracy) with a joint model than with a pipeline approach (Collins, 1997; Charniak, 2000; Charniak and Johnson, 2005; Petrov et al., 2006).Models that in addition incorporate morphological analysis and segmentation have been explored by Tsarfaty (2006), Cohen and Smith (2007), and Goldberg and Tsarfaty (2008) with special reference to Hebrew parsing.We have presented the first system for joint partof-speech tagging and labeled dependency parsing with non-projective dependency trees.Evaluation on four languages shows consistent improvements in both tagging and parsing accuracy over a pipeline system with state-of-the-art results across the board.The error analysis reveals improvements in tagging accuracy for syntactically central categories, mainly verbs, with improvement in syntactic accuracy for core grammatical functions as a result.In future work we intend to explore joint models that incorporate not only basic part-of-speech tags but also more fine-grained morphological features.
Discriminative Sentence Compression With Soft Syntactic EvidenceWe present a model for sentence compression that uses a discriminative largemargin learning framework coupled with a novel feature set defined on compressed bigrams as well as deep syntactic representations provided by auxiliary dependency and phrase-structure parsers.The parsers are trained out-of-domain and contain a significant amount of noise.We argue that the discriminative nature of the learning algorithm allows the model to learn weights relative to any noise in the feature set to optimize compression accuracy directly.This differs from current state-of-the-art models (Knight and Marcu, 2000) that treat noisy parse trees, for both compressed and uncompressed sentences, as gold standard when calculating model parameters.The ability to compress sentences grammatically with minimal information loss is an important problem in text summarization.Most summarization systems are evaluated on the amount of relevant information retained as well as their compression rate.Thus, returning highly compressed, yet informative, sentences allows summarization systems to return larger sets of sentences and increase the overall amount of information extracted.We focus on the particular instantiation of sentence compression when the goal is to produce the compressed version solely by removing words or phrases from the original, which is the most common setting in the literature (Knight and Marcu, 2000; Riezler et al., 2003; Turner and Charniak, 2005).In this framework, the goal is to find the shortest substring of the original sentence that conveys the most important aspects of the meaning.We will work in a supervised learning setting and assume as input a training set T=(xt,yt)|?| t�1 of original sentences xt and their compressions yt.We use the Ziff-Davis corpus, which is a set of 1087 pairs of sentence/compression pairs.Furthermore, we use the same 32 testing examples from Knight and Marcu (2000) and the rest for training, except that we hold out 20 sentences for the purpose of development.A handful of sentences occur twice but with different compressions.We randomly select a single compression for each unique sentence in order to create an unambiguous training set.Examples from this data set are given in Figure 1.Formally, sentence compression aims to shorten a sentence x = x1 ... xn into a substring y = y1 ... ym, where yi E {x1, ... , xn}.We define the function I(yi) E {1, ... , n} that maps word yi in the compression to the index of the word in the original sentence.Finally we include the constraint I(yi) < I(yi+1), which forces each word in x to occur at most once in the compression y. Compressions are evaluated on three criteria, Typically grammaticality and importance are traded off with compression rate.The longer our compressions, the less likely we are to remove important words or phrases crucial to maintaining grammaticality and the intended meaning.The paper is organized as follows: Section 2 discusses previous approaches to sentence compression.In particular, we discuss the advantages and disadvantages of the models of Knight and Marcu (2000).In Section 3 we present our discriminative large-margin model for sentence compression, including the learning framework and an efficient decoding algorithm for searching the space of compressions.We also show how to extract a rich feature set that includes surfacelevel bigram features of the compressed sentence, dropped words and phrases from the original sentence, and features over noisy dependency and phrase-structure trees for the original sentence.We argue that this rich feature set allows the model to learn which words and phrases should be dropped and which should remain in the compression.Section 4 presents an experimental evaluation of our model compared to the models of Knight and Marcu (2000) and finally Section 5 discusses some areas of future work.Knight and Marcu (2000) first tackled this problem by presenting a generative noisy-channel model and a discriminative tree-to-tree decision tree model.The noisy-channel model defines the problem as finding the compressed sentence with maximum conditional probability P(y) is the source model, which is a PCFG plus bigram language model.P(x|y) is the channel model, the probability that the long sentence is an expansion of the compressed sentence.To calculate the channel model, both the original and compressed versions of every sentence in the training set are assigned a phrase-structure tree.Given a tree for a long sentence x and compressed sentence y, the channel probability is the product of the probability for each transformation required if the tree for y is to expand to the tree for x.The tree-to-tree decision tree model looks to rewrite the tree for x into a tree for y.The model uses a shift-reduce-drop parsing algorithm that starts with the sequence of words in x and the corresponding tree.The algorithm then either shifts (considers new words and subtrees for x), reduces (combines subtrees from x into possibly new tree constructions) or drops (drops words and subtrees from x) on each step of the algorithm.A decision tree model is trained on a set of indicative features for each type of action in the parser.These models are then combined in a greedy global search algorithm to find a single compression.Though both models of Knight and Marcu perform quite well, they do have their shortcomings.The noisy-channel model uses a source model that is trained on uncompressed sentences, even though the source model is meant to represent the probability of compressed sentences.The channel model requires aligned parse trees for both compressed and uncompressed sentences in the training set in order to calculate probability estimates.These parses are provided from a parsing model trained on out-of-domain data (the WSJ), which can result in parse trees with many mistakes for both the original and compressed versions.This makes alignment difficult and the channel probability estimates unreliable as a result.On the other hand, the decision tree model does not rely on the trees to align and instead simply learns a tree-totree transformation model to compress sentences.The primary problem with this model is that most of the model features encode properties related to including or dropping constituents from the tree with no encoding of bigram or trigram surface features to promote grammaticality.As a result, the model will sometimes return very short and ungrammatical compressions.Both models rely heavily on the output of a noisy parser to calculate probability estimates for the compression.We argue in the next section that ideally, parse trees should be treated solely as a source of evidence when making compression decisions to be balanced with other evidence such as that provided by the words themselves.Recently Turner and Charniak (2005) presented supervised and semi-supervised versions of the Knight and Marcu noisy-channel model.The resulting systems typically return informative and grammatical sentences, however, they do so at the cost of compression rate.Riezler et al. (2003) present a discriminative sentence compressor over the output of an LFG parser that is a packed representation of possible compressions.Though this model is highly likely to return grammatical compressions, it required the training data be human annotated with syntactic trees.For the rest of the paper we use x = x1 ... xn to indicate an uncompressed sentence and y = y1 ... ym a compressed version of x, i.e., each yj indicates the position in x of the jth word in the compression.We always pad the sentence with dummy start and end words, x1 = -START- and xn = -END-, which are always included in the compressed version (i.e. y1 = x1 and ym = xn).In this section we described a discriminative online learning approach to sentence compression, the core of which is a decoding algorithm that searches the entire space of compressions.Let the score of a compression y for a sentence x as In particular, we are going to factor this score using a first-order Markov assumption on the words in the compressed sentence Finally, we define the score function to be the dot product between a high dimensional feature representation and a corresponding weight vector Note that this factorization will allow us to define features over two adjacent words in the compression as well as the words in-between that were dropped from the original sentence to create the compression.We will show in Section 3.2 how this factorization also allows us to include features on dropped phrases and subtrees from both a dependency and a phrase-structure parse of the original sentence.Note that these features are meant to capture the same information in both the source and channel models of Knight and Marcu (2000).However, here they are merely treated as evidence for the discriminative learner, which will set the weight of each feature relative to the other (possibly overlapping) features to optimize the models accuracy on the observed data.We define a dynamic programming table C[i] which represents the highest score for any compression that ends at word xi for sentence x.We define a recurrence as follows It is easy to show that C[n] represents the score of the best compression for sentence x (whose length is n) under the first-order score factorization we made.We can show this by induction.If we assume that C[j] is the highest scoring compression that ends at word xj, for all j < i, then C[i] must also be the highest scoring compression ending at word xi since it represents the max combination over all high scoring shorter compressions plus the score of extending the compression to the current word.Thus, since xn is by definition in every compressed version of x (see above), then it must be the case that C[n] stores the score of the best compression.This table can be filled in O(n2).This algorithm is really an extension of Viterbi to the case when scores factor over dynamic substrings of the text (Sarawagi and Cohen, 2004; McDonald et al., 2005a).As such, we can use back-pointers to reconstruct the highest scoring compression as well as k-best decoding algorithms.This decoding algorithm is dynamic with respect to compression rate.That is, the algorithm will return the highest scoring compression regardless of length.This may seem problematic since longer compressions might contribute more to the score (since they contain more bigrams) and thus be preferred.However, in Section 3.2 we define a rich feature set, including features on words dropped from the compression that will help disfavor compressions that drop very few words since this is rarely seen in the training data.In fact, it turns out that our learned compressions have a compression rate very similar to the gold standard.That said, there are some instances when a static compression rate is preferred.A user may specifically want a 25% compression rate for all sentences.This is not a problem for our decoding algorithm.We simply augment the dynamic programming table and calculate C[i][r], which is the score of the best compression of length r that ends at word xi.This table can be filled in as follows Thus, if we require a specific compression rate, we simple determine the number of words r that satisfy this rate and calculate C[n][r].The new complexity is O(n2r).So far we have defined the score of a compression as well as a decoding algorithm that searches the entire space of compressions to find the one with highest score.This all relies on a score factorization over adjacent words in the compression, s(x, I(yj−1), I(yj)) = w · f(x, I(yj−1), I(yj)).In Section 3.3 we describe an online large-margin method for learning w. Here we present the feature representation f(x, I(yj−1), I(yj)) for a pair of adjacent words in the compression.These features were tuned on a development data set.The first set of features are over adjacent words yj−1 and yj in the compression.These include the part-of-speech (POS) bigrams for the pair, the POS of each word individually, and the POS context (bigram and trigram) of the most recent word being added to the compression, yj.These features are meant to indicate likely words to include in the compression as well as some level of grammaticality, e.g., the adjacent POS features “JJ&VB” would get a low weight since we rarely see an adjective followed by a verb.We also add a feature indicating if yj−1 and yj were actually adjacent in the original sentence or not and we conjoin this feature with the above POS features.Note that we have not included any lexical features.We found during experiments on the development data that lexical information was too sparse and led to overfitting, so we rarely include such features.Instead we rely on the accuracy of POS tags to provide enough evidence.Next we added features over every dropped word in the original sentence between yj−1 and yj, if there were any.These include the POS of each dropped word, the POS of the dropped words conjoined with the POS of yj−1 and yj.If the dropped word is a verb, we add a feature indicating the actual verb (this is for common verbs like “is”, which are typically in compressions).Finally we add the POS context (bigram and trigram) of each dropped word.These features represent common characteristics of words that can or should be dropped from the original sentence in the compressed version (e.g. adjectives and adverbs).We also add a feature indicating whether the dropped word is a negation (e.g., not, never, etc.).We also have a set of features to represent brackets in the text, which are common in the data set.The first measures if all the dropped words between yj−1 and yj have a mismatched or inconsistent bracketing.The second measures if the left and right-most dropped words are themselves both brackets.These features come in handy for examples like, The Associated Press ( AP ) reported the story, where the compressed version is The Associated Press reported the story.Information within brackets is often redundant.The previous set of features are meant to encode common POS contexts that are commonly retained or dropped from the original sentence during compression.However, they do so without a larger picture of the function of each word in the sentence.For instance, dropping verbs is not that uncommon - a relative clause for instance may be dropped during compression.However, dropping the main verb in the sentence is uncommon, since that verb and its arguments typically encode most of the information being conveyed.An obvious solution to this problem is to include features over a deep syntactic analysis of the sentence.To do this we parse every sentence twice, once with a dependency parser (McDonald et al., 2005b) and once with a phrase-structure parser (Charniak, 2000).These parsers have been trained out-of-domain on the Penn WSJ Treebank and as a result contain noise.However, we are merely going to use them as an additional source of features.We call this soft syntactic evidence since the deep trees are not used as a strict goldstandard in our model but just as more evidence for or against particular compressions.The learning algorithm will set the feature weight accordingly depending on each features discriminative power.It is not unique to use soft syntactic features in this way, as it has been done for many problems in language processing.However, we stress this aspect of our model due to the history of compression systems using syntax to provide hard structural constraints on the output.Lets consider the sentence x = Mary saw Ralph on Tuesday after lunch, with corresponding parses given in Figure 2.In particular, lets consider the feature representation f(x,3,6).That is, the feature representation of making Ralph and after adjacent in the compression and dropping the prepositional phrase on Tuesday.The first set of features we consider are over dependency trees.For every dropped word we add a feature indicating the POS of the words parent in the tree.For example, if the dropped words parent is root, then it typically means it is the main verb of the sentence and unlikely to be dropped.We also add a conjunction feature of the POS tag of the word being dropped and the POS of its parent as well as a feature indicating for each word being dropped whether it is a leaf node in the tree.We also add the same features for the two adjacent words, but indicating that they are part of the compression.For the phrase-structure features we find every node in the tree that subsumes a piece of dropped text and is not a child of a similar node.In this case the PP governing on Tuesday.We then add features indicating the context from which this node was dropped.For example we add a feature specifying that a PP was dropped which was the child of a VP.We also add a feature indicating that a PP was dropped which was the left sibling of another PP, etc.Ideally, for each production in the tree we would like to add a feature indicating every node that was dropped, e.g.“VP→VBD NP PP PP ⇒ VP→VBD NP PP”.However, we cannot necessarily calculate this feature since the extent of the production might be well beyond the local context of first-order feature factorization.Furthermore, since the training set is so small, these features are likely to be observed very few times.In this section we have described a rich feature set over adjacent words in the compressed sentence, dropped words and phrases from the original sentence, and properties of deep syntactic trees of the original sentence.Note that these features in many ways mimic the information already present in the noisy-channel and decision-tree models of Knight and Marcu (2000).Our bigram features encode properties that indicate both good and bad words to be adjacent in the compressed sentence.This is similar in purpose to the source model from the noisy-channel system.However, in that system, the source model is trained on uncompressed sentences and thus is not as representative of likely bigram features for compressed sentences, which is really what we desire.Our feature set also encodes dropped words and phrases through the properties of the words themselves and through properties of their syntactic relation to the rest of the sentence in a parse tree.These features represent likely phrases to be dropped in the compression and are thus similar in nature to the channel model in the noisy-channel system as well as the features in the tree-to-tree decision tree system.However, we use these syntactic constraints as soft evidence in our model.That is, they represent just another layer of evidence to be considered during training when setting parameters.Thus, if the parses have too much noise, the learning algorithm can lower the weight of the parse features since they are unlikely to be useful discriminators on the training data.This differs from the models of Knight and Marcu (2000), which treat the noisy parses as gold-standard when calculating probability estimates.An important distinction we should make is the notion of supported versus unsupported features (Sha and Pereira, 2003).Supported features are those that are on for the gold standard compressions in the training.For instance, the bigram feature “NN&VB” will be supported since there is most likely a compression that contains a adjacent noun and verb.However, the feature “JJ&VB” will not be supported since an adjacent adjective and verb most likely will not be observed in any valid compression.Our model includes all features, including those that are unsupported.The advantage of this is that the model can learn negative weights for features that are indicative of bad compressions.This is not difficult to do since most features are POS based and the feature set size even with all these features is only 78,923.Having defined a feature encoding and decoding algorithm, the last step is to learn the feature weights w. We do this using the Margin Infused Relaxed Algorithm (MIRA), which is a discriminative large-margin online learning technique shown in Figure 3 (Crammer and Singer, 2003).On each iteration, MIRA considers a single instance from the training set (xt, yt) and updates the weights so that the score of the correct compression, yt, is greater than the score of all other compressions by a margin proportional to their loss.Many weight vectors will satisfy these constraints so we pick the one with minimum change from the previous setting.We define the loss to be the number of words falsely retained or dropped in the incorrect compression relative to the correct one.For instance, if the correct compression of the sentence in Figure 2 is Mary saw Ralph, then the compression Mary saw after lunch would have a loss of 3 since it incorrectly left out one word and included two others.Of course, for a sentence there are exponentially many possible compressions, which means that this optimization will have exponentially many constraints.We follow the method of McDonald et al. (2005b) and create constraints only on the k compressions that currently have the highest score, bestk(x; w).This can easily be calculated by extending the decoding algorithm with standard Viterbi k-best techniques.On the development data, we found that k = 10 provided the best performance, though varying k did not have a major impact overall.Furthermore we found that after only 3-5 training epochs performance on the development data was maximized.The final weight vector is the average of all weight vectors throughout training.Averaging has been shown to reduce overfitting (Collins, 2002) as well as reliance on the order of the examples during training.We found it to be particularly important for this data set.We use the same experimental methodology as Knight and Marcu (2000).We provide every compression to four judges and ask them to evaluate each one for grammaticality and importance on a scale from 1 to 5.For each of the 32 sentences in our test set we ask the judges to evaluate three systems: human annotated, the decision tree model of Knight and Marcu (2000) and our system.The judges were told all three compressions were automatically generated and the order in which they were presented was randomly chosen for each sentence.We compared our system to the decision tree model of Knight and Marcu instead of the noisy-channel model since both performed nearly as well in their evaluation, and the compression rate of the decision tree model is nearer to our system (around 57-58%).The noisy-channel model typically returned longer compressions.Results are shown in Table 1.We present the average score over all judges as well as the standard deviation.The evaluation for the decision tree system of Knight and Marcu is strikingly similar to the original evaluation in their work.This provides strong evidence that the evaluation criteria in both cases were very similar.Table 1 shows that all models had similar compressions rates, with humans preferring to compress a little more aggressively.Not surprisingly, the human compressions are practically all grammatical.A quick scan of the evaluations shows that the few ungrammatical human compressions were for sentences that were not really grammatical in the first place.Of greater interest is that the compressions of our system are typically more grammatical than the decision tree model of Knight and Marcu.When looking at importance, we see that our system actually does the best – even better than humans.The most likely reason for this is that our model returns longer sentences and is thus less likely to prune away important information.For example, consider the sentence The chemical etching process used for glare protection is effective and will help if your office has the fluorescent-light overkill that’s typical in offices The human compression was Glare protection is effective, whereas our model compressed the sentence to The chemical etching process used for glare protection is effective.A primary reason that our model does better than the decision tree model of Knight and Marcu is that on a handful of sentences, the decision tree compressions were a single word or noun-phrase.For such sentences the evaluators typically rated the compression a 1 for both grammaticality and importance.In contrast, our model never failed in such drastic ways and always output something reasonable.This is quantified in the standard deviation of the two systems.Though these results are promising, more large scale experiments are required to really ascertain the significance of the performance increase.Ideally we could sample multiple training/testing splits and use all sentences in the data set to evaluate the systems.However, since these systems require human evaluation we did not have the time or the resources to conduct these experiments.Here we aim to give the reader a flavor of some common outputs from the different models.Three examples are given in Table 4.1.The first shows two properties.First of all, the decision tree model completely breaks and just returns a single noun-phrase.Our system performs well, however it leaves out the complementizer of the relative clause.This actually occurred in a few examples and appears to be the most common problem of our model.A post-processing rule should eliminate this.The second example displays a case in which our system and the human system are grammatical, but the removal of a prepositional phrase hurts the resulting meaning of the sentence.In fact, without the knowledge that the sentence is referring to broadband, the compressions are meaningless.This appears to be a harder problem – determining which prepositional phrases can be dropped and which cannot.The final, and more interesting, example presents two very different compressions by the human and our automatic system.Here, the human kept the relative clause relating what languages the source code is available in, but dropped the main verb phrase of the sentence.Our model preferred to retain the main verb phrase and drop the relative clause.This is most likely due to the fact that dropping the main verb phrase of a sentence is much less likely in the training data than dropping a relative clause.Two out of four evaluators preferred the compression returned by our system and the other two rated them equal.In this paper we have described a new system for sentence compression.This system uses discriminative large-margin learning techniques coupled with a decoding algorithm that searches the space of all compressions.In addition we defined a rich feature set of bigrams in the compression and dropped words and phrases from the original sentence.The model also incorporates soft syntactic evidence in the form of features over dependency and phrase-structure trees for each sentence.This system has many advantages over previous approaches.First of all its discriminative nature allows us to use a rich dependent feature set and to optimize a function directly related to compresThe fi rst new product, ATF Protype , is a line of digital postscript typefaces that will be sold in packages of up to six fonts.ATF Protype is a line of digital postscript typefaces that will be sold in packages of up to six fonts .The fi rst new product .ATF Protype is a line of digital postscript typefaces will be sold in packages of up to six fonts .Finally, another advantage of broadband is distance.Another advantage is distance.Another advantage of broadband is distance.Another advantage is distance.The source code , which is available for C , Fortran , ADA and VHDL , can be compiled and executed on the same system or ported to other target platforms .The source code is available for C , Fortran , ADA and VHDL .The source code is available for C .The source code can be compiled and executed on the same system or ported to other target platforms . sion accuracy during training, both of which have been shown to be beneficial for other problems.Furthermore, the system does not rely on the syntactic parses of the sentences to calculate probability estimates.Instead, this information is incorporated as just another form of evidence to be considered during training.This is advantageous because these parses are trained on out-of-domain data and often contain a significant amount of noise.A fundamental flaw with all sentence compression systems is that model parameters are set with the assumption that there is a single correct answer for each sentence.Of course, like most compression and translation tasks, this is not true, consider, TapeWare , which supports DOS and NetWare 286 , is a value-added process that lets you directly connect the QA150-EXAT to a file server and issue a command from any workstation to back up the server The human annotated compression is, TapeWare supports DOS and NetWare 286.However, another completely valid compression might be, TapeWare lets you connect the QA150-EXAT to a fi le server.These two compressions overlap by a single word.Our learning algorithm may unnecessarily lower the score of some perfectly valid compressions just because they were not the exact compression chosen by the human annotator.A possible direction of research is to investigate multilabel learning techniques for structured data (McDonald et al., 2005a) that learn a scoring function separating a set of valid answers from all invalid answers.Thus if a sentence has multiple valid compressions we can learn to score each valid one higher than all invalid compressions during training to avoid this problem.The author would like to thank Daniel Marcu for providing the data as well as the output of his and Kevin Knight’s systems.Thanks also to Hal Daum´e and Fernando Pereira for useful discussions.Finally, the author thanks the four reviewers for evaluating the compressed sentences.This work was supported by NSF ITR grants 0205448 and 0428193.
Personalizing PageRank for Word Sense DisambiguationIn this paper we propose a new graphbased method that uses the knowledge in a LKB (based on WordNet) in order to perform unsupervised Word Sense Disambiguation.Our algorithm uses the full graph of the LKB efficiently, performing better than previous approaches in English all-words datasets.We also show that the algorithm can be easily ported to other languages with good results, with the only requirement of having a wordnet.In addition, we make an analysis of the performance of the algorithm, showing that it is efficient and that it could be tuned to be faster.Word Sense Disambiguation (WSD) is a key enabling-technology that automatically chooses the intended sense of a word in context.Supervised WSD systems are the best performing in public evaluations (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007) but they need large amounts of hand-tagged data, which is typically very expensive to build.Given the relatively small amount of training data available, current state-of-the-art systems only beat the simple most frequent sense (MFS) baseline1 by a small margin.As an alternative to supervised systems, knowledge-based WSD systems exploit the information present in a lexical knowledge base (LKB) to perform WSD, without using any further corpus evidence.Traditional knowledge-based WSD systems assign a sense to an ambiguous word by comparing each of its senses with those of the surrounding context.Typically, some semantic similarity metric is used for calculating the relatedness among senses (Lesk, 1986; McCarthy et al., 2004).One of the major drawbacks of these approaches stems from the fact that senses are compared in a pairwise fashion and thus the number of computations can grow exponentially with the number of words.Although alternatives like simulated annealing (Cowie et al., 1992) and conceptual density (Agirre and Rigau, 1996) were tried, most of past knowledge based WSD was done in a suboptimal word-by-word process, i.e., disambiguating words one at a time.Recently, graph-based methods for knowledgebased WSD have gained much attention in the NLP community (Sinha and Mihalcea, 2007; Navigli and Lapata, 2007; Mihalcea, 2005; Agirre and Soroa, 2008).These methods use well-known graph-based techniques to find and exploit the structural properties of the graph underlying a particular LKB.Because the graph is analyzed as a whole, these techniques have the remarkable property of being able to find globally optimal solutions, given the relations between entities.Graphbased WSD methods are particularly suited for disambiguating word sequences, and they manage to exploit the interrelations among the senses in the given context.In this sense, they provide a principled solution to the exponential explosion problem, with excellent performance.Graph-based WSD is performed over a graph composed by senses (nodes) and relations between pairs of senses (edges).The relations may be of several types (lexico-semantic, coocurrence relations, etc.) and may have some weight attached to them.The disambiguation is typically performed by applying a ranking algorithm over the graph, and then assigning the concepts with highest rank to the corresponding words.Given the computational cost of using large graphs like WordNet, many researchers use smaller subgraphs built online for each target context.In this paper we present a novel graph-based WSD algorithm which uses the full graph of WordNet efficiently, performing significantly better that previously published approaches in English all-words datasets.We also show that the algorithm can be easily ported to other languages with good results, with the only requirement of having a wordnet.The algorithm is publicly available2 and can be applied easily to sense inventories and knowledge bases different from WordNet.Our analysis shows that our algorithm is efficient compared to previously proposed alternatives, and that a good choice of WordNet versions and relations is fundamental for good performance.The paper is structured as follows.We first describe the PageRank and Personalized PageRank algorithms.Section 3 introduces the graph based methods used for WSD.Section 4 shows the experimental setting and the main results, and Section 5 compares our methods with related experiments on graph-based WSD systems.Section 6 shows the results of the method when applied to a Spanish dataset.Section 7 analyzes the performance of the algorithm.Finally, we draw some conclusions in Section 8.The celebrated PageRank algorithm (Brin and Page, 1998) is a method for ranking the vertices in a graph according to their relative structural importance.The main idea of PageRank is that whenever a link from vi to vj exists in a graph, a vote from node i to node j is produced, and hence the rank of node j increases.Besides, the strength of the vote from i to j also depends on the rank of node i: the more important node i is, the more strength its votes will have.Alternatively, PageRank can also be viewed as the result of a random walk process, where the final rank of node i represents the probability of a random walk over the graph ending on node i, at a sufficiently large time.Let G be a graph with N vertices vi, ... , vN and di be the outdegree of node i; let M be a In the equation, v is a N × 1 vector whose elements are 1N and c is the so called damping factor, a scalar value between 0 and 1.The first term of the sum on the equation models the voting scheme described in the beginning of the section.The second term represents, loosely speaking, the probability of a surfer randomly jumping to any node, e.g. without following any paths on the graph.The damping factor, usually set in the [0.85..0.95] range, models the way in which these two terms are combined at each step.The second term on Eq.(1) can also be seen as a smoothing factor that makes any graph fulfill the property of being aperiodic and irreducible, and thus guarantees that PageRank calculation converges to a unique stationary distribution.In the traditional PageRank formulation the vector v is a stochastic normalized vector whose element values are all 1N, thus assigning equal probabilities to all nodes in the graph in case of random jumps.However, as pointed out by (Haveliwala, 2002), the vector v can be non-uniform and assign stronger probabilities to certain kinds of nodes, effectively biasing the resulting PageRank vector to prefer these nodes.For example, if we concentrate all the probability mass on a unique node i, all random jumps on the walk will return to i and thus its rank will be high; moreover, the high rank of i will make all the nodes in its vicinity also receive a high rank.Thus, the importance of node i given by the initial distribution of v spreads along the graph on successive iterations of the algorithm.In this paper, we will use traditional PageRank to refer to the case when a uniform v vector is used in Eq.(1); and whenever a modified v is used, we will call it Personalized PageRank.The next section shows how we define a modified v. PageRank is actually calculated by applying an iterative algorithm which computes Eq.(1) successively until convergence below a given threshold is achieved, or, more typically, until a fixed number of iterations are executed.Regarding PageRank implementation details, we chose a damping value of 0.85 and finish the calculation after 30 iterations.We did not try other damping factors.Some preliminary experiments with higher iteration counts showed that although sometimes the node ranks varied, the relative order among particular word synsets remained stable after the initial iterations (cf.Section 7 for further details).Note that, in order to discard the effect of dangling nodes (i.e. nodes without outlinks) we slightly modified Eq.(1).For the sake of brevity we omit the details, which the interested reader can check in (Langville and Meyer, 2003).In this section we present the application of PageRank to WSD.If we were to apply the traditional PageRank over the whole WordNet we would get a context-independent ranking of word senses, which is not what we want.Given an input piece of text (typically one sentence, or a small set of contiguous sentences), we want to disambiguate all open-class words in the input taken the rest as context.In this framework, we need to rank the senses of the target words according to the other words in the context.Theare two main alternatives to achieve this: The first method has been explored in the literature (cf.Section 5), and we also presented a variant in (Agirre and Soroa, 2008) but the second method is novel in WSD.In both cases, the algorithms return a list of ranked senses for each target word in the context.We will see each of them in turn, but first we will present some notation and a preliminary step.A LKB is formed by a set of concepts and relations among them, and a dictionary, i.e., a list of words (typically, word lemmas) each of them linked to at least one concept of the LKB.Given any such LKB, we build an undirected graph G = (V, E) where nodes represent LKB concepts (vi), and each relation between concepts vi and vj is represented by an undirected edge ei,j.In our experiments we have tried our algorithms using three different LKBs: Given an input text, we extract the list Wi i = 1... m of content words (i.e. nouns, verbs, adjectives and adverbs) which have an entry in the dictionary, and thus can be related to LKB concepts.Let Conceptsi = {v1, ... , vi.} be the im associated concepts of word Wi in the LKB graph.Note that monosemous words will be related to just one concept, whereas polysemous words may be attached to several.As a result of the disambiguation process, every concept in Conceptsi, i = 1, ... , m receives a score.Then, for each target word to be disambiguated, we just choose its associated concept in G with maximal score.In our experiments we build a context of at least 20 content words for each sentence to be disambiguated, taking the sentences immediately before and after it in the case that the original sentence was too short.We follow the algorithm presented in (Agirre and Soroa, 2008), which we explain here for completeness.The main idea of the subgraph method is to extract the subgraph of GKB whose vertices and relations are particularly relevant for a given input context.Such a subgraph is called a “disambiguation subgraph” GD, and it is built in the following way.For each word Wi in the input context and each concept vi E Conceptsi, a standard breathfirst search (BFS) over GKB is performed, starting at node vi.Each run of the BFS calculates the minimum distance paths between vi and the rest of concepts of GKB .In particular, we are interested in the minimum distance paths between vi and the concepts associated to the rest of the words in the context, vj E Uj=,4i Conceptsj.Let mdpvi be the set of these shortest paths.This BFS computation is repeated for every concept of every word in the input context, storing mdpvi accordingly.At the end, we obtain a set of minimum length paths each of them having a different concept as a source.The disambiguation graph GD is then just the union of the vertices and edges of the shortest paths, GD = Umi=1{mdpv,/vj E Conceptsi}.The disambiguation graph GD is thus a subgraph of the original GKB graph obtained by computing the shortest paths between the concepts of the words co-occurring in the context.Thus, we hypothesize that it captures the most relevant concepts and relations in the knowledge base for the particular input context.Once the GD graph is built, we compute the traditional PageRank algorithm over it.The intuition behind this step is that the vertices representing the correct concepts will be more relevant in GD than the rest of the possible concepts of the context words, which should have less relations on average and be more isolated.As usual, the disambiguation step is performed by assigning to each word Wi the associated concept in Conceptsi which has maximum rank.In case of ties we assign all the concepts with maximum rank.Note that the standard evaluation script provided in the Senseval competitions treats multiple senses as if one was chosen at random, i.e. for evaluation purposes our method is equivalent to breaking ties at random.As mentioned before, personalized PageRank allows us to use the full LKB.We first insert the context words into the graph G as nodes, and link them with directed edges to their respective concepts.Then, we compute the personalized PageRank of the graph G by concentrating the initial probability mass uniformly over the newly introduced word nodes.As the words are linked to the concepts by directed edges, they act as source nodes injecting mass into the concepts they are associated with, which thus become relevant nodes, and spread their mass over the LKB graph.Therefore, the resulting personalized PageRank vector can be seen as a measure of the structural relevance of LKB concepts in the presence of the input context.One problem with Personalized PageRank is that if one of the target words has two senses which are related by semantic relations, those senses reinforce each other, and could thus dampen the effect of the other senses in the context.With this observation in mind we devised a variant (dubbed Ppr w2w), where we build the graph for each target word in the context: for each target word Wi, we concentrate the initial probability mass in the senses of the words surrounding Wi, but not in the senses of the target word itself, so that context words increase its relative importance in the graph.The main idea of this approach is to avoid biasing the initial score of concepts associated to target word Wi, and let the surrounding words decide which concept associated to Wi has more relevance.Contrary to the other two approaches, Ppr w2w does not disambiguate all target words of the context in a single run, which makes it less efficient (cf.Section 7).In this paper we will use two datasets for comparing graph-based WSD methods, namely, the Senseval-2 (S2AW) and Senseval-3 (S3AW) all words datasets (Snyder and Palmer, 2004; Palmer et al., 2001), which are both labeled with WordNet 1.7 tags.We did not use the Semeval dataset, for the sake of comparing our results to related work, none of which used Semeval data.Table 1 shows the results as recall of the graph-based WSD system over these datasets on the different LKBs.We detail overall results, as well as results per PoS, and the confidence interval for the overall results.The interval was computed using bootstrap resampling with 95% confidence.The table shows that Ppr w2w is consistently the best method in both datasets and for all LKBs.Ppr and Spr obtain comparable results, which is remarkable, given the simplicity of the Ppr algobaseline and the best results of supervised systems at competition time (SMUaw,GAMBL). rithm, compared to the more elaborate algorithm to construct the graph.The differences between methods are not statistically significant, which is a common problem on this relatively small datasets (Snyder and Palmer, 2004; Palmer et al., 2001).Regarding LKBs, the best results are obtained using WordNet 1.7 and eXtended WordNet.Here the differences are in many cases significant.These results are surprising, as we would expect that the manually disambiguated gloss relations from WordNet 3.0 would lead to better results, compared to the automatically disambiguated gloss relations from the eXtended WordNet (linked to version 1.7).The lower performance of WNet30+gloss can be due to the fact that the Senseval all words data set is tagged using WordNet 1.7 synsets.When using a different LKB for WSD, a mapping to WordNet 1.7 is required.Although the mapping is cited as having a correctness on the high 90s (Daude et al., 2000), it could have introduced sufficient noise to counteract the benefits of the hand-disambiguated glosses.Table 1 also shows the most frequent sense (MFS), as well as the best supervised systems (Snyder and Palmer, 2004; Palmer et al., 2001) that participated in each competition (SMUaw and GAMBL, respectively).The MFS is a baseline for supervised systems, but it is considered a difficult competitor for unsupervised systems, which rarely come close to it.In this case the MFS baseline was computed using previously availabel training data like SemCor.Our best results are close to the MFS in both Senseval-2 and Senseval-3 datasets.The results for the supervised system are given for reference, and we can see that the gap is relatively small, specially for Senseval3.In this section we will briefly describe some graph-based methods for knowledge-based WSD.The methods here presented cope with the problem of sequence-labeling, i.e., they disambiguate all the words coocurring in a sequence (typically, all content words of a sentence).All the methods rely on the information represented on some LKB, which typically is some version of WordNet, sometimes enriched with proprietary relations.The results on our datasets, when available, are shown in Table 2.The table also shows the performance of supervised systems.The TexRank algorithm (Mihalcea, 2005) for WSD creates a complete weighted graph (e.g. a graph where every pair of distinct vertices is connected by a weighted edge) formed by the synsets of the words in the input context.The weight of the links joining two synsets is calculated by executing Lesk’s algorithm (Lesk, 1986) between them, i.e., by calculating the overlap between the words in the glosses of the correspongind senses.Once the complete graph is built, the PageRank algorithm is executed over it and words are assigned to the most relevant synset.In this sense, PageRank is used an alternative to simulated annealing to find the optimal pairwise combinations.The method was evaluated on the Senseval-3 dataset, as shown in row Mih05 on Table 2.(Sinha and Mihalcea, 2007) extends their previous work by using a collection of semantic similarity measures when assigning a weight to the links across synsets.They also compare different graph-based centrality algorithms to rank the vertices of the complete graph.They use different similarity metrics for different POS types and a voting scheme among the centrality algorithm ranks.Here, the Senseval-3 corpus was used as a development data set, and we can thus see those results as the upper-bound of their method.We can see in Table 2 that the methods presented in this paper clearly outperform both Mih05 and Sin07.This result suggests that analyzing the LKB structure as a whole is preferable than computing pairwise similarity measures over synsets.The results of various in-house made experiments replicating (Mihalcea, 2005) also confirm this observation.Note also that our methods are simpler than the combination strategy used in (Sinha and Mihalcea, 2007), and that we did not perform any parameter tuning as they did.In (Navigli and Velardi, 2005) the authors develop a knowledge-based WSD method based on lexical chains called structural semantic interconnections (SSI).Although the system was first designed to find the meaning of the words in WordNet glosses, the authors also apply the method for labeling text sequences.Given a text sequence, SSI first identifies monosemous words and assigns the corresponding synset to them.Then, it iteratively disambiguates the rest of terms by selecting the senses that get the strongest interconnection with the synsets selected so far.The interconnection is calculated by searching for paths on the LKB, constrained by some hand-made rules of possible semantic patterns.The method was evaluated on the Senseval-3 dataset, as shown in row Nav05 on Table 2.Note that the method labels an instance with the most frequent sense of the word if the algorithm produces no output for that instance, which makes comparison to our system unfair, specially given the fact that the MFS performs better than SSI.In fact it is not possible to separate the effect of SSI from that of the MFS.For this reason we place this method close to the MFS baseline in Table 2.In (Navigli and Lapata, 2007), the authors perform a two-stage process for WSD.Given an input context, the method first explores the whole LKB in order to find a subgraph which is particularly relevant for the words of the context.Then, they study different graph-based centrality algorithms for deciding the relevance of the nodes on the subgraph.As a result, every word of the context is attached to the highest ranking concept among its possible senses.The Spr method is very similar to (Navigli and Lapata, 2007), the main difference lying on the initial method for extracting the context subgraph.Whereas (Navigli and Lapata, 2007) apply a depth-first search algorithm over the LKB graph —and restrict the depth of the subtree to a value of 3—, Spr relies on shortest paths between word synsets.Navigli and Lapata don’t report overall results and therefore, we can’t directly compare our results with theirs.However, we can see that on a PoS-basis evaluation our results are consistently better for nouns and verbs (especially the Ppr w2w method) and rather similar for adjectives.(Tsatsaronis et al., 2007) is another example of a two-stage process, the first one consisting on finding a relevant subgraph by performing a BFS dataset, including MFS and the best supervised system in the competition. search over the LKB.The authors apply a spreading activation algorithm over the subgraph for node ranking.Edges of the subgraph are weighted according to its type, following a tf.idf like approach.The results show that our methods clearly outperform Tsatsa07.The fact that the Spr method works better suggests that the traditional PageRank algorithm is a superior method for ranking the subgraph nodes.As stated before, all methods presented here use some LKB for performing WSD.(Mihalcea, 2005) and (Sinha and Mihalcea, 2007) use WordNet relations as a knowledge source, but neither of them specify which particular version did they use.(Tsatsaronis et al., 2007) uses WordNet 1.7 enriched with eXtended WordNet relations, just as we do.Both (Navigli and Velardi, 2005; Navigli and Lapata, 2007) use WordNet 2.0 as the underlying LKB, albeit enriched with several new relations, which are manually created.Unfortunately, those manual relations are not publicly available, so we can’t directly compare their results with the rest of the methods.In (Agirre and Soroa, 2008) we experiment with different LKBs formed by combining relations of different MCR versions along with relations extracted from SemCor, which we call supervised and unsupervised relations, respectively.The unsupervised relations that yielded bests results are also used in this paper (c.f Section 3.1).Our WSD algorithm can be applied over nonenglish texts, provided that a LKB for this particular language exists.We have tested the graphalgorithms proposed in this paper on a Spanish dataset, using the Spanish WordNet as knowledge source (Atserias et al., 2004a).We used the Semeval-2007 Task 09 dataset as evaluation gold standard (M`arquez et al., 2007).The dataset contains examples of the 150 most frequent nouns in the CESS-ECE corpus, manually annotated with Spanish WordNet synsets.It is split into a train and test part, and has an “all words” shape i.e. input consists on sentences, each one having at least one occurrence of a target noun.We ran the experiment over the test part (792 instances), and used the train part for calculating the MFS baseline.We used the Spanish WordNet as LKB, enriched with eXtended WordNet relations.It contains 105, 501 nodes and 623,316 relations.The results in Table 3 are consistent with those for English, with our algorithm approaching MFS performance.Note that for this dataset the supervised algorithm could barely improve over the MFS, suggesting that for this particular dataset MFS is particularly strong.Table 4 shows the time spent by the different algorithms when applied to the Senseval-2 all words dataset, using the WNet17 + Xwn as LKB.The dataset consists on 2473 word instances appearing on 476 different sentences.The experiments were done on a computer with four 2.66 Ghz processors and 16 Gb memory.The table shows that the time elapsed by the algorithms varies between 30 minutes for the Ppr method (which thus disambiguates circa 82 instances per minute) to almost 3 hours spent by the Ppr w2w method (circa 15 instances per minute).The Spr method lies in between, requiring 2 hours for completing the task, but its overall performance is well below the PageRank based Ppr w2w method.Note that the algorithm is coded in C++ for greater efficiency, and uses the Boost Graph Library.Regarding PageRank calculation, we have tried different numbers of iterations, and analyze the rate of convergence of the algorithm.Figure 1 depicts the performance of the Ppr w2w method for different iterations of the algorithm.As before, the algorithm is applied over the MCR17 + Xwn LKB, and evaluated on the Senseval-2 all words dataset.The algorithm converges very quickly: one sole iteration suffices for achieving a relatively high performance, and 20 iterations are enough for achieving convergence.The figure shows that, depending on the LKB complexity, the user can tune the algorithm and lower the number of iterations, thus considerably reducing the time required for disambiguation.In this paper we propose a new graph-based method that uses the knowledge in a LKB (based on WordNet) in order to perform unsupervised Word Sense Disambuation.Our algorithm uses the full graph of the LKB efficiently, performing better than previous approaches in English all-words datasets.We also show that the algorithm can be easily ported to other languages with good results, with the only requirement of having a wordnet.Both for Spanish and English the algorithm attains performances close to the MFS.The algorithm is publicly available5 and can be applied easily to sense inventories and knowledge bases different from WordNet.Our analysis shows that our algorithm is efficient compared to previously proposed alternatives, and that a good choice of WordNet versions and relations is fundamental for good performance.This work has been partially funded by the EU Commission (project KYOTO ICT-2007-211423) and Spanish Research Department (project KNOW TIN2006-15049-C03-01).
An Efficient Method For Determining Bilingual Word ClassesIn statistical natural language processing we always face the problem of sparse data.One way to reduce this problem is to group words into equivalence classes which is a standard method in statistical language modeling.In this paper we describe a method to determine bilingual word classes suitable for statistical machine translation.We develop an optimization criterion based on a maximumlikelihood approach and describe a clustering algorithm.We will show that the usage of the bilingual word classes we get can improve statistical machine translation.Word classes are often used in language modelling to solve the problem of sparse data.Various clustering techniques have been proposed (Brown et al., 1992; Jardino and Adda, 1993; Martin et al., 1998) which perform automatic word clustering optimizing a maximum-likelihood criterion with iterative clustering algorithms.In the field of statistical machine translation we also face the problem of sparse data.Our aim is to use word classes in statistical machine translation to allow for more robust statistical translation models.A naive approach for doing this would be the use of mono-lingually optimized word classes in source and target language.Unfortunately we can not expect these independently optimized classes to be correspondent.Therefore mono-lingually optimized word classes do not seem to be useful for machine translation (see also (Fung and Wu, 1995)).We define bilingual word clustering as the process of forming corresponding word classes suitable for machine translation purposes for a pair of languages using a parallel training corpus.The described method to determine bilingual word classes is an extension and improvement of the method mentioned in (Och and Weber, 1998).Our approach is simpler and computationally more efficient than (Wang et al., 1996).The task of a statistical language model is to estimate the probability Pr(wiev) of a sequence of words wiv = wi 'WN.A simple approximation of Pr(41) is to model it as a product of bigram probabilities: Pr (wPI) = HiN_, p(wi *2_1).If we want to estimate the bigram probabilities p(wlw') using a realistic natural language corpus we are faced with the problem that most of the bigrams are rarely seen.One possibility to solve this problem is to partition the set of all words into equivalence classes.The function C maps words w to their classes C(w).Rewriting the corpus probability using classes we arrive at the following probability model p(wiv IC): In this model we have two types of probabilities: the transition probability p(C1C1) for class C given its predecessor class C' and the membership probability p(wIC) for word w given class C. To determine the optimal classes C for a given number of classes M we perform a maximumlikelihood approach: = arg mrc p(wiv IC) (2) We estimate the probabilities of Eq.(1) by relative frequencies: p(CIC&quot;) := n(C1C1')In(C'), p(wIC) = n(w)In(C).The function n(-) provides the frequency of a uni- or bigram in the training corpus.If we insert this into Eq.(2) and apply the negative logarithm and change the summation order we arrive at the following optimization Proceedings of EACL '99 criterion LP,.(Kneser and Ney, 1991): The function h(n) is a shortcut for n • log(n).It is necessary to fix the number of classes in C in advance as the optimum is reached if every word is a class of its own.Because of this it is necessary to perform an additional optimization process which determines the number of classes.The use of leaving-one-out in a modified optimization criterion as in (Kneser and Ney, 1993) could in principle solve this problem.An efficient optimization algorithm for LPI is described in section 4.In bilingual word clustering we are interested in classes F and E which form partitions of the vocabulary of two languages.To perform bilingual word clustering we use a maximum-likelihood approach as in the monolingual case.We maximize the joint probability of a bilingual training corpus To perform the maximization of Eq.(6) we have to model the monolingual a priori probability p(ef1E) and the translation probability p(f lel; e, F).For the first we use the class-based bigram probability from Eq.(1).To model p(fillef; E, .7) we assume the existence of an alignment af.We assume that every word fj is produced by the word ea, at position a3 in the training corpus with the probability P(filea,): The word alignment ail is trained automatically using statistical translation models as described in (Brown et al., 1993; Vogel et al., 1996).The idea is to introduce the unknown alignment a as hidden variable into a statistical model of the translation probability p(glef).By applying the EMalgorithm we obtain the model parameters.The alignment cif that we use is the Viterbi-Alignment of an HMM alignment model similar to (Vogel et al., 1996).By rewriting the translation probability using word classes, we obtain (corresponding to Eq.(1)): (8) The variables F and E denote special classes in and E. We use relative frequencies to estimate p(FIE) and p(fIF): The function nt(FIE) counts how often the words in class F are aligned to words in class E. If we insert these relative frequencies into Eq.(8) and apply the same transformations as in the monolingual case we obtain a similar optimization criterion for the translation probability part of Eq.(6).Thus the full optimization criterion for bilingual word classes is: The two count functions n(E1E) and nt(FIE) can be combined into one count function ng (X1Y) n(X1Y) + nt (X1Y) as for all words f and all words e and e' holds n(f le) = 0 and nt(ele1) = 0.Using the function n9 we arrive at the following optimization criterion: Here we defined ng,i (X) = Ex, ng (XIX') and n9,2(X) = Ex, n9(X11X).The variable X runs over the classes in £ and F. In the optimization process it cannot be allowed that words of different languages occur in one class.It can be seen that Eq.(3) is a special case of Eq.(9) with ng,1 = n9,2.Another possibility to perform bilingual word clustering is to apply a two-step approach.In a first step we determine classes S optimizing only the monolingual part of Eq.(6) and secondly we determine classes F optimizing the bilingual part (without changing 6): By using these two optimization processes we enforce that the classes E are mono-lingually 'good' classes and that the classes .7- correspond to 6.Interestingly enough this results in a higher translation quality (see section 5).An efficient optimization algorithm for LPI is the exchange algorithm (Martin et al., 1998).For the optimization of LP2 we can use the same algorithm with small modifications.Our starting point is a random partition of the training corpus vocabulary.This initial partition is improved iteratively by moving a single word from one class to another.The algorithm to determine bilingual classes is depicted in Figure 1.If only one word w is moved between the partitions C and C' the change LP(C,n9)— LP(C',n9) can be computed efficiently looking only at classes C for which ng (w, C) > 0 or ng(C,w) >0.We define Mc, to be the average number of seen predecessor and successor word classes.With the notation I for the number of iterations needed for convergence, B for the number of word bigrams, M for the number of classes and V for the vocabulary size the computational complexity of this algorithm is roughly I (B • log2 (B IV) +V M • Mo).A detailed analysis of the complexity can be found in (Martin et al., 1998).The algorithm described above provides only a local optimum.The quality of the resulting local optima can be improved if we accept a short-term degradation of the optimization criterion during the optimization process.We do this in our implementation by applying the optimization method threshold accepting (Dueck and Scheuer, 1990) which is an efficient simplification of simulated annealing.The statistical machine-translation method described in (Och and Weber, 1998) makes use of bilingual word classes.The key element of this approach are the alignment templates (originally referred to as translation rules) which are pairs of phrases together with an alignment between the words of the phrases.Examples of alignment templates are shown in Figure 2.The advantage of the alignment template approach against word-based statistical translation models is that word context and local re-orderings are explicitly taken into account.The alignment templates are automatically trained using a parallel training corpus.The translation of a sentence is done by a search process which determines the set of alignment templates which optimally cover the source sentence.The bilingual word classes are used to generalize the applicability of the alignment templates in search.If there exists a class which contains all cities in source and target language it is possible that an alignment template containing a special city can be generalized to all cities.More details are given in (Och and Weber, 1998; Och and Ney, 1999).We demonstrate results of our bilingual clustering method for two different bilingual corpora (see Tables 1 and 2).The EuTRANs-I corpus is a subtask of the &quot;Traveller Task&quot; (Vidal, 1997) which is an artificially generated Spanish-English corpus.The domain of the corpus is a humanto-human communication situation at a reception Table 3: Example of bilingual word classes (corpus EuTRANs-I, method BIL-2).El: how it pardon what when where which• who why E2: my our E3: today tomorrow E4: ask call make E5: carrying changing giving looking moving putting sending showing waking E6: full half quarter Si: c'omo cu'al cu'ando cu'anta d'onde dice dicho hace qu'e qui'en tiene desk of a hotel.The EuTRANs-II corpus is a natural German-English corpus consisting of different text types belonging to the domain of tourism: bilingual Web pages of hotels, bilingual touristic brochures and business correspondence.The target language of our experiments is English.We compare the three described methods to generate bilingual word classes.The classes MONO are determined by monolingually optimizing source and target language classes with Eq.(4).The classes BIL are determined by bilingually optimizing classes with Eq.(10).The classes BIL-2 are determined by first optimizing mono-lingually classes for the target language (English) and afterwards optimizing classes for the source language (Eq.(11) and Eq.(12)).For EuTRANs-I we used 60 classes and for EuTRANs-II we used 500 classes.We chose the number of classes in such a way that the final performance of the translation system was optimal.The CPU time for optimization of bilingual word classes on an Alpha workstation was under 20 seconds for EuTRANs-I and less than two hours for EuTRANs-II.Table 3 provides examples of bilingual word classes for the EuTRANs-I corpus.It can be seen that the resulting classes often contain words that are similar in their syntactic and semantic functions.The grouping of words with a different meaning like today and tomorrow does not imply that these words should be translated by the same Spanish word, but it does imply that the translations of these words are likely to be in the same Spanish word class.To measure the quality of our bilingual word classes we applied two different evaluation measures: exp (J-1 E maxi log (p (C ( f j) (ei)))) 3=1 Both measures determine the extent to which the translation probability is spread out.A small value means that the translation probability is very focused and that the knowledge of the source language class provides much information about the target language class. sertions/deletions/substitutions relative to a reference translation.As expected the translation quality improves using classes.For the small EuTRANs-I task the word error rates reduce significantly.The word error rates for the EuTRANs-II task are much larger because the task has a very large vocabulary and is more complex.The bilingual classes show better results than the monolingual classes MONO.One explanation for the improvement in translation quality is that the bilingually optimized classes result in an increased average size of used alignment templates.For example the average length of alignment templates with the EuTRANs-I corpus using WORD is 2.85 and using BIL-2 it is 5.19.The longer the average alignment template length, the more context is used in the translation and therefore the translation quality is higher.An explanation for the superiority of BIL-2 over BIL is that by first optimizing the English classes mono-lingually, it is much more probable that longer sequences of classes occur more often thereby increasing the average alignment template size.By applying a maximum-likelihood approach to the joint probability of a parallel corpus we obtained an optimization criterion for bilingual word classes which is very similar to the one used in monolingual maximum-likelihood word clustering.For optimization we used the exchange algorithm.The obtained word classes give a low translation lexicon perplexity and improve the quality of staProceedings of EACL '99 tistical machine translation.We expect improvements in translation quality by allowing that words occur in more than one class and by performing a hierarchical clustering.Acknowledgements This work has been partially supported by the European Community under the ESPRIT project number 30268 (EuTrans).
Non-Projective Dependency Parsing Using Spanning Tree AlgorithmsWe formalize weighted dependency pars ing as searching for maximum spanning trees (MSTs) in directed graphs.Using this representation, the parsing algorithmof Eisner (1996) is sufficient for search ing over all projective trees in O(n3) time.More surprisingly, the representation isextended naturally to non-projective pars ing using Chu-Liu-Edmonds (Chu andLiu, 1965; Edmonds, 1967) MST algorithm, yielding an O(n2) parsing al gorithm.We evaluate these methodson the Prague Dependency Treebank using online large-margin learning tech niques (Crammer et al, 2003; McDonald et al, 2005) and show that MST parsingincreases efficiency and accuracy for lan guages with non-projective dependencies.Dependency parsing has seen a surge of interest lately for applications such as relation extraction (Culotta and Sorensen, 2004), machine translation (Ding and Palmer, 2005), synonym genera tion (Shinyama et al, 2002), and lexical resource augmentation (Snow et al, 2004).The primary reasons for using dependency structures instead of more informative lexicalized phrase structures is that they are more efficient to learn and parse whilestill encoding much of the predicate-argument infor mation needed in applications.root John hit the ball with the bat Figure 1: An example dependency tree.Dependency representations, which link words to their arguments, have a long history (Hudson, 1984).Figure 1 shows a dependency tree for the sentence John hit the ball with the bat.We restrict ourselvesto dependency tree analyses, in which each word de pends on exactly one parent, either another word or a dummy root symbol as shown in the figure.The tree in Figure 1 is projective, meaning that if we put the words in their linear order, preceded by the root, theedges can be drawn above the words without cross ings, or, equivalently, a word and its descendants form a contiguous substring of the sentence.In English, projective trees are sufficient to ana lyze most sentence types.In fact, the largest sourceof English dependency trees is automatically gener ated from the Penn Treebank (Marcus et al, 1993)and is by convention exclusively projective.However, there are certain examples in which a non projective tree is preferable.Consider the sentenceJohn saw a dog yesterday which was a Yorkshire Ter rier.Here the relative clause which was a YorkshireTerrier and the object it modifies (the dog) are sep arated by an adverb.There is no way to draw the dependency tree for this sentence in the plane withno crossing edges, as illustrated in Figure 2.In lan guages with more flexible word order than English, such as German, Dutch and Czech, non-projective dependencies are more frequent.Rich inflection systems reduce reliance on word order to express 523 root John saw a dog yesterday which was a Yorkshire Terrier root O to nove?ve?ts?inou nema?ani za?jem a taky na to ve?ts?inou nema?pen??ze He is mostly not even interested in the new things and in most cases, he has no money for it either.Figure 2: Non-projective dependency trees in English and Czech.grammatical relations, allowing non-projective dependencies that we need to represent and parse ef ficiently.A non-projective example from the Czech Prague Dependency Treebank (Hajic?et al, 2001) is also shown in Figure 2.Most previous dependency parsing models have focused on projective trees, including the work of Eisner (1996), Collins et al (1999), Yamada and Matsumoto (2003), Nivre and Scholz (2004), and McDonald et al (2005).These systems have shown that accurate projective dependency parsers can be automatically learned from parsed data.However, non-projective analyses have recently attracted some interest, not only for languages with freer word order but also for English.In particular, Wang and Harper (2004) describe a broad coverage non-projectiveparser for English based on a hand-constructed constraint dependency grammar rich in lexical and syntactic information.Nivre and Nilsson (2005) presented a parsing model that allows for the introduc tion of non-projective edges into dependency trees through learned edge transformations within their memory-based parser.They test this system onCzech and show improved accuracy relative to a projective parser.Our approach differs from those ear lier efforts in searching optimally and efficiently the full space of non-projective trees.The main idea of our method is that dependencyparsing can be formalized as the search for a maximum spanning tree in a directed graph.This formalization generalizes standard projective parsing mod els based on the Eisner algorithm (Eisner, 1996) toyield efficient O(n2) exact parsing methods for nonprojective languages like Czech.Using this spanning tree representation, we extend the work of McDonald et al (2005) on online large-margin discriminative training methods to non-projective depen dencies.The present work is related to that of Hirakawa(2001) who, like us, reduces the problem of depen dency parsing to spanning tree search.However, his parsing method uses a branch and bound algorithm that is exponential in the worst case, even thoughit appears to perform reasonably in limited experi ments.Furthermore, his work does not adequately address learning or measure parsing accuracy on held-out data.Section 2 describes an edge-based factorizationof dependency trees and uses it to equate depen dency parsing to the problem of finding maximumspanning trees in directed graphs.Section 3 out lines the online large-margin learning framework used to train our dependency parsers.Finally, in Section 4 we present parsing results for Czech.The trees in Figure 1 and Figure 2 are untyped, that is, edges are not partitioned into types representingadditional syntactic information such as grammati cal function.We study untyped dependency treesmainly, but edge types can be added with simple ex tensions to the methods discussed here.2.1 Edge Based Factorization.In what follows, x = x1 ? ?xn represents a genericinput sentence, and y represents a generic depen dency tree for sentence x. Seeing y as the set of tree edges, we write (i, j) ? y if there is a dependency in y from word xi to word xj .In this paper we follow a common method of fac toring the score of a dependency tree as the sum of the scores of all edges in the tree.In particular, wedefine the score of an edge to be the dot product be 524 tween a high dimensional feature representation of the edge and a weight vector, s(i, j) = w ? f(i, j) Thus the score of a dependency tree y for sentence x is, s(x,y) = ?(i,j)?y s(i, j) = ?(i,j)?y w ? f(i, j) Assuming an appropriate feature representation as well as a weight vector w, dependency parsing is the task of finding the dependency tree y with highest score for a given sentence x. For the rest of this section we assume that the weight vector w is known and thus we know the score s(i, j) of each possible edge.In Section 3 we present a method for learning the weight vector.2.2 Maximum Spanning Trees.We represent the generic directed graph G = (V,E) by its vertex set V = {v1, . . ., vn} and set E ? [1 : n]?[1 : n] of pairs (i, j) of directed edges vi ? vj .Each such edge has a score s(i, j).Since G is di rected, s(i, j) does not necessarily equal s(j, i).A maximum spanning tree (MST) of G is a tree y ? E that maximizes the value ?(i,j)?y s(i, j) such thatevery vertex in V appears in y. The maximum pro jective spanning tree of G is constructed similarlyexcept that it can only contain projective edges rel ative to some total order on the vertices of G. The MST problem for directed graphs is also known as the maximum arborescence problem.For each sentence x we define the directed graph Gx = (Vx, Ex) given by Vx = {x0 = root, x1, . . ., xn} Ex = {(i, j) : i 6= j, (i, j) ? [0 : n] ? [1 : n]} That is, Gx is a graph with the sentence words and the dummy root symbol as vertices and a directed edge between every pair of distinct words and fromthe root symbol to every word.It is clear that dependency trees for x and spanning trees for Gx co incide, since both kinds of trees are required to be rooted at the dummy root and reach all the wordsin the sentence.Hence, finding a (projective) depen dency tree with highest score is equivalent to finding a maximum (projective) spanning tree in Gx.Chu-Liu-Edmonds(G, s) Graph G = (V, E) Edge weight function s : E ? R 1.Let M = {(x?, x) : x ? V, x?= arg maxx?s(x?, x)}.2.Let GM = (V, M).4.Otherwise, find a cycle C in GM.5.Let GC = contract(G, C, s).6.Let y = Chu-Liu-Edmonds(GC , s).7.Find a vertex x ? C s. t.(x?, x) ? y, (x??, x) ? C. 8.return y ? C ? {(x??, x)} contract(G = (V, E), C, s) 1.Let GC be the subgraph of G excluding nodes in C. 2.Add a node c to GC representing cycle C. Add edge (c, x) to GC with s(c, x) = maxx??C s(x?, x) 4.For x ? V ? C : ?x??C(x, x?)E. Add edge (x, c) to GC with s(x, c) = maxx??C [s(x, x?)s(a(x?), x?)+ s(C)] where a(v) is the predecessor of v in C and s(C) = Pv?C s(a(v), v) 5.return GC Figure 3: Chu-Liu-Edmonds algorithm for finding maximum spanning trees in directed graphs.2.2.1 Non-projective Trees To find the highest scoring non-projective tree we simply search the entire space of spanning trees with no restrictions.Well-known algorithms exist for theless general case of finding spanning trees in undi rected graphs (Cormen et al, 1990).Efficient algorithms for the directed case are less well known, but they exist.We will use here the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965;Edmonds, 1967), sketched in Figure 3 follow ing Leonidas (2003).Informally, the algorithm has each vertex in the graph greedily select the incoming edge with highest weight.If a tree results, it must be the maximum spanning tree.If not, there must be a cycle.The procedure identifies a cycle and contracts it into a single vertex and recalculates edge weights going into and out of the cycle.It can be shown that a maximum spanning tree on the contracted graph isequivalent to a maximum spanning tree in the orig inal graph (Leonidas, 2003).Hence the algorithm can recursively call itself on the new graph.Naively,this algorithm runs in O(n3) time since each recur sive call takes O(n2) to find the highest incoming edge for each word and to contract the graph.There are at most O(n) recursive calls since we cannot contract the graph more then n times.However, 525 Tarjan (1977) gives an efficient implementation of the algorithm with O(n2) time complexity for dense graphs, which is what we need here.To find the highest scoring non-projective tree for a sentence, x, we simply construct the graph Gx and run it through the Chu-Liu-Edmonds algorithm.The resulting spanning tree is the best non-projective dependency tree.We illustrate here the application of the Chu-Liu-Edmonds algorithm to dependency parsing on the simple example x = John saw Mary, with directed graph representation Gx, root saw John Mary 10 9 9 30 3020 3 0 11 The first step of the algorithm is to find, for each word, the highest scoring incoming edge root saw John Mary30 3020 If the result were a tree, it would have to be the maximum spanning tree.However, in this case we have a cycle, so we will contract it into a single node and recalculate edge weights according to Figure 3.root saw John Mary 40 9 30 31 wjs The new vertex wjs represents the contraction of vertices John and saw.The edge from wjs to Mary is 30 since that is the highest scoring edge from any vertex in wjs.The edge from root into wjs is set to40 since this represents the score of the best span ning tree originating from root and including only the vertices in wjs.The same leads to the edge from Mary to wjs.The fundamental property of the Chu-Liu-Edmonds algorithm is that an MST in thisgraph can be transformed into an MST in the orig inal graph (Leonidas, 2003).Thus, we recursively call the algorithm on this graph.Note that we need to keep track of the real endpoints of the edges into and out of wjs for reconstruction later.Running the algorithm, we must find the best incoming edge to all words root saw John Mary 40 30 wjs This is a tree and thus the MST of this graph.We now need to go up a level and reconstruct the graph.The edge from wjs to Mary originally was from the word saw, so we include that edge.Furthermore, the edge from root to wjs represented a tree from root to saw to John, so we include all those edges to get the final (and correct) MST, root saw John Mary 10 3030 A possible concern with searching the entire spaceof spanning trees is that we have not used any syntactic constraints to guide the search.Many lan guages that allow non-projectivity are still primarily projective.By searching all possible non-projective trees, we run the risk of finding extremely bad trees.We address this concern in Section 4.2.2.2 Projective TreesIt is well known that projective dependency pars ing using edge based factorization can be handledwith the Eisner algorithm (Eisner, 1996).This algorithm has a runtime of O(n3) and has been employed successfully in both generative and discrimi native parsing models (Eisner, 1996; McDonald et al., 2005).Furthermore, it is trivial to show that the Eisner algorithm solves the maximum projective spanning tree problem.The Eisner algorithm differs significantly from the Chu-Liu-Edmonds algorithm.First of all, it is abottom-up dynamic programming algorithm as opposed to a greedy recursive one.A bottom-up al gorithm is necessary for the projective case since it must maintain the nested structural constraint, which is unnecessary for the non-projective case.2.3 Dependency Trees as MSTs: Summary.In the preceding discussion, we have shown that nat ural language dependency parsing can be reduced to finding maximum spanning trees in directed graphs.This reduction results from edge-based factoriza tion and can be applied to projective languages with 526the Eisner parsing algorithm and non-projective languages with the Chu-Liu-Edmonds maximum span ning tree algorithm.The only remaining problem is how to learn the weight vector w. A major advantage of our approach over other dependency parsing models is its uniformity and simplicity.By viewing dependency structures asspanning trees, we have provided a general framework for parsing trees for both projective and non projective languages.Furthermore, the resultingparsing algorithms are more efficient than lexi calized phrase structure approaches to dependencyparsing, allowing us to search the entire space with out any pruning.In particular the non-projective parsing algorithm based on the Chu-Liu-EdmondsMST algorithm provides true non-projective parsing.This is in contrast to other non-projective meth ods, such as that of Nivre and Nilsson (2005), who implement non-projectivity in a pseudo-projective parser with edge transformations.This formulation also dispels the notion that non-projective parsing is?harder?than projective parsing.In fact, it is easier since non-projective parsing does not need to en force the non-crossing constraint of projective trees.As a result, non-projective parsing complexity is justO(n2), against the O(n3) complexity of the Eisner dynamic programming algorithm, which by con struction enforces the non-crossing constraint.In this section, we review the work of McDonald etal.(2005) for online large-margin dependency pars ing.As usual for supervised learning, we assume a training set T = {(xt,yt)}Tt=1, consisting of pairs of a sentence xt and its correct dependency tree yt.In what follows, dt(x) denotes the set of possible dependency trees for sentence x. The basic idea is to extend the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer,2003; Crammer et al, 2003) to learning with struc tured outputs, in the present case dependency trees.Figure 4 gives pseudo-code for the MIRA algorithmas presented by McDonald et al (2005).An on line learning algorithm considers a single training instance at each update to w. The auxiliary vector v accumulates the successive values of w, so that thefinal weight vector is the average of the weight vec Training data: T = {(xt, yt)}Tt=1 1.w0 = 0; v = 0; i = 0 2.for n : 1..N 3.for t : 1..T 4.min ? ?w(i+1) ? w(i) ? ?s.t. s(xt, yt) ? s(xt, y?)L(yt, y?), ?y? ? dt(xt) 5.v = v + w(i+1) 6.i = i + 1 7.w = v/(N ? T ) Figure 4: MIRA learning algorithm.tors after each iteration.This averaging effect has been shown to help overfitting (Collins, 2002).On each update, MIRA attempts to keep the new weight vector as close as possible to the old weight vector, subject to correctly classifying the instance under consideration with a margin given by the loss of the incorrect classifications.For dependency trees, the loss of a tree is defined to be the number of words with incorrect parents relative to the correct tree.This is closely related to the Hamming loss that is often used for sequences (Taskar et al, 2003).For arbitrary inputs, there are typically exponen tially many possible parses and thus exponentially many margin constraints in line 4 of Figure 4.3.1 Single-best MIRA.One solution for the exponential blow-up in number of trees is to relax the optimization by using only the single margin constraint for the tree with the highest score, s(x,y).The resulting online update (to be inserted in Figure 4, line 4) would then be: min ? ?w(i+1) ? w(i) ? ?s.t. s(xt,yt) ? s(xt,y?) ? L(yt,y?) where y?= arg maxy?s(xt,y?) McDonald et al (2005) used a similar update with k constraints for the k highest-scoring trees, and showed that small values of k are sufficient toachieve the best accuracy for these methods.However, here we stay with a single best tree because k best extensions to the Chu-Liu-Edmonds algorithm are too inefficient (Hou, 1996).This model is related to the averaged perceptron algorithm of Collins (2002).In that algorithm, the single highest scoring tree (or structure) is used toupdate the weight vector.However, MIRA aggres sively updates w to maximize the margin between 527 the correct tree and the highest scoring tree, which has been shown to lead to increased accuracy.3.2 Factored MIRA.It is also possible to exploit the structure of the output space and factor the exponential number of mar gin constraints into a polynomial number of local constraints (Taskar et al, 2003; Taskar et al, 2004).For the directed maximum spanning tree problem,we can factor the output by edges to obtain the fol lowing constraints: min ? ?w(i+1) ? w(i) ? ?s.t. s(l, j) ? s(k, j) ? 1 ?(l, j) ? yt, (k, j) /?yt This states that the weight of the correct incomingedge to the word xj and the weight of all other in coming edges must be separated by a margin of 1.It is easy to show that when all these constraintsare satisfied, the correct spanning tree and all incor rect spanning trees are separated by a score at least as large as the number of incorrect incoming edges.This is because the scores for all the correct arcs can cel out, leaving only the scores for the errors causingthe difference in overall score.Since each single er ror results in a score increase of at least 1, the entirescore difference must be at least the number of er rors.For sequences, this form of factorization has been called local lattice preference (Crammer et al, 2004).Let n be the number of nodes in graph Gx.Then the number of constraints is O(n2), since for each node we must maintain n ? 1 constraints.The factored constraints are in general more re strictive than the original constraints, so they mayrule out the optimal solution to the original problem.McDonald et al (2005) examines briefly factored MIRA for projective English dependency pars ing, but for that application, k-best MIRA performs as well or better, and is much faster to train.We performed experiments on the Czech Prague De pendency Treebank (PDT) (Hajic?, 1998; Hajic?et al,2001).We used the predefined training, develop ment and testing split of this data set.Furthermore, we used the automatically generated POS tags that are provided with the data.Czech POS tags are very complex, consisting of a series of slots that may ormay not be filled with some value.These slots rep resent lexical and grammatical properties such as standard POS, case, gender, and tense.The result is that Czech POS tags are rich in information, but quite sparse when viewed as a whole.To reduce sparseness, our features rely only on the reducedPOS tag set from Collins et al (1999).The num ber of features extracted from the PDT training set was 13, 450, 672, using the feature set outlined by McDonald et al (2005).Czech has more flexible word order than Englishand as a result the PDT contains non-projective de pendencies.On average, 23% of the sentences in the training, development and test sets have at least one non-projective dependency.However, less than2% of total edges are actually non-projective.There fore, handling non-projective edges correctly has a relatively small effect on overall accuracy.To show the effect more clearly, we created two Czech data sets.The first, Czech-A, consists of the entire PDT.The second, Czech-B, includes only the 23% of sen tences with at least one non-projective dependency.This second set will allow us to analyze the effectiveness of the algorithms on non-projective mate rial.We compared the following systems: 1.COLL1999: The projective lexicalized phrase-structure.parser of Collins et al (1999).2.N&N2005: The pseudo-projective parser of Nivre and Nilsson (2005).3.McD2005: The projective parser of McDonald et al.(2005) that uses the Eisner algorithm for both training and testing.This system uses k-best MIRA with k=5.4. Single-best MIRA: In this system we use the Chu-Liu-.Edmonds algorithm to find the best dependency tree for Single-best MIRA training and testing.based on edge factorization as described in Section 3.2.We use the Chu-Liu-Edmonds algorithm to find the best tree for the test data.4.1 Results.Results are shown in Table 1.There are two mainmetrics.The first and most widely recognized is Ac curacy, which measures the number of words that correctly identified their parent in the tree.Completemeasures the number of sentences in which the re sulting tree was completely correct.Clearly, there is an advantage in using the ChuLiu-Edmonds algorithm for Czech dependency pars 528 Czech-A Czech-B Accuracy Complete Accuracy CompleteCOLL1999 82.8 - - N&N2005 80.0 31.8 - McD2005 83.3 31.3 74.8 0.0 Single-best MIRA 84.1 32.2 81.0 14.9 Factored MIRA 84.4 32.3 81.5 14.3 Table 1: Dependency parsing results for Czech.Czech-B is the subset of Czech-A containing only sentences with at least one non-projective dependency.ing.Even though less than 2% of all dependenciesare non-projective, we still see an absolute improve ment of up to 1.1% in overall accuracy over the projective model.Furthermore, when we focus on the subset of data that only contains sentences with at least one non-projective dependency, the effect is amplified.Another major improvement here isthat the Chu-Liu-Edmonds non-projective MST al gorithm has a parsing complexity of O(n2), versusthe O(n3) complexity of the projective Eisner algo rithm, which in practice leads to improvements in parsing time.The results also show that in terms of Accuracy, factored MIRA performs better than single-best MIRA.However, for the factored model,we do have O(n2) margin constraints, which re sults in a significant increase in training time over single-best MIRA.Furthermore, we can also see that the MST parsers perform favorably compared to the more powerful lexicalized phrase-structure parsers, such as those presented by Collins et al (1999) andZeman (2004) that use expensive O(n5) parsing al gorithms.We should note that the results in Collins et al (1999) are different then reported here due to different training and testing data sets.One concern raised in Section 2.2.1 is that search ing the entire space of non-projective trees couldcause problems for languages that are primarily projective.However, as we can see, this is not a prob lem.This is because the model sets its weights with respect to the parsing algorithm and will disfavor features over unlikely non-projective edges.Since the space of projective trees is a subset ofthe space of non-projective trees, it is natural to won der how the Chu-Liu-Edmonds parsing algorithm performs on projective data since it is asymptotically better than the Eisner algorithm.Table 2 shows theresults for English projective dependency trees ex tracted from the Penn Treebank (Marcus et al, 1993) using the rules of Yamada and Matsumoto (2003).English Accuracy Complete McD2005 90.9 37.5 Single-best MIRA 90.2 33.2 Factored MIRA 90.2 32.3Table 2: Dependency parsing results for English us ing spanning tree algorithms.This shows that for projective data sets, training and testing with the Chu-Liu-Edmonds algorithm is worse than using the Eisner algorithm.This is notsurprising since the Eisner algorithm uses the a pri ori knowledge that all trees are projective.We presented a general framework for parsing dependency trees based on an equivalence to maximum spanning trees in directed graphs.This frame work provides natural and efficient mechanismsfor parsing both projective and non-projective languages through the use of the Eisner and Chu-Liu Edmonds algorithms.To learn these structures we used online large-margin learning (McDonald et al,2005) that empirically provides state-of-the-art per formance for Czech.A major advantage of our models is the ability to naturally model non-projective parses.Non projective parsing is commonly considered more difficult than projective parsing.However, under our framework, we show that the opposite is actuallytrue that non-projective parsing has a lower asymptotic complexity.Using this framework, we pre sented results showing that the non-projective modeloutperforms the projective model on the Prague De pendency Treebank, which contains a small number of non-projective edges.Our method requires a tree score that decomposes according to the edges of the dependency tree.One might hope that the method would generalize to 529include features of larger substructures.Unfortu nately, that would make the search for the best tree intractable (Ho?ffgen, 1993).Acknowledgments We thank Lillian Lee for bringing an importantmissed connection to our attention, and Koby Cram mer for his help with learning algorithms.This work has been supported by NSF ITR grants 0205448 and 0428193.
Syntax Annotation for the GENIA CorpusLinguistically annotated corpus based on texts in biomedical domain has been constructed to tune natural language processing (NLP) tools for bio textmining.As the focus of information extraction is shifting from "nominal" information such as named entity to "verbal" information such as function and interaction of substances, applica tion of parsers has become one of the key technologies and thus the corpus annotated for syntactic structure of sen tences is in demand.A subset of the GENIA corpus consisting of 500 MEDLINE abstracts has been annotated for syntactic structure in an XML based format based on Penn Treebank II (PTB) scheme.Inter-annotator agreement test indicated that the writ ing style rather than the contents of the research abstracts is the source of the difficulty in tree annotation, and that annotation can be stably done by linguists without much knowledge of bi ology with appropriate guidelines regarding to linguistic phenomena par ticular to scientific texts.Research and development for information extraction from biomedical literature (bio textmining) has been rapidly advancing due to demands caused by information overload in the genome-related field.Natural language process ing (NLP) techniques have been regarded as useful for this purpose.Now that focus of in formation extraction is shifting from extraction of ?nominal?information such as named entity to ?verbal?information such as relations of enti ties including events and functions, syntactic analysis is an important issue of NLP application in biomedical domain.In extraction of rela tion, the roles of entities participating in the relation must be identified along with the verb that represents the relation itself.In text analysis, this corresponds to identifying the subjects, ob jects, and other arguments of the verb.Though rule-based relation information ex traction systems using surface pattern matching and/or shallow parsing can achieve high precision (e.g. Koike et al, 2004) in a particular target domain, they tend to suffer from low recall due to the wide variation of the surface ex pression that describe a relation between a verb and its arguments.In addition, the portability of such systems is low because the system has to be re-equipped with different set of rules when different kind of relation is to be extracted.One solution to this problem is using deep parsers which can abstract the syntactic variation of a relation between a verb and its arguments repre sented in the text, and constructing extraction rule on the abstract predicate-argument structure.To do so, wide-coverage and high-precision parsers are required.While basic NLP techniques are relatively general and portable from domain to domain, customization and tuning are inevitable, especially in order to apply the techniques effec tively to highly specialized literatures such as research papers and abstracts.As recent advances in NLP technology depend on machine learning techniques, annotated corpora from which system can acquire rules (including grammar rules, lexicon, etc.) are indispensable 220 resources for customizing general-purpose NLP tools.In bio-textmining, for example, training on part-of-speech (POS)-annotated GENIA cor pus was reported to improve the accuracy of JunK tagger (English POS tagger) (Kazama et al., 2001) from 83.5% to 98.1% on MEDLINE abstracts (Tateisi and Tsujii, 2004), and the FraMed corpus (Wermter and Hahn, 2004) was used to train TnT tagger on German (Brants, 2000) to improve its accuracy from 95.7% to 98% on clinical reports and other biomedical texts.Corpus annotated for syntactic structures is expected to play a similar role in tuning parsers to biomedical domain, i.e., similar improve ment on the performance of parsers is expected by using domain-specific treebank as a resource for learning.For this purpose, we construct GENA Treebank (GTB), a treebank on research abstracts in biomedical domain.The base text of GTB is that of the GENIA cor pus constructed at University of Tokyo (Kim et al., 2003), which is a collection of research ab stracts selected from the search results of MEDLINE database with keywords (MeSH terms) human, blood cells and transcription factors.In the GENIA corpus, the abstracts are en coded in an XML scheme where each abstract is numbered with MEDLINE UID and contains title and abstract.The text of title and abstract is segmented into sentences in which biological terms are annotated with their semantic classes.The GENIA corpus is also annotated for part-of speech (POS) (Tateisi and Tsujii, 2004), and coreference is also annotated in a part of the GENIA corpus by MedCo project at Institute for Infocomm Research, Singapore (Yang et al 2004).GTB is the addition of syntactic information to the GENIA corpus.By annotating various linguistic information on a same set of text, the GENIA corpus will be a resource not only for individual purpose such as named entity extrac tion or training parsers but also for integrated systems such as information extraction using deep linguistic analysis.Similar attempt of con structing integrated corpora is being done in University of Pennsylvania, where a corpus of MEDLINE abstracts in CYP450 and oncology domains where annotated for named entities, POS, and tree structure of sentences (Kulick et al, 2004).2.1 Annotation Scheme.The annotation scheme basically follows the Penn Treebank II (PTB) scheme (Beis et al 1995), encoded in XML.A non-null constituent is marked as an element, with its syntactic cate gory (which may be combined with its function tags indicating grammatical roles such as -SBJ, -PRD, and -ADV) used as tags.A null constitu ent is marked as a childless element whose tag corresponds to its categories.Other function tags are encoded as attributes.Figure 1 shows an ex ample of annotated sentence in XML, and the corresponding PTB notation.The label ?S? means ?sentence?, ?NP?noun phrase, ?PP?prepositional phrase, and ?VP?verb phrase.The label ?NP-SBJ?means that the element is an NP that serves as the subject of the sentence.A null element, the trace of the object of ?stud ied?moved by passivization, is denoted by ? <NP NULL="NONE" ref="i55"/>?in XML and ?*-55?in PTB notation.The number ?55?which refers to the identifier of the moved ele ment, is denoted by ?id? and ?ref?attributes in XML, and is denoted as a part of a label in PTB.In addition to changing the encoding, we made some modifications to the scheme.First, analysis within the noun phrase is simplified.Second, semantic division of adverbial phrases such as ??TMP?(time) and ??MNR?(manner) are not used: adverbial constituents other than ?ADVP?(adverbial phrases) or ?PP?used ad verbially are marked with ?ADV tags but not with semantic tags.Third, a coordination struc ture is explicitly marked with the attribute SYN=?COOD?whereas in the original PTB scheme it is not marked as such.In our GTB scheme, ?NX?(head of a com plex noun phrase) and ?NAC?(a certain kind of nominal modifier within a noun phrase) of the PTB scheme are not used.A noun phrase is gen erally left unstructured.This is mainly in order to simplify the process of annotation.In case of biomedical abstracts, long noun phrases often involve multi-word technical terms whose syn tactic structure is difficult to determine without deep domain knowledge.However, the structure of noun phrases are usually independent of the structure outside the phrase, so that it would be 221 easier to analyze the phrases involving such terms independently (e.g. by biologists) and later merge the two analysis together.Thus we have decided that we leave noun phrases unstructured in GTB annotation unless their analy sis is necessary for determining the structure outside the phrase.One of the exception is the cases that involves coordination where it is nec essary to explicitly mark up the coordinated constituents.In addition, we have added special attributes ?TXTERR?, ?UNSURE?, and ?COMMENT?for later inspection.The ?TXTERR?is used when the annotator suspects that there is a grammatical error in the original text; the ?UNSURE?attribute is used when the annotator is not confident; and the ?COMMENT?is used for free comments (e.g. reason of using ?UNSURE?)by the annotator.2.2 Annotation Process.The sentences in the titles and abstracts of the base text of GENIA corpus are annotated manu ally using an XML editor used for the Global Document Annotation project (Hasida 2000).Although the sentence boundaries were adopted from the corpus, the tree structure annotation was done independently of POS- and term- an notation already done on the GENIA corpus.The annotator was a Japanese non-biologist who has previously involved in the POS annotation of the GENIA corpus and accustomed to the style of research abstracts in English.Manually annotated abstracts are automatically converted to the PTB format, merged with the POS annota tion of the GENIA corpus (version 3.02).So far, 500 abstracts are annotated and converted to the merged PTB format.In the merg ing process, we found several annotation errors.The 500 abstracts with correction of these errors are made publicly available as ?The GENIA Treebank Beta Version?(GTB-beta).For further clean-up, we also tried to parse the corpus by the Enju parser (Miyao and Tsujii 2004), and identify the error of the corpus by investigating into the parse errors.Enju is an HPSG parser that can be trained with PTB-type corpora which is reported to have 87% accuracy on Wall Street Journal portion of Penn Treebank corpus.Currently the accuracy of the parser drops down to 82% on GTB-beta, and although proper quantitative analysis is yet to be done, it was found that the mismatches between labels of the treebank and the GENIA POS corpus (e.g. an ?ing form labeled as noun in the POS corpus and as the head of a verb phrase in the tree corpus) are a major source of parse error.The cor rection is complicated because several errors in the GENIA POS corpus were found in this cleaning-up process.When the cleaning-up process is done, we will make the corpus pub licly available as the proper release.<S><PP>In <NP>the present paper </NP></PP>, <NP-SBJ id="i55"><NP>the binding </NP><PP>of <NP>a [125I]-labeled aldosterone derivative </NP></PP><PP>to <NP><NP>plasma membrane rich fractions </NP><PP>of HML </PP></NP></PP></NP-SBJ><VP>was <VP>studied <NP NULL="NONE" ref="i55"/></VP> </VP>.</S>We have also checked inter-annotator agreement.Although the PTB scheme is popular among natural language processing society, applicabil ity of the scheme to highly specialized text such as research abstract is yet to be discussed.Espe cially, when the annotation is done by linguists, lack of domain knowledge might decrease the stability and accuracy of annotation.A small part of the base text set (10 ab stracts) was annotated by another annotator.The 10 abstracts were chosen randomly, had 6 to 17 sentences per abstract (total 108 sentences).The new annotator had a similar background as the first annotator that she is a Japanese non biologist who has experiences in translation of (S (PP In (NP the present paper)), (NP-SBJ-55 (NP the binding) (PP of (NP a [125I]-labeled aldosterone derivative)) (PP to (NP (NP plasma membrane rich fractions) (PP of HML)))) (VP was (VP studied * 55)).)Figure 1.The sentence ?In the present paper, the binding of a [125I]-labeled aldosterone derivative to plasma mem brane rich fractions of HML was studied?annotated in XML and PTB formats.222 technical documents in English and in corpus annotation of English texts.The two results were examined manually, and there were 131 disagreements.Almost every sentence had at least one disagreement.We have made the ?gold standard?from the two sets of abstracts by resolving the disagreements, and the accuracy of the annotators against this gold standard were 96.7% for the first annotator and 97.4% for the second annotator.Of the disagreement, the most prominent were the cases involving coordination, espe cially the ones with ellipsis.For example, one annotator annotated the phrase ?IL-1- and IL-18 mediated function?as in Figure 2a, the other annotated as Figure 2b.Such problem is addressed in the PTB guideline and both formats are allowed as alter natives.As coordination with ellipsis occurs rather frequently in research abstracts, this kind of phenomena has higher effect on decrease of the agreement rate than in Penn Treebank.Of the 131 disagreements, 25 were on this type of coordination.Another source of disagreement is the at tachment of modifiers such as prepositional phrases and pronominal adjectives.However, most are ?benign ambiguity?where the difference of the structure does not affect on interpre tation, such as ?high expression of STAT in monocytes?where the prepositional phrase ?in monocytes?can attach to ?expression?or ?STAT?without much difference in meaning, and ?is augmented when the sensitizing tumor is a genetically modified variant?where the whclause can attach to ?is augmented?or ?aug mented?without changing the meaning.The PTB guideline states that the modifier should be attached at the higher level in the former case and at the lower case in the latter.In the annota tion results, one annotator consistently attached the modifiers in both cases at the higher level, and the other consistently at the lower level, in dicating that the problem is in understanding the scheme rather than understanding the sentence.Only 15 cases were true ambiguities that needed knowledge of biology to solve, in which 5 in volved coordination (e.g., the scope of ?various?in ?various T cell lines and peripheral blood cells?)Although the number was small, there were disagreements on how to annotate a mathematical formula such as ?n=2?embedded in the sen tence, since mathematical formulae were outside the scope of the original PTB scheme.One annotator annotated this kind of phrase consis tently as a phrase with ?=?as an adjective, the other annotated as phrase with ?=?as a verb.There were 6 such cases.Another disagreement particular to abstracts is a treatment of labeled sentences.There were 8 sentences in two ab stracts where there is a label like ?Background:?.One annotator included the colon (?:?)in the la bel, while the other did not.Yet another is that one regarded the phrase ?Author et al as coor dination, and the other regarded ?et al as a modifier.<NP SYN="COOD"> <NP><ADJP>IL-1- <ADJP NULL="QSTN"/></ADJP> <NP NULL="RNR" ref="i20"/></NP> and <NP>IL-18-mediated <NP NULL="RNR" ref="i20"/></NP> <NP id="i20">function </NP> Other disagreements are more general type such as regarding ?-ed?form of a verb as an ad jective or a participle, miscellaneous errors such as omission of a subtype of label (such as ? PRD?or ?-SBJ) or the position of <PRN> tags <NP> <ADJP SYN="COOD"> <ADJP>IL-1- <ADJP NULL="QSTN"/></ADJP> and <ADJP>IL-18-mediated </ADJP></ADJP> function </NP> NP ADJP Function ADJP and ADJP IL-1 * IL-18 mediated Figure 2a.Annotation of a coordinated phrase by the first annotator.A* denotes a null constituent.</NP> NP NP And NP ADJP *20 IL-18 meidiated NP IL-1 * function20 Figure 2b.Annotation of the same phrase as in Figure 2a by the second annotator.A * denotes a null constituent and ?20?denotes coindexing.223 with regards to ?,?for the inserted phrase, or the errors which look like just ?careless?.Such dis agreements and mistakes are at least partially eliminated when reliable taggers and parsers are available for preprocessingThe result of the inter-annotator agreement test indicates that the writing style rather than the contents of the research abstracts is the source of the difficulty in tree annotation.Con trary to the expectation that the lack of domain knowledge causes a problem in annotation on attachments of modifiers, the number of cases where annotation of modifier attachment needs domain knowledge is small.This indicates that linguists can annotate most of syntactic structure without an expert level of domain knowledge.A major source of difficulty is coordination, especially the ones involving ellipsis.Coordination is reported to be difficult phenomena in an notation of different levels in the GENIA corpus (Tateisi and Tsujii, 2004), (Kim et al, 2003).In addition to the fact that this is the major source of inter-annotator agreement, the annotator often commented the coordinated structure as ?unsure?.The problem of coordination can be divided into two with different nature: one is that the annota tion policy is still not well-established for the coordination involving ellipsis, and the other is an ambiguity when the coordinated phrase has modifiers.Syntax annotation of coordination with ellipsis is difficult in general but the more so in an notation of abstracts than in the case of general texts, because in abstracts authors tend to pack information in limited number of words.The PTB guideline dedicates a long section for this phenomena and allows alternatives in annotation, but there are still cases which are not well covered by the scheme.For example, in addition to the disagreement, the phrase illustrated in Figure 2a and Figure 2b shows another problem of the annotation scheme.Both annotators fail to indicate that it is ?mediated?that was to be after ?IL-1?because there is no mechanism of coindexing a null element with a part of a token.This problem of ellipsis can frequently occur in research abstracts, and it can be argued that the tokenization criteria must be changed for texts in biomedical domain (Yamamoto and Sa tou, 2004) so that such fragment as ?IL-18?and ?mediated?in ?IL-18-ediated?should be regarede as separate tokens.The Pennsylvania biology corpus (Kulick et al, 2004) partially solves this problem by separating a token where two or more subtokens are connected with hyphens, but in the cases where a shared part of the word is not separated by a hyphen (e.g. ?metric?of ?ste reo- and isometric alleles?)the word including the part is left uncut.The current GTB follows the GENIA corpus that it retains the tokeniza tion criteria of the original Penn Treebank, but this must be reconsidered in future.For analysis of coordination with ellipsis, if the information on full forms is available, one strategy would be to leave the inside structure of coordination unannotated in the treebank corpus (and in the phase of text analysis the structure is not established in the phase of parsing but with a different mechanism) and later merge it with the coordination structure annotation.The GENIA term corpus annotates the full form of a techni cal term whose part is omitted in the surface as an attribute of the ?<cons>?element indicating a technical term (Kim et al, 2003).In the above mentioned Pennsylvania corpus, a similar mechanism (?chaining?)is used for recovering the full form of named entities.However, in both corpora, no such information is available outside the terms/entities.The cases where scope of modification in coordinated phrases is problematic are few but they are more difficult in abstracts than in gen eral texts because the resolution of ambiguity needs domain knowledge.If term/entity annota tion is already done, that information can help resolve this type of ambiguity, but again the problem is that outside the terms/entities such information is not available.It would be practi cal to have the structure flat but specially marked when the tree annotators are unsure and have a domain expert resolve the ambiguity, as the sentences that needs such intervention seems few.Some cases of ambiguity in modifier at tachment (which do not involve coordination) can be solved with similar process.We believe that other type of disagreements can be solved with supplementing criteria for linguistic phenomena not well-covered by the scheme, and annotator training.Automatic pre processing by POS taggers and parsers can also help increase the consistent annotation.224A subset of the GENIA corpus is annotated for syntactic (tree) structure.Inter-annotator agreement test indicated that the annotation can be done stably by linguists without much knowledge in biology, provided that proper guideline is established for linguistic phenomena particular to scientific research abstracts.We have made the 500-abstract corpus in both XML and PTB formats and made it publicly available as ?the GENIA Treebank beta version?(GTB beta).We are in further cleaning up process of the 500-abstract set, and at the same time, initial annotation of the remaining abstracts is being done, so that the full GENIA set of 2000 ab stracts will be annotated with tree structure.For parsers to be useful for information ex traction, they have to establish a map between syntactic structure and more semantic predicate argument structure, and between the linguistic predicate-argument structures to the factual relation to be extracted.Annotation of various in formation on a same set of text can help establish these maps.For the factual relations, we are annotating relations between proteins and genes in cooperation with a group of biologists.For predicate-argument annotation, we are in vestigating the use of the parse results of the Enju parser.Acknowledgments The authors are grateful to annotators and colleagues that helped the construction of the corpus.This work is partially supported by Grant in-Aid for Scientific Research on Priority Area C ?Genome Information Science?from the Min istry of Education, Culture, Sports, Science and Technology of Japan.
Statistical Machine Translation With Scarce Resources Using Morpho-Syntactic InformationIn statistical machine translation, correspondences between the words in the source and the target language are learned from parallel corpora, and often little or no linguistic knowledge is used to structure the underlying models.In particular, existing statistical systems for machine translation often treat different inflectedforms of the same lemma as if they were independent ofone another.The bilingual training data can be better exploited by explicitly taking into account the interdependencies of related inflected forms.We propose the construction of hierarchical lexicon models on the basis of equivalence classes of words.In addition, we introduce sentence-level restructuring transformations which aim at the assimilation of word order in related sentences.We have systematically investigated the amount of bilingual training data required to maintain an acceptable quality of machine translation.The combination of the suggested methods for improving translation quality in frameworks with scarce resources has been successfully tested: We were able to reduce the amount of bilingual training data to less than 10% of the original corpus, while losing only 1.6% in translation quality.The improvement of the translation results is demonstrated on two German-English corpora taken from the Uerbmobil task and the Nespole! task.In statistical machine translation, correspondences between the words in the source and the target language are learned from parallel corpora, and often little or no linguistic knowledge is used to structure the underlying models.In particular, existing statistical systems for machine translation often treat different inflectedforms of the same lemma as if they were independent ofone another.The bilingual training data can be better exploited by explicitly taking into account the interdependencies of related inflected forms.We propose the construction of hierarchical lexicon models on the basis of equivalence classes of words.In addition, we introduce sentence-level restructuring transformations which aim at the assimilation of word order in related sentences.We have systematically investigated the amount of bilingual training data required to maintain an acceptable quality of machine translation.The combination of the suggested methods for improving translation quality in frameworks with scarce resources has been successfully tested: We were able to reduce the amount of bilingual training data to less than 10% of the original corpus, while losing only 1.6% in translation quality.The improvement of the translation results is demonstrated on two German-English corpora taken from the Uerbmobil task and the Nespole! task.The statistical approach to machine translation has proved successful in various comparative evaluations since its revival by the work of the IBM research group more than a decade ago.The IBM group dispensed with linguistic analysis, at least in its earliest publications.Although the IBM group finally made use of morphological and syntactic information to enhance translation quality (Brown et al. 1992; Berger et al.1996), most of today’s statistical machine translation systems still consider only surface forms and use no linguistic knowledge about the structure of the languages involved.In many applications only small amounts of bilingual training data are available for the desired domain and language pair, and it is highly desirable to avoid at least parts of the costly data collection process.The main objective of the work reported in this article is to introduce morphological knowledge in order to reduce the amount of bilingual data necessary to sufficiently cover the vocabulary expected in testing.This is achieved by explicitly taking into account the interdependencies of related inflected forms.In this work, a hierarchy of equivalence classes at different levels of abstraction is proposed.Features from those hierarchy levels are combined to form hierarchical lexicon models, which can replace the standard probabilistic lexicon used in most statistical machine translation systems.Apart from the improved coverage, the proposed lexicon models enable the disambiguation of ambiguous word forms by means of annotation with morpho-syntactic tags.The article is organized as follows.After briefly reviewing the basic concepts of the statistical approach to machine translation, we discuss the state of the art and related work as regards the incorporation of morphological and syntactic information into systems for natural language processing.Section 2 describes the information provided by morpho-syntactic analysis and introduces a suitable representation of the analyzed corpus.Section 3 suggests solutions for two specific aspects of structural difference, namely, question inversion and separated verb prefixes.Section 4 is dedicated to hierarchical lexicon models.These models are able to infer translations of word forms from the translations of other word forms of the same lemma.Furthermore, they use morpho-syntactic information to resolve categorial ambiguity.In Section 5, we describe how disambiguation between different readings and their corresponding translations can be performed when no context is available, as is typically the case for conventional electronic dictionaries.Section 6 provides an overview of our procedure for training model parameters for statistical machine translation with scarce resources.Experimental results are reported in Section 7.Section 8 concludes the presentation with a discussion of the achievements of this work.In statistical machine translation, every target language string eI1 = e1 · · · eI is assigned a probability Pr(eI1) of being a valid word sequence in the target language and a probability Pr(eI1|f J1) of being a translation for the given source language string f1J = f1 · · · fJ.According to Bayes’ decision rule, the optimal translation for f1J is the target string that maximizes the product of the target language model Pr(eI1) and the string translation model Pr(fJ1|eI1).Many existing systems for statistical machine translation (Garcia-Varea and Casacuberta 2001; Germann et al. 2001; Nießen et al.1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.The probability that a certain target language word will occur in the target string is assumed to depend basically only on the source words aligned with it.1.3.1 Morphology.Some publications have already dealt with the treatment of morphology in the framework of language modeling and speech recognition: Kanevsky, Roukos, and Sedivy (1997) propose a statistical language model for inflected languages.They decompose word forms into stems and affixes.Maltese and Mancini (1992) report that a linear interpolation of word n-grams, part of speech n-grams, and lemma n-grams yields lower perplexity than pure word-based models.Larson et al. (2000) apply a data-driven algorithm for decomposing compound words in compounding languages as well as for recombining phrases to enhance the pronunciation lexicon and the language model for large-vocabulary speech recognition systems.As regards machine translation, the treatment of morphology is part of the analysis and generation step in virtually every symbolic machine translation system.For this purpose, the lexicon should contain base forms of words and the grammatical category, subcategorization features, and semantic information in order to enable the size of the lexicon to be reduced and in order to account for unknown word forms, that is, word forms not present explicitly in the dictionary.Today’s statistical machine translation systems build upon the work of P. F. Brown and his colleagues at IBM.The translation models they presented in various papers between 1988 and 1993 (Brown et al. 1988; Brown et al.1990; Brown, Della Pietra, Della Pietra, and Mercer 1993) are commonly referred to as IBM models 1–5, based on the numbering in Brown, Della Pietra, Della Pietra, and Mercer (1993).The underlying (probabilistic) lexicon contains only pairs of full forms.On the other hand, Brown et al. (1992) had already suggested word forms be annotated with morpho-syntactic information, but they did not perform any investigation on the effects.et al. (2000), have dealt with the problem of translation with scarce resources.AlOnaizan et al. report on an experiment involving Tetun-to-English translation by different groups, including one using statistical machine translation.Al-Onaizan et al. assume the absence of linguistic knowledge sources such as morphological analyzers and dictionaries.Nevertheless, they found that the human mind is very well capable of deriving dependencies such as morphology, cognates, proper names, and spelling variations and that this capability was finally at the basis of the better results produced by humans compared to corpus-based machine translation.The additional information results from complex reasoning, and it is not directly accessible from the full-wordform representation in the data.This article takes a different point of view: Even if full bilingual training data are scarce, monolingual knowledge sources like morphological analyzers and data for training the target language model as well as conventional dictionaries (one word and its translation[s] per entry) may be available and of substantial usefulness for improving the performance of statistical translation systems.This is especially the case for more-inflecting major languages like German.The use of dictionaries to augment or replace parallel corpora has already been examined by Brown, Della Pietra, Della Pietra, and Goldsmith (1993) and Koehn and Knight (2001), for instance.A prerequisite for the methods for improving the quality of statistical machine translation described in this article is the availability of various kinds of morphological and syntactic information.This section describes the output resulting from morphosyntactic analysis and explains which parts of the analysis are used and how the output is represented for further processing.For obtaining the required morpho-syntactic information, the following analyzers for German and English were applied: gertwol and engtwol for lexical analysis and gercg and engcg for morphological and syntactic disambiguation.For a description of the underlying approach, the reader is referred to Karlsson (1990).Tables 1 and 2 give examples of the information provided by these tools.The examples in Tables 1 and 2 demonstrate the capability of the tools to disambiguate among different readings: For instance, they infer that the word wollen is a verb in the indicative present first-person plural form.Without any context taken into account, Sample analysis of a German sentence.Input: Wir wollen nach dem Abendessen nach Essen aufbrechen.(In English: We want to start for Essen after dinner.)Original Base form Tags Wir wir personal-pronoun plural first nominative wollen wollen verb indicative present plural first nach nach preposition dative dem das definite-article singular dative neuter Abendessen Abend#essen noun neuter singular dative nach nach preposition dative Essen Essen noun name neuter singular dative Esse noun feminine plural dative Essen noun neuter plural dative Essen noun neuter singular dative aufbrechen auflbrechen verb separable infinitive Sample analysis of an English sentence.Input: Do we have to reserve rooms?.Original Base form Tags Do do verb present not-singular-third finite auxiliary we we personal-pronoun nominative plural first subject have have verb infinitive not-finite main to to infinitive-marker reserve reserve verb infinitive not-finite main rooms room noun nominative plural object wollen has other readings.It can even be interpreted as derived from an adjective with the meaning “made of wool.” The inflected word forms on the German part of the Verbmobil (cf.Section 7.1.1) corpus have on average 2.85 readings (1.86 for the English corpus), 58% of which can be eliminated by the syntactic analyzers on the basis of sentence context.Common bilingual corpora normally contain full sentences, which provide enough context information for ruling out all but one reading for an inflected word form.To reduce the remaining uncertainty, preference rules have been implemented.For instance, it is assumed that the corpus is correctly true-case-converted beforehand, and as a consequence, non-noun readings of uppercase words are dropped.Furthermore, indicative verb readings are preferred to subjunctive or imperative.In addition, some simple domain-specific heuristics are applied.The reading “plural of Esse” for the German word form Essen, for instance, is much less likely in the domain of appointment scheduling and travel arrangements than the readings “proper name of the town Essen” or the German equivalent of the English word meal.As can be seen in Table 3, the reduction in the number of readings resulting from these preference rules is fairly small in the case of the Verbmobil corpus.The remaining ambiguity often lies in those parts of the information which are not used or which are not relevant to the translation task.For example, the analyzers cannot tell accusative from dative case in German, but the case information is not essential for the translation task (see also Table 4).Section 2.4 describes a method for selecting morpho-syntactic tags considered relevant for the translation task, which results in a further reduction in the number of readings per word form to 1.06 for German and 1.01 for English.In these rare cases of ambiguity it is admissible to resort to the unambiguous parts of the readings, that is, to drop all tags causing mixed interpretations.Table 3 summarizes the gradual resolution of ambiguity.The analysis of conventional dictionaries poses some special problems, because they do not provide enough context to enable effective disambiguation.For handling this special situation, dedicated methods have been implemented; these are presented in Section 5.1.A full word form is represented by the information provided by the morpho-syntactic analysis: from the interpretation gehen verb indicative present first singular, that is, the base form plus part of speech plus the other tags, the word form gehe can be restored.It has already been mentioned that the analyzers can disambiguate among different readings on the basis of context information.In this sense, the information inherent in the original word forms is augmented by the disambiguating analyzer.This can be useful for choosing the correct translation of ambiguous words.Of course, these disambiguation clues result in an enlarged vocabulary.The vocabulary of the new representation of the German part of the Verbmobil corpus, for example, in which full word forms are replaced by base form plus morphological and syntactic tags (lemmatag representation), is one and a half times as large as the vocabulary of the original corpus.On the other hand, the information in the lemma-tag representation can be accessed gradually and ultimately reduced: For example, certain instances of words can be considered equivalent.This fact is used to better exploit the bilingual training data along two directions: detecting and omitting unimportant information (see Section 2.4) and constructing hierarchical translation models (see Section 4).To summarize, the lemma-tag representation of a corpus has the following main advantages: It makes context information locally available, and it allows information to be explicitly accessed at different levels of abstraction.Inflected word forms in the input language often contain information that is not relevant for translation.This is especially true for the task of translating from a more inflecting language like German into English, for instance: In parallel German/English corpora, the German part contains many more distinct word forms than the English part (see, for example, Table 5).It is useful for the process of statistical machine translation to define equivalence classes of word forms which tend to be translated by the same target language word: The resulting statistical translation lexicon becomes smoother, and the coverage is considerably improved.Such equivalence classes are constructed by omitting those items of information from morpho-syntactic analysis which are not relevant for translation.The lemma-tag representation of the corpus helps to identify the unimportant information.The definition of relevant and unimportant information, respectively, depends on many factors like the languages involved, the translation direction, and the choice of the models.We detect candidates for equivalence classes of words automatically from the probabilistic lexicon trained for translation from German to English.For this purpose, those inflected forms of the same base form which result in the same translation are inspected.For each set of tags T, the algorithm counts how often an additional tag t1 can be replaced with a certain other tag t2 without effect on the translation.As an example, let T = ‘blau-adjective’, t1 =‘masculine’ and t2 =‘feminine’.The two entries (‘blau-adjective-masculine’|‘blue’) and (‘blau-adjective-feminine’|‘blue’) are hints for detecting gender as nonrelevant when translating adjectives into English.Table 4 lists some of the most frequently identified candidates to be ignored while translating: The gender of nouns is irrelevant for their translation (which is straightforward, as the gender of a noun is unambiguous), as are the cases nominative, dative, accusative.(For the genitive forms, the translation in English differs.)For verbs the candidates number and person were found: The translation of the first-person singular form of a verb, for example, is often the same as the translation of the third-person plural form.Ignoring (dropping) those tags most often identified as irrelevant for translation results in the building of equivalence classes of words.Doing so results in a smaller vocabulary, one about 65.5% the size of the vocabulary of the full lemmatag representation of the Verbmobil corpus, for example—it is even smaller than the vocabulary of the original full-form corpus.The information described in this section is used to improve the quality of statistical machine translation and to better exploit the available bilingual resources.Difference in sentence structure is one of the main sources of errors in machine translation.It is thus promising to “harmonize” the word order in corresponding sentences.The presentation in this section focuses on the following aspects: question inversion and separated verb prefixes.For a more detailed discussion of restructuring for statistical machine translation the reader is referred to NieBen and Ney (2000, 2001).In many languages, the sentence structure of questions differs from the structure in declarative sentences in that the order of the subject and the corresponding finite verb is inverted.From the perspective of statistical translation, this behavior has some disadvantages: The algorithm for training the parameters of the target language model Pr(eI1), which is typically a standard n-gram model, cannot deduce the probability of a word sequence in an interrogative sentence from the corresponding declarative form.The same reasoning is valid for the lexical translation probabilities of multiwordphrase pairs.To harmonize the word order of questions with the word order in declarative sentences, the order of the subject (including the appendant articles, adjectives etc.) and the corresponding finite verb is inverted.In English questions supporting dos are removed.The application of the described preprocessing step in the bilingual training corpus implies the necessity of restoring the correct forms of the translations produced by the machine translation algorithm.This procedure was suggested by Brown et al. (1992) for the language pair English and French, but they did not report on experimental results revealing the effect of the restructuring on the translation quality.German prefix verbs consist of a main part and a detachable prefix, which can be shifted to the end of the clause.For the automatic alignment process, it is often difficult to associate one English word with more than one word in the corresponding German sentence, namely, the main part of the verb and the separated prefix.To solve the problem of separated prefixes, all separable word forms of verbs are extracted from the training corpus.The resulting list contains entries of the form prefix|main.In all clauses containing a word matching a main part and a word matching the corresponding prefix part occurring at the end of the clause, the prefix is prepended to the beginning of the main part.In general, the probabilistic lexicon resulting from training the translation model contains all word forms occurring in the training corpus as separate entries, not taking into account whether or not they are inflected forms of the same lemma.Bearing in mind that typically more than 40% of the word forms are seen only once in training (see, for example, Table 5), it is obvious that for many words, learning the correct translations is difficult.Furthermore, new input sentences are expected to contain unknown word forms, for which no translation can be retrieved from the lexicon.This problem is especially relevant for more-inflecting languages like German: Texts in German contain many more distinct word forms than their English translations.Table 5 also reveals that these words are often generated via inflection from a smaller set of base forms.As mentioned in Section 2.3, the lemma-tag representation of the information from morpho-syntactic analysis makes it possible to gradually access information with different grades of abstraction.Consider, for example, the German verb form ankomme, which is the indicative present first-person singular form of the lemma ankommen and can be translated into English by arrive.The lemma-tag representation provides an “observation tuple” consisting of In the following, ti0 = t0,..., ti denotes the representation of a word where the base form t0 and i additional tags are taken into account.For the example above, t0 = ankommen, t1 = verb, and so on.The hierarchy of equivalence classes F0, ... ,Fn is as follows: where n is the maximum number of morpho-syntactic tags.The mapping from the full lemma-tag representation back to inflected word forms is generally unambiguous; thus Fn contains only one element, namely, ankomme.Fn−1 contains the forms ankomme, ankommst, and ankommt; in Fn−2 the number (singular or plural) is ignored, and so on.The largest equivalence class contains all inflected forms of the base form ankommen.1 Section 4.2 introduces the concept of combining information at different levels of abstraction.In modeling for statistical machine translation, a hidden variable aJ1, denoting the hidden alignment between the words in the source and target languages, is usually introduced into the string translation probability: In the following, Tj = �tn � j denotes the lemma-tag representation of the jth word in 0 the input sentence.The sequence TJ1 stands for the sequence of readings for the word sequence fJ1 and can be introduced as a new hidden variable: 1 The order of omitting tags can be defined in a natural way depending on the part of speech.In principle this decision can also be left to the maximum-entropy training, when features for all possible sets of tags are defined, but this would cause the number of parameters to explode.As the experiments in this work have been carried out only with up to three levels of abstraction as defined in Section 4.2, the set of tags of the intermediate level is fixed, and thus the priority of the tags needs not be specified.The relation between this equivalence class hierarchy and the suggestions in Section 2.4 is clear: Choosing candidates for morpho-syntactic tags not relevant for translation amounts to fixing a level in the hierarchy.This is exactly what has been done to define the intermediate level in Section 4.2.Nießen and Ney SMT with Scarce Resources Let T (fj) be the set of interpretations which are regarded valid readings of fj by the morpho-syntactic analyzers on the basis of the whole-sentence context fJ1.We assume that the probability functions defined above yield zero for all other readings, that is, when Tj V T (fj).Under the usual independence assumption, which states that the probability of the translation of words depends only on the identity of the words associated with each other by the word alignment, we get As has been argued in Section 2.2, the number of readings |T (fj) |per word form can be reduced to one for the tasks for which experimental results are reported here.The elements in equation (4) are the joint probabilities p(f, T|e) off and the readings T of f given the target language word e. The maximum-entropy principle recommends choosing for p the distribution which preserves as much uncertainty as possible in terms of maximizing the entropy, while requiring p to satisfy constraints which represent facts known from the data.These constraints are encoded on the basis of feature functions hm(x), and the expectation of each feature hm over the model p is required to be equal to the observed expectation.The maximum-entropy model can be shown to be unique and to have an exponential form involving a weighted sum over the feature functions hm (Ratnaparkhi 1997).In equation (5), the notation tn0 is used again for the lemma-tag representation of an input word (this was denoted by T in equations (2)–(4) for notational simplicity): where Λ = {Am} is the set of model parameters with one weight Am for each feature function hm.These model parameters can be trained using converging iterative training procedures like the ones described by Darroch and Ratcliff (1972) or Della Pietra, Della Pietra, and Lafferty (1995).In the experiments presented in this article, the sum over the word forms f˜ and the readings ˜tn0 in the denominator of equation (5) is restricted to the readings of word forms having the same base form and partial reading as a word form f&quot; aligned at least once to e. The new lexicon model pΛ(f, tn0|e) can now replace the usual lexicon model p(f |e), over which it has the following main advantages: f (that is, for tn0 E� T (f)) amounts to making context information from the complete sentence f1J locally available: The sentence context was taken into account by the morpho-syntactic analyzer, which chose the valid readings T (f).4.2.1 Definition of Feature Functions.There are numerous possibilities for defining feature functions.We do not need to require that they all have the same parametric form or that the components be disjoint and statistically independent.Still, it is necessary to restrict the number of parameters so that optimizing them is practical.We used the following types of feature functions, which have been defined on the basis of the lemma-tag representation (see Section 2.3): First level: m = IL,˜e}, where L is the base form: Second level: m = IT, L,˜e}, with subsets T of cardinality < n of morpho-syntactic tags considered relevant (see Section 2.4 for a description of the detection of relevant tags): In terms of the hierarchy introduced in Section 4.1, this means that information at three different levels in the hierarchy is combined.The subsets T of relevant tags mentioned previously fix the intermediate level.2 This choice of the types of features as well as the choice of the subsets T is reasonable but somewhat arbitrary.Alternatively one can think of defining a much more general set of features and applying some method of feature selection, as has been done, for example, by Foster (2000), who compared different methods for feature selection within the task of translation modeling for statistical machine translation.Note that the log-linear model introduced here uses one parameter per feature.For the Verbmobil task, for example, there are approximately 162, 000 parameters: 47,800 for the first-order features, 55,700 for the second-order features, and 58,500 for the third-order features.No feature selection or threshold was applied: All features seen in training were used. lexicon models is depicted in Figure 1.This figure includes the possibility of using restructuring operations as suggested in Section 3 in order to deal with structural differences between the languages involved.This can be especially advantageous in the case of multiword phrases which jointly fulfill a syntactic function: Not merging them 2 Of course, there is not only one set of relevant tags, but at least one per part of speech.In order to keep the notation as simple as possible, this fact is not accounted for in the formulas and the textual descriptions.Training and test with hierarchical lexicon.“(Inverse) restructuring,” “analyze,” and “annotation” all require morpho-syntactic analysis of the transformed sentences. would raise the question of how to distribute the syntactic tags which have been associated with the whole phrase.In Section 5.2 we describe a method of learning multiword phrases using conventional dictionaries.The alignment on the training corpus is trained using the original source language corpus containing inflected word forms.This alignment is then used to count the co-occurrences of the annotated “words” in the lemma-tag representation of the source language corpus with the words in the target language corpus.These event counts are used for the maximum-entropy training of the model parameters Λ.The probability mass is distributed over (all readings of) the source language word forms to be supported for test (not necessarily restricted to those occurring during training).The only precondition is that the firing features for these unseen events are known.This “vocabulary supported in test,” as it is called in Figure 1, can be a predefined closed vocabulary, as is the case in Verbmobil, in which the output of a speech recognizer with limited output vocabulary is to be translated.In the easiest case it is identical to the vocabulary found in the source language part of the training corpus.The other extreme would be an extended vocabulary containing all automatically generated inflected forms of all base forms occurring in the training corpus.This vocabulary is annotated with morpho-syntactic tags, ideally under consideration of all possible readings of all word forms.To enable the application of the hierarchical lexicon model, the source language input sentences in test have to be analyzed and annotated with their lemma-tag representation before the actual translation process.So far, the sum over the readings in equation (4) has been ignored, because when the techniques for reducing the amount of ambiguity described in Section 2.2 and the disambiguated conventional dictionaries resulting from the approach presented in Section 5.1 are applied, there remains almost always only one reading per word form.Conventional dictionaries are often used as additional evidence to better train the model parameters in statistical machine translation.The expression conventional dictionary here denotes bilingual collections of word or phrase pairs predominantly collected “by hand,” usually by lexicographers, as opposed to the probabilistic lexica, which are learned automatically.Apart from the theoretical problem of how to incorporate external dictionaries in a mathematically sound way into a statistical framework for machine translation (Brown, Della Pietra, Della Pietra, and Goldsmith 1993) there are also some pragmatic difficulties: As discussed in Section 2.2, one of the disadvantages of these conventional dictionaries as compared to full bilingual corpora is that their entries typically contain single words or short phrases on each language side.Consequently, it is not possible to distinguish among the translations for different readings of a word.In normal bilingual corpora, the words can often be disambiguated by taking into account the sentence context in which they occur.For example, from the context in the sentence Ich werde die Zimmer buchen, it is possible to infer that Zimmer in this sentence is plural and has to be translated by rooms in English, whereas the correct translation of Zimmer in the sentence Ich h¨atte gerne ein Zimmer is the singular form room.The dictionary used by our research group for augmenting the bilingual data contains two entries for Zimmer: (‘Zimmer’|‘room’) and (‘Zimmer’|‘rooms’).The approach described in this section is based on the observation that in many of the cases of ambiguous entries in dictionaries, the second part of the entry—that is, the other-language side—contains the information necessary to decide upon the interpretation.In some other cases, the same kind of ambiguity is present in both languages, and it would be possible and desirable to associate the (semantically) corresponding readings with one another.The method proposed here takes advantage of these facts in order to disambiguate dictionary entries.Figure 2 sketches the procedure for the disambiguation of a conventional dictionary D. In addition to D, a bilingual corpus C1 of the same language pair is required to train the probability model for tag sequence translations.The word forms in C1 need not match those in D. C1 is not necessarily the training corpus for the translation task in which the disambiguated version of D will be used.It does not even have to be taken from the same domain.A word alignment between the sentences in C1 is trained with some automatic alignment algorithm.Then the words in the bilingual corpus are replaced by a reduced form of their lemma-tag representation, in which only a subset of their morpho-syntactic tags is retained—even the base form is dropped.The remaining subset of tags, in the following denoted by Tf for the source language and Te for the target language, consists of tags considered relevant for the task of aligning corresponding readings.This is not necessarily the same set of tags considered relevant for the task of translation which was used, for example, to fix the intermediate level for the log-linear lexicon Disambiguation of conventional dictionaries.“Learn phrases,” “analyze,” and “annotation” require morpho-syntactic analysis of the transformed sentences. combination in Section 4.2.1.In the case of the Verbmobil corpus, the maximum length of a tag sequence is five.The alignment is used to count the frequency of a certain tag sequence tf in the source language to be associated with another tag sequence te in the target language and to compute the tag sequence translation probabilities p(tf|te) as relative frequencies.For the time being, these tag sequence translation probabilities associate readings of words in one language with readings of words in the other language: Multiword sequences are not accounted for.To alleviate this shortcoming it is possible and advisable to automatically detect and merge multiword phrases.As will be described in Section 5.2, the conventional bilingual dictionary itself can be used to learn and validate these phrases.The resulting multiword phrases Pe for the target language and Pf for the source language are afterwards concatenated within D to form entries consisting of pairs of “units.” The next step is to analyze the word forms in D and generate all possible readings of all entries.It is also possible to ignore those readings that are considered unlikely for the task under consideration by applying the domain-specific preference rules proposed in Section 2.2.The process of generating all readings includes replacing word forms with their lemma-tag representation, which is thereafter reduced by dropping all morpho-syntactic tags not contained in the tag sets Tf and Te.Using the tag sequence translation probabilities p(tf|te), the readings in one language are aligned with readings in the other language.These alignments are applied to the full lemma-tag representation (not only tags in Tf and Te) of the expanded dictionary containing one entry per reading of the original word forms.The highest-ranking aligned readings according to p(tf|te) for each lemma are preserved.The resulting disambiguated dictionary contains two entries for the German word Zimmer: (‘Zimmer-noun-sg.’|‘room-noun-sg.’) and (‘Zimmer-noun-pl.’|‘room-nounpl.’).The target language part is then reduced to the surface forms: (‘Zimmer-noun-sg.’| ‘room’) and (‘Zimmer-noun-pl.’|‘rooms’).Note that this augmented dictionary, in the following denoted by D', has more entries than D as a result of the step of generating all readings.The two entries (‘beabsichtigt’|‘intends’) and (‘beabsichtigt’|‘intended’), for example, produce three new entries: (‘beabsichtigt-verb-ind.-pres.-sg.3rd’|‘intends’), (‘beabsichtigt-verb-past-part.’|‘intended’), and (‘beabsichtigtadjective-pos.’|‘intended’).Some recent publications deal with the automatic detection of multiword phrases (Och and Weber 1998; Tillmann and Ney 2000).These methods are very useful, but they have one drawback: They rely on sufficiently large training corpora, because they detect the phrases from automatically learned word alignments.In this section a method for detecting multiword phrases is suggested which merely requires monolingual syntactic analyzers and a conventional dictionary.Some multiword phrases which jointly fulfill a syntactic function are provided by the analyzers.The phrase irgend etwas (‘anything’), for example, may form either an indefinite determiner or an indefinite pronoun. irgend=etwas is merged by the analyzer in order to form one single vocabulary entry.In the German part of the Verbmobil training corpus 26 different, nonidiomatic multiword phrases are merged, while there are 318 phrases suggested for the English part.In addition, syntactic information like the identification of infinitive markers, determiners, modifying adjectives (for example, single room), premodifying adverbials (more comfortable), and premodifying nouns (account number) are used for detecting multiword phrases.When applied to the English part of the Verbmobil training corpus, these hints suggest 7,225 different phrases.Altogether, 26 phrases for German and about 7,500 phrases for English are detected in this way.It is quite natural that there are more multiword phrases found for English, as German, unlike English, uses compounding.But the experiments show that it is not advantageous to use all these phrases for English.Electronic dictionaries can be useful for detecting those phrases which are important in a statistical machine translation context: A multiword phrase is considered useful if it is translated into a single word or a distinct multiword phrase (suggested in a similar way by syntactic analysis) in another language.There are 290 phrases chosen in this way for the English language.Taking into account the interdependencies of inflected forms of the same base form is especially relevant when inflected languages like German are involved and when training data are sparse.In this situation many of the inflected word forms to account for in test do not occur during training.Sparse bilingual training data also make additional conventional dictionaries especially important.Enriching the dictionaries by aligning corresponding readings is particularly useful when the dictionaries are used in conjunction with a hierarchical lexicon, which can access the information necessary to distinguish readings via morpho-syntactic tags.The restructuring operations described in Section 3 also help in coping with the data sparseness problem, because they make corresponding sentences more similar.This section proposes a procedure for combining all these methods in order to improve the translation quality despite sparseness of data.Figure 3 sketches the proposed procedure.Training with scarce resources.“Restructuring,” “learn phrases,” and “annotation” all require morpho-syntactic analysis of the transformed sentences.Two different bilingual corpora C1 and C2, one monolingual target language corpus, and a conventional bilingual dictionary D can contribute in various ways to the overall result.It is important to note here that C1 and C2 can, but need not, be distinct, and that the monolingual corpus can be identical to the target language part of C2.Furthermore these corpora can be taken from different domains, and C1 can be (very) small.Only C2 has to represent the domain and the vocabulary for which the translation system is built, and only the size of C2 and the monolingual corpus have a substantial effect on the translation quality.It is interesting to note, though, that a basic statistical machine translation system with an accuracy near 50% can be built without any domain-specific bilingual corpus C2, solely on the basis of a disambiguated dictionary and the hierarchical lexicon models, as Table 9 shows. can be comparatively small, given the limited number of tag sequence pairs (tf|te) for which translation probabilities must be provided: In the Verbmobil training corpus, for example, there are only 261 different German and 110 different English tag sequences.• In the next step, the second bilingual corpus C2 and D' are combined, and a word alignment A for both is trained.C2, D', and A are presented as input to the maximum-entropy training of a hierarchical lexicon model as described in Section 4.2.• The language model can be trained on a separate monolingual corpus.As monolingual data are much easier and cheaper to compile, this corpus might be (substantially) larger than the target language part of C2.Tests were carried out on Verbmobil data and on Nespole! data.As usual, the sentences from the test sets were not used for training.The training corpora were used for training the parameters of IBM model 4.7.1.1 Verbmobil.Verbmobil was a project for automatic translation of spontaneously spoken dialogues.A detailed description of the statistical translation system within Verbmobil is given by Ney et al. (2000) and by Och (2002).Table 5 summarizes the characteristics of the English and German parallel corpus used for training the parameters of IBM model 4.A conventional dictionary complements the training corpus (see Table 6 for the statistics).The vocabulary in Verbmobil was considered closed: There are official lists of word forms which can be produced by the speech recognizers.Such lists exist for German and English (see Table 7).Table 8 lists the characteristics of the two test sets Test and Develop taken from the end-to-end evaluation in Verbmobil, the development part being meant to tune system parameters on a held-out corpus different from the training as well as the test corpus.As no parameters are optimized on the development set for the methods described in this article, most of the experiments were carried out on a joint set containing both test sets.7.1.2 Nespole!.Nespole! is a research project that ran from January 2000 to June 2002.It aimed to provide multimodel support for negotiation (Nespole!2000; Lavie et al. 2001).Table 5 summarizes the corpus statistics of the Nespole! training set.Table 8 provides the corresponding figures for the test set used in this work.For testing we used the alignment template translation system, described in Och, Tillmann, and Ney (1999).Training the parameters for this system entails training of IBM model 4 parameters in both translation directions and combining the resulting alignments into one symmetrized alignment.From this symmetrized alignment, the lexicon probabilities as well as the so-called alignment templates are extracted.The latter are translation patterns which capture phrase-level translation pairs.The following evaluation criteria were used in the experiments: BLEU (Bilingual Evaluation Understudy): This score, proposed by Papineni et al. (2001), is based on the notion of modified n-gram precision, with n ∈ {1,...,4}: All candidate unigram, bigram, trigram, and four-gram counts are collected and clipped against their corresponding maximum reference counts.The reference n-gram counts are calculated on a corpus of reference translations for each input sentence.The clipped candidate counts are summed and normalized by the total number of candidate ngrams.The geometric mean of the modified precision scores for a test corpus is calculated and multiplied by an exponential brevity penalty factor to penalize too-short translations.BLEU is an accuracy measure, while the others are error measures. m-WER (multireference word error rate): For each test sentence there is a set of reference translations.For each translation hypothesis, the edit distance (number of substitutions, deletions, and insertions) to the most similar reference is calculated.SSER (subjective sentence error rate): Each translated sentence is judged by a human examiner according to an error scale from 0.0 (semantically and syntactically correct) to 1.0 (completely wrong).ISER (information item semantic error rate): The test sentences are segmented into information items; for each of these items, the translation candidates are assigned either “OK” or an error class.If the intended information is conveyed, the translation of an information item is considered correct, even if there are slight syntactic errors which do not seriously deteriorate the intelligibility.For evaluating the SSER and the ISER, we have used the evaluation tool EvalTrans (Nießen and Leusch 2000), which is designed to facilitate the work of manually judging evaluation quality and to ensure consistency over time and across evaluators.It is a costly and time-consuming task to compile large texts and have them translated to form bilingual corpora suitable for training the model parameters for statistical machine translation.As a consequence, it is important to investigate the amount of data necessary to sufficiently cover the vocabulary expected in testing.Furthermore, we want to examine to what extent the incorporation of morphological knowledge sources can reduce this amount of necessary data.Figure 4 shows the relation between the size of a typical German corpus and the corresponding number of different full forms.At the size of 520,000 words, the size of the Verbmobil corpus used for training, this curve still has a high growth rate.To investigate the impact of the size of the bilingual corpus available for training, on translation quality three different setups for training the statistical lexicon on Verbmobil data have been defined: The language model is always trained on the full English corpus.The argument for this is that monolingual corpora are always easier and less expensive to obtain than bilingual corpora.A conventional dictionary is used in all three setups to complement Impact of corpus size (measured in number of running words in the corpus) on vocabulary size (measured in number of different full-form words found in the corpus) for the German part of the Verbmobil corpus. the bilingual corpus.In the last setup, the lexicon probabilities are trained exclusively on this dictionary As Table 9 shows, the quality of translation drops significantly when the amount of bilingual data available during training is reduced: When the training corpus is restricted to 5,000 sentences, the SSER increases by about 7% and the ISER by about 3%.As could be expected, the translations produced by the system trained exclusively on a conventional dictionary are very poor: The SSER jumps over 60%.7.5.1 Results on the Verbmobil Task.As was pointed out in Section 4, the hierarchical lexicon is expected to be especially useful in cases in which many of the inflected word forms to be accounted for in test do not occur during training.To systematically investigate the model’s generalization capability, it has been applied on the three different setups described in Section 7.4.The training procedure was the one proposed in Section 6, which includes restructuring transformations in training and test.Table 9 summarizes the improvement achieved for all three setups.Training on 58,000 sentences plus conventional dictionary: Compared to the effect of restructuring, the additional improvement achieved with the hierarchical lexicon is relatively small in this setup.The combination of all methods results in a relative improvement in terms of SSER of almost 13% and in terms of information ISER of more than 16% as compared to the baseline.Training on 5,000 sentences plus conventional dictionary: Restructuring alone can improve the translation quality from 37.3% to 33.6%.The benefit from the hierarchical lexicon is larger in this setup, and the resulting in SSER is 31.8%.This is a relative improvement of almost 15%.The relative improvement in terms of ISER is almost 22%.Note that by applying the methods proposed here, the corpus for training can be reduced to less than 10% of the original size while increasing the SSER only from 30.2% to 31.8% compared to the baseline when using the full corpus.Training only on conventional dictionary: In this setup the impact of the hierarchical lexicon is clearly larger than the effect of the restructuring methods, because here the data sparseness problem is much more important than the word order problem.The overall relative reduction in terms of SSER is 13.7% and in terms of ISER 19.1%.An error rate of about 52% is still very poor, but it is close to what might be acceptable when only the gist of the translated document is needed, as is the case in the framework of document classification or multilingual information retrieval.Examples taken from the Verbmobil Eval-2000 test set are given in Table 10.Smoothing the lexicon probabilities over the inflected forms of the same lemma enables the translation of sind as would instead of are.The smoothed lexicon contains the translation convenient for any inflected form of bequem.The comparative more convenient would be the completely correct translation.The last two examples in the table demonstrate the effect of the disambiguating analyzer, which on the basis of the sentence context identifies Zimmer as plural (it has been translated into the singular form room by the baseline system) and das as an article to be translated by the instead of a pronoun which would be translated as that.The last example demonstrates that overfitting on domain-specific training can be problematic in some cases: Generally, because is a good translation for the co-ordinating conjunction denn, but in the appointmentscheduling domain, denn is often an adverb, and it often occurs in the same sentence as dann, as in Wie w¨are es denn dann?.The translation for this sentence is something like How about then?.Because of the frequency of this domain-specific language use, the word form denn is often aligned to then in the training corpus.The hierarchical Examples of the effect of the hierarchical lexicon.Input sind Sie mit einem Doppelzimmer einverstanden?Baseline are you agree with a double room?Hierarchical lexicon would you agree with a double room?Input mit dem Zug ist es bequemer.Baseline by train it is UNKNOWN-bequemer.Hierarchical lexicon by train it is convenient.Input wir haben zwei Zimmer.Baseline we have two room.Hierarchical lexicon we have two rooms.Input ich w¨urde das Hilton vorschlagen denn es ist das beste.Baseline I would suggest that Hilton then it is the best.Hierarchical lexicon I would suggest the Hilton because it is the best. lexicon distinguishes the adverb reading and the conjunction reading, and the correct translation because is the highest-ranking one for the conjunction.7.5.2 Results on the Nespole!Task.We were provided with a small German-English corpus from the Nespole! project (see Section 7.1 for a description).From Table 5 it is obvious that this task is an example of very scarce training data, and it is thus interesting to test the performance of the methods proposed in this article on this task.The same conventional dictionary as was used for the experiments on Verbmobil data (cf.Table 6) complemented the small bilingual training corpus.Furthermore, the (monolingual) English part of the Verbmobil corpus was used in addition to the English part of the Nespole! corpus for training the language model.Table 11 summarizes the results.Information items have not been defined for this test set.An overall relative improvement of 16.5% in the SSER can be achieved.In this article we have proposed methods of incorporating morphological and syntactic information into systems for statistical machine translation.The overall goal was to improve translation quality and to reduce the amount of parallel text necessary to Results for hierarchical lexicon model Nespole!“Restructuring” entails treatment of question inversion and separated verb prefixes as well as merging of phrases in both languages.The same conventional dictionary was used as in the experiments the Verbmobil.The language model was trained on a combination of the English parts of the Nespole! corpus and the Verbmobil corpus. train the model parameters.Substantial improvements on the Verbmobil task and the Nespole! task were achieved.Some sentence-level restructuring transformations have been introduced which are motivated by knowledge about the sentence structure in the languages involved.These transformations aim at the assimilation of word orders in related sentences.A hierarchy of equivalence classes has been defined on the basis of morphological and syntactic information beyond the surface forms.The study of the effect of using information from either degree of abstraction led to the construction of hierarchical lexicon models, which combine different items of information in a log-linear way.The benefit from these combined models is twofold: First, the lexical coverage is improved, because the translation of unseen word forms can be derived by considering information from lower levels in the hierarchy.Second, category ambiguity can be resolved, because syntactical context information is made locally accessible by means of annotation with morpho-syntactic tags.As a side effect of the preparative work for setting up the underlying hierarchy of morpho-syntactic information, those pieces of information inherent in fully inflected word forms that are not relevant for translation are detected.A method for aligning corresponding readings in conventional dictionaries containing pairs of fully inflected word forms has been proposed.The approach uses information deduced from one language side to resolve category ambiguity in the corresponding entry in the other language.The resulting disambiguated dictionaries have proven to be better suited for improving the quality of machine translation, especially if they are used in combination with the hierarchical lexicon models.The amount of bilingual training data required to achieve an acceptable quality of machine translation has been systematically investigated.All the methods mentioned previously contribute to a better exploitation of the available bilingual data and thus to improving translation quality in frameworks with scarce resources.Three setups for training the parameters of the statistical lexicon on Verbmobil data have been examined: (1) Using the full 58,000 sentences comprising the bilingual training corpus, (2) restricting the corpus to 5,000 sentences, and (3) using only a conventional dictionary.For each of these setups, a relative improvement in terms of subjective sentence error rate between 13% and 15% as compared to the baseline could be obtained using combinations of the methods described in this article.The amount of bilingual training data could be reduced to less than 10% of the original corpus, while losing only 1.6% in accuracy as measured by the subjective sentence error rate.A relative improvement of 16.5% in terms of subjective sentence error rate could also be achieved on the Nespole! task.This work has been partially supported as part of the Verbmobil project (contract number 01 IV 701 T4) by the German Federal Ministry of Education, Science, Research and Technology and as part of the EuTrans project (project number 30268) by the European Union.For the provision of the Nespole! data we thank the Nespole! consortium, listed on the project’s home page (Nespole!2000).Special thanks to Alon Lavie, Lori Levin, Stephan Vogel, and Alex Waibel (in alphabetical order).
Articles: Recognizing Contextual Polarity: An Exploration of Features for Phrase-Level Sentiment AnalysisMany approaches to automatic sentiment analysis begin with a large lexicon of words marked with their prior polarity (also called semantic orientation).However, the contextual polarity of the phrase in which a particular instance of a word appears may be quite different from the word’s prior polarity.Positive words are used in phrases expressing negative sentiments, or vice versa.Also, quite often words that are positive or negative out of context are neutral in context, meaning they are not even being used to express a sentiment.The goal of this work is to automatically distinguish between prior and contextual polarity, with a focus on understanding which features are important for this task.Because an important aspect of the problem is identifying when polar terms are being used in neutral contexts, features for distinguishing between neutral and polar instances are evaluated, as well as features for distinguishing between positive and negative contextual polarity.The evaluation includes assessing the performance of features across multiple machine learning algorithms.For all learning algorithms except one, the combination of all features together gives the best performance.Another facet of the evaluation considers how the presence of neutral instances affects the performance offeatures for distinguishing between positive and negative polarity.These experiments show that the presence of neutral instances greatly degrades the performance of these features, and that perhaps the best way to improve performance across all polarity classes is to improve the system’s ability to identify when an instance is neutral.Many approaches to automatic sentiment analysis begin with a large lexicon of words marked with their prior polarity (also called semantic orientation).However, the contextual polarity of the phrase in which a particular instance of a word appears may be quite different from the word’s prior polarity.Positive words are used in phrases expressing negative sentiments, or vice versa.Also, quite often words that are positive or negative out of context are neutral in context, meaning they are not even being used to express a sentiment.The goal of this work is to automatically distinguish between prior and contextual polarity, with a focus on understanding which features are important for this task.Because an important aspect of the problem is identifying when polar terms are being used in neutral contexts, features for distinguishing between neutral and polar instances are evaluated, as well as features for distinguishing between positive and negative contextual polarity.The evaluation includes assessing the performance of features across multiple machine learning algorithms.For all learning algorithms except one, the combination of all features together gives the best performance.Another facet of the evaluation considers how the presence of neutral instances affects the performance offeatures for distinguishing between positive and negative polarity.These experiments show that the presence of neutral instances greatly degrades the performance of these features, and that perhaps the best way to improve performance across all polarity classes is to improve the system’s ability to identify when an instance is neutral.Sentiment analysis is a type of subjectivity analysis (Wiebe 1994) that focuses on identifying positive and negative opinions, emotions, and evaluations expressed in natural language.It has been a central component in applications ranging from recognizing inflammatory messages (Spertus 1997), to tracking sentiments over time in online discussions (Tong 2001), to classifying positive and negative reviews (Pang, Lee, and Vaithyanathan 2002; Turney 2002).Although a great deal of work in sentiment analysis has targeted documents, applications such as opinion question answering (Yu and Hatzivassiloglou 2003; Maybury 2004; Stoyanov, Cardie, and Wiebe 2005) and review mining to extract opinions about companies and products (Morinaga et al. 2002; Nasukawa and Yi 2003) require sentence-level or even phrase-level analysis.For example, if a question answering system is to successfully answer questions about people’s opinions, it must be able not only to pinpoint expressions of positive and negative sentiments, such as we find in sentence (1), but also to determine when an opinion is not being expressed by a word or phrase that typically does evoke one, such as condemned in sentence (2).A common approach to sentiment analysis is to use a lexicon with information about which words and phrases are positive and which are negative.This lexicon may be manually compiled, as is the case with the General Inquirer (Stone et al. 1966), a resource often used in sentiment analysis.Alternatively, the information in the lexicon may be acquired automatically.Acquiring the polarity of words and phrases is itself an active line of research in the sentiment analysis community, pioneered by the work of Hatzivassiloglou and McKeown (1997) on predicting the polarity or semantic orientation of adjectives.Various techniques have been proposed for learning the polarity of words.They include corpus-based techniques, such as using constraints on the co-occurrence in conjunctions of words with similar or opposite polarity (Hatzivassiloglou and McKeown 1997) and statistical measures of word association (Turney and Littman 2003), as well as techniques that exploit information about lexical relationships (Kamps and Marx 2002; Kim and Hovy 2004) and glosses (Esuli and Sebastiani 2005; Andreevskaia and Bergler 2006) in resources such as WordNet.Acquiring the polarity of words and phrases is undeniably important, and there are still open research challenges, such as addressing the sentiments of different senses of words (Esuli and Sebastiani 2006b; Wiebe and Mihalcea 2006), and so on.However, what the polarity of a given word or phrase is when it is used in a particular context is another problem entirely.Consider, for example, the underlined positive and negative words in the following sentence.The first underlined word is Trust.Although many senses of the word trust express a positive sentiment, in this case, the word is not being used to express a sentiment at all.It is simply part of an expression referring to an organization that has taken on the charge of caring for the environment.The adjective well is considered positive, and indeed it is positive in this context.However, the same is not true for the words reason and reasonable.Out of context, we would consider both of these words to be positive.1 In context, the word reason is being negated, changing its polarity from positive to negative.The phrase no reason at all to believe changes the polarity of the proposition that follows; because reasonable falls within this proposition, its polarity becomes negative.The word polluters has a negative connotation, but here in the context of the discussion of the article and its position in the sentence, polluters is being used less to express a sentiment and more to objectively refer to companies that pollute.To clarify how the polarity of polluters is affected by its subject role, consider the purely negative sentiment that emerges when it is used as an object: They are polluters.We call the polarity that would be listed for a word in a lexicon the word’s prior polarity, and we call the polarity of the expression in which a word appears, considering the context of the sentence and document, the word’s contextual polarity.Although words often do have the same prior and contextual polarity, many times a word’s prior and contextual polarities differ.Words with a positive prior polarity may have a negative contextual polarity, or vice versa.Quite often words that are positive or negative out of context are neutral in context, meaning that they are not even being used to express a sentiment.Similarly, words that are neutral out of context, neither positive or negative, may combine to create a positive or negative expression in context.The focus of this work is on the recognition of contextual polarity—in particular, disambiguating the contextual polarity of words with positive or negative prior polarity.We begin by presenting an annotation scheme for marking sentiment expressions and their contextual polarity in the Multi-perspective Question Answering (MPQA) opinion corpus.We show that, given a set of subjective expressions (identified from the existing annotations in the MPQA corpus), contextual polarity can be annotated reliably.Using the contextual polarity annotations, we conduct experiments in automatically distinguishing between prior and contextual polarity.Beginning with a large lexicon of clues tagged with prior polarity, we identify the contextual polarity of the instances of those clues in the corpus.The process that we use has two steps, first classifying each clue as being in a neutral or polar phrase, and then disambiguating the contextual polarity of the clues marked as polar.For each step in the process, we experiment with a variety of features and evaluate the performance of the features using several different machine learning algorithms.Our experiments reveal a number of interesting findings.First, being able to accurately identify neutral contextual polarity—when a positive or negative clue is not being used to express a sentiment—is an important aspect of the problem.The importance of neutral examples has previously been noted for classifying the sentiment of documents (Koppel and Schler 2006), but ours is the first work to explore how neutral instances affect classifying the contextual polarity of words and phrases.In particular, we found that the performance of features for distinguishing between positive and negative polarity greatly degrades when neutral instances are included in the experiments.We also found that achieving the best performance for recognizing contextual polarity requires a wide variety of features.This is particularly true for distinguishing between neutral and polar instances.Although some features help to increase polar or neutral recall or precision, it is only the combination of features together that achieves significant improvements in accuracy over the baselines.Our experiments show that for distinguishing between positive and negative instances, features capturing negation are clearly the most important.However, there is more to the story than simple negation.Features that capture relationships between instances of clues also perform well, indicating that identifying features that represent more complex interdependencies between sentiment clues may be an important avenue for future research.The remainder of this article is organized as follows.Section 2 gives an overview of some of the things that can influence contextual polarity.In Section 3, we describe our corpus and present our annotation scheme and inter-annotator agreement study for marking contextual polarity.Sections 4 and 5 describe the lexicon used in our experiments and how the contextual polarity annotations are used to determine the gold-standard tags for instances from the lexicon.In Section 6, we consider what kind of performance can be expected from a simple, prior-polarity classifier.Section 7 describes the features that we use for recognizing contextual polarity, and our experiments and results are presented in Section 8.In Section 9 we discuss related work, and we conclude in Section 10.Phrase-level sentiment analysis is not a simple problem.Many things besides negation can influence contextual polarity, and even negation is not always straightforward.Negation may be local (e.g., not good), or involve longer-distance dependencies such as the negation of the proposition (e.g., does not look very good) or the negation of the subject (e.g., no one thinks that it’s good).In addition, certain phrases that contain negation words intensify rather than change polarity (e.g., not only good but amazing).Contextual polarity may also be influenced by modality: whether the proposition is asserted to be real (realis) or not real (irrealis) (no reason at all to believe is irrealis, for example); word sense (e.g., Environmental Trust vs.He has won the people’s trust); the syntactic role of a word in the sentence: whether the word is the subject or object of a copular verb (consider polluters are versus they are polluters); and diminishers such as little (e.g., little truth, little threat).Polanyi and Zaenen (2004) give a detailed discussion of many of these types of polarity influencers.Many of these contextual polarity influencers are represented as features in our experiments.Contextual polarity may also be influenced by the domain or topic.For example, the word cool is positive if used to describe a car, but it is negative if it is used to describe someone’s demeanor.Similarly, a word such as fever is unlikely to be expressing a sentiment when used in a medical context.We use one feature in our experiments to represent the topic of the document.Another important aspect of contextual polarity is the perspective of the person who is expressing the sentiment.For example, consider the phrase failed to defeat in the sentence Israel failed to defeat Hezbollah.From the perspective of Israel, failed to defeat is negative.From the perspective of Hezbollah, failed to defeat is positive.Therefore, the contextual polarity of this phrase ultimately depends on the perspective of who is expressing the sentiment.Although automatically detecting this kind of pragmatic influence on polarity is beyond the scope of this work, this as well as the other types of polarity influencers all are considered when annotating contextual polarity.For the experiments in this work, we need a corpus that is annotated comprehensively for sentiment expressions and their contextual polarity.Rather than building a corpus from scratch, we chose to add contextual polarity annotations to the existing annotations in the Multi-perspective Question Answering (MPQA) opinion corpus2 (Wiebe, Wilson, and Cardie 2005).The MPQA corpus is a collection of English-language versions of news documents from the world press.The documents contain detailed, expression-level annotations of attributions and private states (Quirk et al. 1985).Private states are mental and emotional states; they include beliefs, speculations, intentions, and sentiments, among others.Although sentiments are not distinguished from other types of private states in the existing annotations, they are a subset of what already is annotated.This makes the annotations in the MPQA corpus a good starting point for annotating sentiment expressions and their contextual polarity.When developing our annotation scheme for sentiment expressions and contextual polarity, there were three main questions to address.First, which of the existing annotations in the MPQA corpus have the possibility of being sentiment expressions?Second, which of the possible sentiment expressions actually are expressing sentiments?Third, what coding scheme should be used for marking contextual polarity?The MPQA annotation scheme has four types of annotations: objective speech event frames, two types of private state frames, and agent frames that are used for marking speakers of speech events and experiencers of private states.A full description of the MPQA annotation scheme and an agreement study evaluating key aspects of the scheme are found in Wiebe, Wilson, and Cardie (2005).The two types of private state frames, direct subjective frames and expressive subjective element frames, are where we will find sentiment expressions.Direct subjective frames are used to mark direct references to private states as well as speech events in which private states are being expressed.For example, in the following sentences, fears, praised, and said are all marked as direct subjective annotations.The word fears directly refers to a private state; praised refers to a speech event in which a private state is being expressed; and said is marked as direct subjective because a private state is being expressed within the speech event referred to by said.Expressive subjective elements indirectly express private states through the way something is described or through a particular wording.In example (6), the phrase full of absurdities is an expressive subjective element.Subjectivity (Banfield 1982; Wiebe 1994) refers to the linguistic expression of private states, hence the names for the two types of private state annotations.All expressive subjective elements are included in the set of annotations that have the possibility of being sentiment expressions, but the direct subjective frames to include in this set can be pared down further.Direct subjective frames have an attribute, expression intensity, that captures the contribution of the annotated word or phrase to the overall intensity of the private state being expressed.Expression intensity ranges from neutral to high.In the given sentences, fears and praised have an expression intensity of medium, and said has an expression intensity of neutral.A neutral expression intensity indicates that the direct subjective phrase itself is not contributing to the expression of the private state.If this is the case, then the direct subjective phrase cannot be a sentiment expression.Thus, only direct subjective annotations with a non-neutral expression intensity are included in the set of annotations that have the possibility of being sentiment expressions.We call this set of annotations, the union of the expressive subjective elements and the direct subjective frames with a non-neutral intensity, the subjective expressions in the corpus; these are the annotations we will mark for contextual polarity.Table 1 gives a sample of subjective expressions marked in the MPQA corpus.Although many of the words and phrases express what we typically think of as sentiments, others do not, for example, believes, very definitely, and unconditionally and without delay.Now that we have identified which annotations have the possibility of being sentiment expressions, the next question is which of these annotated words and phrases are actually expressing sentiments.We define a sentiment as a positive or negative emotion, evaluation, or stance.On the left of Table 2 are examples of positive sentiments; examples of negative sentiments are on the right.Sample of subjective expressions from the MPQA corpus. victory of justice and freedom such a disadvantageous situation grown tremendously must such animosity not true at all throttling the voice imperative for harmonious society disdain and wrath glorious so exciting disastrous consequences could not have wished for a better situation believes freak show the embodiment of two-sided justice if you’re not with us, you’re against us appalling vehemently denied very definitely everything good and nice once and for all under no circumstances shameful mum most fraudulent, terrorist and extremist enthusiastically asked number one democracy hate seems to think gross misstatement indulging in blood-shed and their lunaticism surprised, to put it mildly take justice to pre-historic times unconditionally and without delay so conservative that it makes Pat Buchanan look vegetarian those digging graves for others, get engraved themselves lost the reputation of commitment to principles of human justice ultimately the demon they have reared will eat up their own vitals The final issue to address is the actual annotation scheme for marking contextual polarity.The scheme we developed has four tags: positive, negative, both, and neutral.The positive tag is used to mark positive sentiments.The negative tag is used to mark negative sentiments.The both tag is applied to expressions in which both a positive and negative sentiment are being expressed.Subjective expressions with positive, negative, or both tags are our sentiment expressions.The neutral tag is used for all other subjective expressions, including emotions, evaluations, and stances that are neither positive or negative.Instructions for the contextual-polarity annotation scheme are available at http://www.cs.pitt.edu/mpqa/databaserelease/polarityCodingInstructions.txt.Following are examples from the corpus of each of the different contextual-polarity annotations.Each underlined word or phrase is a subjective expression that was marked in the original MPQA annotations.3 In bold following each subjective expression is the contextual polarity with which it was annotated.To measure the reliability of the polarity annotation scheme, we conducted an agreement study with two annotators4 using 10 documents from the MPQA corpus.The 10 documents contain 447 subjective expressions.Table 3 shows the contingency table for the two annotators’ judgments.Overall agreement is 82%, with a kappa value of 0.72.As part of the annotation scheme, annotators are asked to judge how certain they are in their polarity tags.For 18% of the subjective expressions, at least one annotator used the uncertain tag when marking polarity.If we consider these cases to be borderline and exclude them from the study, percent agreement increases to 90% and kappa rises to 0.84.Table 4 shows the revised contingency table with the uncertain cases removed.This shows that annotator agreement is especially high when both annotators are certain, and that annotators are certain for over 80% of their tags.Note that all annotations are included in the experiments.In total, all 19,962 subjective expressions in the 535 documents (11,112 sentences) of the MPQA corpus were annotated with their contextual polarity as just described.5 Three annotators carried out the task: the two who participated in the annotation study and a third who was trained later.6 Table 5 gives the distribution of the contextual polarity tags.Looking at this table, we see that a small majority of subjective expressions (54.6%) are expressing a positive, negative, or both (positive and negative) sentiment.We refer to these expressions as polar in context.Many of the subjective expressions are neutral and do not express a sentiment.This suggests that, although sentiment is a major type of subjectivity, distinguishing other prominent types of subjectivity will be important for future work in subjectivity analysis.As many NLP applications operate at the sentence level, one important issue to consider is the distribution of sentences with respect to the subjective expressions they contain.In the 11,112 sentences in the MPQA corpus, 28% contain no subjective expressions, 24% contain only one, and 48% contain two or more.Of the 5,304 sentences containing two or more subjective expressions, 17% contain mixtures of positive and negative expressions, and 61% contain mixtures of polar (positive/negative/both) and neutral subjective expressions.For the experiments in this article, we use a lexicon of over 8,000 subjectivity clues.Subjectivity clues are words and phrases that may be used to express private states.In other words, subjectivity clues have subjective usages, though they may have objective usages as well.For this work, only single-word clues are used.To compile the lexicon, we began with the list of subjectivity clues from Riloff and Wiebe (2003), which includes the positive and negative adjectives from Hatzivassiloglou and McKeown (1997).The words in this list were grouped in previous work according to their reliability as subjectivity clues.Words that are subjective in most contexts are considered strong subjective clues, indicated by the strongsubj tag.Words that may only have certain subjective usages are considered weak subjective clues, indicated by the weaksubj tag.We expanded the list using a dictionary and a thesaurus, and added words from the General Inquirer positive and negative word lists (Stone et al. 1966) that we judged to be potentially subjective.7 We also gave the new words strongsubj and weaksubj reliability tags.The final lexicon has a coverage of 67% of subjective expressions in the MPQA corpus, where coverage is the percentage of subjective expressions containing one or more instances of clues from the lexicon.The coverage of just sentiment expressions is even higher: 75%.The next step was to tag the clues in the lexicon with their prior polarity: positive, negative, both, or neutral.A word in the lexicon is tagged as positive if out of context it seems to evoke something positive, and negative if it seems to evoke something negative.If a word has both positive and negative meanings, it is tagged with the polarity that seems the most common.A word is tagged as both if it is at the same time both positive and negative.For example, the word bittersweet evokes something both positive and negative.Words like brag are also tagged as both, because the one who is bragging is expressing something positive, yet at the same time describing someone as bragging is expressing a negative evaluation of that person.A word is tagged as neutral if it does not evoke anything positive or negative.For words that came from positive and negative word lists (Stone et al. 1966; Hatzivassiloglou and McKeown 1997), we largely retained their original polarity.However, we did change the polarity of a word if we strongly disagreed with its original class.8 For example, the word apocalypse is listed as positive in the General Inquirer; we changed its prior polarity to negative for our lexicon.By far, the majority of clues in the lexicon (92.8%) are marked as having either positive (33.1%) or negative (59.7%) prior polarity.Only a small number of clues (0.3%) are marked as having both positive and negative polarity.We refer to the set of clues marked as positive, negative, or both as sentiment clues.A total of 6.9% of the clues in the lexicon are marked as neutral.Examples of neutral clues are verbs such as feel, look, and think, and intensifiers such as deeply, entirely, and practically.Although the neutral clues make up a small proportion of the total words in the lexicon, we retain them for our later experiments in recognizing contextual polarity because many of them are good clues that a sentiment is being expressed (e.g., feels slighted, feels satisfied, look kindly on, look forward to).Including them increases the coverage of the system.At the end of the previous section, we considered the distribution of sentences in the MPQA corpus with respect to the subjective expressions they contain.It is interesting to compare that distribution with the distribution of sentences with respect to the instances they contain of clues from the lexicon.We find that there are more sentences with two or more clue instances (62%) than sentences with two or more subjective expressions (48%).More importantly, many more sentences have mixtures of positive and negative clue instances than actually have mixtures of positive and negative subjective expressions.Only 880 sentences have a mixture of both positive and negative subjective expressions, whereas 3,234 sentences have a mixture of positive and negative clue instances.Thus, a large number of positive and negative instances are either neutral in context, or they are combining to form more complex polarity expressions.Either way, this provides strong evidence of the need to be able to disambiguate the contextual polarity of subjectivity and sentiment clues.In the experiments described in the following sections, the goal is to classify the contextual polarity of the expressions that contain instances of the subjectivity clues in our lexicon.However, determining which clue instances are part of the same expression and identifying expression boundaries are not the focus of this work.Thus, instead of trying to identify and label each expression, in the following experiments, each clue instance is labeled individually as to its contextual polarity.We define the gold-standard contextual polarity of a clue instance in terms of the manual annotations (Section 3) as follows.If a clue instance is not in a subjective expression (and therefore not in a sentiment expression), its gold class is neutral.If a clue instance appears in just one subjective expression or in multiple subjective expressions with the same contextual polarity, its gold class is the contextual polarity of the subjective expression(s).If a clue instance appears in a mixture of negative and neutral subjective expressions, its gold class is negative; if it is in a mixture of positive and neutral subjective expressions, its gold class is positive.Finally, if a clue instance appears in at least one positive and one negative subjective expression (or in a subjective expression marked as both), then its gold class is both.A clue instance can appear in more than one subjective expression because in the MPQA annotation scheme, it is possible for direct subjective frames and expressive subjective elements frames to overlap.Before delving into the task of recognizing contextual polarity, an important question to address is how useful prior polarity alone is for identifying contextual polarity.To answer this question, we create a classifier that simply assumes the contextual polarity of a clue instance is the same as the clue’s prior polarity.We explore this classifier’s performance on a small amount of development data, which is not part of the data used in the subsequent experiments.This simple classifier has an accuracy of 48%.From the confusion matrix given in Table 6, we see that 76% of the errors result from words with non-neutral prior polarity appearing in phrases with neutral contextual polarity.Only 12% of the errors result from words with neutral prior polarity appearing in expressions with non-neutral contextual polarity, and only 11% of the errors come from words with a positive or negative prior polarity appearing in expressions with the opposite contextual polarity.Table 6 also shows that positive clues tend to be used in negative expressions far more often than negative clues tend to be used in positive expressions.Given that by far the largest number of errors come from clues with positive, negative, or both prior polarity appearing in neutral contexts, we were motivated to try a two-step approach to the problem of sentiment classification.The first step, Neutral– Polar Classification, tries to determine if an instance is neutral or polar in context.The second step, Polarity Classification, takes all instances that step one classified as polar, and tries to disambiguate their contextual polarity.This two-step approach is illustrated in Figure 1.The features used in our experiments were motivated both by the literature and by exploration of the contextual-polarity annotations in our development data.A number Two-step approach to recognizing contextual polarity. of features were inspired by the paper on contextual-polarity influencers by Polanyi and Zaenan (2004).Other features are those that have been found useful in the past for recognizing subjective sentences (Wiebe, Bruce, and O’Hara 1999; Wiebe and Riloff 2005).For distinguishing between neutral and polar instances, we use the features listed in Table 7.For ease of description, we group the features into six sets: word features, general modification features, polarity modification features, structure features, sentence features, and one document feature.Word Features In addition to the word token (the token of the clue instance being classified), the word features include the parts of speech of the previous word, the word itself, and the next word.The prior polarity and reliability class features represent those pieces of information about the clue which are taken from the lexicon.General Modification Features These are binary features that capture different types of relationships involving the clue instance.The first four features involve relationships with the word immediately before or after the clue instance.The preceded by adjective feature is true if the clue instance is a noun preceded by an adjective.The preceded by adverb feature is true if the preceding word is an adverb other than not.The preceded by intensifier feature is true if the preceding word is an intensifier, and the self intensifier feature is true if the clue instance itself is an intensifier.A word is considered to be an intensifier if it appears in a list of intensifiers and if it precedes a word of the appropriate part of speech (e.g., an intensifier adjective must come before a noun).The list of intensifiers is a compilation of those listed in Quirk et al. (1985), intensifiers identified from existing entries in the subjectivity lexicon, and intensifiers identified during explorations of the development data.The modifies/modifed by features involve the dependency parse tree of the sentence, obtained by first parsing the sentence (Collins 1997) and then converting the tree into its dependency representation (Xia and Palmer 2001).In a dependency representation, every node in the tree structure is a surface word (i.e., there are no abstract nodes such as NP or VP).The parent word is called the head, and its children are its modifiers.The edge between a parent and a child specifies the grammatical relationship between the two words.Figure 2 shows an example of a dependency parse tree.Instances of clues in the tree are marked with the clue’s prior polarity and reliability class from the lexicon.For each clue instance, the modifies/modifed by features capture whether there are adj, mod, or vmod relationships between the clue instance and any other instances from the lexicon.Specifically, the modifies strongsubj feature is true if the clue instance and its parent share an adj, mod, or vmod relationship, and if its parent is an instance of a strongsubj clue from the lexicon.The modifies weaksubj feature is the same, except that it looks in the parent for an instance of a weaksubj clue.The modified by strongsubj The dependency tree for the sentence The human rights report poses a substantial challenge to the U.S. interpretation of good and evil.Prior polarity and reliability class are marked in parentheses for words that match clues from the lexicon. feature is true for a clue instance if one of its children is an instance of a strongsubj clue, and if the clue instance and its child share an adj, mod, or vmod relationship.The modified by weaksubj feature is the same, except that it looks for instances of weaksubj clues in the children.Although the adj and vmod relationships are typically local, the mod relationship involves longer-distance as well as local dependencies.Figure 2 helps to illustrate these features.The modifies weaksubj feature is true for substantial, because substantial modifies challenge, which is an instance of a weaksubj clue.For rights, the modifies weaksubj feature is false, because rights modifies report, which is not an instance of a weaksubj clue.The modified by weaksubj feature is false for substantial, because it has no modifiers that are instances of weaksubj clues.For challenge, the modified by weaksubj feature is true because it is being modified by substantial, which is an instance of a weaksubj clue.Polarity Modification Features The modifies polarity, modified by polarity, and conj polarity features capture specific relationships between the clue instance and other sentiment clues it may be related to.If the clue instance and its parent in the dependency tree share an obj, adj, mod, or vmod relationship, the modifies polarity feature is set to the prior polarity of the parent.If the parent is not in the prior-polarity lexicon, its prior polarity is considered neutral.If the clue instance is at the root of the tree and has no parent, the value of the feature is notmod.The modified by polarity feature is similar, looking for adj, mod, and vmod relationships and other sentiment clues in the children of the clue instance.The conj polarity feature determines if the clue instance is in a conjunction.If so, the value of this feature is its sibling’s prior polarity.As before, if the sibling is not in the Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity lexicon, its prior polarity is neutral.If the clue instance is not in a conjunction, the value for this feature is notmod.Figure 2 also helps to illustrate these modification features.The word substantial with positive prior polarity modifies the word challenge with negative prior polarity.Therefore the modifies polarity feature is negative for substantial, and the modified by polarity feature is positive for challenge.The words good and evil are in a conjunction together; thus the conj polarity feature is negative for good and positive for evil.Structure Features These are binary features that are determined by starting with the clue instance and climbing up the dependency parse tree toward the root, looking for particular relationships, words, or patterns.The in subject feature is true if we find a subj relationship on the path to the root.The in copular feature is true if in subject is false and if a node along the path is both a main verb and a copular verb.The in passive feature is true if a passive verb pattern is found on the climb.The in subject and in copular features were motivated by the intuition that the syntactic role of a word may influence whether a word is being used to express a sentiment.For example, consider the word polluters in each of the following two sentences.In the first sentence, polluters is simply being used as a referring expression.In the second sentence, polluters is clearly being used to express a negative evaluation of the farmers.The motivation for the in passive feature was previous work by Riloff and Wiebe (2003), who found that different words are more or less likely to be subjective depending on whether they are in the active or passive.Sentence Features These are features that previously were found useful for sentence-level subjectivity classification (Wiebe, Bruce, and O’Hara 1999; Wiebe and Riloff 2005).They include counts of strongsubj and weaksubj clue instances in the current, previous and next sentences, counts of adjectives and adverbs other than not in the current sentence, and binary features to indicate whether the sentence contains a pronoun, a cardinal number, and a modal other than will.Document Feature There is one document feature representing the topic or domain of the document.The motivation for this feature is that whether or not a word is expressing a sentiment or even a private state in general may depend on the subject of the discourse.For example, the words fever and sufferer may express a negative sentiment in certain contexts, but probably not in a health or medical context, as is the case in the following sentence.(14) The disease can be contracted if a person is bitten by a certain tick or if a person comes into contact with the blood of a congo fever sufferer.In the creation of the MPQA corpus, about two-thirds of the documents were selected to be on one of the 10 topics listed in Table 8.The documents for each topic were identified by human searches and by an information retrieval system.The remaining documents were semi-randomly selected from a very large pool of documents from the world press.In the corpus, these documents are listed with the topic miscellaneous.Rather than leaving these documents unlabeled, we chose to label them using the following general domain categories: economics, general politics, health, report events, and war and terrorism.Table 9 lists the features that we use for step two, polarity classification.Word token, word prior polarity, and the polarity-modification features are the same as described for neutral–polar classification.We use two features to capture two different types of negation.The negated feature is a binary feature that is used to capture more local negations: Its value is true if a negation word or phrase is found within the four words preceding the clue instance, and if the negation word is not also in a phrase that acts as an intensifier rather than a negator.Examples of phrases that intensify rather than negate are not only and nothing if not.The negated subject feature captures a longer-distance type of negation.This feature is true if the subject of the clause containing the clue instance is negated.For example, the negated subject feature is true for support in the following sentence.(15) No politically prudent Israeli could support either of them.The last three polarity features look in a window of four words before the clue instance, searching for the presence of particular types of polarity influencers.General polarity shifters reverse polarity (e.g., little truth, little threat).Negative polarity shifters typically make the polarity of an expression negative (e.g., lack of understanding).Positive polarity shifters typically make the polarity of an expression positive (e.g., abate the damage).The polarity influencers that we used were identified through explorations of the development data.We have two primary goals with our experiments in recognizing contextual polarity.The first is to evaluate the features described in Section 7 as to their usefulness for this task.The second is to investigate the importance of recognizing neutral instances— recognizing when a sentiment clue is not being used to express a sentiment—for classifying contextual polarity.To evaluate features, we investigate their performance, both together and separately, across several different learning algorithms.Varying the learning algorithm allows us to verify that the features are robust and that their performance is not the artifact of a particular algorithm.We experiment with four different types of machine learning: boosting, memory-based learning, rule learning, and support vector learning.For boosting, we use BoosTexter (Schapire and Singer 2000) AdaBoost.MH.For rule learning, we use Ripper (Cohen 1996).For memory-based learning, we use TiMBL (Daelemans et al. 2003b) IB1 (k-nearest neighbor).For support vector learning, we use SVM-light and SVM-multiclass (Joachims 1999).SVM-light is used for the experiments involving binary classification (neutral–polar classification), and SVM-multiclass is used for experiments with more than two classes.These machine learning algorithms were chosen because they have been used successfully for a number of natural language processing tasks, and they represent several different types of learning.For all of the classification algorithms except for SVM, the features for a clue instance are represented as they are presented in Section 7.For SVM, the representations for numeric and discrete-valued features are changed.Numeric features, such as the count of strongsubj clue instances in a sentence, are scaled to range between 0 and 1.Discrete-valued features, such as the reliability class feature, are converted into multiple binary features.For example, the reliability class feature is represented by two binary features: one for whether the clue instance is a strongsubj clue and one for whether the clue instance is a weaksubj clue.To investigate the importance of recognizing neutral instances, we perform two sets of polarity classification (step two) experiments.First, we experiment with classifying the polarity of all gold-standard polar instances—the clue instances identified as polar in context by the manual polarity annotations.Second, we experiment with using the polar instances identified automatically by the neutral–polar classifiers.Because the second set of experiments includes the neutral instances misclassified in step one, we can compare results for the two sets of experiments to see how the noise of neutral instances affects the performance of the polarity features.All experiments are performed using 10-fold cross validation over a test set of 10,287 sentences from 494 MPQA corpus documents.We measure performance in terms of accuracy, recall, precision, and F-measure.Accuracy is simply the total number of instances correctly classified.Recall, precision, and F-measure for a given class C are defined as follows.Recall is the percentage of all instances of class C correctly identified.|all instances of C | Precision is the percentage of instances classified as class C that are class C in truth.Prec(C) = |instances of C correctly identified | |all instances identified as C | F-measure is the harmonic mean of recall and precision.In our two-step process for recognizing contextual polarity, the first step is neutral–polar classification, determining whether each instance of a clue from the lexicon is neutral or polar in context.In our test set, there are 26,729 instances of clues from the lexicon.The features we use for this step were listed above in Table 7 and described in Section 7.1.In this section, we perform two sets of experiments.In the first, we compare the results of neutral–polar classification using all the neutral–polar features against two baselines.The first baseline uses just the word token feature.The second baseline (word+priorpol) uses the word token and prior polarity features.In the second set of experiments, we explore the performance of different sets of features for neutral–polar classification.Research has shown that the performance of learning algorithms for NLP tasks can vary widely depending on their parameter settings, and that the optimal parameter settings can also vary depending on the set of features being evaluated (Daelemans et al. 2003a; Hoste 2005).Although the goal of this work is not to identify the optimal configuration for each algorithm and each set of features, we still want to make a reasonable attempt to find a good configuration for each algorithm.To do this, we perform 10-fold cross validation of the more challenging baseline classifier (word+priorpol) on the development data, varying select parameter settings.The results from those experiments are then used to select the parameter settings for each algorithm.For BoosTexter, we vary the number of rounds of boosting.For TiMBL, we vary the value for k (the number of neighbors) and the distance metric (overlap or modified value difference metric [MVDM]).For Ripper, we vary whether negative tests are disallowed for nominal (-!n) and set (-!s) valued attributes and how much to simplify the hypothesis (-S).For SVM, we experiment with linear, polynomial, and radial basis function kernels.Table 10 gives the settings selected for the neutral–polar classification experiments for the different learning algorithms.Table 11.For each algorithm, we give the results for the two baseline classifiers, followed by the results for the classifier trained using all the neutral–polar features.The results shown in bold are significantly better than both baselines (two-sided t-test, p ≤ 0.05) for the given algorithm.Working together, how well do the neutral–polar features perform?For BoosTexter, TiMBL, and Ripper, the classifiers trained using all the features improve significantly over the two baselines in terms of accuracy, polar recall, polar F-measure, and neutral precision.Neutral F-measure is also higher, but not significantly so.These consistent results across three of the four algorithms show that the neutral–polar features are helpful for determining when a sentiment clue is actually being used to express a sentiment.Interestingly, Ripper is the only algorithm for which the word-token baseline performed better than the word+priorpol baseline.Nevertheless, the prior polarity feature is an important component in the performance of the Ripper classifier using all the features.Excluding prior polarity from this classifier results in a significant decrease in performance for every metric.Decreases range from 2.5% for neutral recall to 9.5% for polar recall.The best SVM classifier is the word+priorpol baseline.In terms of accuracy, this classifier does not perform much worse than the BoosTexter and TiMBL classifiers that use all the neutral–polar features: The SVM word+priorpol baseline classifier has an accuracy of 75.6%, and both the BoosTexter and TiMBL classifiers have an accuracy of 76.5%.However, the BoosTexter and TiMBL classifiers using all the features perform notably better in terms of polar recall and F-measure.The BoosTexter and TiMBL classifiers have polar recalls that are 7% and 9.2% higher than the SVM baseline.Polar F-measures for BoosTexter and TiMBL are 3.9% and 4.5% higher.These increases are significant for p < 0.01.8.1.2 Feature Set Evaluation.To evaluate the contribution of the various features for neutral–polar classification, we perform a series of experiments in which different sets of neutral–polar features are added to the word+priorpol baseline and new classifiers are trained.We then compare the performance of these new classifiers to the word+priorpol baseline, with the exception of the Ripper classifiers, which we compare to the higher word baseline.Table 12 lists the sets of features tested in these experiments.The feature sets generally correspond to how the neutral–polar features are presented in Table 7, although some of the groups are broken down into more fine-grained sets that we believe capture meaningful distinctions.Table 13 gives the results for these experiments.Increases and decreases for a given metric as compared to the word+priorpol baseline (word baseline for Ripper) are indicated by + or –, respectively.Where changes are significant at the p < 0.1 level, ++ or – – are used, and where changes are significant at the p < 0.05 level, +++ or – – – are used.An “nc” indicates no change (a change of less than ± 0.05) compared to the baseline.What does Table 13 reveal about the performance of various feature sets for neutral– polar classification?Most noticeable is that no individual feature sets stand out as strong performers.The only significant improvements in accuracy come from the PARTSOF-SPEECH and RELIABILITY-CLASS feature sets for Ripper.These improvements are perhaps not surprising given that the Ripper baseline was much lower to begin with.Very few feature sets show any improvement for SVM.Again, this is not unexpected given that all the features together performed worse than the word+priorpol baseline Increases and decreases for a given metric as compared to the word+priorpol baseline (word baseline for Ripper) are indicated by + or –, respectively; ++ or – – indicates the change is significant at the p < 0.1 level; +++ or – – – indicates significance at the p < 0.05 level; nc indicates no change. for SVM.The performance of the feature sets for BoosTexter and TiMBL are perhaps the most revealing.In the previous experiments using all the features together, these algorithms produced classifiers with the same high performance.In these experiments, six different feature sets for each algorithm show improvements in accuracy over the baseline, yet none of those improvements are significant.This suggests that achieving the highest performance for neutral–polar classification requires a wide variety of features working together in combination.We further test this result by evaluating the effect of removing the features that produced either no change or a drop in accuracy from the respective all-feature classifiers.For example, we train a TiMBL neutral–polar classifier using all the features except for those in the PRECEDED-POS, INTENSIFY, STRUCTURE, CURSENT-COUNTS, and TOPIC feature sets, and then compare the performance of this new classifier to the TiMBL, allfeature classifier.Although removing the non-performing features has little effect for BoosTexter, performance does drop for both TiMBL and Ripper.The primary source of this performance drop is a decrease in polar recall: 2% for TiMBL and 3.2% for Ripper.Although no feature sets stand out in Table 13 as far as giving an overall high performance, there are some features that consistently improve performance across the different algorithms.The reliability class of the clue instance (RELIABILITY-CLASS) improves accuracy over the baseline for all four algorithms.It is the only feature that does so.The RELCLASS-MOD features give improvements for all metrics for BoosTexter, Ripper, and TiMBL, as well as improving polar F-measure for SVM.The PARTS-OFSPEECH features are also fairly consistent, improving performance for all the algorithms except for SVM.There are also a couple of feature sets that consistently do not improve performance for any of the algorithms: the INTENSIFY and PRECEDED-POS features.For the second step of recognizing contextual polarity, we classify the polarity of all clue instances identified as polar in step one.The features for polarity classification were listed in Table 9 and described in Section 7.2.We investigate the performance of the polarity features under two conditions: (1) perfect neutral–polar recognition and (2) automatic neutral–polar recognition.For condition 1, we identify the polar instances according to the gold-standard, manual contextual-polarity annotations.In the test data, 9,835 instances of the clues from the lexicon are polar in context according to the manual annotations.Experiments under condition 1 classify these instances as having positive, negative, or both (positive and negative) polarity.For condition 2, we take the best performing neutral–polar classifier for each algorithm and use the output from those algorithms to identify the polar instances.Because polar instances now are being identified automatically, there will be noise in the form of misclassified neutral instances.Therefore, for experiments under condition 2 we include the neutral class and perform four-way classification instead of three-way.Condition 1 allows us to investigate the performance of the different polarity features without the noise of misclassified neutral instances.Also, because the set of polar instances being classified is the same for all the algorithms, condition 1 allows us to compare the performance of the polarity features across the different algorithms.However, condition 2 is the more natural one.It allows us to see how the noise of neutral instances affects the performance of the polarity features.The following sections describe three sets of experiments.First, we investigate the performance of the polarity features used together for polarity classification under condition 1.As before, the word and word+priorpol classifiers provide our baselines.In the second set of experiments, we explore the performance of different sets of features for polarity classification, again assuming perfect recognition of the polar instances.Finally, we experiment with polarity classification using all the polarity features under condition 2, automatic recognition of the polar instances.As before, we use the development data to select the parameter settings for each algorithm.The settings for polarity classification are given in Table 14.They were selected based on the performance of the word+priorpol baseline classifier under condition 2.8.2.1 Classification Results: Condition 1.The results for polarity classification using all the polarity features, assuming perfect neutral–polar recognition for step one, are given in Table 15.For each algorithm, we give the results for the two baseline classifiers, followed by the results for the classifier trained using all the polarity features.For the metrics where the polarity features perform statistically better than both baselines (two-sided t-test, p ≤ 0.05), the results are given in bold.How well do the polarity features perform working all together?For all algorithms, the polarity classifier using all the features significantly outperforms both baselines in terms of accuracy, positive F-measure, and negative F-measure.These consistent improvements in performance across all four algorithms show that these features are quite useful for polarity classification.One interesting thing that Table 15 reveals is that negative polarity words are much more straightforward to recognize than positive polarity words, at least in this corpus.For the negative class, precisions and recalls for the word+priorpol baseline range from 82.2 to 87.2.For the positive class, precisions and recalls for the word+priorpol baseline range from 63.7 to 76.7.However, it is with the positive class that polarity features seem to help the most.With the addition of the polarity features, positive F-measure improves by 5 points on average; improvements in negative F-measures average only 2.75 points.8.2.2 Feature Set Evaluation.To evaluate the performance of the various features for polarity classification, we again perform a series of ablation experiments.As before, we start with the word+priorpol baseline classifier, add different sets of polarity features, train new classifiers, and compare the results of the new classifiers to the baseline.Increases and decreases for a given metric as compared to the word+priorpol baseline are indicated by + or –, respectively; ++ or – – indicates the change is significant at the p < 0.1 level; +++ or – – – indicates significance at the p < 0.05 level.Table 16 lists the sets of features tested in each experiment, and Table 17 shows the results of the experiments.Results are reported as they were previously in Section 8.1.2, with increases and decreases compared to the baseline for a given metric indicated by + or –, respectively.Looking at Table 17, we see that all three sets of polarity features help to increase performance as measured by accuracy and positive and negative F-measures.This is true for all the classification algorithms.As we might expect, including the negation features has the most marked effect on the performance of polarity classification, with statistically significant improvements for most metrics across all the algorithms.9 The polarity-modification features also seem to be important for polarity classification, in particular for disambiguating the positive instances.For all the algorithms except TiMBL, including the polarity-modification features results in significant improvements for at least one of the positive metrics.The polarity shifters also help classification, but they seem to be the weakest of the features: Including them does not result in significant improvements for any algorithm.Another question that is interesting to consider is how much the word token feature contributes to polarity classification, given all the other polarity features.Is it enough to know the prior polarity of a word, whether it is being negated, and how it is related to other polarity influencers?To answer this question, we train classifiers using all the polarity features except for word token.Table 18 gives the results for these classifiers; for comparison, the results for the all-feature polarity classifiers are also given.Interestingly, excluding the word token feature produces only small changes in the overall results.The results for BoosTexter and Ripper are slightly lower, and the results for SVM are practically unchanged.TiMBL actually shows a slight improvement, with the exception of the both class.This provides further evidence of the strength of the polarity features.Also, a classifier not tied to actual word tokens may potentially be a more domain-independent classifier.8.2.3 Classification Results: Condition 2.The experiments in Section 8.2.1 show that the polarity features perform well under the ideal condition of perfect recognition of polar instances.The next question to consider is how well the polarity features perform under the more natural but less-than-perfect condition of automatic recognition of polar instances.To investigate this, the polarity classifiers (including the baselines) for each algorithm in these experiments start with the polar instances identified by the best performing neutral–polar classifier for that algorithm (from Section 8.1.1).The results for these experiments are given in Table 19.As before, statistically significant improvements over both baselines are given in bold.How well do the polarity features perform in the presence of noise from misclassified neutral instances?Our first observation comes from comparing Table 15 with Table 19: Polarity classification results are much lower for all classifiers with the noise of neutral instances.Yet in spite of this, the polarity features still produce classifiers that outperform the baselines.For three of the four algorithms, the classifier using all the polarity features has the highest accuracy.For BoosTexter and TiMBL, the improvements in accuracy over both baselines are significant.Also for all algorithms, using the polarity features gives the highest positive and negative F-measures.Because the set of polarity instances being classified by each algorithm is different, we cannot directly compare the results from one algorithm to the next.Although the two-step approach to recognizing contextual polarity allows us to focus our investigation on the performance of features for both neutral–polar classification and polarity classification, the question remains: How does the two-step approach compare to recognizing contextual polarity in a single classification step?The results shown in Table 20 help to answer this question.The first row in Table 20 for each algorithm shows the combined result for the two stages of classification.For BoosTexter, TiMBL, and Ripper, this is the combination of results from using all the neutral–polar features for step one, together with the results from using all of the polarity features for step two.10 For SVM, this is the combination of results from the word+priorpol baseline from step one, together with results for using all the polarity features for step two.Recall that the word+priorpol classifier was the best neutral–polar classifier for SVM (see Table 11).The second rows for BoosTexter, TiMBL, and Ripper show the results of a single classifier trained to recognize contextual polarity using all the neutral–polar and polarity features together.For SVM, the second row shows the results of classifying the contextual polarity using just the word token feature.This classifier outperformed all others for SVM.In the table, the best result for each metric for each algorithm is highlighted in bold.When comparing the two-step and one-step approaches, contrary to our expectations, we see that the one-step approach performs about as well or better than the two-step approach for recognizing contextual polarity.For SVM, the improvement in accuracy achieved by the two-step approach is significant, but this is not true for the other algorithms.One fairly consistent difference between the two approaches is that the two-step approach scores slightly higher for neutral F-measure, and the onestep approach achieves higher F-measures for the polarity classes.The difference in negative F-measure is significant for BoosTexter, TiMBL, and Ripper.The exception to this is SVM.For SVM, the two-step approach achieves significantly higher positive and negative F-measures.One last question we consider is how much the neutral–polar features contribute to the performance of the one-step classifiers.The third line in Table 20 for BoosTexter, TiMBL, and Ripper gives the results for a one-step classifier trained without the neutral– polar features.Although the differences are not always large, excluding the neutral– polar features consistently degrades performance in terms of accuracy and positive, negative, and neutral F-measures.The drop in negative F-measure is significant for all three algorithms, the drop in neutral F-measure is significant for BoosTexter and TiMBL, and the drop in accuracy is significant for TiMBL and Ripper (and for BoosTexter at the p ≤ 0.1 level).The modest drop in performance that we see when excluding the neutral–polar features in the one-step approach seems to suggest that discriminating between neutral and polar instances is helpful but not necessarily crucial.However, consider Figure 3.In this figure, we show the F-measures for the positive, negative, and both classes for the BoosTexter polarity classifier that uses the gold-standard neutral/polar instances (from Table 15) and for the BoosTexter one-step polarity classifier that uses all features (from Table 20).Plotting the same sets of results for the other three algorithms produces very similar figures.The difference when the classifiers have to contend with the noise from neutral instances is dramatic.Although Table 20 shows that there is room for improvement across all the contextual polarity classes, Figure 3 shows us that perhaps the best way to achieve these improvements is to improve the ability to discriminate the neutral class from the others.Other researchers who have worked on classifying the contextual polarity of sentiment expressions are Yi et al. (2003), Popescu and Etzioni (2005), and Suzuki, Takamura, and Okumura (2006).Yi et al. use a lexicon and manually developed patterns to classify contextual polarity.Their patterns are high-quality, yielding quite high precision over the set of expressions that they evaluate.Popescu and Etzioni use an unsupervised classification technique called relaxation labeling (Hummel and Zucker 1983) to recognize the contextual polarity of words that are at the heads of select opinion phrases.They take an iterative approach, using relaxation labeling first to determine the contextual polarities of the words, then again to label the polarities of the words with respect to their targets.A third stage of relaxation labeling then is used to assign final polarities to the words, taking into consideration the presence of other polarity terms and negation.As we do, Popescu and Etzioni use features that represent conjunctions and dependency relations between polarity words.Suzuki et al. use a bootstrapping approach to classify the polarity of tuples of adjectives and their target nouns in Japanese blogs.Included in the features that they use are the words that modify the adjectives and the word that the adjective modifies.They consider the effect of a single negation term, the Japanese equivalent of not.Our work in recognizing contextual polarity differs from this research on expression-level sentiment analysis in several ways.First, the set of expressions they evaluate is limited either to those that target specific items of interest, such as products and product features, or to tuples of adjectives and nouns.In contrast, we seek to classify the contextual polarity of all instances of words from a large lexicon of subjectivity clues that appear in the corpus.Included in the lexicon are not only adjectives, but nouns, verbs, adverbs, and even modals.Our work also differs from other research in the variety of features that we use.As other researchers do, we consider negation and the words that directly modify or are modified by the expression being classified.However, with negation, we have features for both local and longer-distance types of negation, and we take care to count negation terms only when they are actually being used to negate, excluding, for example, negation terms when they are used in phrases that intensify (e.g., not only).We also include contextual features to capture the presence of other clue instances in the surrounding sentences, and features that represent the reliability of clues from the lexicon.Finally, a unique aspect of the work presented in this article is the evaluation of different features for recognizing contextual polarity.We first presented the features explored in this research in Wilson, Wiebe, and Hoffman (2005), but this work significantly extends that initial evaluation.We explore the performance of features across different learning algorithms, and we evaluate not only features for discriminating between positive and negative polarity, but features for determining when a word is or is not expressing a sentiment in the first place (neutral in context).This is also the first work to evaluate the effect of neutral instances on the performance of features for discriminating between positive and negative contextual polarity.Recognizing contextual polarity is just one facet of the research in automatic sentiment analysis.Research ranges from work on learning the prior polarity (semantic orientation) of words and phrases (e.g., Hatzivassiloglou and McKeown 1997; Kamps and Marx 2002; Turney and Littman 2003; Hu and Liu 2004; Kim and Hovy 2004; Esuli and Sebastiani 2005; Takamura, Inui, and Okumura 2005; Popescu and Etzioni 2005; Andreevskaia and Bergler 2006; Esuli and Sebastiani 2006a; Kanayama and Nasukawa 2006) to characterizing the sentiment of documents, such as recognizing inflammatory messages (Spertus 1997), tracking sentiment over time in online discussions (Tong 2001), and classifying the sentiment of online messages (e.g., Das and Chen 2001; Koppel and Schler 2006), customer feedback data (Gamon 2004), or product and movie reviews (e.g., Turney 2002; Pang, Lee, and Vaithyanathan 2002; Dave, Lawrence, and Pennock 2003; Beineke, Hastie, and Vaithyanathan 2004; Mullen and Collier 2004; Bai, Padman, and Airoldi 2005; Whitelaw, Garg, and Argamon 2005; Kennedy and Inkpen 2006; Koppel and Schler 2006).Identifying prior polarity is a different task than recognizing contextual polarity, although the two tasks are complementary.The goal of identifying prior polarity is to automatically acquire the polarity of words or phrases for listing in a lexicon.Our work on recognizing contextual polarity begins with a lexicon of words with established prior polarities and then disambiguates in the corpus the polarity being expressed by the phrases in which instances of those words appear.To make the relationship between that task and ours clearer, some word lists that are used to evaluate methods for recognizing prior polarity (positive and negative word lists from the General Inquirer [Stone et al. 1966] and lists of positive and negative adjectives created for evaluation by Hatzivassiloglou and McKeown [1997]) are included in the prior-polarity lexicon used in our experiments.For the most part, the features explored in this work differ from the ones used to identify prior polarity with just a few exceptions.Using a feature to capture conjunctions between clue instances was motivated in part by the work of Hatzivassiloglou and McKeown (1997).They use constraints on the co-occurrence in conjunctions of words with similar or opposite polarity to predict the prior polarity of adjectives.Esuli and Sebastiani (2005) consider negation in some of their experiments involving WordNet glosses.Takamura et al. (2005) use negation words and phrases, including phrases such as lack of that are members in our lists of polarity shifters, and conjunctive expressions that they collect from corpora.Esuli and Sebastiani (2006a) is the only work in prior-polarity identification to include a neutral (objective) category and to consider a three-way classification between positive, negative, and neutral words.Although identifying prior polarity is a different task, they report a finding similar to ours, namely, that accuracy is lower when neutral words are included.Some research in sentiment analysis classifies the sentiments of sentences.Morinaga et al. (2002), Yu and Hatzivassiloglou (2003), Kim and Hovy (2004), Hu and Liu (2004), and Grefenstette et al.(2004)11 all begin by first creating prior-polarity lexicons.Yu and Hatzivassiloglou then assign a sentiment to a sentence by averaging the prior semantic orientations of instances of lexicon words in the sentence.Thus, they do not identify the contextual polarity of individual phrases containing clue instances, which is the focus of this work.Morinaga et al. only consider the positive or negative clue instance in each sentence that is closest to some target reference; Kim and Hovy, Hu and Liu, and Grefenstette et al. multiply or count the prior polarities of clue instances in the sentence.These researchers also consider local negation to reverse polarity, with Morinaga et al. also taking into account the negating effect of words like insufficient.However, they do not use the other types of features that we consider in our experiments.Kaji and Kitsuregawa (2006) take a different approach to recognizing positive and negative sentences.They bootstrap from information easily obtained in “Pro” and “Con” HTML tables and lists, and from one high-precision linguistic pattern, to automatically construct a large corpus of positive and negative sentences.They then use this corpus to train a naive Bayes sentence classifier.In contrast to our work, sentiment classification in all of this research is restricted to identifying only positive and negative sentences (excluding our both and neutral categories).In addition, only one sentiment is assigned per sentence; our system assigns contextual polarity to individual expressions, which would allow for a sentence to be assigned to multiple sentiment categories.As we saw when exploring the contextual polarity annotations, it is not uncommon for sentences to contain more than one sentiment expression.Classifying the sentiment of documents is a very different task than recognizing the contextual polarity of words and phrases.However, some researchers have reported findings about document-level classification that are similar to our findings about phrase-level classification.Bai et al. (2005) argue that dependencies among key sentiment terms are important for classifying document sentiment.Similarly, we show that features for capturing when clue instances modify each other are important for phrase-level classification, in particular, for identifying positive expressions.Gamon (2004) achieves his best results for document classification using a wide variety of features, including rich linguistic features, such as features that capture constituent structure, features that combine part-of-speech and semantic relations (e.g., sentence subject or negated context), and features that capture tense information.We also achieve our best results for phrase-level classification using a wide variety of features, many of which are linguistically rich.Kennedy and Inkpen (2006) report consistently higher results for document sentiment classification when select polarity influencers, including negators and intensifiers, are included.12 Koppel and Schler (2006) demonstrate the importance of neutral examples for document-level classification.In this work, we show that being able to correctly identify neutral instances is also very important for phraselevel sentiment analysis.Being able to determine automatically the contextual polarity of words and phrases is an important problem in sentiment analysis.In the research presented in this article, we tackle this problem and show that it is much more complex than simply determining whether a word or phrase is positive or negative.In our analysis of a corpus with annotations of subjective expressions and their contextual polarity, we find that positive and negative words from a lexicon are used in neutral contexts much more often than they are used in expressions of the opposite polarity.The importance of identifying when contextual polarity is neutral is further revealed in our classification experiments: When neutral instances are excluded, the performance of features for distinguishing between positive and negative polarity greatly improves.A focus of this research is on understanding which features are important for recognizing contextual polarity.We experiment with a wide variety of linguistically motivated features, and we evaluate the performance of these features using several different machine learning algorithms.Features for distinguishing between neutral and polar instances are evaluated, as well as features for distinguishing between positive and negative contextual polarity.For classifying neutral and polar instances, we find that, although some features produce significant improvements over the baseline in terms of polar or neutral recall or precision, it is the combination of features together that is needed to achieve significant improvements in accuracy.For classifying positive and negative contextual polarity, features for capturing negation prove to be the most important.However, we find that features that also perform well are those that capture when a word is (or is not) modifying or being modified by other polarity terms.This suggests that identifying features that represent more complex interdependencies between polarity clues will be an important avenue for future research.Another direction for future work will be to expand our lexicon using existing techniques for acquiring the prior polarity of words and phrases.It follows that a larger lexicon will have a greater coverage of sentiment expressions.However, expanding the lexicon with automatically acquired prior-polarity tags may result in an even greater proportion of neutral instances to contend with.Given the degradation in performance created by the neutral instances, whether expanding the lexicon automatically will result in improved performance for recognizing contextual polarity is an empirical question.Finally, the overall goal of our research is to use phrase-level sentiment analysis in higher-level NLP tasks, such as opinion question answering and summarization.We would like to thank the anonymous reviewers for their valuable comments and suggestions.This work was supported in part by an Andrew Mellow Predoctoral Fellowship, by the NSF under grant IIS-0208798, by the Advanced Research and Development Activity (ARDA), and by the European IST Programme through the AMIDA Integrated Project FP6-0033812.
PCFG Models Of Linguistic Tree RepresentationsThe kinds of tree representations used in a treebank corpus can have a dramatic effect on performance of a parser based on the PCFG estimated from that corpus, causing the estimated likelihood of a tree to differ substantially from its frequency in the training corpus.This paper points out that the Penn II treebank representations are of the kind predicted to have such an effect, and describes a simple node relabeling transformation that improves a treebank PCFG-based parser's average precision and recall by around 8%, or approximately half of the performance difference between a simple PCFG model and the best broad-coverage parsers available today.This performance variation comes about because any PCFG, and hence the corpus of trees from which the PCFG is induced, embodies independence assumptions about the distribution of words and phrases.The particular independence assumptions implicit in a tree representation can be studied theoretically and investigated empirically by means of a tree transformation / detransformation process.The kinds of tree representations used in a treebank corpus can have a dramatic effect on performance of a parser based on the PCFG estimated from that corpus, causing the estimated likelihood of a tree to differ substantially from its frequency in the training corpus.This paper points out that the Penn II treebank representations are of the kind predicted to have such an effect, and describes a simple node relabeling transformation that improves a treebank PCFG-based parser's average precision and recall by around 8%, or approximately half of the performance difference between a simple PCFG model and the best broad-coverage parsers available today.This performance variation comes about because any PCFG, and hence the corpus of trees from which the PCFG is induced, embodies independence assumptions about the distribution of words and phrases.The particular independence assumptions implicit in a tree representation can be studied theoretically and investigated empirically by means of a tree transformation / detransformation process.Probabalistic context-free grammars (PCFGs) provide simple statistical models of natural languages.The relative frequency estimator provides a straightforward way of inducing these grammars from treebank corpora, and a broad-coverage parsing system can be obtained by using a parser to find a maximum-likelihood parse tree for the input string with respect to such a treebank grammar.PCFG parsing systems often perform as well as other simple broad-coverage parsing system for predicting tree structure from part-of-speech (POS) tag sequences (Chamiak 1996).While PCFG models do not perform as well as models that are sensitive to a wider range of dependencies (Collins 1996), their simplicity makes them straightforward to analyze both theoretically and empirically.Moreover, since more sophisticated systems can be viewed as refinements of the basic PCFG model (Charniak 1997), it seems reasonable to first attempt to better understand the properties of PCFG models themselves.It is well known that natural language exhibits dependencies that context-free grammars (CFGs) cannot describe (Culy 1985; Shieber 1985).But the statistical independence assumptions embodied in a particular PCFG description of a particular natural language construction are in general much stronger than the requirement that the construction be generated by a CFG.We show below that the PCFG extension of what seems to be an adequate CFG description of PP attachment constructions performs no better than PCFG models estimated from non-CFG accounts of the same constructions.More specifically, this paper studies the effect of varying the tree structure representation of PP modification from both a theoretical and an empirical point of view.It compares PCFG models induced from treebanks using several different tree representations, including the representation used in the Penn II treebank corpora (Marcus, Santorini, and Marcinkiewicz 1993) and the &quot;Chomsky adjunction&quot; representation now standardly assumed in generative linguistics.One of the weaknesses of a PCFG model is that it is insensitive to nonlocal relationships between nodes.If these relationships are significant then a PCFG will be a poor language model.Indeed, the sense in which the set of trees generated by a CFG is &quot;context free&quot; is precisely that the label on a node completely characterizes the relationships between the subtree dominated by the node and the nodes that properly dominate this subtree.Roughly speaking, the more nodes in the trees of the training corpus, the stronger the independence assumptions in the PCFG language model induced from those trees.For example, a PCFG induced from a corpus of completely flat trees (i.e., consisting of the root node immediately dominating a string of terminals) generates precisely the strings of training corpus with likelihoods equal to their relative frequencies in that corpus.Thus the location and labeling on the nonroot nonterminal nodes determine how a PCFG induced from a treebank generalizes from that training data.Generally, one might expect that the fewer the nodes in the training corpus trees, the weaker the independence assumptions in the induced language model.For this reason, a &quot;flat&quot; tree representation of PP modification is investigated here as well.A second method of relaxing the independence assumptions implicit in a PCFG is to encode more information in each node's label.Here the intuition is that the label on a node is a &quot;communication channel&quot; that conveys information between the subtree dominated by the node and the part of the tree not dominated by this node, so all other things being equal, appending to the node's label additional information about the context in which the node appears should make the independence assumptions implicit in the PCFG model weaker.The effect of adding a particularly simple kind of contextual information—the category of the node's parent—is also studied in this paper.Whether either of these two PCFG models outperforms a PCFG induced from the original treebank is a separate question.We face a classical &quot;bias versus variance&quot; dilemma here (Geman, Bienenstock, and Doursat 1992): as the independence assumptions implicit in the PCFG model are weakened, the number of parameters that must be estimated (i.e., the number of productions) increases.Thus while moving to a class of models with weaker independence assumptions permits us to more accurately describe a wider class of distributions (i.e., it reduces the bias implicit in the estimator), in general our estimate of these parameters will be less accurate simply because there are more of them to estimate from the same data (i.e., the variance in the estimator increases).This paper studies the effects of these differing tree representations of PP modification theoretically by considering their effect on very simple corpora, and empirically by means of a tree transformation/detransformation methodology introduced below.The corpus used as the source for the empirical study is version II of the Wall Street Journal (WSJ) corpus constructed at the University of Pennsylvania, modified as described in Charniak (1996), in that:The theory of PCFGs is described elsewhere (e.g., Charniak [1993]), so it is only summarized here.A PCFG is a CFG in which each production A -4 a in the grammar's set of productions P is associated with an emission probability P(A a) that satisfies a normalization constraint and a consistency or tightness constraint not discussed here, that PCFGs estimated from tree banks using the relative frequency estimator always satisfy (Chi and Geman 1998).A PCFG defines a probability distribution over the (finite) parse trees generated by the grammar, where the probability of a tree T is given by where Cr (A --> a) is the number of times the production A a is used in the derivation T. The PCFG that assigns maximum likelihood to the sequence i= of trees in a treebank corpus is given by the relative frequency estimator.Here C,-- (A a) is the number of times the production A a is used in derivations of the trees in f. This estimation procedure can be used in a broad-coverage parsing procedure as follows: A PCFG G is estimated from a treebank corpus :1- of training data.In the work presented here the actual lexical items (words) are ignored, and the terminals of the trees are taken to be the part-of-speech (POS) tags assigned to the lexical items.Given a sequence of POS tags to be analyzed, a dynamic programming method based on the CKY algorithm (Aho and Ullman 1972) is used to search for a maximum-likelihood parse using this PCFG.For something so apparently fundamental to syntactic research, there is considerable disagreement among linguists as to just what the right tree structure analysis of various linguistic constructions ought to be.Figure 1 shows some of the variation in PP modification structures postulated in generative syntactic approaches over the past 30 years.The flat attachment structure was popular in the early days of transformational grammar, and is used to represent VPs in the WSJ corpus.In this representation both arguments and adjuncts are sisters to the lexical head, and so are not directly distinguished in the tree structure.The adjunction representation was introduced by Chomsky (it is often called &quot;Chomsky adjunction&quot;); in that representation arguments are sisters to the lexical head, while adjuncts are adjoined as sisters to a phrasal node: either a maximal projection (as shown in Figure 1) or a &quot;1-bar&quot; projection in the &quot;X-bar&quot; theory of grammar and its descendants.The third representation depicted in Figure 1 is a mixed representation in which phrases with adjuncts have exactly two levels of phrasal projection.The lower level contains the lexical head, and all adjuncts are attached as sisters to a maximal projection at the higher level.To a first approximation, this is the representation used for NPs with PP modifiers or complements in the WSJ corpus used in this study.1 If the standard linguistic intuition that the number of PP modifiers permitted in natural language is unbounded is correct, then only the Chomsky adjunction representation trees can be generated by a CFG, as the other two representations depicted in Figure 1 require a different production for each possible number of PP modifiers.For example, the rule schema VP —4 V NP PP*, which generates the flat attachment structure, abbreviates an infinite number of CF productions.In addition, if a treebank using the two-level representation contains at least one node with a single PP modifier, then the PCFG induced from it will generate Chomsky adjunction representations of multiple PP modification, in addition to the two-level representations used in the treebank.(Note that this is not a criticism of the use of this representation in a treebank, but of modeling such a representation with a PCFG).This raises the question: how should a parse tree be interpreted that does not fit the representational scheme used to construct the treebank training data?As noted above, the WSJ corpus represents PP modification to NPs using the twolevel representation.The PCFG estimated from sections 2-21 of this corpus contains the following two productions: These productions generate the two-level representations of one and two PP adjunctions to NP, as explained above.However, the second of these productions will never be used in a maximum-likelihood parse, as the parse of sequence NP PP PP involving two applications of the first rule has a higher estimated likelihood.In fact, all of the productions of the form NP NP Pr where n> 1 in the PCFG induced from sections 2-21 of the WSJ corpus are subsumed by the NP NP PP production in this way.Thus PP adjunctions to NP in the maximum-likelihood parses using this PCFG always appear as Chomsky adjunctions, even though the original treebank uses a two-level representation!A large number of productions in the PCFG induced from sections 2-21 of the WSJ corpus are subsumed by higher-likelihood combinations of shorter, higher-probability productions.Of the 14,962 productions in the PCFG, 1,327 productions, or just under 9%, are subsumed by combinations of two or more productions.'Since the subsumed productions are never used to construct a maximum-likelihood parse, they can be ignored if only maximum-likelihood parses are required.Moreover, since these subsumed productions tend to be longer than the productions that subsume them, removing them from the grammar reduces the average parse time of the exhaustive PCFG parser used here by more than 9%.Finally, note that the overgeneration of the PCFG model of the two-level adjunction structures is due to an independence assumption implicit in the PCFG model; specifically, that the upper and lower NPs in the two-level structure have the same expansions, and that these expansions have the same distributions.This assumption is clearly incorrect for the two-level tree representations.If we systematically relabel one of these NPs with a fresh label, then a PCFG induced from the resulting transformed treebank no longer has this property.The &quot;parent annotation&quot; transform discussed below, which appends the category of a parent node onto the label of all of its nonterminal children as sketched in Figure 2, has just this effect.Charniak and Carroll (1994) describe this transformation as adding &quot;pseudo context-sensitivity&quot; to the language model because the distribution of expansions of a node depends on nonlocal context, viz,, the category of its parent.3 This nonlocal information is sufficient to distinguish the upper and lower NPs in the structures considered here.Indeed, even though the PCFG estimated from the trees obtained by applying the &quot;parent annotation&quot; transformation to sections 2-21 of the WSJ corpus contains 22,773 productions (i.e., 7,811 more than the PCFG estimated from the untransformed corpus), only 965 of them, or just over 4%, are subsumed by two or more other productions.We can gain some theoretical insight into the effect that different tree representations have on PCFG language models by considering several artifical corpora whose estimated PCFGs are simple enough to study analytically.PP attachment was chosen for investigation here because the alternative structures are simple and clear, but presumably the same points could be made for any construction that has several alternative tree representations.Correctly resolving PP attachment ambiguities requires information, such as lexical information (Hindle and Rooth 1993), that is simply not available to the PCFG models considered here.Still, one might hope that a PCFG model might be able to accurately reflect general statistical trends concerning attachment preferences in the training data, even if it lacks the information to correctly resolve individual cases.But as the analysis in this section makes clear, even this is not always obtained.For example, suppose our corpora only contain two trees, both of which have yields V Det N P Det N, are always analyzed as a VP with a direct object NP and a PP, and differ only as to whether the PP modifies the NP or the VP.The corpora differ as to how these modifications are represented as trees.The dependencies in these corpora (specifically, the fact that the PP is either attached to the NP or to the VP) violate the independence assumptions implicit in a PCFG model, so one should not expect a PCFG model to exactly reproduce any of these corpora.As a CL reviewer points out, the results presented here depend on the assumption that there is exactly one PP.Nevertheless, the analysis of these corpora highlights two important points: Suppose we train a PCFG on a corpus f1 consisting only of two different tree structures: the NP attachment structure labeled (A1) and the VP attachment tree labeled (BO depicted in Figure 3.These trees are called the &quot;Penn II&quot; tree representations here because these are the representations used to encode PP modification in version II of the WSJ corpus constructed at the University of Pennsylvania.Suppose that (A1) The training corpus&quot;fi.This corpus, which uses Penn II tree representations, consists of the trees (Al) with relative frequency f and the trees (Bi) with relative frequency 1 — f. The PCFG P1 is estimated from this corpus. occurs in the corpus with relative frequency f and (Bi) occurs with relative frequency In fact, in the WSJ corpus, structure (A1) occurs 7,033 times in sections 2-21 and 279 times in section 22, while structure (Bi) occurs 7,717 times in sections 2-21 and 299 times in section 22.Thus f 0.48 in both the F2-21 subcorpora and the F22 corpus.Returning to the theoretical analysis, the relative frequency counts C1 and the nonunit production probability estimates Pi for the PCFG induced from this two-tree corpus are as follows: Of course, in a real treebank the counts of all these productions would also include their occurrences in other constructions, so the theoretical analysis presented here is but a crude idealization.Empirical studies using actual corpus data are presented in Section 5.Thus the estimated likelihoods using Pi of the tree structures (Ai) and (Bi) are: Clearly Pi (Ai) <f and Pi (B1) < (1 — f) except at f =- 0 and f = 1, so in general the estimated frequencies using P1 differ from the frequencies of (A1) and (B1) in the training corpus.This is not too surprising, as the PCFG ii assigns nonzero probability to trees not in the training corpus (e.g., to trees with more than one PP).In any case, in the parsing applications mentioned earlier the absolute magnitude of the probability of a tree is not of direct interest; rather we are concerned with its probability relative to the probabilities of other, alternative tree structures for the same yield.Thus it is arguably more reasonable to ignore the &quot;spurious&quot; tree structures The estimated relative frequency./ of NP attachment.This graph shows I as a function of the relative frequency f of NP attachment in the training data for various models discussed in the text. generated by Pi but not present in the training corpus, and compare the estimated relative frequencies of (A1) and (Bi) under Pi to their frequencies in the training data.Ideally the estimated relative frequency fi of (Al) will be close to its actual frequencyf in the training corpus.The relationship between f and fi is plotted in Figure 4.As inspection of Figure 4 makes clear, the value of fi can diverge substantially from f. For example, at f = 0.48 (the estimate obtained from the WSJ corpus presented above) 11 = 0.15.Thus a PCFG language model induced from the simple two-tree corpus above can underestimate the relative frequency of NP attachment by a factor of more than 3.Now suppose that the corpus contains the following two trees (A2) and (B2) of Figure 5, which are the Chomsky adjunction representations of NP attached and VP attached PP's, respectively, with relative frequencies f and 1 — f as before.Note that unlike the Penn II representations, the Chomsky adjunction representation represents NP and VP modification by PPs symmetrically.The training corpus rf2.This two-free corpus, which uses Chomsky adjunction tree representations, consists of the trees (A2) with relative frequency f and the trees (B2) with relative frequency 1 —f.The PCFG 132 is estimated from this corpus.The counts C2 and the nonunit production probability estimates 152 for the PCFG induced from this two-tree corpus are as follows: As in the previous subsection, 152(A2) <f and P2(B2) < (1 —f) because the PCFG assigns nonzero probability to trees not in the training corpus.Again, we calculate the estimated relative frequencies of (A2) and (B2) under P2.The relationship betweenf and1.2 is also plotted in Figure 4.The value of.12 can diverge from f, although not as widely asfi.For example, at f = 0.4812 = 0.36.Thus the precise tree structure representations used to train a PCFG can have a marked effect on the probability distribution that it generates.The previous subsection showed that inserting additional nodes into the tree structure can result in a PCFG language model that better models the distribution of trees in the training corpus.This subsection investigates the effect of removing the lower NP node in the WSJ NP modification structure, again resulting in a pair of more symmetric tree The training corpus i'3.The NP modification tree representation used in the Penn II WSJ corpus is &quot;flattened&quot; to make it similar to the VP modification representation.The PCFG P3 is estimated from this corpus. structures, as shown in Figure 6.As explained in Section 1, flattening the tree structures in general corresponds to weakening the independence assumptions in the induced PCFG models, so one might expect this to improve the induced language model.The counts C3 and the nonunit production probability estimates P3 for the PCFG induced from this two-tree corpus are as follows: The relationship between f and h is also plotted in Figure 4.The value of h diverges from f, as before: at f = 0.48/3 -= 0.23.As Figure 4 shows, the estimated relative frequency /3 using the flattened tree representations is always closer to f than the estimated relative frequency fi using the Penn II representations, but is only closer to f than the estimated relative frequency f2 using the Chomsky adjunction representations for f greater than approximately 0.7.The training corpus 77'4.This corpus, which uses Penn II treebank tree representations in which each preterminal node's parent's category is appended onto its own label, consists of the trees (A4) with relative frequency f and the trees (B4) with relative frequency 1 —f.The PCFG P4 is estimated from this corpus.As mentioned in Section 1, another way of relaxing the independence assumptions implicit in a PCFG model is to systematically encode more information in node labels about their context.This subsection explores a particularly simple kind of contextual encoding: the label of the parent of each nonroot nonpreterminal node is appended to that node's label.The labels of the root node and the terminal and preterminal nodes are left unchanged.For example, assuming that the Penn II format trees (A1) and (B1) of Section 4.1 are immediately dominated by a node labeled S, this relabeling applied to those trees produces the trees (A4) and (B4) depicted in Figure 7.We can perform the same theoretical analysis on this two-tree corpus that we applied to the previous corpora to investigate the effect of this relabeling on the PCFG modeling of PP attachment structures.The counts C4 and the nonunit production probability estimates P4 for the PCFG induced from this two-tree corpus are as follows: As in the previous subsection P4(A4) <f and P4(B4) < (1 -f).Again, we calculate the estimated relative frequencies of (A4) and (B4) under P4.The relationship between f and .11 is plotted in Figure 4.The value of 1.4 can diverge from f, just like the other estimates.For example, at f = 0.48 fi = 0.46, which is closer to f than any of the other relative frequency estimates presented earlier.(However, for f less than approximately 0.38, the relative frequency estimate using the Chomsky adjunction representations j2 is closer to f than f4).Thus as expected, increasing the context information in the form of an enriched node-labeling scheme can improve the performance of a PCFG language model.The previous section presented theoretical evidence that varying the tree representations used to estimate a PCFG language model can have a noticeable impact on that model's performance.However, as anyone working with statistical language models knows, the actual performance of a language model on real language data can often differ dramatically from one's expectations, even when it has an apparently impeccable theoretical basis.For example, on the basis of the theoretical models presented in the last section (and, undoubtedly, a background in theoretical linguistics) I expected that PCFG models induced from Chomksy adjunction tree representations would perform better than models induced from the Penn II representations.However, as shown in this section, this is not the case, but some of the other tree representations investigated here induce PCFGs that do perform noticeably better than the Penn II representations.It is fairly straightforward to mechanically transform the Penn II tree representations in the WSJ corpus into something close to the alternative tree representations described above, although the diversity of local trees in the WSJ corpus makes this task more difficult.For example, what is the Chomsky adjunction representation of a VP with no apparent verbal head?In addition, the Chomsky adjunction representation requires argument PPs to be attached as sisters of the lexical head, while adjunct PPs are attached as sisters of a nonlexical projection.Argument PPs are not systematically distinguished from adjunct PPs in the Penn II tree representations, and reliably determining whether a particular PP is an argument or an adjunct is extremely difficult, even for trained linguists.Nevertheless, the tree transformations investigated below should give at least an initial idea as to the influence of different kinds of tree representation on the induced PCFG language models.The tree transformations investigated in this section are listed below.Each is given a short name, which is used to identify it in the rest of the paper.Designing the tree transformations is complicated by the fact that there are in general many different tree transformations that correctly transform the simple cases discussed in Section 4, but behave differently on more complex constructions that appear in the WSJ corpus.The actual transformations investigated here have the advantage of simplicity, but many other different transformations would correctly transform the trees discussed in Sections 3 and 4 and be just as linguistically plausible as the transforms below, yet would presumably induce PCFGs with very different properties.Id is an identity transformation, i.e., it does not modify the trees at all.This condition studies the behavior of the Penn II tree representation used in the WSJ corpus.NP-VP produces trees that represent PP modification of both NPs and VPs using Chomsky adjunction.The NP-VP transform is the result of exhaustively applying all four of the tree transforms depicted in Figure 8.The first and fourth transforms turn NP and VP nodes whose rightmost child is a PP into Chomsky adjunction structures, and the second and third transforms adjoin final PPs with a following comma punctuation into Chomsky adjunction structures.The constraints that U> 1 and > 2 ensures that these transforms will only apply a finite number of time to any given subtree.N'-V' produces trees that represent PP modification of NPs and VPs with a Chomsky adjunction representation that uses an intermediate level of X' structure.This is the result of repeatedly applying the four transformations depicted in Figure 8 as in the NP-VP transform, with the modification that the new nonmaximal nodes are labeled N' or V' as appropriate (rather than NP or VP).Flatten produces trees in which NPs have a flatter structure than the two-level representation of NPs used in the Penn II treebank.Only subtrees consisting of a parent node labeled NP whose first child is also labeled NP are affected by this transformation.The effect of this transformation is to excise all the children nodes labeled NP from the tree, and to attach their children as direct descendants of the parent node, as depicted in the schema below.Parent appends to each nonroot nonterminal node's label its parent's category.The effect of this transformation is to produce trees of the kind discussed in Section 4.4.It is straightforward to estimate PCFGs using the relative frequency estimator from the sequences of trees produced by applying these transforms to the WSJ corpus.We turn now to the question of evaluating the different PCFGs so obtained.None of the PCFGs induced from the various tree representations discussed here reliably identifies the correct tree representations on sentences from held-out data.It is standard to evaluate broad-coverage parsers using less-stringent criteria that measure how similiar the trees produced by the parser are to the &quot;correct&quot; analysis trees in a portion of the treebank held out for testing purposes.This study uses the 1,578 sentences in section 22 of the WSJ corpus of length 40 or less for this purpose.The labeled precision and recall figures are obtained by regarding the sequence of trees :f produced by a parser as a multiset or bag E(i-) of edges, i.e., triples (N r) where N is a nonterminal label and 1 and r are left and right string positions in yield of the entire corpus.(Root nodes and preterminal nodes are not included in these edge sets, as they are given as input to the parser).Relative to a test sequence of trees (here section 22 of the WSJ corpus) the labeled precision and recall of a sequence of trees f- with the same yield as are calculated as follows, where the n operation denotes multiset intersection.Thus, precision is the fraction of edges in the tree sequence to be evaluated that also appear in the test tree sequence, and recall is the fraction of edges in the test tree sequence that also appear in tree sequence to be evaluated.It is straightforward to use the PCFG estimation techniques described in Section 2 to estimate PCFGs from the result of applying these transformations to sections 221 of the Penn II WSJ corpus.The resulting PCFGs can be used with a parser to obtain maximum-likelihood parse trees for the POS tag yields of the trees of the heldout test corpus (section 22 of the WSJ corpus).While the resulting parse trees can be compared to the trees in the test corpus using the precision and recall measures described above, the results would not be meaningful as the parse trees reflect a different tree representation to that used in the test corpus, and thus are not directly comparable with the test corpus trees.For example, the node labels used in the PCFG induced from trees produced by applying the parent transform are pairs of categories from the original Penn II WSJ tree bank, and so the labeled precision and recall measures obtained by comparing the parse trees obtained using this PCFG with the trees from the tree bank would be close to zero.One might try to overcome this by applying the same transformation to the test trees as was used to obtain the training trees for the PCFG, but then the resulting precision and recall measures would not be comparable across transformations.For example, as two different Penn II format trees may map to the same flattened tree, the flatten transformation is in general not invertible.Thus a parsing system that produces perfect flat tree representations provides less information than one that produces perfect Penn II tree representations, and one might expect that all else being equal, a parsing system using flat representations will score higher (or at least differently) in terms of precision and recall than an equivalent one producing Penn II representations.The approach developed here overcomes this problem by applying an additional tree transformation step that converts the parse trees produced using the PCFG back to the Penn II tree representations, and compares these trees to the held-out test trees using the labeled precision and recall trees.This transformation/detransformation process is depicted in Figure 9.It has the virtue that all precision and recall measures involve trees using the Penn II tree representations, but it does involve an additional detransformation step.It is straightforward to define detransformers for all of the tree transformations described in this section except for the flattening transform.The difficulty in this case is that several different Penn II format trees may map onto the same flattened tree, as mentioned above.The detransformer for the flattening transform was obtained by recording for each distinct local tree in the flattened tree representation of the training corpus the various tree fragments in the Penn II format training corpus it could have been derived from.The detransformation of a flattened tree is effected by replacing each local tree in the parse tree with its most frequently occuring Penn II format fragment.This detransformation step is in principle an additional source of error, in that a parser could produce flawless parse trees in its particular tree representation, but the transformation to the corresponding Penn II tree representations might itself introduce errors.For example, it might be that several different Penn II tree representations can correspond to a single parse tree, as is the case with a parser producing flattened tree representations.To determine if detransformation can be done reliably, for each tree transformation, labeled precision and recall measures were calculated comparing the result of applying the transformation and the corresponding detransformation to the test corpus trees with the original trees of the test corpus.In all cases except for the flattening transform these precision and recall measures were always greater than 99.5%, indicating that the transformation/detransformation process is quite reliable.For the flattening transform the measures were greater than 97.5%, suggesting that while the error introduced by this process is noticable, the transformation/detransformation process does not introduce a very large error on its own.Table 1 presents an analysis of the sequences of trees produced via this detransformation process applied to the maximum-likelihood-parse trees.The columns of this table correspond to sequences of parse trees for section 22 of the WSJ corpus.The column labeled &quot;22&quot; describes the trees given in section 22 of the WSJ corpus, and the column labeled &quot;22 Id&quot; describes the maximum-likelihood-parse trees of section 22 of the WSJ corpus using the PCFG induced from those very trees.This is thus an example of training on the test data, and is often assumed to provide an upper bound on the performance of a learning algorithm.The remaining columns describe the sequences of trees produced using the transformation/detransformation process described above.The first three rows of the table show the number of productions in each PCFG (which is the number of distinct local trees in the corresponding transformed training corpus), and the labeled precision and recall measures for the detransformed parse trees.Randomization tests for paired sample data were performed to assess the significance of the difference between the labeled precision and recall scores for the output of the Id PCFG and the other PCFGs (Cohen 1995).The labeled precision and recall scores for the Flatten and Parent transforms differed significantly from each other and also from the Id transform at the 0.01 level, while neither the NP-VP nor the N'-V' transform differed significantly from each other or the Id transform at the 0.1 level.The remaining rows of Table 1 show the number of times certain tree schema appear in these (detransformed) tree sequences.The rows labeled NP attachments and VP attachments provide the number of times the following tree schema, which As expected, the PCFGs induced from the output of the Flatten transform and Parent transform significantly improve precision and recall over the original treebank PCFG (i.e., the PCFG induced from the output of the Id transform).The PCFG induced from the output of the Parent transform performed significantly better than any other PCFG investigated here.As discussed above, both the Parent and the Flatten transforms induce PCFGs that are sensitive to what would be non-CF dependencies in the original treebank trees, which perhaps accounts for their superior performance.Both the Flatten and Parent transforms induced PCFGs that have substantially more productions than the original treebank grammar, perhaps reflecting the fact that they encode more contextual information than the original treebank grammar, albeit in different ways.Their superior performance suggests that the reduction in bias obtained by the weakening of independence assumptions that these transformations induce more than outweighs any associated increase in variance.The various adjunction transformations only had minimal effect on labeled precision and recall.Perhaps this is because PP attachment ambiguities, despite their important role in linguistic and parsing theory, are just one source of ambiguity among many in real language, and the effect of the alternative representations is only minor.Indeed, moving to the purportedly linguistically more realistic Chomsky adjunction representations did not improve performance on these measures.On reflection, perhaps this should not be surprising.The Chomsky adjunction representations are motivated within the theoretical framework of Transformational Grammar, which explicitly argues for nonlocal, indeed, non-context-free, dependencies.Thus its poor performance when used as input to a statistical model that is insensitive to such dependencies is perhaps to be expected.Indeed, it might be the case that inserting the additional adjunction nodes inserted by the NP-VP and N'-V' transformations above have the effect of converting a local dependency (which can be described by a PCFG) into a nonlocal dependency (which cannot).Another initially surprising property of the tree sequences produced by the PCFGs is that they do not reflect at all well the frequency of the different kinds of PP attachment found in the Penn II corpus.This is in fact to be expected, since the sequences consist of maximum-likelihood parses.To see this, consider any of the examples analyzed in Section 4.In all of these cases, the corpora contained two tree structures, and the induced PCFG associates each with an estimated likelihood.If these likelihoods differ, then a maximum-likelihood parser will always return the same maximum-likelihood tree structure each time it is presented with its yield, and will never return the tree structure with lower likelihood, even though the PCFG assigns it a nonzero likelihood.Thus the surprising fact is that these PCFG parsers ever produce a nonzero number of NP attachments and VP attachments in the same tree sequence.This is possible because the node label V in the attachment schema above abbreviates several different preterminal labels (i.e., the set of all verbal tags).Further investigation shows that once the V label in NP attachment and VP attachment schemas is instantiated with a particular verbal tag, only either the relevant NP attachment schema or the VP attachment schema appears in the tree sequence.For instance, in the Id tree sequence (i.e., produced by the standard tree bank grammar) the 67 NP attachments all occurred with the V label instantiated to the verbal tag AUX.'It is worth noting that the 8% improvement in average precision and recall obtained by the parent annotation transform is approximately half of the performance difference between a parser using a PCFG induced directly from the tree bank (i.e., using the Id transform above) and the best currently available broad-coverage parsing systems, which exploit lexical as well as purely syntactic information (Charniak 1997).In order to better understand just why the parent annotation transform performs so much better than the other transforms, transformation/detransformation experiments were performed in which the parent annotation transform was performed selectively either on all nodes with a given category label, or all nodes with a given category label and parent category label.Figure 10 depicts the effect of selective application of the parent annotation transform on the change of the average of precision and recall with respect to the Id transform.It is clear that distinguishing the context of NP and S nodes is responsible for an important part of the improvement in performance.Merely distinguishing root from nonroot S nodes—a distinction made in early transformational grammar but ignored in more recent work—improves average precision and recall by approximately 3%.Thus it is possible that the performance gains achieved by the parent annotation transform have little to do with PP attachment.This paper has presented theoretical and empirical evidence that the choice of tree representation can make a significant difference to the performance of a PCFG-based parsing system.What makes a tree representation a good choice for PCFG modeling seems to be quite different to what makes it a good choice for a representation of a linguistic theory.In conventional linguistic theories the choice of rules, and hence trees, The effects of selective application of the Parent transform.Each point corresponds to a PCFG induced after selective application of the Parent transform.The point labeled All corresponds to the PCFG induced after the Parent transform to all nonroot nonterminal nodes, as before.Points labeled with a single category A correspond to PCFGs induced after applying the Parent transform to just those nodes labeled A, while points labeled with a pair of categories KB correspond to PCFGs induced applying the Parent transform to nodes labeled A with parents labeled B.(Some labels are elided to make the remaining labels legible).The x-axis shows the difference in number of productions in the PCFG after selective parent transform and the untransformed treebank PCFG, and the y-axis shows the difference in the average of the precision and recall scores. is usually influenced by considerations of parsimony; thus the Chomsky adjunction representation of PP modification may be preferred because it requires only a single context-free rule, rather than a rule schema abbreviating a potentially unbounded number of rules that would be required in flat tree representations of adjunction.But in a PCFG model the additional nodes required by the Chomsky adjunction representation represent independence assumptions that seem not to be justified.In general, in selecting a tree structure one faces a bias/variance trade-off, in that tree structures with fewer nodes and/or richer node labels reduce bias, but possibly at the expense of an increase in variance.A tree transformation/detransformation methodology for empirically evaluating the effect of different tree representations on parsing systems was developed in this paper.The results presented earlier show that the tree representations that incorporated weaker independence assumptions performed signficantly better in the empirical studies than the more linguistically motivated Chomsky adjunction structures.Of course, there is nothing particularly special about the particular tree transformations studied in this paper: other transforms could—and should—be studied in exactly the same manner.For example, I am currently using this methodology to study the interaction between tree structure and a &quot;slash category&quot; node labeling in tree representations with empty categories (Gazdar et al. 1985).While the work presented here focussed on PCFG parsing models, it seems that the general transformation/detransformation approach can be applied to a wider range of probA(Average of Precision and Recall) lems.For example, it would be interesting to know to what extent the performance of more sophisticated parsing systems, such as those described by Collins (1996) and Charniak (1997), depends on the particular tree representations they are trained on.I would like to thank Dick Oehrle and Chris Manning, Eugene Charniak and my other colleagues at Brown, and the CL reviewers for their excellent advice in this research.This material is based on work supported by the National Science Foundation under Grants No.SBR-9720368 and SBR-9812169.
Syntax-Based Alignment Of Multiple Translations: Extracting Paraphrases And Generating New SentencesWe describe a syntax-based algorithm that automatically builds Finite State Automata (word lattices) from semantically equivalent translation sets.These FSAs are good representations of paraphrases.They can be used to extract lexical and syntactic paraphrase pairs and to generate new, unseen sentences that express the same meaning as the sentences in the input sets.Our FSAs can also predict the correctness of alternative semantic renderings, which may be used to evaluate the quality of translations.In the past, paraphrases have come under the scrutiny of many research communities.Information retrieval researchers have used paraphrasing techniques for query reformulation in order to increase the recall of information retrieval engines (Sparck Jones and Tait, 1984).Natural language generation researchers have used paraphrasing to increase the expressive power of generation systems (Iordanskaja et al., 1991; Lenke, 1994; Stede, 1999).And researchers in multi-document text summarization (Barzilay et al., 1999), information extraction (Shinyama et al., 2002), and question answering (Lin and Pantel, 2001; Hermjakob et al., 2002) have focused on identifying and exploiting paraphrases in the context of recognizing redundancies, alternative formulations of the same meaning, and improving the performance of question answering systems.In previous work (Barzilay and McKeown, 2001; Lin and Pantel, 2001; Shinyama et al., 2002), paraphrases are represented as sets or pairs of semantically equivalent words, phrases, and patterns.Although this is adequate in the context of some applications, it is clearly too weak from a generative perspective.Assume, for example, that we know that text pairs (stock market rose, stock market gained) and (stock market rose, stock prices rose) have the same meaning.If we memorized only these two pairs, it would be impossible to infer that, in fact, consistent with our intuition, any of the following sets of phrases are also semantically equivalent: {stock market rose, stock market gained, stock prices rose, stock prices gained } and {stock market, stock prices } in the context of rose or gained; {market rose }, {market gained }, {prices rose } and {prices gained } in the context of stock; and so on.In this paper, we propose solutions for two problems: the problem ofparaphrase representation and the problem of paraphrase induction.We propose a new, finite-statebased representation of paraphrases that enables one to encode compactly large numbers of paraphrases.We also propose algorithms that automatically derive such representations from inputs that are now routinely released in conjunction with large scale machine translation evaluations (DARPA, 2002): multiple English translations of many foreign language texts.For instance, when given as input the 11 semantically equivalent English translations in Figure 1, our algorithm automatically induces the FSA in Figure 2, which represents compactly 49 distinct renderings of the same semantic meaning.Our FSAs capture both lexical paraphrases, such as {fighting, battle}, {died, were killed} and structural paraphrases such as {last week’s fighting, the battle of last week}.The contexts in which these are correct paraphrases are also conveniently captured in the representation.In previous work, Langkilde and Knight (1998) used word lattices for language generation, but their method involved hand-crafted rules.Bangalore et al. (2001) and Barzilay and Lee (2002) both applied the technique of multi-sequence alignment (MSA) to align parallel corpora and produced similar FSAs.For their purposes, they mainly need to ensure the correctness of consensus among different translations, so that different constituent orderings in input sentences do not pose a serious problem.In contrast, we want to ensure the correctness of all paths represented by the FSAs, and direct application of MSA in the presence of different constituent orderings can be problematic.For example, when given as input the same sentences in Figure 1, one instantiation of the MSA algorithm produces the FSA in Figure 3, which contains many “bad” paths such as the battle of last week’s fighting took at least 12 people lost their people died in the fighting last week’s fighting (See Section 4.2.2 for a more quantitative analysis.).It’s still possible to use MSA if, for example, the input is pre-clustered to have the same constituent ordering (Barzilay and Lee (2003)).But we chose to approach this problem from another direction.As a result, we propose a new syntax-based algorithm to produce FSAs.In this paper, we first introduce the multiple translation corpus that we use in our experiments (see Section 2).We then present the algorithms that we developed to induce finite-state paraphrase representations from such data (see Section 3).An important part of the paper is dedicated to evaluating the quality of the finite-state representations that we derive (see Section 4).Since our representations encode thousands and sometimes millions of equivalent verbalizations of the same meaning, we use both manual and automatic evaluation techniques.Some of the automatic evaluations we perform are novel as well.The data we use in this work is the LDC-available Multiple-Translation Chinese (MTC) Corpus1 developed for machine translation evaluation, which contains 105 news stories (993 sentences) from three sources of journalistic Mandarin Chinese text.These stories were independently translated into English by 11 translation agencies.Each sentence group, which consists of 11 semantically equivalent translations, is a rich source for learning lexical and structural paraphrases.In our experiments, we use 899 of the sentence groups — the sentence groups with sentences longer than 45 words were dropped.Our syntax-based alignment algorithm, whose pseudocode is shown in Figure 4, works in three steps.In the first step (lines 1-5 in Figure 4), we parse every sentence in a sentence group and merge all resulting parse trees into a parse forest.In the second step (line 6), we extract an FSA from the parse forest and then we compact it further using a limited form of bottom-up alignment, which we call squeezing (line 7).In what follows, we describe each step in turn.Top-down merging.Given a sentence group, we pass each of the 11 sentences to Charniak’s (2000) parser to get 11 parse trees.The first step in the algorithm is to merge these parse trees into one parse-forest-like structure using a top-down process.Let’s consider a simple case in which the parse forest contains one single tree, Tree 1 in Figure 5, and we are adding Tree 2 to it.Since the two trees correspond to sentences that have the same meaning and since both trees expand an S node into an NP and a V P, it is reasonable to assume that NP1 is a paraphrase of NP2 and V P1 is a paraphrase of V P2.We merge NP1 with NP2 and V P1 with V P2 and continue the merging process on each of the subtrees recursively, until we either reach the leaves of the trees or the two nodes that we examine are expanded using different syntactic rules.When we apply this process to the trees in Figure 5, the NP nodes are merged all the way down to the leaves, and we get “12” as a paraphrase of “twelve” and “people” as a paraphrase of “persons”; in contrast, the two VPs are expanded in different ways, so no merging is done beyond this level, and we are left with the information that “were killed” is a paraphrase of “died”.We repeat this top-down merging procedure with each of the 11 parse trees in a sentence group.So far, only constituents with same syntactic type are treated as paraphrases.However, later we shall see that we can match word spans whose syntactic types differ.Keyword checking.The matching process described above appears quite strict – the expansions must match exactly for two nodes to be merged.But consider the following parse trees: 1.(S (NP1 people)(VP1 were killed in this battle)) 2.(S (NP2 this battle)(VP2 killed people)) If we applied the algorithm described above, we would mistakenly align NP1 with NP2 and V P1 with V P2 — the algorithm described so far makes no use of lexical information.To prevent such erroneous alignments, we also implement a simple keyword checking procedure.We note that since the word “battle” appears in both V P1 and NP2, this can serve as an evidence against the merging of (NP1, NP2) and (V P1, V P2).A similar argument can be constructed for the word “people”.So in this example we actually have double evidence against merging; in general, one such clue suffices to stop the merging.Our keyword checking procedure acts as a filter.A list of keywords is maintained for each node in a syntactic tree.This list contains all the nouns, verbs, and adjectives that are spanned by a syntactic node.Before merging two nodes, we check to see whether the keyword lists associated with them share words with other nodes.That is, supposed we just merged nodes A and B, and they are expanded with the same syntactic rule into A1A2...An and B1B2...Bn respectively; before we merge each Ai with Bi, we check for each Bi if its keyword list shares common words with any Aj (j =� i).If they do not, we continue the top-down merging process; otherwise we stop.In our current implementation, a pair of synonyms can not stop an otherwise legitimate merging, but it’s possible to extend our keyword checking process with the help of lexical resources such as WordNet in future work.Mapping Parse Forests into Finite State Automata.The process of mapping Parse Forests into Finite State Automata is simple.We simply traverse the parse forest top-down and create alternative paths for every merged node.For example, the parse forest in Figure 5 is mapped into the FSA shown at the bottom of the same figure.In the FSA, there is a word associated with each edge.Different paths between any two nodes are assumed to be paraphrases of each other.Each path that starts from the BEGIN node and ends at the END node corresponds to either an original input sentence or a paraphrase sentence.Squeezing.Since we adopted a very strict matching criterion in top-down merging, a small difference in the syntactic structure of two trees prevents some legitimate mergings from taking place.This behavior is also exacerbated by errors in syntactic parsing.Hence, for instance, three edges labeled detroit at the leftmost of the top FSA in Figure 6 were kept apart.To compensate for this effect, our algorithm implements an additional step, which we call squeezing.If two different edges that go into (or out of) the same node in an FSA are labeled with the same word, the nodes on the other end of the edges are merged.We apply this operation exhaustively over the FSAs produced by the top-down merging procedure.Figure 6 illustrates the effect of this operation: the FSA at the top of this figure is compressed into the more compact FSA shown at the bottom of it.Note that in addition to reducing the redundant edges, this also gives us paraphrases not available in the FSA before squeezing (e.g.{reduced to rubble, blasted to ground}).Therefore, the squeezing operation, which implements a limited form of lexically driven alignment similar to that exploited by MSA algorithms, leads to FSAs that have a larger number of paths The evaluation for our finite state representations and algorithm requires careful examination.Obviously, what counts as a good result largely depends on the application one has in mind.If we are extracting paraphrases for question-reformulation, it doesn’t really matter if we output a few syntactically incorrect paraphrases, as long as we produce a large number of semantically correct ones.If we want to use the FSA for MT evaluation (for example, comparing a sentence to be evaluated with the possible paths in FSA), we would want all paths to be relatively good (which we will focus on in this paper), while in some other applications, we may only care about the quality of the best path (not addressed in this paper).Section 4.1 concentrates on evaluating the paraphrase pairs that can be extracted from the FSAs built by our system, while Section 4.2 is dedicated to evaluating the FSAs directly.By construction, different paths between any two nodes in the FSA representations that we derive are paraphrases (in the context in which the nodes occur).To evaluate our algorithm, we extract paraphrases from our FSAs and ask human judges to evaluate their correctness.We compare the paraphrases we collect with paraphrases that are derivable from the same corpus using a cotraining-based paraphrase extraction algorithm (Barzilay and McKeown, 2001).To the best of our knowledge, this is the most relevant work to compare against since it aims at extracting paraphrase pairs from parallel corpus.Unlike our syntax-based algorithm which treats a sentence as a tree structure and uses this hierarchical structural information to guide the merging process, their algorithm treats a sentence as a sequence of phrases with surrounding contexts (no hierarchical structure involved) and cotrains classifiers to detect paraphrases and contexts for paraphrases.It would be interesting to compare the results from two algorithms so different from each other.For the purpose of this experiment, we randomly selected 300 paraphrase pairs (Ssyn) from the FSAs produced by our system.Since the co-training-based algorithm of Barzilay and McKeown (2001) takes parallel corpus as input, we created out of the MTC corpus 55 × 993 sentence pairs (Each equivalent translation set of cardinality 11 was mapped into (2) equivalent translation pairs.).Regina Barzilay kindly provided us the list of paraphrases extracted by their algorithm from this parallel corpus, from which we randomly selected another set of 300 paraphrases (Scotr). phrases produced by the syntax-based alignment (Ssyn) and co-training-based (Scotr) algorithms.The resulting 600 paraphrase pairs were mixed and presented in random order to four human judges.Each judge was asked to assess the correctness of 150 paraphrase pairs (75 pairs from each system) based on the context, i.e., the sentence group, from which the paraphrase pair was extracted.Judges were given three choices: “Correct”, for perfect paraphrases, “Partially correct”, for paraphrases in which there is only a partial overlap between the meaning of two paraphrases (e.g. while {saving set, aid package} is a correct paraphrase pair in the given context, {set, aide package} is considered partially correct), and “Incorrect”.The results of the evaluation are presented in Table 1.Although the four evaluators were judging four different sets, each clearly rated a higher percentage of the outputs produced by the syntax-based alignment algorithm as “Correct”.We should note that there are parameters specific to the co-training algorithm that we did not tune to work for this particular corpus.In addition, the cotraining algorithm recovered more paraphrase pairs: the syntax-based algorithm extracted 8666 pairs in total with 1051 of them extracted at least twice (i.e. more or less reliable), while the numbers for the co-training algorithm is 2934 out of a total of 16993 pairs.This means we are not comparing the accuracy on the same recall level.Aside from evaluating the correctness of the paraphrases, we are also interested in the degree of overlap between the paraphrase pairs discovered by the two algorithms so different from each other.We find that out of the 1051 paraphrase pairs that were extracted from more than one sentence group by the syntax-based algorithm, 62.3% were also extracted by the co-training algorithm; and out of the 2934 paraphrase pairs from the results of co-training algorithm, 33.4% were also extracted by the syntax-based algorithm.This shows that in spite of the very different cues the two different algorithms rely on, they do discover a lot of common pairs.In order to (roughly) estimate the recall (of lexical synonyms) of our algorithm, we use the synonymy relation in WordNet to extract all the synonym pairs present in our corpus.This extraction process yields the list of all WordNet-consistent synonym pairs that are present in our data.(Note that some of the pairs identified as synonyms by WordNet, like “follow/be”, are not really synonyms in the contexts defined in our data set, which may lead to artificial deflation of our recall estimate.)Once we have the list of WordNet-consistent paraphrases, we can check how many of them are recovered by our method.Table 2 gives the percentage of pairs recovered for each range of average sentence length (ASL) in the group.Not surprisingly, we get higher recall with shorter sentences, since long sentences tend to differ in their syntactic structures fairly high up in the parse trees, which leads to fewer mergings at the lexical level.The recall on the task of extracting lexical synonyms, as defined by WordNet, is not high.But after all, this is not what our algorithm has been designed for.It’s worth noticing that the syntax-based algorithm also picks up many paraphrases that are not identified as synonyms in WordNet.Out of 3217 lexical paraphrases that are learned by our system, only 493 (15.3%) are WordNet synonyms, which suggests that paraphrasing is a much richer and looser relation than synonymy.However, the WordNetbased recall figures suggest that WordNet can be used as an additional source of information to be exploited by our algorithm.We noted before that apart from being a natural representation of paraphrases, the FSAs that we build have their own merit and deserve to be evaluated directly.Since our FSAs contain large numbers of paths, we design automatic evaluation metrics to assess their qualities.If we take our claims seriously, each path in our FSAs that connects the start and end nodes should correspond to a well-formed sentence.We are interested in both quantity (how many sentences our automata are able to produce) and quality (how good these sentences are).To answer the first question, we simply count the number of paths produced by our FSAs.Table 3 gives the statistics on the number of paths produced by our FSAs, reported by the average length of sentences in the input sentence groups.For example, the sentence groups that have between 10 and 20 words produce, on average, automata that can yield 4468 alternative, semantically equivalent formulations.Note that if we always get the same degree of merging per word across all sentence groups, the number of paths would tend to increase with the sentence length.This is not the case here.Apparently we are getting less merging with longer sentences.But still, given 11 sentences, we are capable of generating hundreds, thousands, and in some cases even millions of sentences.Obviously, we should not get too happy with our ability to boost the number of equivalent meanings if they are incorrect.To assess the quality of the FSAs generated by our algorithm, we use a language model-based metric.We train a 4-gram model over one year of the Wall Street Journal using the CMU-Cambridge Statistical Language Modeling toolkit (v2).For each sentence group SG, we use this language model to estimate the average entropy of the 11 original sentences in that group (ent(SG)).We also compute the average entropy of all the sentences in the corresponding FSA built by our syntax-based algorithm (ent(FSA)).As the statistics in Table 4 show, there is little difference between the average entropy of the original sentences and the average entropy of the paraphrase sentences we produce.To better calibrate this result, we compare it with the average entropy of 6 corresponding machine translation outputs (ent(MTS)), which were also made available by LDC in conjunction with the same corpus.As one can see, the difference between the average entropy of the machine produced output and the average entropy of the original 11 sentences is much higher than the difference between the average entropy of the FSA-produced outputs and the average entropy of the original 11 sentences.Obviously, this does not mean that our FSAs only produce well-formed sentences.But it does mean that our FSAs produce sentences that look more like human produced sentences than machine produced ones according to a language model.Not surprisingly, the language model we used in Section 4.2.1 is far from being a perfect judge of sentence quality.Recall the example of “bad” path we gave in Section 1: the battle of last week’s fighting took at least 12 people lost their people died in the fighting last week’s fighting.Our 4-gram based language model will not find any fault with this sentence.Notice, however, that some words (such as “fighting” and “people”) appear at least twice in this path, although they are not repeated in any of the source sentences.These erroneous repetitions indicate mis-alignment.By measuring the frequency of words that are mistakenly repeated, we can now examine quantitatively whether a direct application of the MSA algorithm suffers from different constituent orderings as we expected.For each sentence group, we get a list of words that never appear more than once in any sentence in this group.Given a word from this list and the FSA built from this group, we count the total number of paths that contain this word (C) and the number of paths in which this word appears at least twice (CT, i.e. number of erroneous repetitions).We define the repetition ratio to be CT/C, which is the proportion of “bad” paths in this FSA according to this word.If we compute this ratio for all the words in the lists of the first 499 groups2 and the corresponding FSAs produced by an instantiation of the MSA algorithm3, the average repetition ratio is 0.0304992 (14.76% of the words have a non-zero repetition ratio, and the average ratio for these words is 0.206671).In comparison, the average repetition ratio for our algorithm is 0.0035074 (2.16% of the words have a non-zero repetition ratio4, and the average ratio for these words is 0.162309).The presence of different constituent orderings does pose a more serious problem to the MSA algorithm.Recently, Papineni et al. (2002) have proposed an automatic MT system evaluation technique (the BLEU score).Given an MT system output and a set of reference translations, one can estimate the “goodness” of the MT output by measuring the n-gram overlap between the output and the reference set.The higher the overlap, i.e., the closer an output string is to a set of reference translations, the better a translation it is.We hypothesize that our FSAs provide a better representation against which the outputs of MT systems can be evaluated because they encode not just a few but thousands of equivalent semantic formulations of the desired meaning.Ideally, if the FSAs we build accept all and only the correct renderings of a given meaning, we can just give a test sentence to the reference FSA and see if it is accepted by it.Since this is not a realistic expectation, we measure the edit distance between a string and an FSA instead: the smaller this distance is, the closer it is to the meaning represented by the FSA.To assess whether our FSAs are more appropriate representations for evaluating the output of MT systems, we perform the following experiment.For each sentence group, we hold out one sentence as test sentence, and try to evaluate how much of it can be predicted from the other 10 sentences.We compare two different ways of estimating the predictive power.(a) we compute the edit distance between the test sentence and the other 10 sentences in the set.The minimum of this distance is ed(input).(b) we use dynamic programming to efficiently compute the minimum distance (ed(FSA)) between the test sentence and all the paths in the FSA built from the other 10 sentences.The smaller the edit distance is, the better we are predicting a test sentence.Mathematically, the difference between these two measures ed(input) − ed(FSA) characterizes how much is gained in predictive power by building the FSA.We carry out the experiment described above in a “leave-one-out” fashion (i.e. each sentence serves as a test sentence once).Now let edgain be the average of ed(input) − ed(FSA) over the 11 runs for a given group.We compute this for all 899 groups and find the mean for edgain to be 0.91 (std. dev = 0.78).Table 5 gives the count for groups whose edgain falls into the specified range.We can see that the majority of edgain falls under 2.We are also interested in the relation between the predictive power of the FSAs and the number of reference translations they are derived from.For a given group, we randomly order the sentences in it, set the last one as the test sentence, and try to predict it with the first 1, 2, 3, ... 10 sentences.We investigate whether more sentences Let ed(FSAn) be the edit distance from the test sentence to the FSA built on the first n sentences; similarly, let ed(inputn) be the minimum edit distance from the test sentence to an input set that consists of only the first n sentences.Table 6 reports the effect of using different number of reference translations.The first column shows that each translation is contributing to the predictive power of our FSA.Even when we add the tenth translation to our FSA, we still improve its predictive power.The second column shows that the more sentences we add to the FSA the larger the difference between its predictive power and that of a simple set.The results in Table 6 suggest that our FSA may be used in order to refine the BLEU metric (Papineni et al., 2002).In this paper, we presented a new syntax-based algorithm that learns paraphrases from a newly available dataset.The multiple translation corpus that we use in this paper is the first instance in a series of similar corpora that are built and made publicly available by LDC in the context of a series of DARPA-sponsored MT evaluations.The algorithm we proposed constructs finite state representations of paraphrases that are useful in many contexts: to induce large lists of lexical and structural paraphrases; to generate semantically equivalent renderings of a given meaning; and to estimate the quality of machine translation systems.More experiments need to be carried out in order to assess extrinsically whether the FSAs we produce can be used to yield higher agreement scores between human and automatic assessments of translation quality.In our future work, we wish to experiment with more flexible merging algorithms and to integrate better the top-down and bottom-up processes that are used to induce FSAs.We also wish to extract more abstract paraphrase patterns from the current representation.Such patterns are more likely to get reused – which would help us get reliable statistics for them in the extraction phase, and also have a better chance of being applicable to unseen data.We thank Hal Daum´e III, Ulrich Germann, and Ulf Hermjakob for help and discussions; Eric Breck, Hubert Chen, Stephen Chong, Dan Kifer, and Kevin O’Neill for participating in the human evaluation; and the Cornell NLP group and the reviewers for their comments on this paper.We especially want to thank Regina Barzilay and Lillian Lee for many valuable suggestions and help at various stages of this work.Portions of this work were done while the first author was visiting Information Sciences Institute.This work was supported by the Advanced Research and Development Activity (ARDA)’s Advance Question Answering for Intelligence (AQUAINT) Program under contract number MDA908-02-C-0007, the National Science Foundation under ITR/IM grant IIS0081334 and a Sloan Research Fellowship to Lillian Lee.Any opinions, findings, and conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the views of the National Science Foundation or the Sloan Foundation.
Discriminative Reranking For Machine TranslationThis paper describes the application of discriminative reranking techniques to the problem of machine translation.For each sentence in the source language, we obtain from a baseline statistical machine translation system, a ranked best list of candidate translations in the target language.We introduce two novel perceptroninspired reranking algorithms that improve on the quality of machine translation over the baseline system based on evaluation using the BLEU metric.We provide experimental results on the NIST 2003 Chinese-English large data track evaluation.We also provide theoretical analysis of our algorithms and experiments that verify that our algorithms provide state-of-theart performance in machine translation.The noisy-channel model (Brown et al., 1990) has been the foundation for statistical machine translation (SMT) for over ten years.Recently so-called reranking techniques, such as maximum entropy models (Och and Ney, 2002) and gradient methods (Och, 2003), have been applied to machine translation (MT), and have provided significant improvements.In this paper, we introduce two novel machine learning algorithms specialized for the MT task.Discriminative reranking algorithms have also contributed to improvements in natural language parsing and tagging performance.Discriminative reranking algorithms used for these applications include Perceptron, Boosting and Support Vector Machines (SVMs).In the machine learning community, some novel discriminative ranking (also called ordinal regression) algorithms have been proposed in recent years.Based on this work, in this paper, we will present some novel discriminative reranking techniques applied to machine translation.The reranking problem for natural language is neither a classification problem nor a regression problem, and under certain conditions MT reranking turns out to be quite different from parse reranking.In this paper, we consider the special issues of applying reranking techniques to the MT task and introduce two perceptron-like reranking algorithms for MT reranking.We provide experimental results that show that the proposed algorithms achieve start-of-the-art results on the NIST 2003 Chinese-English large data track evaluation.The seminal IBM models (Brown et al., 1990) were the first to introduce generative models to the MT task.The IBM models applied the sequence learning paradigm well-known from Hidden Markov Models in speech recognition to the problem of MT.The source and target sentences were treated as the observations, but the alignments were treated as hidden information learned from parallel texts using the EM algorithm.This sourcechannel model treated the task of finding the probability , where is the translation in the target (English) language for a given source (foreign) sentence , as two generative probability models: the language model which is a generative probability over candidate translations and the translation model which is a generative conditional probability of the source sentence given a candidate translation .The lexicon of the single-word based IBM models does not take word context into account.This means unlikely alignments are being considered while training the model and this also results in additional decoding complexity.Several MT models were proposed as extensions of the IBM models which used this intuition to add additional linguistic constraints to decrease the decoding perplexity and increase the translation quality.Wang and Waibel (1998) proposed an SMT model based on phrase-based alignments.Since their translation model reordered phrases directly, it achieved higher accuracy for translation between languages with different word orders.In (Och and Weber, 1998; Och et al., 1999), a two-level alignment model was employed to utilize shallow phrase structures: alignment between templates was used to handle phrase reordering, and word alignments within a template were used to handle phrase to phrase translation.However, phrase level alignment cannot handle long distance reordering effectively.Parse trees have also been used in alignment models.Wu (1997) introduced constraints on alignments using a probabilistic synchronous context-free grammar restricted to Chomskynormal form.(Wu, 1997) was an implicit or selforganizing syntax model as it did not use a Treebank.Yamada and Knight (2001) used a statistical parser trained using a Treebank in the source language to produce parse trees and proposed a tree to string model for alignment.Gildea (2003) proposed a tree to tree alignment model using output from a statistical parser in both source and target languages.The translation model involved tree alignments in which subtree cloning was used to handle cases of reordering that were not possible in earlier tree-based alignment models.Och and Ney (2002) proposed a framework for MT based on direct translation, using the conditional model estimated using a maximum entropy model.A small number of feature functions defined on the source and target sentence were used to rerank the translations generated by a baseline MT system.While the total number of feature functions was small, each feature function was a complex statistical model by itself, as for example, the alignment template feature functions used in this approach.Och (2003) described the use of minimum error training directly optimizing the error rate on automatic MT evaluation metrics such as BLEU.The experiments showed that this approach obtains significantly better results than using the maximum mutual information criterion on parameter estimation.This approach used the same set of features as the alignment template approach in (Och and Ney, 2002).SMT Team (2003) also used minimum error training as in Och (2003), but used a large number of feature functions.More than 450 different feature functions were used in order to improve the syntactic well-formedness of MT output.By reranking a 1000-best list generated by the baseline MT system from Och (2003), the BLEU (Papineni et al., 2001) score on the test dataset was improved from 31.6% to 32.9%.Like machine translation, parsing is another field of natural language processing in which generative models have been widely used.In recent years, reranking techniques, especially discriminative reranking, have resulted in significant improvements in parsing.Various machine learning algorithms have been employed in parse reranking, such as Boosting (Collins, 2000), Perceptron (Collins and Duffy, 2002) and Support Vector Machines (Shen and Joshi, 2003).The reranking techniques have resulted in a 13.5% error reduction in labeled recall/precision over the previous best generative parsing models.Discriminative reranking methods for parsing typically use the notion of a margin as the distance between the best candidate parse and the rest of the parses.The reranking problem is reduced to a classification problem by using pairwise samples.In (Shen and Joshi, 2004), we have introduced a new perceptron-like ordinal regression algorithm for parse reranking.In that algorithm, pairwise samples are used for training and margins are defined as the distance between parses of different ranks.In addition, the uneven margin technique has been used for the purpose of adapting ordinal regression to reranking tasks.In this paper, we apply this algorithm to MT reranking, and we also introduce a new perceptron-like reranking algorithm for MT.In the field of machine learning, a class of tasks (called ranking or ordinal regression) are similar to the reranking tasks in NLP.One of the motivations of this paper is to apply ranking or ordinal regression algorithms to MT reranking.In the previous works on ranking or ordinal regression, the margin is defined as the distance between two consecutive ranks.Two large margin approaches have been used.One is the PRank algorithm, a variant of the perceptron algorithm, that uses multiple biases to represent the boundaries between every two consecutive ranks (Crammer and Singer, 2001; Harrington, 2003).However, as we will show in section 3.7, the PRank algorithm does not work on the reranking tasks due to the introduction of global ranks.The other approach is to reduce the ranking problem to a classification problem by using the method of pairwise samples (Herbrich et al., 2000).The underlying assumption is that the samples of consecutive ranks are separable.This may become a problem in the case that ranks are unreliable when ranking does not strongly distinguish between candidates.This is just what happens in reranking for machine translation.The reranking approach for MT is defined as follows: First, a baseline system generates -best candidates.Features that can potentially discriminate between good vs. bad translations are extracted from these -best candidates.These features are then used to determine a new ranking for the -best list.The new top ranked candidate in this -best list is our new best candidate translation.Discriminative reranking allows us to use global features which are unavailable for the baseline system.Second, we can use features of various kinds and need not worry about fine-grained smoothing issues.Finally, the statistical machine learning approach has been shown to be effective in many NLP tasks.Reranking enables rapid experimentation with complex feature functions, because the complex decoding steps in SMT are done once to generate the N-best list of translations.First, we consider how to apply discriminative reranking to machine translation.We may directly use those algorithms that have been successfully used in parse reranking.However, we immediately find that those algorithms are not as appropriate for machine translation.Let be the candidate ranked at the th position for the source sentence, where ranking is defined on the quality of the candidates.In parse reranking, we look for parallel hyperplanes successfully separating and for all the source sentences, but in MT, for each source sentence, we have a set of reference translations instead of a single gold standard.For this reason, it is hard to define which candidate translation is the best.Suppose we have two translations, one of which is close to reference translation ref while the other is close to reference translation ref .It is difficult to say that one candidate is better than the other.Although we might invent metrics to define the quality of a translation, standard reranking algorithms cannot be directly applied to MT.In parse reranking, each training sentence has a ranked list of 27 candidates on average (Collins, 2000), but for machine translation, the number of candidate translations in the -best list is much higher.(SMT Team, 2003) show that to get a reasonable improvement in the BLEU score at least 1000 candidates need to be considered in the -best list.In addition, the parallel hyperplanes separating and actually are unable to distinguish good translations from bad translations, since they are not trained to distinguish any translations in .Furthermore, many good translations in may differ greatly from , since there are multiple references.These facts cause problems for the applicability of reranking algorithms.Our first attempt to handle this problem is to redefine the notion of good translations versus bad translations.Instead of separating and , we say the top of the -best translations are good translations, and the bottom of the -best translations are bad translations, where .Then we look for parallel hyperplanes splitting the top translations and bottom translations for each sentence.Figure 1 illustrates this situation, where and .Furthermore, if we only look for the hyperplanes to separate the good and the bad translations, we, in fact, discard the order information of translations of the same class.Maybe knowing that is better than may be useless for training to some extent, but knowing is better than is useful, if .Although we cannot give an affirmative answer at this time, it is at least reasonable to use the ordering information.The problem is how to use the ordering information.In addition, we only want to maintain the order of two candidates if their ranks are far away from each other.On the other hand, we do not care the order of two translations whose ranks are very close, e.g.100 and 101.Thus insensitive ordinal regression is more desirable and is the approach we follow in this paper.However, reranking is not an ordinal regression problem.In reranking evaluation, we are only interested in the quality of the translation with the highest score, and we do not care the order of bad translations.Therefore we cannot simply regard a reranking problem as an ordinal regression problem, since they have different definitions for the loss function.As far as linear classifiers are concerned, we want to maintain a larger margin in translations of high ranks and a smaller margin in translations of low ranks.For example, The reason is that the scoring function will be penalized There are quite a few linear classifiers1 that can separate samples with large margin, such as SVMs (Vapnik, 1998), Boosting (Schapire et al., 1997), Winnow (Zhang, 2000) and Perceptron (Krauth and Mezard, 1987).The performance of SVMs is superior to other linear classifiers because of their ability to margin maximization.However, SVMs are extremely slow in training since they need to solve a quadratic programming search.For example, SVMs even cannot be used to train on the whole Penn Treebank in parse reranking (Shen and Joshi, 2003).Taking this into account, we use perceptron-like algorithms, since the perceptron algorithm is fast in training which allow us to do experiments on real-world data.Its large margin version is able to provide relatively good results in general.In previous work on the PRank algorithm, ranks are defined on the entire training and test data.Thus we can define boundaries between consecutive ranks on the entire data.But in MT reranking, ranks are defined over every single source sentence.For example, in our data set, the rank of a translation is only the rank among all the translations for the same sentence.The training data includes about 1000 sentences, each of which normally has 1000 candidate translations with the exception of short sentences that have a smaller number of candidate translations.As a result, we cannot use the PRank algorithm in the reranking task, since there are no global ranks or boundaries for all the samples.However, the approach of using pairwise samples does work.By pairing up two samples, we compute the relative distance between these two samples in the scoring metric.In the training phase, we are only interested in whether the relative distance is positive or negative.However, the size of generated training samples will be very large.For samples, the total number of pairwise samples in (Herbrich et al., 2000) is roughly .In the next section, we will introduce two perceptron-like algorithms that utilize pairwise samples while keeping the complexity of data space unchanged.Considering the desiderata discussed in the last section, we present two perceptron-like algorithms for MT reranking.The first one is a splitting algorithm specially designed for MT reranking, which has similarities to a classification algorithm.We also experimented with an ordinal regression algorithm proposed in (Shen and Joshi, 2004).For the sake of completeness, we will briefly describe the algorithm here.In this section, we will propose a splitting algorithm which separates translations of each sentence into two parts, the top translations and the bottom translations.All the separating hyperplanes are parallel by sharing the same weight vector .The margin is defined on the distance between the top items and the bottom items in each cluster, as shown in Figure 1.Let be the feature vector of the translation of the sentence, and be the rank for this translation among all the translations for the sentence.Then the set of training samples is: where is the number of clusters and is the length of ranks for each cluster.Let be a linear function, where is the feature vector of a translation, and is a weight vector.We construct a hypothesis function with as follows. where is a function that takes a list of scores for the candidate translations computed according to the evaluation metric and returns the rank in that list.For example .The splitting algorithm searches a linear function that successfully splits the top -ranked and bottom -ranked translations for each sentence, where .Formally, let for any linear function .We look for the function such that which means that can successfully separate the good translations and the bad translations.Suppose there exists a linear function satisfying (1) and (2), we say is by given and .Furthermore, we can define the splitting margin for the translations of the sentence as follows.The minimal splitting margin, , for given and is defined as follows.Algorithm 1 splitting Require: , and a positive learning margin .Algorithm 1 is a perceptron-like algorithm that looks for a function that splits the training data.The idea of the algorithm is as follows.For every two translations and , if the rank of is higher than or equal to , , the rank of is lower than ,, the weight vector can not successfully separate and with a learning margin , , then we need to update with the addition of .However, the updating is not executed until all the inconsistent pairs in a sentence are found for the purpose of speeding up the algorithm.When sentence is selected, we first compute and store for all .Thus we do not need to recompute again in the inner loop.Now the complexity of a repeat iteration is , where is the average number of active features in vector .If we updated the weight vector whenever an inconsistent pair was found, the complexity of a loop would be .The following theorem will show that Algorithm 1 will stop in finite steps, outputting a function that splits the training data with a large margin, if the training data is splittable.Due to lack of space, we omit the proof for Theorem 1 in this paper.Theorem 1 Suppose the training samples Let .Then Algorithm 1 makes at most mistakes on the pairwise samples during the Algorithm 2 ordinal regression with uneven margin Require: a positive learning margin .The second algorithm that we will use for MT reranking is the -insensitive ordinal regression with uneven margin, which was proposed in (Shen and Joshi, 2004), as shown in Algorithm 2.In Algorithm 2, the function is used to control the level of insensitivity, and the function is used to control the learning margin between pairs of translations with different ranks as described in Section 3.5.There are many candidates for .The following definition for is one of the simplest solutions.We will use this function in our experiments on MT reranking.We provide experimental results on the NIST 2003 Chinese-English large data track evaluation.We use the data set used in (SMT Team, 2003).The training data consists of about 170M English words, on which the baseline translation system is trained.The training data is also used to build language models which are used to define feature functions on various syntactic levels.The development data consists of 993 Chinese sentences.Each Chinese sentence is associated with 1000-best English translations generated by the baseline MT system.The development data set is used to estimate the parameters for the feature functions for the purpose of reranking.The test data consists of 878 Chinese sentences.Each Chinese sentence is associated with 1000-best English translations too.The test set is used to assess the quality of the reranking output.In (SMT Team, 2003), 450 features were generated.Six features from (Och, 2003) were used as baseline features.Each of the 450 features was evaluated independently by combining it with 6 baseline features and assessing on the test data with the minimum error training.The baseline BLEU score on the test set is 31.6%.Table 1 shows some of the best performing features.In (SMT Team, 2003), aggressive search was used to combine features.After combining about a dozen features, the BLEU score did not improve any more, and the score was 32.9%.It was also noticed that the major improvement came from the Model 1 feature.By combining the four features, Model 1, matched parentheses, matched quotation marks and POS language model, the system achieved a BLEU score of 32.6%.In our experiments, we will use 4 different kinds of feature combinations: Baseline: The 6 baseline features used in (Och, 2003), such as cost of word penalty, cost of aligned template penalty.Best Feature: Baseline + IBM Model 1 + matched parentheses + matched quotation marks + POS language model.Top Twenty: Baseline + 14 features with individual BLEU score no less than 31.9% with the minimum error training.Large Set: Baseline + 50 features with individual BLEU score no less than 31.7% with the minimum error training.Since the baseline is 31.6% and the 95% confidence range is 0.9%, most of the features in this set are not individually discriminative with respect to the BLEU metric.We apply Algorithm 1 and 2 to the four feature sets.For algorithm 1, the splitting algorithm, we set in the 1000-best translations given by the baseline MT system.For algorithm 2, the ordinal regression algorithm, we set the updating condition as and , which means one’s rank number is at most half of the other’s and there are at least 20 ranks in between.Figures 2-9 show the results of using Algorithm 1 and 2 with the four feature sets.The -axis represents the number of iterations in the training.The left -axis stands for the BLEU% score on the test data, and the right -axis stands for log of the loss function on the development data.Algorithm 1, the splitting algorithm, converges on the first three feature sets.The smaller the feature set is, the faster the algorithm converges.It achieves a BLEU score of 31.7% on the Baseline, 32.8% on the Best Feature, but only 32.6% on the Top Twenty features.However it is within the range of 95% confidence.Unfortunately on the Large Set, Algorithm 1 converges very slowly.In the Top Twenty set there are a fewer number of individually non-discriminative feature making the pool of features “better”.In addition, generalization performance in the Top Twenty set is better than the Large Set due to the smaller set of “better” features, cf.(Shen and Joshi, 2004).If the number of the non-discriminative features is large enough, the data set becomes unsplittable.We have tried using the trick as in (Li et al., 2002) to make data separable artificially, but the performance could not be improved with such features.We achieve similar results with Algorithm 2, the ordinal regression with uneven margin.It converges on the first 3 feature sets too.On the Baseline, it achieves 31.4%.We notice that the model is over-trained on the development data according to the learning curve.In the Best Feature category, it achieves 32.7%, and on the Top Twenty features, it achieves 32.9%.This algorithm does not converge on the Large Set in 10000 iterations.We compare our perceptron-like algorithms with the minimum error training used in (SMT Team, 2003) as shown in Table 2.The splitting algorithm achieves slightly better results on the Baseline and the Best Feature set, while the minimum error training and the regression algorithm tie for first place on feature combinations.However, the differences are not significant.We notice in those separable feature sets the performance on the development data and the test data are tightly consistent.Whenever the log-loss on the development set is decreased, and BLEU score on the test set goes up, and vice versa.This tells us the merit of these two algorithms; By optimizing on the loss function for the development data, we can improve performance on the test data.This property is guaranteed by the theoretical analysis and is borne out in the experimental results.In this paper, we have successfully applied the discriminative reranking to machine translation.We applied a new perceptron-like splitting algorithm and ordinal regression algorithm with uneven margin to reranking in MT.We provide a theoretical justification for the performance of the splitting algorithms.Experimental results provided in this paper show that the proposed algorithms provide state-of-the-art performance in the NIST 2003 Chinese-English large data track evaluation.This material is based upon work supported by the National Science Foundation under Grant No.0121285.The first author was partially supported by JHU postworkshop fellowship and NSF Grant ITR-0205456.The second author is partially supported by NSERC, Canada (RGPIN: 264905).We thank the members of the SMT team of JHU Workshop 2003 for help on the dataset and three anonymous reviewers for useful comments.
Accurate Information Extraction From Research Papers Using Conditional Random FieldsWith the increasing use of research paper search engines, such as CiteSeer, for both literature search and hiring decisions, the accuracy of such systems is of paramount importance.This paper employs Conditional Random Fields (CRFs) for the task of extracting various common fields from the headers and citation of research papers.The basic theory of CRFs is becoming well-understood, but best-practices for applying them to real-world data requires additional exploration.This paper makes an empirical exploration of several factors, including variations on Gaussian, expoand priors for improved regularization, and several classes of features and Markov order.On a standard benchmark data set, we achieve new state-of-the-art performance, reducing error in average F1 by 36%, and word error rate by 78% in comparison with the previous best SVM results.Accuracy compares even more favorably against HMMs.Research paper search engines, such as CiteSeer (Lawrence et al., 1999) and Cora (McCallum et al., 2000), give researchers tremendous power and convenience in their research.They are also becoming increasingly used for recruiting and hiring decisions.Thus the information quality of such systems is of significant importance.This quality critically depends on an information extraction component that extracts meta-data, such as title, author, institution, etc, from paper headers and references, because these meta-data are further used in many component applications such as field-based search, author analysis, and citation analysis.Previous work in information extraction from research papers has been based on two major machine learning techniques.The first is hidden Markov models (HMM) (Seymore et al., 1999; Takasu, 2003).An HMM learns a generative model over input sequence and labeled sequence pairs.While enjoying wide historical success, standard HMM models have difficulty modeling multiple non-independent features of the observation sequence.The second technique is based on discriminatively-trained SVM classifiers (Han et al., 2003).These SVM classifiers can handle many nonindependent features.However, for this sequence labeling problem, Han et al. (2003) work in a two stages process: first classifying each line independently to assign it label, then adjusting these labels based on an additional classifier that examines larger windows of labels.Solving the information extraction problem in two steps looses the tight interaction between state transitions and observations.In this paper, we present results on this research paper meta-data extraction task using a Conditional Random Field (Lafferty et al., 2001), and explore several practical issues in applying CRFs to information extraction in general.The CRF approach draws together the advantages of both finite state HMM and discriminative SVM techniques by allowing use of arbitrary, dependent features and joint inference over entire sequences.CRFs have been previously applied to other tasks such as name entity extraction (McCallum and Li, 2003), table extraction (Pinto et al., 2003) and shallow parsing (Sha and Pereira, 2003).The basic theory of CRFs is now well-understood, but the best-practices for applying them to new, real-world data is still in an early-exploration phase.Here we explore two key practical issues: (1) regularization, with an empirical study of Gaussian (Chen and Rosenfeld, 2000), exponential (Goodman, 2003), and hyperbolic-Ll (Pinto et al., 2003) priors; (2) exploration of various families of features, including text, lexicons, and layout, as well as proposing a method for the beneficial use of zero-count features without incurring large memory penalties.We describe a large collection of experimental results on two traditional benchmark data sets.Dramatic improvements are obtained in comparison with previous SVM and HMM based results.Conditional random fields (CRFs) are undirected graphical models trained to maximize a conditional probability (Lafferty et al., 2001).A common special-case graph structure is a linear chain, which corresponds to a finite state machine, and is suitable for sequence labeling.A linear-chain CRF with parameters A = {A,...} defines a conditional probability for a state (or label1) sequence y = y1...yT given an input sequence x = x1...xT to be where ZX is the normalization constant that makes the probability of all state sequences sum to one, fk(yt−1, yt, x, t) is a feature function which is often binary-valued, but can be real-valued, and Ak is a learned weight associated with feature fk.The feature functions can measure any aspect of a state transition, yt−1 → yt, and the observation sequence, x, centered at the current time step, t. For example, one feature function might have value 1 when yt−1 is the state TITLE, yt is the state AUTHOR, and xt is a word appearing in a lexicon of people’s first names.Large positive values for Ak indicate a preference for such an event, while large negative values make the event unlikely.Given such a model as defined in Equ.(1), the most probable labeling sequence for an input x, can be efficiently calculated by dynamic programming using the Viterbi algorithm.Calculating the marginal probability of states or transitions at each position in the sequence by a dynamic-programming-based inference procedure very similar to forward-backward for hidden Markov models.The parameters may be estimated by maximum likelihood—maximizing the conditional probability of a set of label sequences, each given their corresponding input sequences.The log-likelihood of training set Maximizing (2) corresponds to satisfying the following equality, wherein the the empirical count of each feature matches its expected count according to the model PΛ(y|x).CRFs share many of the advantageous properties of standard maximum entropy models, including their convex likelihood function, which guarantees that the learning procedure converges to the global maximum.Traditional maximum entropy learning algorithms, such as GIS and IIS (Pietra et al., 1995), can be used to train CRFs, however, it has been found that a quasi-Newton gradient-climber, BFGS, converges much faster (Malouf, 2002; Sha and Pereira, 2003).We use BFGS for optimization.In our experiments, we shall focus instead on two other aspects of CRF deployment, namely regularization and selection of different model structure and feature types.To avoid over-fitting, log-likelihood is often penalized by some prior distribution over the parameters.Figure 1 shows an empirical distribution of parameters, A, learned from an unpenalized likelihood, including only features with non-zero count in the training set.Three prior distributions that have shape similar to this empirical distribution are the Gaussian prior, exponential prior, and hyperbolic-L1 prior, each shown in Figure 2.In this paper we provide an empirical study of these three priors.With a Gaussian prior, log-likelihood (2) is penalized as follows: This adjusted constraint (as well as the adjustments imposed by the other two priors) is intuitively understandable: rather than matching exact empirical feature frequencies, the model is tuned to match discounted feature frequencies.Chen and Rosenfeld (2000) discuss this in the context of other discounting procedures common in language modeling.We call the term subtracted from the empirical counts (in this case λk/σ2) a discounted value.The variance can be feature dependent.However for simplicity, constant variance is often used for all features.In this paper, however, we experiment with several alternate versions of Gaussian prior in which the variance is feature dependent.Although Gaussian (and other) priors are gradually overcome by increasing amounts of training data, perhaps not at the right rate.The three methods below all provide ways to alter this rate by changing the variance of the Gaussian prior dependent on feature counts. ckxσ2 where σ is a constant over all features.In this way, we increase the smoothing on the low frequency features more so than the high frequency features. λk fck/ xσ2 where ck is the count of features, N is the bin size, and ra] is the ceiling function.Alternatively, the variance in each bin may be set independently by cross-validation.Whereas the Gaussian prior penalizes according to the square of the weights (an L2 penalizer), the intention here is to create a smoothly differentiable analogue to penalizing the absolute-value of the weights (an L1 penalizer).L1 penalizers often result in more “sparse solutions,” in which many features have weight nearly at zero, and thus provide a kind of soft feature selection that improves generalization.Goodman (2003) proposes an exponential prior, specifically a Laplacian prior, as an alternative to Gaussian prior.Under this prior, This corresponds to the absolute smoothing method in language modeling.We set the αk = α; i.e. all features share the same constant whose value can be determined using absolute discounting α = n1 n1+2n2 , where n1 and n2 are the number of features occurring once and twice (Ney et al., 1995).Another L1 penalizer is the hyperbolic-L1 prior, described in (Pinto et al., 2003).The hyperbolic distribution has log-linear tails.Consequently the class of hyperbolic distribution is an important alternative to the class of normal distributions and has been used for analyzing data from various scientific areas such as finance, though less frequently used in natural language processing.Under a hyperbolic prior, The hyperbolic prior was also tested with CRFs in McCallum and Li (2003).Wise choice of features is always vital the performance of any machine learning solution.Feature induction (McCallum, 2003) has been shown to provide significant improvements in CRFs performance.In some experiments described below we use feature induction.The focus in this section is on three other aspects of the feature space.In CRFs, state transitions are also represented as features.The feature function fk(yt−1, yt, x, t) in Equ.(1) is a general function over states and observations.Different state transition features can be defined to form different Markov-order structures.We define four different state transitions features corresponding to different Markov order for different classes of features.Higher order features model dependencies better, but also create more data sparse problem and require more memory in training.Before the use of prior distributions over parameters was common in maximum entropy classifiers, standard practice was to eliminate all features with zero count in the training data (the so-called unsupported features).However, unsupported, zero-count features can be extremely useful for pushing Viterbi inference away from certain paths by assigning such features negative weight.The use of a prior allows the incorporation of unsupported features, however, doing so often greatly increases the number parameters and thus the memory requirements.Below we experiment with models containing and not containing unsupported features—both with and without regularization by priors, and we argue that non-supported features are useful.We present here incremental support, a method of introducing some useful unsupported features without exploding the number of parameters with all unsupported features.The model is trained for several iterations with supported features only.Then inference determines the label sequences assigned high probability by the model.Incorrect transitions assigned high probability by the model are used to selectively add to the model those unsupported features that occur on those transitions, which may help improve performance by being assigned negative weight in future training.If desired, several iterations of this procedure may be performed.One of the advantages of CRFs and maximum entropy models in general is that they easily afford the use of arbitrary features of the input.One can encode local spelling features, layout features such as positions of line breaks, as well as external lexicon features, all in one framework.We study all these features in our research paper extraction problem, evaluate their individual contributions, and give some guidelines for selecting good features.Here we also briefly describe a HMM model we used in our experiments.We relax the independence assumption made in standard HMM and allow Markov dependencies among observations, e.g., P(otlst, ot−1).We can vary Markov orders in state transition and observation transitions.In our experiments, a model with second order state transitions and first order observation transitions performs the best.The state transition probabilities and emission probabilities are estimated using maximum likelihood estimation with absolute smoothing, which was found to be effective in previous experiments, including Seymore et al. (1999).We experiment with two datasets of research paper content.One consists of the headers of research papers.The other consists of pre-segmented citations from the reference sections of research papers.These data sets have been used as standard benchmarks in several previous studies (Seymore et al., 1999; McCallum et al., 2000; Han et al., 2003).The header of a research paper is defined to be all of the words from the beginning of the paper up to either the first section of the paper, usually the introduction, or to the end of the first page, whichever occurs first.It contains 15 fields to be extracted: title, author, affiliation, address, note, email, date, abstract, introduction, phone, keywords, web, degree, publication number, and page (Seymore et al., 1999).The header dataset contains 935 headers.Following previous research (Seymore et al., 1999; McCallum et al., 2000; Han et al., 2003), for each trial we randomly select 500 for training and the remaining 435 for testing.We refer this dataset as H. The reference dataset was created by the Cora project (McCallum et al., 2000).It contains 500 references, we use 350 for training and the rest 150 for testing.References contain 13 fields: author, title, editor, booktitle, date, journal, volume, tech, institution, pages, location, publisher, note.We refer this dataset as R. To give a comprehensive evaluation, we measure performance using several different metrics.In addition to the previously-used word accuracy measure (which overemphasizes accuracy of the abstract field), we use perfield F1 measure (both for individual fields and averaged over all fields—called a “macro average” in the information retrieval literature), and whole instance accuracy for measuring overall performance in a way that is sensitive to even a single error in any part of header or citation.Thus, we consider both word accuracy and average F-measure in evaluation.3.Whole instance accuracy: An instance here is defined to be a single header or reference.Whole instance accuracy is the percentage of instances in which every word is correctly labeled.We first report the overall results by comparing CRFs with HMMs, and with the previously best benchmark results obtained by SVMs (Han et al., 2003).We then break down the results to analyze various factors individually.Table 1 shows the results on dataset H with the best results in bold; (intro and page fields are not shown, following past practice (Seymore et al., 1999; Han et al., 2003)).The results we obtained with CRFs use secondorder state transition features, layout features, as well as supported and unsupported features.Feature induction is used in experiments on dataset R; (it didn’t improve accuracy on H).The results we obtained with the HMM model use a second order model for transitions, and a first order for observations.The results on SVM is obtained from (Han et al., 2003) by computing F1 measures from the precision and recall numbers they report.Table 2 shows the results on dataset R. SVM results are not available for these datasets.From Table (1, 2), one can see that CRF performs significantly better than HMMs, which again supports the previous findings (Lafferty et al., 2001; Pinto et al., 2003).CRFs also perform significantly better than SVMbased approach, yielding new state of the art performance on this task.CRFs increase the performance on nearly all the fields.The overall word accuracy is improved from 92.9% to 98.3%, which corresponds to a 78% error rate reduction.However, as we can see word accuracy can be misleading since HMM model even has a higher word accuracy than SVM, although it performs much worse than SVM in most individual fields except abstract.Interestingly, HMM performs much better on abstract field (98% versus 93.8% F-measure) which pushes the overall accuracy up.A better comparison can be made by comparing the field-based F-measures.Here, in comparison to the SVM, CRFs improve the F1 measure from 89.7% to 93.9%, an error reduction of 36%.The results of different regularization methods are summarized in Table (3).Setting Gaussian variance of features depending on feature count performs better, from 90.5% to 91.2%, an error reduction of 7%, when only using supported features, and an error reduction of 9% when using supported and unsupported features.Results are averaged over 5 random runs, with an average variance of 0.2%.In our experiments we found the Gaussian prior to consistently perform better than the others.Surprisingly, exponential prior hurts the performance significantly.It over penalizes the likelihood (significantly increasing cost—defined as negative penalized log-likelihood).We hypothesized that the problem could be that the choice of constant α is inappropriate.So we tried varying α instead of computing it using absolute discounting, but found the alternatives to perform worse.These results suggest that Gaussian prior is a safer prior non-regularized, Gaussian variance = X sets variance to be X. Gaussian cut 7 refers to the Threshold Cut method, Gaussian divide count refers to the Divide Count method, Gaussian bin N refers to the Bin-Based method with bin size equals N, as described in 2.1.1 to use in practice.State transition features and unsupported features.We summarize the comparison of different state transition models using or not using unsupported features in Table 4.The first column describes the four different state transition models, the second column contains the overall word accuracy of these models using only support features, and the third column contains the result of using all features, including unsupported features.Comparing the rows, one can see that the second-order model performs the best, but not dramatically better than the firstorder+transitions and the third order model.However, the first-order model performs significantly worse.The difference does not come from sharing the weights, but from ignoring the f(yt−1i yt).The first order transition feature is vital here.We would expect the third order model to perform better if enough training data were available.Comparing the second and the third columns, we can see that using all features including unsupported features, consistently performs better than ignoring them.Our preliminary experiments with incremental support have shown performance in between that of supported-only and all features, and are still ongoing.Effects of layout features To analyze the contribution of different kinds of features, we divide the features into three categories: local features, layout features, and external lexicon resources.The features we used are summarized in Table 5.The results of using different features are shown in Table 6.The layout feature dramatically increases the performance, raising the F1 measure from 88.8% to 93.9%, whole sentence accuracy from 40.1% to 72.4%.Adding lexicon features alone improves the performance.However, when combing lexicon features and layout features, the performance is worse than using layout features alone.The lexicons were gathered from a large collection of BibTeX files, and upon examination had difficult to remove noise, for example words in the author lexicon that were also affiliations.In previous work, we have gained significant benefits by dividing each lexicon into sections based on point-wise information gain with respect to the lexicon’s class.errors happen at the boundaries between two fields.Especially the transition from author to affiliation, from abstract to keyword.The note field is the one most confused with others, and upon inspection is actually labeled inconsistently in the training data.Other errors could be fixed with additional feature engineering—for example, including additional specialized regular expressions should make email accuracy nearly perfect.Increasing the amount of training data would also be expected to help significantly, as indicated by consistent nearly perfect accuracy on the training set.This paper investigates the issues of regularization, feature spaces, and efficient use of unsupported features in CRFs, with an application to information extraction from research papers.For regularization we find that the Gaussian prior with variance depending on feature frequencies performs better than several other alternatives in the literature.Feature engineering is a key component of any machine learning solution—especially in conditionally-trained models with such freedom to choose arbitrary features—and plays an even more important role than regularization.We obtain new state-of-the-art performance in extracting standard fields from research papers, with a significant error reduction by several metrics.We also suggest better evaluation metrics to facilitate future research in this task—especially field-F1, rather than word accuracy.We have provided an empirical exploration of a few previously-published priors for conditionally-trained loglinear models.Fundamental advances in regularization for CRFs remains a significant open research area.This work was supported in part by the Center for Intelligent Information Retrieval, in part by SPAWARSYSCEN-SD grant number N66001-02-18903, in part by the National Science Foundation Cooperative Agreement number ATM-9732665 through a subcontract from the University Corporation for Atmospheric Research (UCAR) and in part by The Central Intelligence Agency, the National Security Agency and National Science Foundation under NSF grant #IIS0326249.Any opinions, findings and conclusions or rectitle auth. pubnum date abs. aff. addr. email deg. note ph. intro k.w. web title 3446 0 6 0 22 0 0 0 9 25 0 0 12 0 author 0 2653 0 0 7 13 5 0 14 41 0 0 12 0 pubnum 0 14 278 2 0 2 7 0 0 39 0 0 0 0 date 0 0 3 336 0 1 3 0 0 18 0 0 0 0 abstract 0 0 0 0 53262 0 0 1 0 0 0 0 0 0 affil.19 13 0 0 10 3852 27 0 28 34 0 0 0 1 address 0 11 3 0 0 35 2170 1 0 21 0 0 0 0 email 0 0 1 0 12 2 3 461 0 2 2 0 15 0 degree 2 2 0 2 0 2 0 5 465 95 0 0 2 0 note 52 2 9 6 219 52 59 0 5 4520 4 3 21 3 phone 0 0 0 0 0 0 0 1 0 2 215 0 0 0 intro 0 0 0 0 0 0 0 0 0 32 0 625 0 0 keyword 57 0 0 0 18 3 15 0 0 91 0 0 975 0 web 0 0 0 0 2 0 0 0 0 31 0 0 0 294 ommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor.
Exploring Content Models for Multi-Document SummarizationWe present an exploration of generative probabilistic models for multi-document summarization.Beginning with a simple word frequency based model (Nenkova and Vanderwende, 2005), we construct a sequence of models each injecting more structure into the representation of document set content and exhibiting ROUGE gains along the way.Our model, utilizes a hierarchical LDA-style model (Blei et al., 2004) to represent content specificity as a hierarchy of topic vocabulary distributions.At the task of producing generic DUC-style summaries, state-of-the-art ROUGE performance and in pairwise user evaluation strongly outperforms Toutanova et al. (2007)’s state-of-the-art discriminative system.We explore capacity to produce multiple ‘topical summaries’ in order to facilitate content discovery and navigation.Over the past several years, there has been much interest in the task of multi-document summarization.In the common Document Understanding Conference (DUC) formulation of the task, a system takes as input a document set as well as a short description of desired summary focus and outputs a word length limited summary.1 To avoid the problem of generating cogent sentences, many systems opt for an extractive approach, selecting sentences from the document set which best reflect its core content.2 There are several approaches to modeling document content: simple word frequency-based methods (Luhn, 1958; Nenkova and Vanderwende, 2005), graph-based approaches (Radev, 2004; Wan and Yang, 2006), as well as more linguistically motivated techniques (Mckeown et al., 1999; Leskovec et al., 2005; Harabagiu et al., 2007).Another strand of work (Barzilay and Lee, 2004; Daum´e III and Marcu, 2006; Eisenstein and Barzilay, 2008), has explored the use of structured probabilistic topic models to represent document content.However, little has been done to directly compare the benefit of complex content models to simpler surface ones for generic multi-document summarization.In this work we examine a series of content models for multi-document summarization and argue that LDA-style probabilistic topic models (Blei et al., 2003) can offer state-of-the-art summarization quality as measured by automatic metrics (see section 5.1) and manual user evaluation (see section 5.2).We also contend that they provide convenient building blocks for adding more structure to a summarization model.In particular, we utilize a variation of the hierarchical LDA topic model (Blei et al., 2004) to discover multiple specific ‘subtopics’ within a document set.The resulting model, HIERSUM (see section 3.4), can produce general summaries as well as summaries for any of the learned sub-topics.The task we will consider is extractive multidocument summarization.In this task we assume a document collection D consisting of documents Di, ... , D,,, describing the same (or closely related) narrative (Lapata, 2003). set of events.Our task will be to propose a summary S consisting of sentences in D totaling at most L words.3 Here as in much extractive summarization, we will view each sentence as a bag-of-words or more generally a bag-of-ngrams (see section 5.1).The most prevalent example of this data setting is document clusters found on news aggregator sites.For model development we will utilize the DUC 2006 evaluation set4 consisting of 50 document sets each with 25 documents; final evaluation will utilize the DUC 2007 evaluation set (section 5).Automated evaluation will utilize the standard DUC evaluation metric ROUGE (Lin, 2004) which represents recall over various n-grams statistics from a system-generated summary against a set of humangenerated peer summaries.5 We compute ROUGE scores with and without stop words removed from peer and proposed summaries.In particular, we utilize R-1 (recall against unigrams), R-2 (recall against bigrams), and R-SU4 (recall against skip-4 bigrams)6.We present R-2 without stop words in the running text, but full development results are presented in table 1.Official DUC scoring utilizes the jackknife procedure and assesses significance using bootstrapping resampling (Lin, 2004).In addition to presenting automated results, we also present a user evaluation in section 5.2.We present a progression of models for multidocument summarization.Inference details are given in section 4.The SUMBASIC algorithm, introduced in Nenkova and Vanderwende (2005), is a simple effective procedure for multi-document extractive summarization.Its design is motivated by the observation that the relative frequency of a non-stop word in a document set is a good predictor of a word appearing in a human summary.In SUMBASIC, each sentence where PD(·) initially reflects the observed unigram probabilities obtained from the document collection D. A summary S is progressively built by adding the highest scoring sentence according to (1).7 In order to discourage redundancy, the words in the selected sentence are updated PDnew (w) a til the summary word limit has been reached.Despite its simplicity, SUMBASIC yields 5.3 R-2 without stop words on DUC 2006 (see table 1).8 By comparison, the highest-performing ROUGE system at the DUC 2006 evaluation, SUMFOCUS, was built on top of SUMBASIC and yielded a 6.0, which is not a statistically significant improvement (Vanderwende et al., 2007).9 Intuitively, SUMBASIC is trying to select a summary which has sentences where most words have high likelihood under the document set unigram distribution.One conceptual problem with this objective is that it inherently favors repetition of frequent non-stop words despite the ‘squaring’ update.Ideally, a summarization criterion should be more recall oriented, penalizing summaries which omit moderately frequent document set words and quickly diminishing the reward for repeated use of word.Another more subtle shortcoming is the use of the raw empirical unigram distribution to represent content significance.For instance, there is no distinction between a word which occurs many times in the same document or the same number of times across several documents.Intuitively, the latter word is more indicative of significant document set content.The KLSUM algorithm introduces a criterion for selecting a summary S given document collection D, where PS is the empirical unigram distribution of the candidate summary S and KL(P Q) represents the Kullback-Lieber (KL) divergence given by divergence between the true distribution P (here the document set unigram distribution) and the approximating distribution Q (the summary distribution).This criterion casts summarization as finding a set of summary sentences which closely match the document set unigram distribution.Lin et al. (2006) propose a related criterion for robust summarization evaluation, but to our knowledge this criteria has been unexplored in summarization systems.We address optimizing equation (2) as well as summary sentence ordering in section 4.KLSUM yields 6.0 R-2 without stop words, beating SUMBASIC but not with statistical significance.It is worth noting however that KLSUM’s performance matches SUMFOCUS (Vanderwende et al., 2007), the highest R-2 performing system at DUC 2006.As mentioned in section 3.2, the raw unigram distribution PD(·) may not best reflect the content of D for the purpose of summary extraction.We propose TOPICSUM, which uses a simple LDA-like topic model (Blei et al., 2003) similar to Daum´e III and Marcu (2006) to estimate a content distribu10In order to ensure finite values of KL-divergence we smoothe PS(·) so that it has a small amount of mass on all document set words. tion for summary extraction.11 We extract summary sentences as before using the KLSUM criterion (see equation (2)), plugging in a learned content distribution in place of the raw unigram distribution.First, we describe our topic model (see figure 1) which generates a collection of document sets.We assume a fixed vocabulary V :12 11A topic model is a probabilistic generative process that generates a collection of documents using a mixture of topic vocabulary distributions (Steyvers and Griffiths, 2007).Note this usage of topic is unrelated to the summary focus given for document collections; this information is ignored by our models.12In contrast to previous models, stop words are not removed in pre-processing.13DIRICHLET(V,A) represents the symmetric Dirichlet prior distribution over V each with a pseudo-count of A.Concrete pseudo-count values will be given in section 4.4.For each sentence S of each document D, draw a distribution ψT over topics (CONTENT, DOCSPECIFIC, BACKGROUND) from a Dirichlet prior with pseudo-counts (1.0, 5.0,10.0).14 For each word position in the sentence, we draw a topic Z from ψT, and a word W from the topic distribution Z indicates.Our intent is that φC represents the core content of a document set.Intuitively, φC does not include words which are common amongst several document collections (modeled with the BACKGROUND topic), or words which don’t appear across many documents (modeled with the DOCSPECIFIC topic).Also, because topics are tied together at the sentence level, words which frequently occur with other content words are more likely to be considered content words.We ran our topic model over the DUC 2006 document collections and estimated the distribution φC(·) for each document set.15 Then we extracted a summary using the KLSUM criterion with our estimated φC in place of the the raw unigram distribution.Doing so yielded 6.3 R-2 without stop words (see TOPICSUM in table 1); while not a statistically significant improvement over KLSUM, it is our first model which outperforms SUMBASIC with statistical significance.Daum´e III and Marcu (2006) explore a topic model similar to ours for query-focused multidocument summarization.16 Crucially however, Daum´e III and Marcu (2006) selected sentences with the highest expected number of CONTENT words.17 We found that in our model using this extraction criterion yielded 5.3 R-2 without stop words, significantly underperforming our TOPICSUM model.One reason for this may be that Daum´e III and Marcu (2006)’s criterion encourages selecting sentences which have words that are confidently generated by the CONTENT distribution, but not necessarily sentences which contain a plurality of it’s mass.TENT distribution by analytically integrating over φC (Blei et al., 2003), doing so gave no benefit.Previous sections have treated the content of a document set as a single (perhaps learned) unigram distribution.However, as Barzilay and Lee (2004) observe, the content of document collections is highly structured, consisting of several topical themes, each with its own vocabulary and ordering preferences.For concreteness consider the DUC 2006 document collection describing the opening of Star Wars: Episode 1 (see figure 2(a)).While there are words which indicate the general content of this document collection (e.g. star, wars), there are several sub-stories with their own specific vocabulary.For instance, several documents in this collection spend a paragraph or two talking about the financial aspect of the film’s opening and use a specific vocabulary there (e.g.$, million, record).A user may be interested in general content of a document collection or, depending on his or her interests, one or more of the sub-stories.We choose to adapt our topic modeling approach to allow modeling this aspect of document set content.Rather than drawing a single CONTENT distribution 0C for a document collection, we now draw a general content distribution 0C0 from DIRICHLET(V,AG) as well as specific content distributions 0Ci for i = 1, ... , K each from DIRICHLET(V,AS).18 Our intent is that 0C0 represents the 18We choose K=3 in our experiments, but one could flexibly general content of the document collection and each 0Ci represents specific sub-stories.As with TOPICSUM, each sentence has a distribution ψT over topics (BACKGROUND, DOCSPECIFIC, CONTENT).When BACKGROUND or DOCSPECIFIC topics are chosen, the model works exactly as in TOPICSUM.However when the CONTENT topic is drawn, we must decide whether to emit a general content word (from 0C0) or from one of the specific content distributions (from one of 0Ci for i = 1, ... , K).The generative story of TOPICSUM is altered as follows in this case: • General or Specific?We must first decide whether to use a general or specific content word.Each sentence draws a binomial distribution ψG determining whether a CONTENT word in the sentence will be drawn from the general or a specific topic distribution.Reflecting the intuition that the earlier sentences in a document19 describe the general content of a story, we bias ψG to be drawn from BETA(5,2), preferring general content words, and every later sentence from BETA(1,2).20 emitting a topic specific content word, we must decide which of 0Cl, ... , 0CK to use.In order to ensure tight lexical cohesion amongst the specific topics, we assume that each sentence draws a single specific topic ZS used for every specific content word in that sentence.Reflecting intuition that adjacent sentences are likely to share specific content vocabulary, we utilize a ‘sticky’ HMM as in Barzilay and Lee (2004) over the each sentences’ ZS.Concretely, ZS for the first sentence in a document is drawn uniformly from 1, ... , K, and each subsequent sentence’s ZS will be identical to the previous sentence with probability Q, and with probability 1 − Q we select a successor topic from a learned transition distribution amongst 1, ... , K.21 Our intent is that the general content distribution 0C0 now prefers words which not only appear in many documents, but also words which appear consistently throughout a document rather than being concentrated in a small number of sentences.Each specific content distribution 0Ci is meant to model topics which are used in several documents but tend to be used in concentrated locations.HIERSUM can be used to extract several kinds of summaries.It can extract a general summary by plugging 0C0 into the KLSUM criterion.It can also produce topical summaries for the learned specific topics by extracting a summary over each 0Ci distribution; this might be appropriate for a user who wants to know more about a particular substory.While we found the general content distribution (from 0Co) to produce the best single summary, we experimented with utilizing topical summaries for other summarization tasks (see section 6.1).The resulting system, HIERSUM yielded 6.4 R-2 without stop words.While not a statistically significant improvement in ROUGE over TOPICSUM, we found the summaries to be noticeably improved.Since globally optimizing the KLSUM criterion in equation (equation (2)) is exponential in the total number of sentences in a document collection, we 21We choose σ = 0.75 in our experiments. opted instead for a simple approximation where sentences are greedily added to a summary so long as they decrease KL-divergence.We attempted more complex inference procedures such as McDonald (2007), but these attempts only yielded negligible performance gains.All summary sentence ordering was determined as follows: each sentence in the proposed summary was assigned a number in [0, 1] reflecting its relative sentence position in its source document, and sorted by this quantity.All topic models utilize Gibbs sampling for inference (Griffiths, 2002; Blei et al., 2004).In general for concentration parameters, the more specific a distribution is meant to be, the smaller its concentration parameter.Accordingly for TOPICSUM, AG = AD = 1 and AC = 0.1.For HIERSUM we used AG = 0.1 and AS = 0.01.These parameters were minimally tuned (without reference to ROUGE results) in order to ensure that all topic distribution behaved as intended.We present formal experiments on the DUC 2007 data main summarization task, proposing a general summary of at most 250 words22 which will be evaluated automatically and manually in order to simulate as much as possible the DUC evaluation environment.23 DUC 2007 consists of 45 document sets, each consisting of 25 documents and 4 human reference summaries.We primarily evaluate the HIERSUM model, extracting a single summary from the general content distribution using the KLSUM criterion (see section 3.2).Although the differences in ROUGE between HIERSUM and TOPICSUM were minimal, we found HIERSUM summary quality to be stronger.In order to provide a reference for ROUGE and manual evaluation results, we compare against PYTHY, a state-of-the-art supervised sentence extraction summarization system.PYTHY uses humangenerated summaries in order to train a sentence ranking system which discriminatively maximizes ROUGE scores.PYTHY uses several features to rank sentences including several variations of the SUMBASIC score (see section 3.1).At DUC 2007, PYTHY was ranked first overall in automatic ROUGE evaluation and fifth in manual content judgments.As PYTHY utilizes a sentence simplification component, which we do not, we also compare against PYTHY without sentence simplification.ROUGE results comparing variants of HIERSUM and PYTHY are given in table 3.The HIERSUM system as described in section 3.4 yields 7.3 R-2 without stop words, falling significantly short of the 8.7 that PYTHY without simplification yields.Note that R-2 is a measure of bigram recall and HIERSUM does not represent bigrams whereas PYTHY includes several bigram and higher order n-gram statistics.In order to put HIERSUM and PYTHY on equalfooting with respect to R-2, we instead ran HIERSUM with each sentence consisting of a bag of bigrams instead of unigrams.24 All the details of the model remain the same.Once a general content distribution over bigrams has been determined by hierarchical topic modeling, the KLSUM criterion is used as before to extract a summary.This system, labeled HIERSUM bigram in table 3, yields 9.3 R-2 without stop words, significantly outperforming HIERSUM unigram.This model outperforms PYTHY with and without sentence simplification, but not with statistical significance.We conclude that both PYTHY variants and HIERSUM bigram are comparable with respect to ROUGE performance.24Note that by doing topic modeling in this way over bigrams, our model becomes degenerate as it can generate inconsistent bags of bigrams.Future work may look at topic models over n-grams as suggested by Wang et al. (2007).In order to obtain a more accurate measure of summary quality, we performed a simple user study.For each document set in the DUC 2007 collection, a user was given a reference summary, a PYTHY summary, and a HIERSUM summary;25 note that the original documents in the set were not provided to the user, only a reference summary.For this experiment we use the bigram variant of HIERSUM and compare it to PYTHY without simplification so both systems have the same set of possible output summaries.The reference summary for each document set was selected according to highest R-2 without stop words against the remaining peer summaries.Users were presented with 4 questions drawn from the DUC manual evaluation guidelines:26 (1) Overall quality: Which summary was better overall?(2) Non-Redundancy: Which summary was less redundant?(3) Coherence: Which summary was more coherent?(4) Focus: Which summary was more 25The system identifier was of course not visible to the user.The order of automatic summaries was determined randomly. focused in its content, not conveying irrelevant details?The study had 16 users and each was asked to compare five summary pairs, although some did fewer.A total of 69 preferences were solicited.Document collections presented to users were randomly selected from those evaluated fewest.As seen in table 5.2, HIERSUM outperforms PYTHY under all questions.All results are statistically significant as judged by a simple pairwise t-test with 95% confidence.It is safe to conclude that users in this study strongly preferred the HIERSUM summaries over the PYTHY summaries.While it is difficult to qualitatively compare one summarization system over another, we can broadly characterize HIERSUM summaries compared to some of the other systems discussed.For example output from HIERSUM and PYTHY see table 2.On the whole, HIERSUM summaries appear to be significantly less redundant than PYTHY and moderately less redundant than SUMBASIC.The reason for this might be that PYTHY is discriminatively trained to maximize ROUGE which does not directly penalize redundancy.Another tendency is for HIERSUM to select longer sentences typically chosen from an early sentence in a document.As discussed in section 3.4, HIERSUM is biased to consider early sentences in documents have a higher proportion of general content words and so this tendency is to be expected.A common concern in multi-document summarization is that without any indication of user interest or intent providing a single satisfactory summary to a user may not be feasible.While many variants of the general summarization task have been proposed which utilize such information (Vanderwende et al., 2007; Nastase, 2008), this presupposes that a user knows enough of the content of a document collection in order to propose a query.As Leuski et al. (2003) and Branavan et al.(2007) suggest, a document summarization system should facilitate content discovery and yield summaries relevant to a user’s interests.We may use HIERSUM in order to facilitate content discovery via presenting a user with salient words or phrases from the specific content topics parametrized by 0C1, ... , 0CK (for an example see figure 3).While these topics are not adaptive to user interest, they typically reflect lexically coherent vocabularies.In this paper we have presented an exploration of content models for multi-document summarization and demonstrated that the use of structured topic models can benefit summarization quality as measured by automatic and manual metrics.Acknowledgements The authors would like to thank Bob Moore, Chris Brockett, Chris Quirk, and Kristina Toutanova for their useful discussions as well as the reviewers for their helpful comments.
Fast Decoding And Optimal Decoding For Machine TranslationA good decoding algorithm is critical to the success of any statistical machine translation system.The decoder’s job is to find the translation that is most likely according to set of previously learned parameters (and a formula for combining them).Since the space of possible translations is extremely large, typical decoding algorithms are only able to examine a portion of it, thus risking to miss good solutions.In this paper, we compare the speed and output quality of a traditional stack-based decoding algorithm with two new decoders: a fast greedy decoder and a slow but optimal decoder that treats decoding as an integer-programming optimization problem.A statistical MT system that translates (say) French sentences into English, is divided into three parts: (1) a language model (LM) that assigns a probability P(e) to any English string, (2) a translation model (TM) that assigns a probability P(fe) to any pair of English and French strings, and (3) a decoder.The decoder takes a previously unseen sentenceand tries to find the that maximizes P(ef), or equivalently maximizes P(e)P(fe).Brown et al. (1993) introduced a series of TMs based on word-for-word substitution and reordering, but did not include a decoding algorithm.If the source and target languages are constrained to have the same word order (by choice or through suitable pre-processing), then the linear Viterbi algorithm can be applied (Tillmann et al., 1997).If re-ordering is limited to rotations around nodes in a binary tree, then optimal decoding can be carried out by a high-polynomial algorithm (Wu, 1996).For arbitrary word-reordering, the decoding problem is NP-complete (Knight, 1999).A sensible strategy (Brown et al., 1995; Wang and Waibel, 1997) is to examine a large subset of likely decodings and choose just from that.Of course, it is possible to miss a good translation this way.If the decoder returns ebut there exists some e for which P(ef) P(ef), this is called a search error.As Wang and Waibel (1997) remark, it is hard to know whether a search error has occurred—the only way to show that a decoding is sub-optimal is to actually produce a higherscoring one.Thus, while decoding is a clear-cut optimization task in which every problem instance has a right answer, it is hard to come up with good answers quickly.This paper reports on measurements of speed, search errors, and translation quality in the context of a traditional stack decoder (Jelinek, 1969; Brown et al., 1995) and two new decoders.The first is a fast greedy decoder, and the second is a slow optimal decoder based on generic mathematical programming techniques.In this paper, we work with IBM Model 4, which revolves around the notion of a word alignment over a pair of sentences (see Figure 1).A word alignment assigns a single home (English string position) to each French word.If two French words align to the same English word, then that English word is said to have a fertility of two.Likewise, if an English word remains unalignedto, then it has fertility zero.The word alignment in Figure 1 is shorthand for a hypothetical stochastic process by which an English string gets converted into a French string.There are several sets of decisions to be made.First, every English word is assigned a fertility.These assignments are made stochastically according to a table n( e).We delete from the string any word with fertility zero, we duplicate any word with fertility two, etc.If a word has fertility greater than zero, we call it fertile.If its fertility is greater than one, we call it very fertile.After each English word in the new string, we may increment the fertility of an invisible English NULL element with probability p(typically about 0.02).The NULL element ultimately produces “spurious” French words.Next, we perform a word-for-word replacement of English words (including NULL) by French words, according to the table t(f e).Finally, we permute the French words.In permuting, Model 4 distinguishes between French words that are heads (the leftmost French word generated from a particular English word), nonheads (non-leftmost, generated only by very fertile English words), and NULL-generated.Heads.The head of one English word is assigned a French string position based on the position assigned to the previous English word.If an English word e translates into something at French position j, then the French head word of eis stochastically placed in French position k with distortion probability d(k–jclass(e ), class(f)), where “class” refers to automatically determined word classes for French and English vocabulary items.This relative offset k–j encourages adjacent English words to translate into adjacent French words.If e is infertile, then j is taken from e , etc.If e is very fertile, then j is the average of the positions of its French translations.Non-heads.If the head of English word e is placed in French position j, then its first nonhead is placed in French position k ( j) according to another table d (k–jclass(f)).The next non-head is placed at position q with probability d (q–kclass(f)), and so forth.NULL-generated.After heads and non-heads are placed, NULL-generated words are permuted into the remaining vacant slots randomly.If there are NULL-generated words, then any placement scheme is chosen with probability 1/ .These stochastic decisions, starting with e, result in different choices of f and an alignment of f with e. We map an e onto a particulara,f✠pair with probability: d e d NULL where the factors separated bysymbols denote fertility, translation, head permutation, non-head permutation, null-fertility, and null-translation probabilities.1If we observe a new sentence f, then an optimal decoder will search for an e that maximizes P(ef) P(e)P(fe).Here, P(fe) is the sum of P(a,fe) over all possible alignments a.Because this sum involves significant computation, we typically avoid it by instead searching for ane,a pair that maximizes P(e,af) P(e)P(a,fe).We take the language model P(e) to be a smoothed n-gram model of English.The stack (also called A*) decoding algorithm is a kind of best-first search which was first introduced in the domain of speech recognition (Jelinek, 1969).By building solutions incrementally and storing partial solutions, or hypotheses, in a “stack” (in modern terminology, a priority queue), the decoder conducts an ordered search of the solution space.In the ideal case (unlimited stack size and exhaustive search time), a stack decoder is guaranteed to find an optimal solution; our hope is to do almost as well under real-world constraints of limited space and time.The generic stack decoding algorithm follows: Initialize the stack with an empty hypothesis.Pop h, the best hypothesis, off the stack.If h is a complete sentence, output h and terminate.For each possible next word w, extend h by adding w and push the resulting hypothesis onto the stack.Return to the second step (pop).One crucial difference between the decoding process in speech recognition (SR) and machine translation (MT) is that speech is always produced in the same order as its transcription.Consequently, in SR decoding there is always a simple left-to-right correspondence between input and output sequences.By contrast, in MT the leftto-right relation rarely holds even for language pairs as similar as French and English.We address this problem by building the solution from left to right, but allowing the decoder to consume its input in any order.This change makes decoding significantly more complex in MT; instead of knowing the order of the input in advance, we must consider allpermutations of an-word input sentence.Another important difference between SR and MT decoding is the lack of reliable heuristics in MT.A heuristic is used in A* search to estimate the cost of completing a partial hypothesis.A good heuristic makes it possible to accurately compare the value of different partial hypotheses, and thus to focus the search in the most promising direction.The left-to-right restriction in SR makes it possible to use a simple yet reliable class of heuristics which estimate cost based on the amount of input left to decode.Partly because of the absence of left-to-right correspondence, MT heuristics are significantly more difficult to develop (Wang and Waibel, 1997).Without a heuristic, a classic stack decoder is ineffective because shorter hypotheses will almost always look more attractive than longer ones, since as we add words to a hypothesis, we end up multiplying more and more terms to find the probability.Because of this, longer hypotheses will be pushed off the end of the stack by shorter ones even if they are in reality better decodings.Fortunately, by using more than one stack, we can eliminate this effect.In a multistack decoder, we employ more than one stack to force hypotheses to compete fairly.More specifically, we have one stack for each subset of input words.This way, a hypothesis can only be pruned if there are other, better, hypotheses that represent the same portion of the input.With more than one stack, however, how does a multistack decoder choose which hypothesis to extend during each iteration?We address this issue by simply taking one hypothesis from each stack, but a better solution would be to somehow compare hypotheses from different stacks and extend only the best ones.The multistack decoder we describe is closely patterned on the Model 3 decoder described in the (Brown et al., 1995) patent.We build solutions incrementally by applying operations to hypotheses.There are four operations: Add adds a new English word and aligns a single French word to it.AddZfert adds two new English words.The first has fertility zero, while the second is aligned to a single French word.Extend aligns an additional French word to the most recent English word, increasing its fertility.AddNull aligns a French word to the English NULL element.AddZfert is by far the most expensive operation, as we must consider inserting a zero-fertility English word before each translation of each unaligned French word.With an English vocabulary size of 40,000, AddZfert is 400,000 times more expensive than AddNull!We can reduce the cost of AddZfert in two ways.First, we can consider only certain English words as candidates for zero-fertility, namely words which both occur frequently and have a high probability of being assigned frequency zero.Second, we can only insert a zero-fertility word if it will increase the probability of a hypothesis.According to the definition of the decoding problem, a zero-fertility English word can only make a decoding more likely by increasing P(e) more than it decreases P(a,fe).2 By only considering helpful zero-fertility insertions, we save ourselves significant overhead in the AddZfert operation, in many cases eliminating all possibilities and reducing its cost to less than that of AddNull.Over the last decade, many instances of NPcomplete problems have been shown to be solvable in reasonable/polynomial time using greedy methods (Selman et al., 1992; Monasson et al., 1999).Instead of deeply probing the search space, such greedy methods typically start out with a random, approximate solution and then try to improve it incrementally until a satisfactory solution is reached.In many cases, greedy methods quickly yield surprisingly good solutions.We conjectured that such greedy methods may prove to be helpful in the context of MT decoding.The greedy decoder that we describe starts the translation process from an English gloss of the French sentence given as input.The gloss is constructed by aligning each French word f with its most likely English translation ef(ef argmaxt(ef)).For example, in translating the French sentence “Bien entendu , il parle de une belle victoire .”, the greedy decoder initially assumes that a good translation of it is “Well heard , it talking a beautiful victory” because the best translation of “bien” is “well”, the best translation of “entendu” is “heard”, and so on.The alignment corresponding to this translation is shown at the top of Figure 2.Once the initial alignment is created, the greedy decoder tries to improve it, i.e., tries to find an alignment (and implicitly translation) of higher probability, by applying one of the following operations: translateOneOrTwoWords( ,e, ,e) changes the translation of one or two French words, those located at positions and , from e and e into eand e. If eis a word of fertility 1 and eis NULL, then eis deleted from the translation.If eis the NULL word, the word eis inserted into the translation at the position that yields the alignment of highest probability.If e eor e e, this operation amounts to changing the translation of a single word. translateAndInsert( ,e,e) changes the translation of the French word located at positionfrom einto and simulataneously inserts word eat the position that yields the alignment of highest probability.Word is selected from an automatically derived list of 1024 words with high probability of having fertility 0.When ee, this operation amounts to inserting a word of fertility 0 into the alignment. removeWordOfFertility0() deletes the word of fertility 0 at positionin the current alignment. swapSegments( ) creates a new alignment from the old one by swapping non-overlapping English word segments and .During the swap operation, all existing links between English and French words are preserved.The segments can be as small as a word or as long as words, where is the length of the English sentence. joinWords( ) eliminates from the alignment the English word at position (or ) and links the French words generated by (or ) to (or ).In a stepwise fashion, starting from the initial gloss, the greedy decoder iterates exhaustively over all alignments that are one operation away from the alignment under consideration.At every step, the decoder chooses the alignment of highest probability, until the probability of the current alignment can no longer be improved.When it starts from the gloss of the French sentence “Bien entendu, il parle de une belle victoire.”, for example, the greedy decoder alters the initial alignment incrementally as shown in Figure 2, eventually producing the translation “Quite naturally, he talks about a great victory.”.In the process, the decoder explores a total of 77421 distinct alignments/translations, of which “Quite naturally, he talks about a great victory.” has the highest probability.We chose the operation types enumerated above for two reasons: (i) they are general enough to enable the decoder escape local maxima and modify in a non-trivial manner a given alignment in order to produce good translations; (ii) they are relatively inexpensive (timewise).The most time consuming operations in the decoder are swapSegments, translateOneOrTwoWords, and translateAndInsert.SwapSegments iterates over all possible non-overlapping span pairs that can be built on a sequence of length .TranslateOneOrTwoWords iterates over alignments, where is the size of the French sentence andis the number of translations we associate with each word (in our implementation, we limit this number to the top 10 translations).TranslateAndInsert iterates over alignments, where is the size of the list of words with high probability of having fertility 0 (1024 words in our implementation).Knight (1999) likens MT decoding to finding optimal tours in the Traveling Salesman Problem (Garey and Johnson, 1979)—choosing a good word order for decoder output is similar to choosing a good TSP tour.Because any TSP problem instance can be transformed into a decoding problem instance, Model 4 decoding is provably NP-complete in the length of f. It is interesting to consider the reverse direction—is it possible to transform a decoding problem instance into a TSP instance?If so, we may take great advantage of previous research into efficient TSP algorithms.We may also take advantage of existing software packages, obtaining a sophisticated decoder with little programming effort.It is difficult to convert decoding into straight TSP, but a wide range of combinatorial optimization problems (including TSP) can be expressed in the more general framework of linear integer programming.A sample integer program (IP) looks like this: A solution to an IP is an assignment of integer values to variables.Solutions are constrained by inequalities involving linear combinations of variables.An optimal solution is one that respects the constraints and minimizes the value of the objective function, which is also a linear combination of variables.We can solve IP instances with generic problem-solving software such as lp solve or CPLEX.3 In this section we explain tence f = “CE NE EST PAS CLAIR .” There is one city for each word in f. City boundaries are marked with bold lines, and hotels are illustrated with rectangles.A tour of cities is a sequence of hotels (starting at the sentence boundary hotel) that visits each city exactly once before returning to the start. how to express MT decoding (Model 4 plus English bigrams) in IP format.We first create a salesman graph like the one in Figure 3.To do this, we set up a city for each word in the observed sentence f. City boundaries are shown with bold lines.We populate each city with ten hotels corresponding to ten likely English word translations.Hotels are shown as small rectangles.The owner of a hotel is the English word inside the rectangle.If two cities have hotels with the same owner x, then we build a third xowned hotel on the border of the two cities.More generally, ifcities all have hotels owned by x, we build new hotels (one for each non-empty, non-singleton subset of the cities) on various city borders and intersections.Finally, we add an extra city representing the sentence boundary.We define a tour of cities as a sequence and hotels (starting at the sentence boundary hotel) that visits each city exactly once before returning to the start.If a hotel sits on the border between two cities, then staying at that hotel counts as visiting both cities.We can view each tour of cities as corresponding to a potential decodinge,a.The owners of the hotels on the tour give us e, while the hotel locations yield a.The next task is to establish real-valued (asymmetric) distances between pairs of hotels, such that the length of any tour is exactly the negative of log(P(e)P(a,fe)).Because log is monotonic, the shortest tour will correspond to the likeliest decoding.The distance we assign to each pair of hotels consists of some small piece of the Model 4 formula.The usual case is typified by the large black arrow in Figure 3.Because the destination hotel “not” sits on the border between cities NE and PAS, it corresponds to a partial alignment in which the word “not” has fertility two: If we assume that we have already paid the price for visiting the “what” hotel, then our interhotel distance need only account for the partial alignment concerning “not”: NULL-owned hotels are treated specially.We require that all non-NULL hotels be visited before any NULL hotels, and we further require that at most one NULL hotel visited on a tour.Moreover, the NULL fertility sub-formula is easy to compute if we allow only one NULL hotel to be visited:is simply the number of cities that hotel straddles, and is the number of cities minus one.This case is typified by the large gray arrow shown in Figure 3.Between hotels that are located (even partially) in the same city, we assign an infinite distance in both directions, as travel from one to the other can never be part of a tour.For 6-word French sentences, we normally come up with a graph that has about 80 hotels and 3500 finite-cost travel segments.The next step is to cast tour selection as an integer program.Here we adapt a subtour elimination strategy used in standard TSP.We create a binary (0/1) integer variable for each pair of hotels and. if and only if travel from hotelto hotelis on the itinerary.The objective function is straightforward: This minimization is subject to three classes of constraints.First, every city must be visited exactly once.That means exactly one tour segment must exit each city: Second, the segments must be linked to one another, i.e., every hotel has either (a) one tour segment coming in and one going out, or (b) no segments in and none out.To put it another way, every hotel must have an equal number of tour segments going in and out: Third, it is necessary to prevent multiple independent sub-tours.To do this, we require that every proper subset of cities have at least one tour segment leaving it: There are an exponential number of constraints in this third class.Finally, we invoke our IP solver.If we assign mnemonic names to the variables, we can easily extracte,afrom the list of variables and their binary values.The shortest tour for the graph in Figure 3 corresponds to this optimal decoding: it is not clear .We can obtain the second-best decoding by adding a new constraint to the IP to stop it from choosing the same solution again.4In our experiments we used a test collection of 505 sentences, uniformly distributed across the lengths 6, 8, 10, 15, and 20.We evaluated all decoders with respect to (1) speed, (2) search optimality, and (3) translation accuracy.The last two factors may not always coincide, as Model 4 is an imperfect model of the translation process—i.e., there is no guarantee that a numerically optimal decoding is actually a good translation.Suppose a decoder outputs, while the optimal decoding turns out to be.Then we consider six possible outcomes: no error (NE) : , and is a perfect , andis a perfect translation, whileis not. harmless search error (HSE): , butandare both perfectly good translations. compound error (CE): , and neither is a perfect translation.Here, “perfect” refers to a human-judged translation that transmits all of the meaning of the source sentence using flawless target-language syntax.We have found it very useful to have several decoders on hand.It is only through IP decoder output, for example, that we can know the stack decoder is returning optimal solutions for so many sentences (see Table 1).The IP and stack decoders enabled us to quickly locate bugs in the greedy decoder, and to implement extensions to the basic greedy search that can find better solutions.(We came up with the greedy operations discussed in Section 5 by carefully analyzing error logs of the kind shown in Table 1).The results in Table 1 also enable us to prioritize the items on our research agenda.Since the majority of the translation errors can be attributed to the language and translation models we use (see column PME in Table 1), it is clear that significant improvement in translation quality will come from better trigram language model.Greedyand greedyare greedy decoders optimized for speed. models.The results in Table 2, obtained with decoders that use a trigram language model, show that our greedy decoding algorithm is a viable alternative to the traditional stack decoding algorithm.Even when the greedy decoder uses an optimized-forspeed set of operations in which at most one word is translated, moved, or inserted at a time and at most 3-word-long segments are swapped—which is labeled “greedy” in Table 2—the translation accuracy is affected only slightly.In contrast, the translation speed increases with at least one order of magnitude.Depending on the application of interest, one may choose to use a slow decoder that provides optimal results or a fast, greedy decoder that provides non-optimal, but acceptable results.One may also run the greedy decoder using a time threshold, as any instance of anytime algorithm.When the threshold is set to one second per sentence (the greedylabel in Table 1), the performance is affected only slightly.Acknowledgments.This work was supported by DARPA-ITO grant N66001-00-1-9814.
Parameter Estimation For Probabilistic Finite-State Transducersalgebraic path problem (shortest paths; matrix inver- 34(3):191–219.Richard Sproat and Michael Riley.1996.Compilation of weighted finite-state transducers from decision trees. of the 34th Annual Meeting of the Andreas Stolcke and Stephen M. Omohundro.1994.Best-first model merging for hidden Markov model induction.Tech.Report ICSI TR-94-003, Berkeley, CA.Robert Endre Tarjan.1981a.A unified approach to path of the 28(3):577–593, July.Robert Endre Tarjan.1981b.Fast algorithms for solving problems. of the 28(3):594–614, July.G. van Noord and D. Gerdemann.2001.An extendible regular expression compiler for finite-state approaches natural language processing.In Impleno.22 in Springer Lecture Notes in CS.Rational relations on strings have become widespread in language and speech engineering (Roche and Schabes, 1997).Despite bounded memory they are well-suited to describe many linguistic and textual processes, either exactly or approximately.A relation is a set of (input, output) pairs.Relations are more general than functions because they may pair a given input string with more or fewer than one output string.The class of so-called rational relations admits a nice declarative programming paradigm.Source code describing the relation (a regular expression) is compiled into efficient object code (in the form of a 2-tape automaton called a finite-state transducer).The object code can even be optimized for runtime and code size (via algorithms such as determinization and minimization of transducers).This programming paradigm supports efficient nondeterminism, including parallel processing over infinite sets of input strings, and even allows “reverse” computation from output to input.Its unusual flexibility for the practiced programmer stems from the many operations under which rational relations are closed.It is common to define further useful operations (as macros), which modify existing relations not by editing their source code but simply by operating on them “from outside.” ∗A brief version of this work, with some additional material, first appeared as (Eisner, 2001a).A leisurely journal-length version with more details has been prepared and is available.The entire paradigm has been generalized to weighted relations, which assign a weight to each (input, output) pair rather than simply including or excluding it.If these weights represent probabilities P(input, output) or P(output  |input), the weighted relation is called a joint or conditional (probabilistic) relation and constitutes a statistical model.Such models can be efficiently restricted, manipulated or combined using rational operations as before.An artificial example will appear in §2.The availability of toolkits for this weighted case (Mohri et al., 1998; van Noord and Gerdemann, 2001) promises to unify much of statistical NLP.Such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy-channel decoding,' including classic models for speech recognition (Pereira and Riley, 1997) and machine translation (Knight and Al-Onaizan, 1998).Moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources.Unfortunately, there is a stumbling block: Where do the weights come from?After all, statistical models require supervised or unsupervised training.Currently, finite-state practitioners derive weights using exogenous training methods, then patch them onto transducer arcs.Not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens.For example, the forward-backward algorithm (Baum, 1972) trains only Hidden Markov Models, while (Ristad and Yianilos, 1996) trains only stochastic edit distance.In short, current finite-state toolkits include no training algorithms, because none exist for the large space of statistical models that the toolkits can in principle describe and run.'Given output, find input to maximize P(input, output).This paper aims to provide a remedy through a new paradigm, which we call parameterized finitestate machines.It lays out a fully general approach for training the weights of weighted rational relations.First §2 considers how to parameterize such models, so that weights are defined in terms of underlying parameters to be learned.§3 asks what it means to learn these parameters from training data (what is to be optimized?), and notes the apparently formidable bookkeeping involved.§4 cuts through the difficulty with a surprisingly simple trick.Finally, §5 removes inefficiencies from the basic algorithm, making it suitable for inclusion in an actual toolkit.Such a toolkit could greatly shorten the development cycle in natural language engineering.Finite-state machines, including finite-state automata (FSAs) and transducers (FSTs), are a kind of labeled directed multigraph.For ease and brevity, we explain them by example.Fig.1a shows a probabilistic FST with input alphabet E = {a, b}, output alphabet A = {x, z}, and all states final.It may be regarded as a device for generating a string pair in E* x A* by a random walk from Q.Two paths exist that generate both input aabb and output xz: Each of the paths has probability .0002646, so the probability of somehow generating the pair (aabb, xz) is .0002646 + .0002646 = .0005292.Abstracting away from the idea of random walks, arc weights need not be probabilities.Still, define a path’s weight as the product of its arc weights and the stopping weight of its final state.Thus Fig.1a defines a weighted relation f where f(aabb, xz) = .0005292.This particular relation does happen to be probabilistic (see §1).It represents a joint distribution (since Ex,y f(x, y) = 1).Meanwhile, Fig.1c defines a conditional one (bx Ey f(x, y) = 1).This paper explains how to adjust probability distributions like that of Fig.1a so as to model training data better.The algorithm improves an FST’s numeric weights while leaving its topology fixed.How many parameters are there to adjust in Fig.1a?That is up to the user who built it!An FST model with few parameters is more constrained, making optimization easier.Some possibilities: generate E if heads, F if tails.” E*λ = (AE)∗(1−A) means “repeatedly flip an A-weighted coin and keep repeating E as long as it comes up heads.” These 4 parameters have global effects on Fig.1a, thanks to complex parameter tying: arcs ® b:p −) @, ® b:q −) ® in Fig.1b get respective probabilities (1 − A)µν and (1 − µ)ν, which covary with ν and vary oppositely with µ.Each of these probabilities in turn affects multiple arcs in the composed FST of Fig.1a.We offer a theorem that highlights the broad applicability of these modeling techniques.4 If f(input, output) is a weighted regular relation, then the following statements are equivalent: (1) f is a joint probabilistic relation; (2) f can be computed by a Markovian FST that halts with probability 1; (3) f can be expressed as a probabilistic regexp, i.e., a regexp built up from atomic expressions a : b (for a E E U {E}, b E A U {E}) using concatenation, probabilistic union +p, and probabilistic closure *p. For defining conditional relations, a good regexp language is unknown to us, but they can be defined in several other ways: (1) via FSTs as in Fig.1c, (2) by compilation of weighted rewrite rules (Mohri and Sproat, 1996), (3) by compilation of decision trees (Sproat and Riley, 1996), (4) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation (Gerdemann and van Noord, 1999),5 (5) by conditionalization of a joint relation as discussed below.A central technique is to define a joint relation as a noisy-channel model, by composing a joint relation with a cascade of one or more conditional relations as in Fig.1 (Pereira and Riley, 1997; Knight and Graehl, 1998).The general form is illustrated by 3Conceptually, the parameters represent the probabilities of reading another a (A); reading another b (ν); transducing b to p rather than q (µ); starting to transduce p to a rather than x (p).P(v, z) def = Ew,x,y P(v|w)P(w, x)P(y|x)P(z|y), implemented by composing 4 machines.6,7 There are also procedures for defining weighted FSTs that are not probabilistic (Berstel and Reutenauer, 1988).Arbitrary weights such as 2.7 may be assigned to arcs or sprinkled through a regexp (to be compiled into E:E/2.7 −)arcs).A more subtle example is weighted FSAs that approximate PCFGs (Nederhof, 2000; Mohri and Nederhof, 2001), or to extend the idea, weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation.These are parameterized by the PCFG’s parameters, but add or remove strings of the PCFG to leave an improper probability distribution.Fortunately for those techniques, an FST with positive arc weights can be normalized to make it jointly or conditionally probabilistic: ization, which simply divides each f(x, y) by Ex,,y, f(x', y') (joint case) or by Ey, f(x, y') (conditional case).To implement the joint case, just divide stopping weights by the total weight of all paths (which §4 shows how to find), provided this is finite.In the conditional case, let g be a copy of f with the output labels removed, so that g(x) finds the desired divisor; determinize g if possible (but this fails for some weighted FSAs), replace all weights with their reciprocals, and compose the result with f.9 6P(w, x) defines the source model, and is often an “identity FST” that requires w = x, really just an FSA.7We propose also using n-tape automata to generalize to “branching noisy channels” (a case of dendroid distributions).In Ew,x P(v|w)P(v,|w)P(w, x)P(y|x), the true transcription w can be triply constrained by observing speech y and two errorful transcriptions v, v', which independently depend on w. 8A corresponding problem exists in the joint case, but may be easily avoided there by first pruning non-coaccessible states.9It suffices to make g unambiguous (one accepting path per string), a weaker condition than determinism.When this is not possible (as in the inverse of Fig.1b, whose conditionalizaNormalization is particularly important because it enables the use of log-linear (maximum-entropy) parameterizations.Here one defines each arc weight, coin weight, or regexp weight in terms of meaningful features associated by hand with that arc, coin, etc.Each feature has a strength E R>0, and a weight is computed as the product of the strengths of its features.10 It is now the strengths that are the learnable parameters.This allows meaningful parameter tying: if certain arcs such asu:i �—*, �—*, and a:ae o:e �—* share a contextual “vowel-fronting” feature, then their weights rise and fall together with the strength of that feature.The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired.Such approaches have been tried recently in restricted cases (McCallum et al., 2000; Eisner, 2001b; Lafferty et al., 2001).Normalization may be postponed and applied instead to the result of combining the FST with other FSTs by composition, union, concatenation, etc.A simple example is a probabilistic FSA defined by normalizing the intersection of other probabilistic FSAs f1, f2,.... (This is in fact a log-linear model in which the component FSAs define the features: string x has log fi(x) occurrences of feature i.)In short, weighted finite-state operators provide a language for specifying a wide variety of parameterized statistical models.Let us turn to their training.We are primarily concerned with the following training paradigm, novel in its generality.Let fθ : E* xA* —* R>0 be a joint probabilistic relation that is computed by a weighted FST.The FST was built by some recipe that used the parameter vector 0.Changing 0 may require us to rebuild the FST to get updated weights; this can involve composition, regexp compilation, multiplication of feature strengths, etc.(Lazy algorithms that compute arcs and states of tion cannot be realized by any weighted FST), one can sometimes succeed by first intersecting g with a smaller regular set in which the input being considered is known to fall.In the extreme, if each input string is fully observed (not the case if the input is bound by composition to the output of a one-to-many FST), one can succeed by restricting g to each input string in turn; this amounts to manually dividing f(x, y) by g(x). fθ on demand (Mohri et al., 1998) can pay off here, since only part of fθ may be needed subsequently.)As training data we are given a set of observed (input, output) pairs, (xi, yi).These are assumed to be independent random samples from a joint distribution of the form fe(x, y); the goal is to recover the true ˆ0.Samples need not be fully observed (partly supervised training): thus xi C E*, yi C A* may be given as regular sets in which input and output were observed to fall.For example, in ordinary HMM training, xi = E* and represents a completely hidden state sequence (cf.Ristad (1998), who allows any regular set), while yi is a single string representing a completely observed emission sequence.11 What to optimize?Maximum-likelihood estimation guesses 0ˆ to be the 0 maximizing Hi fθ(xi, yi).Maximum-posterior estimation tries to maximize P(0)·Hi fθ(xi, yi) where P(0) is a prior probability.In a log-linear parameterization, for example, a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting (Chen and Rosenfeld, 1999).The EM algorithm (Dempster et al., 1977) can maximize these functions.Roughly, the E step guesses hidden information: if (xi, yi) was generated from the current fθ, which FST paths stand a chance of having been the path used?(Guessing the path also guesses the exact input and output.)The M step updates 0 to make those paths more likely.EM alternates these steps and converges to a local optimum.The M step’s form depends on the parameterization and the E step serves the M step’s needs.Let fθ be Fig.1a and suppose (xi, yi) = (a(a + b)*, xxz).During the E step, we restrict to paths compatible with this observation by computing xi o fθ o yi, shown in Fig.2.To find each path’s posterior probability given the observation (xi, yi), just conditionalize: divide its raw probability by the total probability (Pz� 0.1003) of all paths in Fig.2.11To implement an HMM by an FST, compose a probabilistic FSA that generates a state sequence of the HMM with a conditional FST that transduces HMM states to emitted symbols.But that is not the full E step.The M step uses not individual path probabilities (Fig.2 has infinitely many) but expected counts derived from the paths.Crucially, §4 will show how the E step can accumulate these counts effortlessly.We first explain their use by the M step, repeating the presentation of §2: in Fig.2 is “really” to traverse Q a:x Rosenfeld, 1999).12 For globally normalized, joint models, the predicted vector is ecf(E*, A*).If the log-linear probabilities are conditioned on the state and/or the input, the predicted vector is harder to describe (though usually much easier to compute).13 12IIS is itself iterative; to avoid nested loops, run only one iteration at each M step, giving a GEM algorithm (Riezler,1999).Alternatively, discard EM and use gradient-based optimization.13For per-state conditional normalization, let Dj,a be the set of arcs from state j with input symbol a E E; their weights are normalized to sum to 1.Besides computing c, the E step must count the expected number dj,a of traversals of arcs in each Dj,a.Then the predicted vector given θ is Ej,a dj,a ·(expected feature counts on a randomly chosen arc in Dj,a).Per-state joint normalization (Eisner, 2001b, §8.2) is similar but drops the dependence on a.The difficult case is global conditional normalization.It arises, for example, when training a joint model of the form fθ = · · · (gθ o hθ) · · ·, where hθ is a conditional It is also possible to use this EM approach for discriminative training, where we wish to maximize Hi P(yi  |xi) and fθ(x, y) is a conditional FST that defines P(y  |x).The trick is to instead train a joint model g o fθ, where g(xi) defines P(xi), thereby maximizing Hi P(xi) · P(yi  |xi).(Of course, the method of this paper can train such compositions.)If x1,... xn are fully observed, just define each g(xi) = 1/n.But by choosing a more general model of g, we can also handle incompletely observed xi: training g o fθ then forces g and fθ to cooperatively reconstruct a distribution over the possible inputs and do discriminative training of fθ given those inputs.(Any parameters of g may be either frozen before training or optimized along with the parameters of fθ.)A final possibility is that each xi is defined by a probabilistic FSA that already supplies a distribution over the inputs; then we consider xi o fθ o yi directly, just as in the joint model.Finally, note that EM is not all-purpose.It only maximizes probabilistic objective functions, and even there it is not necessarily as fast as (say) conjugate gradient.For this reason, we will also show below how to compute the gradient of fθ(xi, yi) with respect to 0, for an arbitrary parameterized FST fθ.We remark without elaboration that this can help optimize task-related objective functions, such as E Ey(P(xi, y)α/ Ey' P(xi, y�)α) · error(y, yi). iIt remains to devise appropriate E steps, which looks rather daunting.Each path in Fig.2 weaves together parameters from other machines, which we must untangle and tally.In the 4-coin parameterization, path observed heads and tails of the 4 coins.This nontrivially works out to (4, 1, 0,1,1,1,1, 2).For other parameterizations, the path must instead yield a vector of arc traversal counts or feature counts.Computing a count vector for one path is hard enough, but it is the E step’s job to find the expected value of this vector—an average over the infinitely log-linear model of P(v  |u) for u E E'*, v E 0'*.Then the predicted count vector contributed by h is Ei EuEΣ,∗ P(u xi, yi) · ech(u, 0'*).The term Ei P(u  |xi, yi) computes the expected count of each u E E'*.It may be found by a variant of §4 in which path values are regular expressions over E'*. many paths π through Fig.2 in proportion to their posterior probabilities P(π  |xi, yi).The results for all (xi, yi) are summed and passed to the M step.Abstractly, let us say that each path π has not only a probability P(π) E [0, 1] but also a value val(π) in a vector space V , which counts the arcs, features, or coin flips encountered along path π.The value of a path is the sum of the values assigned to its arcs.The E step must return the expected value of the unknown path that generated (xi, yi).For example, if every arc had value 1, then expected value would be expected path length.Letting H denote the set of paths in xi o fe o yi (Fig.2), the expected value is14 The denominator of equation (1) is the total probability of all accepting paths in xi o f o yi.But while computing this, we will also compute the numerator.The idea is to augment the weight data structure with expectation information, so each weight records a probability and a vector counting the parameters that contributed to that probability.We will enforce an invariant: the weight of any pathset H must be (&EΠ P(π), &EΠ P(π) val(π)) E R>0 x V , from which (1) is trivial to compute.Berstel and Reutenauer (1988) give a sufficiently general finite-state framework to allow this: weights may fall in any set K (instead of R).Multiplication and addition are replaced by binary operations ® and ® on K. Thus ® is used to combine arc weights into a path weight and ® is used to combine the weights of alternative paths.To sum over infinite sets of cyclic paths we also need a closure operation *, interpreted as k* = (D'0 ki.The usual finite-state algorithms work if (K, ®, ®, *) has the structure of a closed semiring.15 Ordinary probabilities fall in the semiring (R>0, +, x, *).16 Our novel weights fall in a novel If an arc has probability p and value v, we give it the weight (p, pv), so that our invariant (see above) holds if H consists of a single length-0 or length-1 path.The above definitions are designed to preserve our invariant as we build up larger paths and pathsets.® lets us concatenate (e.g.) simple paths π1, π2 to get a longer path π with P(π) = P(π1)P(π2) and val(π) = val(π1) + val(π2).The definition of ® guarantees that path π’s weight will be (P(π), P(π) · val(π)).® lets us take the union of two disjoint pathsets, and * computes infinite unions.To compute (1) now, we only need the total weight ti of accepting paths in xi o f o yi (Fig.2).This can be computed with finite-state methods: the machine (exxi)of o(yixc) is aversion that replaces all input:output labels with c: c, so it maps (E, 6) to the same total weight ti.Minimizing it yields a onestate FST from which ti can be read directly!The other “magical” property of the expectation semiring is that it automatically keeps track of the tangled parameter counts.For instance, recall that traversing Q a:x −) Q should have the same effect as traversing both the underlying arcs ® a:p −) ® and © p:x −) ©.And indeed, if the underlying arcs have values v1 and v2, then the composed arc @ a:x −) @ gets weight �,„1,p1v1) ® p ( g �N2,p2v2) = (p1p2, p1p2(v1 + v2)), just as if it had value v1 + v2.Some concrete examples of values may be useful: Really we are manipulating weighted relations, not FSTs.We may combine FSTs, or determinize or minimize them, with any variant of the semiringweighted algorithms.17 As long as the resulting FST computes the right weighted relation, the arrangement of its states, arcs, and labels is unimportant.The same semiring may be used to compute gradients.We would like to find fθ(xi, yi) and its gradient with respect to θ, where fθ is real-valued but need not be probabilistic.Whatever procedures are used to evaluate fθ(xi, yi) exactly or approximately—for example, FST operations to compile fθ followed by minimization of (c x xi) o fθ o (yi x c)—can simply be applied over the expectation semiring, replacing each weight p by (p, Vp) and replacing the usual arithmetic operations with ⊕, ⊗, etc.18 (2)–(4) preserve the gradient ((2) is the derivative product rule), so this computation yields (fθ(xi, yi), Vfθ(xi, yi)).Now for some important remarks on efficiency: • Computing ti is an instance of the well-known algebraic path problem (Lehmann, 1977; Tar an, 1981a).Let Ti = xiofoyi.Then ti is the total semiring weight w0n of paths in Ti from initial state 0 to final state n (assumed WLOG to be unique and unweighted).It is wasteful to compute ti as suggested earlier, by minimizing (cxxi)of o(yixE), since then the real work is done by an c-closure step (Mohri, 2002) that implements the all-pairs version of algebraic path, whereas all we need is the single-source version.If n and m are the number of states and edges,19 then both problems are O(n3) in the worst case, but the single-source version can be solved in essentially O(m) time for acyclic graphs and other reducible flow graphs (Tar an, 1981b).For a general graph Ti, Tar an (1981b) shows how to partition into “hard” subgraphs that localize the cyclicity or irreducibility, then run the O(n3) algorithm on each subgraph (thereby reducing n to as little as 1), and recombine the results.The overhead of partitioning and recombining is essentially only O(m).• For speeding up the O(n3) problem on subgraphs, one can use an approximate relaxation technique (Mohri, 2002).Efficient hardware implementation is also possible via chip-level parallelism (Rote, 1985).• In many cases of interest, Ti is an acyclic graph.20 Then Tar an’s method computes w0j for each j in topologically sorted order, thereby finding ti in a linear number of ⊕ and ⊗ operations.For HMMs (footnote 11), Ti is the familiar trellis, and we would like this computation of ti to reduce to the forwardbackward algorithm (Baum, 1972).But notice that it has no backward pass.In place of pushing cumulative probabilities backward to the arcs, it pushes cumulative arcs (more generally, values in V ) forward to the probabilities.This is slower because our ⊕ and ⊗ are vector operations, and the vectors rapidly lose sparsity as they are added together.We therefore reintroduce a backward pass that lets us avoid ⊕ and ⊗ when computing ti (so they are needed only to construct Ti).This speedup also works for cyclic graphs and for any V .Write wjk as (pjk, vjk), and let w1jk = (p1jk, v1 jk) denote the weight of the edge from j to k.19 Then it can be shown that w0n = (p0n, Ej,k p0jv1jkpkn).The forward and backward probabilities, p0j and pkn, can be computed using single-source algebraic path for the simpler semiring (R, +, x, ∗)—or equivalently, by solving a sparse linear system of equations over R, a much-studied problem at O(n) space, O(nm) time, and faster approximations (Greenbaum, 1997).Here, the forward and backward probabilities can be computed in time only O(m + n log n) (Fredman and Tar an, 1987). k-best variants are also possible.We have exhibited a training algorithm for parameterized finite-state machines.Some specific consequences that we believe to be novel are (1) an EM algorithm for FSTs with cycles and epsilons; (2) training algorithms for HMMs and weighted contextual edit distance that work on incomplete data; (3) endto-end training of noisy channel cascades, so that it is not necessary to have separate training data for each machine in the cascade (cf.Knight and Graehl, 20If xi and yi are acyclic (e.g., fully observed strings), and f (or rather its FST) has no a : a cycles, then composition will “unroll” f into an acyclic machine.If only xi is acyclic, then the composition is still acyclic if domain(f) has no a cycles.1998), although such data could also be used; (4) training of branching noisy channels (footnote 7); (5) discriminative training with incomplete data; (6) training of conditional MEMMs (McCallum et al., 2000) and conditional random fields (Lafferty et al., 2001) on unbounded sequences.We are particularly interested in the potential for quickly building statistical models that incorporate linguistic and engineering insights.Many models of interest can be constructed in our paradigm, without having to write new code.Bringing diverse models into the same declarative framework also allows one to apply new optimization methods, objective functions, and finite-state algorithms to all of them.To avoid local maxima, one might try deterministic annealing (Rao and Rose, 2001), or randomized methods, or place a prior on θ.Another extension is to adjust the machine topology, say by model merging (Stolcke and Omohundro, 1994).Such techniques build on our parameter estimation method.The key algorithmic ideas of this paper extend from forward-backward-style to inside-outside-style methods.For example, it should be possible to do end-to-end training of a weighted relation defined by an interestingly parameterized synchronous CFG composed with tree transducers and then FSTs.
GATE: A Framework And Graphical Development Environment For Robust NLP Tools And Applicationspeg,. pyscropyrssexa pa...Inn/bee(Esrey:,we6onee nocnemere)seressartba 43csrassartsmeuroto-ssoprbonourseLproronparserrna flaw) sror spe6yer — ,r1 Transducerloacleo a erkirlE.FOL.*I PD.021 fP21111“enwialitneI r Figure 2: Unicode text in Gate2 witnessed on the software scene with the emergence of Unicode as a universal standard for representing textual data.GATE supports multilingual data processing using Unicode as its default text encoding.It also provides a means of entering text in various languages, using virtual keyboards where the language is not supported by the underlying operating platform.(Note that although Java represents characters as Unicode, it doesn't support input in many of the languages covered by Unicode.)Currently 28 languages are supported, and more are planned for future releases.Because GATE is an open architecture, new virtual keyboards can be defined by the users and added to the system as needed.For displaying the text, GATE relies on the rendering facilities offered by the Java implementation for the platform it runs on.Figure 2 gives an example of text in various languages displayed by GATE.The ability to handle Unicode data, along with the separation between data and implementation, allows LE systems based on GATE to be ported to new languages with no additional overhead apart from the development of the resources needed for the specific language.These facilities have been developed as part of the EMILLE project (McEnery et al., 2000), which focuses on the construction a 63 million word electronic corpus of South Asian languages.3 Applications One of GATE's strengths is that it is flexible and robust enough to enable the development of a wide range of applications within its framework.In this section, we describe briefly some of the NLP applications we have developed using the GATE architecture.3.1 MUSE The MUSE system (Maynard et al., 2001) is a multi-purpose Named Entity recognition system which is capable of processing texts from widely different domains and genres, thereby aiming to reduce the need for costly and time-consuming adaptation of existing resources to new applications and domains.The system aims to identify the parameters relevant to the creation of a name recognition system across different types of variability such as changes in domain, genre and media.For example, less formal texts may not follow standard capitalisation, punctuation and spelling formats, which can be a problem for many generic NE systems.Current evaluations with this system average around 93% precision and 95% recall across a variety of text types.3.2 ACE The MUSE system has also been adapted to take part in the current ACE (Automatic Content Extraction) program run by NIST.This requires systems to perform recognition and tracking tasks of named, nominal and pronominal entities and their mentions across three types of clean news text (newswire, broadcast news and newspaper) and two types of degraded news text (OCR output and ASR output).3.3 MUMIS The MUMIS (MUltiMedia Indexing and Searching environment) system uses Information Extraction components developed within GATE to produce formal annotations about essential events in football video programme material.This IE system comprises versions of the tokenisation, sentence detection, POS-tagging, and semantic tagging modules developed as part of GATE's standard resources, but also includes morphological analysis, full syntactic parsing and discourse interpretation modules, thereby enabling the production of annotations over text encoding structural, lexical, syntactic and semantic information.The semantic tagging module currently achieves around 91% precision and 76% recall, a significant improvement on a baseline named entity recognition system evaluated against it.4 Processing Resources Provided with GATE is a set of reusable processing resources for common NLP tasks.(None of them are definitive, and the user can replace and/or extend them as necessary.)These are packaged together to form ANNIE, A Nearly- New IE system, but can also be used individually or coupled together with new modules in order to create new applications.For example, many other NLP tasks might require a sentence splitter and POS tagger, but would not necessarily require resources more specific to IE tasks such as a named entity transducer.The system is in use for a variety of IE and other tasks, sometimes in combination with other sets of application-specific modules.ANNIE consists of the following main processing resources: tokeniser, sentence splitter, POS tagger, gazetteer, finite state transducer (based on GATE's built-in regular expressions over annotations language (Cunningham et al., 2002)), orthomatcher and coreference resolver.The resources communicate via GATE's annotation API, which is a directed graph of arcs bearing arbitrary feature/value data, and nodes rooting this data into document content (in this case text). text into simple tokens, such as numbers, punctuation, symbols, and words of different types (e.g. with an initial capital, all upper case, etc.).The aim is to limit the work of the tokeniser to maximise efficiency, and enable greater flexibility by placing the burden of analysis on the grammars.This means that the tokeniser does not need to be modified for different applications or text types. splitter a cascade of finitestate transducers which segments the text into sentences.This module is required for the tagger.Both the splitter and tagger are domainand application-independent. a modified version of the Brill tagger, which produces a part-of-speech tag as an annotation on each word or symbol.Neither the splitter nor the tagger are a mandatory part of the NE system, but the annotations they produce can be used by the grammar (described below), in order to increase its power and coverage. of lists such as cities, organisations, days of the week, etc.It not only consists of entities, but also of names of useful as typical company designators (e.g.'Ltd:), titles, etc.The gazetteer lists are compiled into finite state machines, which can match text tokens. tagger of handcrafted rules written in the JAPE (Java Annotations Pattern Engine) language (Cunningham et al., 2002), which describe patterns to match and annotations to be created as a result.JAPE is a version of CPSL (Common Pattern Specification Language) (Appelt, 1996), which provides finite state transduction over annotations based on regular expressions.A JAPE grammar consists of a set of phases, each of which consists of a set of pattern/action rules, and which run sequentially.Patterns can be specified by describing a specific text string, or annotations previously created by modules such as the tokeniser, gazetteer, or document format analysis.Rule prioritisation (if activated) prevents multiple assignment of annotations to the same text string. another optional module for the IE system.Its primary objective is to perform co-reference, or entity tracking, by recognising relations between entities.It also has a secondary role in improving named entity recognition by assigning annotations to previously unclassified names, based on relations with existing entities. identity relations between entities in the text.For more details see (Dimitrov, 2002).4.1 Implementation The implementation of the processing resources is centred on robustness, usability and the clear distinction between declarative data representations and finite state algorithms The behaviour of all the processors is completely controlled by external resources such as grammars or rule sets, which makes them easily modifiable by users who do not need to be familiar with programming languages.The fact that all processing resources use finite-state transducer technology makes them quite performant in terms of execution times.Our initial experiments show that the full named entity recognition system is capable of processing around 2.5KB/s on a PITT 450 with 256 MB RAM (independently of the size of the input file; the processing requirement is linear in relation to the text size).Scalability was tested by running the ANNIE modules over a randomly chosen part of the British National Corpus (10% of all documents), which contained documents of up to 17MB in size.5 Language Resource Creation Since many NLP algorithms require annotated corpora for training, GATE's development environment provides easy-to-use and extendable facilities for text annotation.In order to test their usability in practice, we used these facilities to build corpora of named entity annotated texts for the MUSE, ACE, and MUMIS applications.The annotation can be done manually by the user or semi-automatically by running some processing resources over the corpus and then correcting/adding new annotations manually.Depending on the information that needs to be annotated, some ANNIE modules can be used or adapted to bootstrap the corpus annotation task.For example, users from the humanities created a gazetteer list with 18th century place names in London, which when supplied to the ANNIE gazetteer, allows the automatic annotation of place information in a large collection of 18th century court reports from the Old Bailey in London.Since manual annotation is a difficult and error-prone task, GATE tries to make it simple to use and yet keep it flexible.To add a new annotation, one selects the text with the mouse (e.g., &quot;Mr. Clever&quot;) and then clicks on the desired annotation type (e.g., Person), which is shown in the list of types on the right-handside of the document viewer (see Figure 1).If however the desired annotation type does not already appear there or the user wants to associate more detailed information with the annotation (not just its type), then an annotation editing dialogue can be used.6 Evaluation A vital part of any language engineering application is the evaluation of its performance, and a development environment for this purpose would not be complete without some mechanisms for its measurement in a large number of test cases.GATE contains two such mechanisms: an evaluation tool (AnnotationDiff) which enables automated performance measurement and visualisation of the results, and a benchmarking tool, which enables the tracking of a system's progress and regression testing.6.1 The AnnotationDiff Tool Gate's AnnotationDiff tool enables two sets of annotations on a document to be compared, in order to either compare a system-annotated text with a reference (hand-annotated) text, or to compare the output of two different versions of the system (or two different systems).For each annotation type, figures are generated for precision, recall, F-measure and false positives.The AnnotationDiff viewer displays the two sets of annotations, marked with different colours (similar to 'visual diff' implementations such as in the MKS Toolkit or TkDiff).Annotations in the key set have two possible colours depending on their state: white for annotations which have a compatible (or partially compatible) annotation in the response set, and orange for annotations which are missing in the response set.Annotations in the response set have three possible colours: green if they are compatible with the key annotation, blue if they Figure 3: Fragment of results from benchmark tool are partially compatible, and red if they are spurious.In the viewer, two annotations will be positioned on the same row if they are co-extensive, and on different rows if not.6.2 Benchmarking tool GATE's benchmarking tool differs from the AnnotationDiff in that it enables evaluation to be carried out over a whole corpus rather than a single document.It also enables tracking of the system's performance over time.The tool requires a clean version of a corpus (with no annotations) and an annotated reference corpus.First of all, the tool is run in generation mode to produce a set of texts annotated by the system.These texts are stored for future use.The tool can then be run in three ways: 1.Comparing the annotated set with the reference set; 2.Comparing the annotated set with the set produced by a more recent version of the system resources (the latest set); 3.Comparing the latest set with the reference set.In each case, performance statistics will be provided for each text in the set, and overall statistics for the entire set, in comparison with the reference set.In case 2, information is also provided about whether the figures have increased or decreased in comparison with the annotated set.The annotated set can be updated at any time by rerunning the tool in generation mode with the latest version of the system resources.Furthermore, the system can be run in verbose mode, where for each figure below a certain threshold (set by the user), the non-coextensive annotations (and their corresponding text) will be displayed.The output of the tool is written to an HTML file in tabular form, as shown in Figure 3.Current evaluations for the MUSE NE system are producing average figures of 90-95% Precision and Recall on a selection of different text types (spoken transcriptions, emails etc.).The default ANNIE system produces figures of between 80-90% Precision and Recall on news texts.This figure is lower than for the MUSE system, because the resources have not been tuned to a specific text type or application, but are intended to be adapted as necessary.Work on resolution of anaphora is currently averaging 63% Precision and 45% Recall, although this work is still very much in progress, and we expect these figures to improve in the near future.7 Related Work GATE draws from a large pool of previous work on infrastructures, architectures and development environments for representing and processing language resources, corpora, and annotations.Due to space limitations here we will discuss only a small subset.For a detailed review and its use for deriving the desiderata for this architecture see (Cunningham, 2000).Work on standard ways to deal with XML data is relevant here, such as the LT XML work at Edinburgh (Thompson and McKelvie, 1997), as is work on managing collections of documents and their formats, e.g.(Brugman et al., 1998; Grishman, 1997; Zajac, 1998).We have also drawn from work on representing information about text and speech, e.g.(Brugman et al., 1998; Mikheev and Finch, 1997; Zajac, 1998; Young et al., 1999), as well as annotation standards, such as the ATLAS project (an architecture for linguistic annotation) at LDC (Bird et kirlactunerkterripNerl ABC19980430.1830.0858.sgm Annotation tope., GPE RecallIncreaseon Atonaltmarked iron 06371426571426571la10 type Organization 1.0 increaseon hurnan-markedto 1 0 09444444444444444 Mug 07, limEll.ncreaseon tom0345to 07, 14153ING ANNOTATIONSIt To automatetetteABC Ir.NNOTATKMISinTo embroil,bath PARTIALLYCORRECT Pl4071&quot;ATIC*15nhe automateIDA, PratotationType Annotation type Person Precision increase on human-marked from 08947368421052632 lc 09444444444444444 03444444444444444 al., 2000).Our approach is also related to work on user interfaces to architectural facilities such as development environments, e.g.(Brugman et al., 1998) and to work on comparing different versions of information, e.g.(Sparck-Jones and Galliers, 1996; Paggio, 1998).This work is particularly novel in that it addresses the complete range of issues in NLP application development in a flexible and extensible way, rather than focusing just on some particular aspect of the development process.In addition, it promotes robustness, re-usability, and scalability as important principles that help with the construction of practical NLP systems.8 Conclusions In this paper we have described an infrastructure for language engineering software which aims to assist the develeopment of robust tools and resources for NLP.One future direction is the integration of processing resources which learn in the background while the user is annotating corpora in GATE's visual environment.Currently, statistical models can be integrated but need to be trained separately.We are also extending the system to handle language generation modules, in order to enable the construction of applications which require language production in addition to analysis, e.g. intelligent report generation from IE data.ferent components, and ensures that the component interactions satisfy the system requirements.As a framework, it provides a reusable design for an LE software system and a set of prefabricated software building blocks that language engineers can use, extend and customise for their specific needs.As a development environment, it helps its users to minimise the time they spend building new LE systems or modifying existing ones, by aiding overall development and providing a debugging mechanism for new modules.Because GATE has a componentbased model, this allows for easy coupling and decoupling of the processors, thereby facilitating comparison of alternative configurations of the system or different implementations of the same module (e.g., different parsers).The availability of tools for easy visualisation of data at each point during the development process aids immediate interpretation of the results.The GATE framework comprises a core library (analogous to a bus backplane) and a set of reusable LE modules.The framework implements the architecture and provides (amongst other things) facilities for processing and visualising resources, including representation, import and export of data.The reusable modules provided with the backplane are able to perform basic language processing tasks such as POS tagging and semantic tagging.This eliminates the need for users to keep recreating the same kind of resources, and provides a good starting point for new applications.The modules are described in more detail in Section 4.Applications developed within GATE can be deployed outside its Graphical User Interface (GUI), using programmatic access via the GATE API (see http: //gat e . ac .uk).In addition, the reusable modules, the document and annotation model, and the visualisation components can all be used independently of the development environment.GATE components may be implemented by a variety of programming languages and databases, but in each case they are represented to the system as a Java class.This class may simply call the underlying program or provide an access layer to a database; alternatively it may implement the whole component.In the rest of this section, we show how the GATE infrastructure takes care of the resource discovery, loading, and execution, and briefly discuss data storage and visualisation.The title expresses succinctly the distinction made in GATE between data, algorithms, and ways of visualising them.In other words, GATE components are one of three types: These resources can be local to the user's machine or remote (available via HTTP), and all can be extended by users without modification to GATE itself.One of the main advantages of separating the algorithms from the data they require is that the two can be developed independently by language engineers with different types of expertise, e.g. programmers and linguists.Similarly, separating data from its visualisation allows users to develop alternative visual resources, while still using a language resource provided by GATE.Collectively, all resources are known as CREOLE (a Collection of REusable Objects for Language Engineering), and are declared in a repository XML file, which describes their name, implementing class, parameters, icons, etc.This repository is used by the framework to discover and load available resources.A parameters tag describes the parameters which each resource needs when created or executed.Parameters can be optional, e.g. if a document list is provided when the corpus is constructed, it will be populated automatically with these documents.When an application is developed within GATE's graphical environment, the user chooses which processing resources go into it (e.g. tokeniser, POS tagger), in what order they will be executed, and on which data (e.g. document or corpus).The execution parameters of each resource are also set there, e.g. a loaded document is given as a parameter to each PR.When the application is run, the modules will be executed in the specified order on the given document.The results can be viewed in the document viewer/editor (see Figure 1).GATE supports a variety of formats including XML, RTF, HTML, SGML, email and plain text.In all cases, when a document is created/opened in GATE, the format is analysed and converted into a single unified model of annotation.The annotation format is a modified form of the TIPSTER format (Grishman, 1997) which has been made largely compatible with the Atlas format (Bird et al., 2000), and uses the now standard mechanism of 'stand-off markup' (Thompson and McKelvie, 1997).The annotations associated with each document are a structure central to GATE, because they encode the language data read and produced by each processing module.The GATE framework also provides persistent storage of language resources.It currently offers three storage mechanisms: one uses relational databases (e.g.Oracle) and the other two are file- based, using Java serialisation or an XML-based internal format.GATE documents can also be exported back to their original format (e.g.SGML/XML for the British National Corpus (BNC)) and the user can choose whether some additional annotations (e.g. named entity information) are added to it or not.To summarise, the existence of a unified data structure ensures a smooth communication between components, while the provision of import and export capabilities makes communication with the outside world simple.In recent years, the emphasis on multilinguality has grown, and important advances have been witnessed on the software scene with the emergence of Unicode as a universal standard for representing textual data.GATE supports multilingual data processing using Unicode as its default text encoding.It also provides a means of entering text in various languages, using virtual keyboards where the language is not supported by the underlying operating platform.(Note that although Java represents characters as Unicode, it doesn't support input in many of the languages covered by Unicode.)Currently 28 languages are supported, and more are planned for future releases.Because GATE is an open architecture, new virtual keyboards can be defined by the users and added to the system as needed.For displaying the text, GATE relies on the rendering facilities offered by the Java implementation for the platform it runs on.Figure 2 gives an example of text in various languages displayed by GATE.The ability to handle Unicode data, along with the separation between data and implementation, allows LE systems based on GATE to be ported to new languages with no additional overhead apart from the development of the resources needed for the specific language.These facilities have been developed as part of the EMILLE project (McEnery et al., 2000), which focuses on the construction a 63 million word electronic corpus of South Asian languages.One of GATE's strengths is that it is flexible and robust enough to enable the development of a wide range of applications within its framework.In this section, we describe briefly some of the NLP applications we have developed using the GATE architecture.The MUSE system (Maynard et al., 2001) is a multi-purpose Named Entity recognition system which is capable of processing texts from widely different domains and genres, thereby aiming to reduce the need for costly and time-consuming adaptation of existing resources to new applications and domains.The system aims to identify the parameters relevant to the creation of a name recognition system across different types of variability such as changes in domain, genre and media.For example, less formal texts may not follow standard capitalisation, punctuation and spelling formats, which can be a problem for many generic NE systems.Current evaluations with this system average around 93% precision and 95% recall across a variety of text types.The MUSE system has also been adapted to take part in the current ACE (Automatic Content Extraction) program run by NIST.This requires systems to perform recognition and tracking tasks of named, nominal and pronominal entities and their mentions across three types of clean news text (newswire, broadcast news and newspaper) and two types of degraded news text (OCR output and ASR output).The MUMIS (MUltiMedia Indexing and Searching environment) system uses Information Extraction components developed within GATE to produce formal annotations about essential events in football video programme material.This IE system comprises versions of the tokenisation, sentence detection, POS-tagging, and semantic tagging modules developed as part of GATE's standard resources, but also includes morphological analysis, full syntactic parsing and discourse interpretation modules, thereby enabling the production of annotations over text encoding structural, lexical, syntactic and semantic information.The semantic tagging module currently achieves around 91% precision and 76% recall, a significant improvement on a baseline named entity recognition system evaluated against it.Provided with GATE is a set of reusable processing resources for common NLP tasks.(None of them are definitive, and the user can replace and/or extend them as necessary.)These are packaged together to form ANNIE, A NearlyNew IE system, but can also be used individually or coupled together with new modules in order to create new applications.For example, many other NLP tasks might require a sentence splitter and POS tagger, but would not necessarily require resources more specific to IE tasks such as a named entity transducer.The system is in use for a variety of IE and other tasks, sometimes in combination with other sets of application-specific modules.ANNIE consists of the following main processing resources: tokeniser, sentence splitter, POS tagger, gazetteer, finite state transducer (based on GATE's built-in regular expressions over annotations language (Cunningham et al., 2002)), orthomatcher and coreference resolver.The resources communicate via GATE's annotation API, which is a directed graph of arcs bearing arbitrary feature/value data, and nodes rooting this data into document content (in this case text).The tokeniser splits text into simple tokens, such as numbers, punctuation, symbols, and words of different types (e.g. with an initial capital, all upper case, etc.).The aim is to limit the work of the tokeniser to maximise efficiency, and enable greater flexibility by placing the burden of analysis on the grammars.This means that the tokeniser does not need to be modified for different applications or text types.The sentence splitter is a cascade of finitestate transducers which segments the text into sentences.This module is required for the tagger.Both the splitter and tagger are domainand application-independent.The tagger is a modified version of the Brill tagger, which produces a part-of-speech tag as an annotation on each word or symbol.Neither the splitter nor the tagger are a mandatory part of the NE system, but the annotations they produce can be used by the grammar (described below), in order to increase its power and coverage.The gazetteer consists of lists such as cities, organisations, days of the week, etc.It not only consists of entities, but also of names of useful indicators, such as typical company designators (e.g.'Ltd:), titles, etc.The gazetteer lists are compiled into finite state machines, which can match text tokens.The semantic tagger consists of handcrafted rules written in the JAPE (Java Annotations Pattern Engine) language (Cunningham et al., 2002), which describe patterns to match and annotations to be created as a result.JAPE is a version of CPSL (Common Pattern Specification Language) (Appelt, 1996), which provides finite state transduction over annotations based on regular expressions.A JAPE grammar consists of a set of phases, each of which consists of a set of pattern/action rules, and which run sequentially.Patterns can be specified by describing a specific text string, or annotations previously created by modules such as the tokeniser, gazetteer, or document format analysis.Rule prioritisation (if activated) prevents multiple assignment of annotations to the same text string.The orthomatcher is another optional module for the IE system.Its primary objective is to perform co-reference, or entity tracking, by recognising relations between entities.It also has a secondary role in improving named entity recognition by assigning annotations to previously unclassified names, based on relations with existing entities.The coreferencer finds identity relations between entities in the text.For more details see (Dimitrov, 2002).The implementation of the processing resources is centred on robustness, usability and the clear distinction between declarative data representations and finite state algorithms The behaviour of all the processors is completely controlled by external resources such as grammars or rule sets, which makes them easily modifiable by users who do not need to be familiar with programming languages.The fact that all processing resources use finite-state transducer technology makes them quite performant in terms of execution times.Our initial experiments show that the full named entity recognition system is capable of processing around 2.5KB/s on a PITT 450 with 256 MB RAM (independently of the size of the input file; the processing requirement is linear in relation to the text size).Scalability was tested by running the ANNIE modules over a randomly chosen part of the British National Corpus (10% of all documents), which contained documents of up to 17MB in size.Since many NLP algorithms require annotated corpora for training, GATE's development environment provides easy-to-use and extendable facilities for text annotation.In order to test their usability in practice, we used these facilities to build corpora of named entity annotated texts for the MUSE, ACE, and MUMIS applications.The annotation can be done manually by the user or semi-automatically by running some processing resources over the corpus and then correcting/adding new annotations manually.Depending on the information that needs to be annotated, some ANNIE modules can be used or adapted to bootstrap the corpus annotation task.For example, users from the humanities created a gazetteer list with 18th century place names in London, which when supplied to the ANNIE gazetteer, allows the automatic annotation of place information in a large collection of 18th century court reports from the Old Bailey in London.Since manual annotation is a difficult and error-prone task, GATE tries to make it simple to use and yet keep it flexible.To add a new annotation, one selects the text with the mouse (e.g., &quot;Mr. Clever&quot;) and then clicks on the desired annotation type (e.g., Person), which is shown in the list of types on the right-handside of the document viewer (see Figure 1).If however the desired annotation type does not already appear there or the user wants to associate more detailed information with the annotation (not just its type), then an annotation editing dialogue can be used.A vital part of any language engineering application is the evaluation of its performance, and a development environment for this purpose would not be complete without some mechanisms for its measurement in a large number of test cases.GATE contains two such mechanisms: an evaluation tool (AnnotationDiff) which enables automated performance measurement and visualisation of the results, and a benchmarking tool, which enables the tracking of a system's progress and regression testing.Gate's AnnotationDiff tool enables two sets of annotations on a document to be compared, in order to either compare a system-annotated text with a reference (hand-annotated) text, or to compare the output of two different versions of the system (or two different systems).For each annotation type, figures are generated for precision, recall, F-measure and false positives.The AnnotationDiff viewer displays the two sets of annotations, marked with different colours (similar to 'visual diff' implementations such as in the MKS Toolkit or TkDiff).Annotations in the key set have two possible colours depending on their state: white for annotations which have a compatible (or partially compatible) annotation in the response set, and orange for annotations which are missing in the response set.Annotations in the response set have three possible colours: green if they are compatible with the key annotation, blue if they are partially compatible, and red if they are spurious.In the viewer, two annotations will be positioned on the same row if they are co-extensive, and on different rows if not.GATE's benchmarking tool differs from the AnnotationDiff in that it enables evaluation to be carried out over a whole corpus rather than a single document.It also enables tracking of the system's performance over time.The tool requires a clean version of a corpus (with no annotations) and an annotated reference corpus.First of all, the tool is run in generation mode to produce a set of texts annotated by the system.These texts are stored for future use.The tool can then be run in three ways: In each case, performance statistics will be provided for each text in the set, and overall statistics for the entire set, in comparison with the reference set.In case 2, information is also provided about whether the figures have increased or decreased in comparison with the annotated set.The annotated set can be updated at any time by rerunning the tool in generation mode with the latest version of the system resources.Furthermore, the system can be run in verbose mode, where for each figure below a certain threshold (set by the user), the non-coextensive annotations (and their corresponding text) will be displayed.The output of the tool is written to an HTML file in tabular form, as shown in Figure 3.Current evaluations for the MUSE NE system are producing average figures of 90-95% Precision and Recall on a selection of different text types (spoken transcriptions, emails etc.).The default ANNIE system produces figures of between 80-90% Precision and Recall on news texts.This figure is lower than for the MUSE system, because the resources have not been tuned to a specific text type or application, but are intended to be adapted as necessary.Work on resolution of anaphora is currently averaging 63% Precision and 45% Recall, although this work is still very much in progress, and we expect these figures to improve in the near future.GATE draws from a large pool of previous work on infrastructures, architectures and development environments for representing and processing language resources, corpora, and annotations.Due to space limitations here we will discuss only a small subset.For a detailed review and its use for deriving the desiderata for this architecture see (Cunningham, 2000).Work on standard ways to deal with XML data is relevant here, such as the LT XML work at Edinburgh (Thompson and McKelvie, 1997), as is work on managing collections of documents and their formats, e.g.(Brugman et al., 1998; Grishman, 1997; Zajac, 1998).We have also drawn from work on representing information about text and speech, e.g.(Brugman et al., 1998; Mikheev and Finch, 1997; Zajac, 1998; Young et al., 1999), as well as annotation standards, such as the ATLAS project (an architecture for linguistic annotation) at LDC (Bird et al., 2000).Our approach is also related to work on user interfaces to architectural facilities such as development environments, e.g.(Brugman et al., 1998) and to work on comparing different versions of information, e.g.(Sparck-Jones and Galliers, 1996; Paggio, 1998).This work is particularly novel in that it addresses the complete range of issues in NLP application development in a flexible and extensible way, rather than focusing just on some particular aspect of the development process.In addition, it promotes robustness, re-usability, and scalability as important principles that help with the construction of practical NLP systems.In this paper we have described an infrastructure for language engineering software which aims to assist the develeopment of robust tools and resources for NLP.One future direction is the integration of processing resources which learn in the background while the user is annotating corpora in GATE's visual environment.Currently, statistical models can be integrated but need to be trained separately.We are also extending the system to handle language generation modules, in order to enable the construction of applications which require language production in addition to analysis, e.g. intelligent report generation from IE data.
Minimum Cut Model For Spoken Lecture SegmentationWe consider the task of unsupervised lecture segmentation.We formalize segmentation as a graph-partitioning task that optimizes the normalized cut criterion.Our approach moves beyond localized comparisons and takes into account longrange cohesion dependencies.Our results demonstrate that global analysis improves the segmentation accuracy and is robust in the presence of speech recognition errors.The development of computational models of text structure is a central concern in natural language processing.Text segmentation is an important instance of such work.The task is to partition a text into a linear sequence of topically coherent segments and thereby induce a content structure of the text.The applications of the derived representation are broad, encompassing information retrieval, question-answering and summarization.Not surprisingly, text segmentation has been extensively investigated over the last decade.Following the first unsupervised segmentation approach by Hearst (1994), most algorithms assume that variations in lexical distribution indicate topic changes.When documents exhibit sharp variations in lexical distribution, these algorithms are likely to detect segment boundaries accurately.For example, most algorithms achieve high performance on synthetic collections, generated by concatenation of random text blocks (Choi, 2000).The difficulty arises, however, when transitions between topics are smooth and distributional variations are subtle.This is evident in the performance of existing unsupervised algorithms on less structured datasets, such as spoken meeting transcripts (Galley et al., 2003).Therefore, a more refined analysis of lexical distribution is needed.Our work addresses this challenge by casting text segmentation in a graph-theoretic framework.We abstract a text into a weighted undirected graph, where the nodes of the graph correspond to sentences and edge weights represent the pairwise sentence similarity.In this framework, text segmentation corresponds to a graph partitioning that optimizes the normalized-cut criterion (Shi and Malik, 2000).This criterion measures both the similarity within each partition and the dissimilarity across different partitions.Thus, our approach moves beyond localized comparisons and takes into account long-range changes in lexical distribution.Our key hypothesis is that global analysis yields more accurate segmentation results than local models.We tested our algorithm on a corpus of spoken lectures.Segmentation in this domain is challenging in several respects.Being less structured than written text, lecture material exhibits digressions, disfluencies, and other artifacts of spontaneous communication.In addition, the output of speech recognizers is fraught with high word error rates due to specialized technical vocabulary and lack of in-domain spoken data for training.Finally, pedagogical considerations call for fluent transitions between different topics in a lecture, further complicating the segmentation task.Our experimental results confirm our hypothesis: considering long-distance lexical dependencies yields substantial gains in segmentation performance.Our graph-theoretic approach compares favorably to state-of-the-art segmentation algorithms and attains results close to the range of human agreement scores.Another attractive property of the algorithm is its robustness to noise: the accuracy of our algorithm does not deteriorate significantly when applied to speech recognition output.Most unsupervised algorithms assume that fragments of text with homogeneous lexical distribution correspond to topically coherent segments.Previous research has analyzed various facets of lexical distribution, including lexical weighting, similarity computation, and smoothing (Hearst, 1994; Utiyama and Isahara, 2001; Choi, 2000; Reynar, 1998; Kehagias et al., 2003; Ji and Zha, 2003).The focus of our work, however, is on an orthogonal yet fundamental aspect of this analysis — the impact of long-range cohesion dependencies on segmentation performance.In contrast to previous approaches, the homogeneity of a segment is determined not only by the similarity of its words, but also by their relation to words in other segments of the text.We show that optimizing our global objective enables us to detect subtle topical changes. mentation Our work is inspired by minimum-cutbased segmentation algorithms developed for image analysis.Shi and Malik (2000) introduced the normalized-cut criterion and demonstrated its practical benefits for segmenting static images.Our method, however, is not a simple application of the existing approach to a new task.First, in order to make it work in the new linguistic framework, we had to redefine the underlying representation and introduce a variety of smoothing and lexical weighting techniques.Second, the computational techniques for finding the optimal partitioning are also quite different.Since the minimization of the normalized cut is NP-complete in the general case, researchers in vision have to approximate this computation.Fortunately, we can find an exact solution due to the linearity constraint on text segmentation.Linguistic research has shown that word repetition in a particular section of a text is a device for creating thematic cohesion (Halliday and Hasan, 1976), and that changes in the lexical distributions usually signal topic transitions.Figure 1 illustrates these properties in a lecture transcript from an undergraduate Physics class.We use the text Dotplotting representation by (Church, 1993) and plot the cosine similarity scores between every pair of sentences in the text.The intensity of a point (i, j) on the plot indicates the degree to which the i-th sentence in the text is similar to the j-th sentence.The true segment boundaries are denoted by vertical lines.This similarity plot reveals a block structure where true boundaries delimit blocks of text with high inter-sentential similarity.Sentences found in different blocks, on the other hand, tend to exhibit low similarity.Formalizing the Objective Whereas previous unsupervised approaches to segmentation rested on intuitive notions of similarity density, we formalize the objective of text segmentation through cuts on graphs.We aim to jointly maximize the intra-segmental similarity and minimize the similarity between different segments.In other words, we want to find the segmentation with a maximally homogeneous set of segments that are also maximally different from each other.Let G = {V, E} be an undirected, weighted graph, where V is the set of nodes corresponding to sentences in the text and E is the set of weighted edges (See Figure 2).The edge weights, w(u, v), define a measure of similarity between pairs of nodes u and v, where higher scores indicate higher similarity.Section 4 provides more details on graph construction.We consider the problem of partitioning the graph into two disjoint sets of nodes A and B.We aim to minimize the cut, which is defined to be the sum of the crossing edges between the two sets of nodes.In other words, we want to split the sentences into two maximally dissimilar classes by choosing A and B to minimize: Decoding Papadimitriou proved that the problem of minimizing normalized cuts on graphs is NP-complete (Shi and Malik, 2000).However, in our case, the multi-way cut is constrained to preserve the linearity of the segmentation.By segmentation linearity, we mean that all of the nodes between the leftmost and the rightmost nodes of a particular partition have to belong to that partition.With this constraint, we formulate a dynamic programming algorithm for exactly finding the minimum normalized multiway cut in polynomial time: However, we need to ensure that the two partitions are not only maximally different from each other, but also that they are themselves homogeneous by accounting for intra-partition node similarity.We formulate this requirement in the framework of normalized cuts (Shi and Malik, 2000), where the cut value is normalized by the volume of the corresponding partitions.The volume of the partition is the sum of its edges to the whole graph: The normalized cut criterion (Ncut) is then defined as follows: By minimizing this objective we simultaneously minimize the similarity across partitions and maximize the similarity within partitions.This formulation also allows us to decompose the objective into a sum of individual terms, and formulate a dynamic programming solution to the multiway cut problem.This criterion is naturally extended to a k-way normalized cut: where A1 ... Ak form a partition of the graph, and V −Ak is the set difference between the entire graph and partition k. C [i, k] is the normalized cut value of the optimal segmentation of the first k sentences into i segments.The i-th segment, Aj,k, begins at node uj and ends at node uk.B [i, k] is the back-pointer table from which we recover the optimal sequence of segment boundaries.Equations 3 and 4 capture respectively the condition that the normalized cut value of the trivial segmentation of an empty text into one segment is zero and the constraint that the first segment starts with the first node.The time complexity of the dynamic programming algorithm is O(KN2), where K is the number of partitions and N is the number of nodes in the graph or sentences in the transcript.Clearly, the performance of our model depends on the underlying representation, the definition of the pairwise similarity function, and various other model parameters.In this section we provide further details on the graph construction process.Preprocessing Before building the graph, we apply standard text preprocessing techniques to the text.We stem words with the Porter stemmer (Porter, 1980) to alleviate the sparsity of word counts through stem equivalence classes.We also remove words matching a prespecified list of stop words.Graph Topology As we noted in the previous section, the normalized cut criterion considers long-term similarity relationships between nodes.This effect is achieved by constructing a fullyconnected graph.However, considering all pairwise relations in a long text may be detrimental to segmentation accuracy.Therefore, we discard edges between sentences exceeding a certain threshold distance.This reduction in the graph size also provides us with computational savings.Similarity Computation In computing pairwise sentence similarities, sentences are represented as vectors of word counts.Cosine similarity is commonly used in text segmentation (Hearst, 1994).To avoid numerical precision issues when summing a series of very small scores, we compute exponentiated cosine similarity scores between pairs of sentence vectors: We further refine our analysis by smoothing the similarity metric.When comparing two sentences, we also take into account similarity between their immediate neighborhoods.The smoothing is achieved by adding counts of words that occur in adjoining sentences to the current sentence feature vector.These counts are weighted in accordance to their distance from the current sentence: e−α(j−i)sj, where si are vectors of word counts, and α is a parameter that controls the degree of smoothing.In the formulation above we use sentences as our nodes.However, we can also represent graph nodes with non-overlapping blocks of words of fixed length.This is desirable, since the lecture transcripts lack sentence boundary markers, and short utterances can skew the cosine similarity scores.The optimal length of the block is tuned on a heldout development set.Lexical Weighting Previous research has shown that weighting schemes play an important role in segmentation performance (Ji and Zha, 2003; Choi et al., 2001).Of particular concern are words that may not be common in general English discourse but that occur throughout the text for a particular lecture or subject.For example, in a lecture about support vector machines, the occurrence of the term “SVM” is not going to convey a lot of information about the distribution of sub-topics, even though it is a fairly rare term in general English and bears much semantic content.The same words can convey varying degrees of information across different lectures, and term weighting specific to individual lectures becomes important in the similarity computation.In order to address this issue, we introduce a variation on the tf-idf scoring scheme used in the information-retrieval literature (Salton and Buckley, 1988).A transcript is split uniformly into N chunks; each chunk serves as the equivalent of documents in the tf-idf computation.The weights are computed separately for each transcript, since topic and word distributions vary across lectures.In this section we present the different corpora used to evaluate our model and provide a brief overview of the evaluation metrics.Next, we describe our human segmentation study on the corpus of spoken lecture data.A heldout development set of three lectures isused for estimating the optimal word block length for representing nodes, the threshold distances for discarding node edges, the number of uniform chunks for estimating tf-idf lexical weights, the alpha parameter for smoothing, and the length of the smoothing window.We use a simple greedy search procedure for optimizing the parameters.We evaluate our segmentation algorithm on three sets of data.Two of the datasets we use are new segmentation collections that we have compiled for this study,1 and the remaining set includes a standard collection previously used for evaluation of segmentation algorithms.Various corpus statistics for the new datasets are presented in Table 1.Below we briefly describe each corpus.Physics Lectures Our first corpus consists of spoken lecture transcripts from an undergraduate Physics class.In contrast to other segmentation datasets, our corpus contains much longer texts.A typical lecture of 90 minutes has 500 to 700 sentences with 8500 words, which corresponds to about 15 pages of raw text.We have access both to manual transcriptions of these lectures and also output from an automatic speech recognition system.The word error rate for the latter is 19.4%,2 which is representative of state-of-the-art performance on lecture material (Leeuwis et al., 2003).The Physics lecture transcript segmentations were produced by the teaching staff of the introductory Physics course at the Massachusetts Institute of Technology.Their objective was to facilitate access to lecture recordings available on the class website.This segmentation conveys the high-level topical structure of the lectures.On average, a lecture was annotated with six segments, and a typical segment corresponds to two pages of a transcript.Artificial Intelligence Lectures Our second lecture corpus differs in subject matter, lecturing style, and segmentation granularity.The graduate Artificial Intelligence class has, on average, twelve segments per lecture, and a typical segment is about half of a page.One segment roughly corresponds to the content of a slide.This time the segmentation was obtained from the lecturer herself.The lecturer went through the transcripts of lecture recordings and segmented the lectures with the objective of making the segments correspond to presentation slides for the lectures.Due to the low recording quality, we were unable to obtain the ASR transcripts for this class.Therefore, we only use manual transcriptions of these lectures.Synthetic Corpus Also as part of our analysis, we used the synthetic corpus created by Choi (2000) which is commonly used in the evaluation of segmentation algorithms.This corpus consists of a set of concatenated segments randomly sampled from the Brown corpus.The length of the segments in this corpus ranges from three to eleven sentences.It is important to note that the lexical transitions in these concatenated texts are very sharp, since the segments come from texts written in widely varying language styles on completely different topics.We use the Pk and WindowDiff measures to evaluate our system (Beeferman et al., 1999; Pevzner and Hearst, 2002).The Pk measure estimates the probability that a randomly chosen pair of words within a window of length k words is inconsistently classified.The WindowDiff metric is a variant of the Pk measure, which penalizes false positives on an equal basis with near misses.Both of these metrics are defined with respect to the average segment length of texts and exhibit high variability on real data.We follow Choi (2000) and compute the mean segment length used in determining the parameter k on each reference text separately.We also plot the Receiver Operating Characteristic (ROC) curve to gauge performance at a finer level of discrimination (Swets, 1988).The ROC plot is the plot of the true positive rate against the false positive rate for various settings of a decision criterion.In our case, the true positive rate is the fraction of boundaries correctly classified, and the false positive rate is the fraction of non-boundary positions incorrectly classified as boundaries.In computing the true and false positive rates, we vary the threshold distance to the true boundary within which a hypothesized boundary is considered correct.Larger areas under the ROC curve of a classifier indicate better discriminative performance.Spoken lectures are very different in style from other corpora used in human segmentation studies (Hearst, 1994; Galley et al., 2003).We are interested in analyzing human performance on a corpus of lecture transcripts with much longer texts and a less clear-cut concept of a sub-topic.We define a segment to be a sub-topic that signals a prominent shift in subject matter.Disregarding this sub-topic change would impair the high-level understanding of the structure and the content of the lecture.As part of our human segmentation analysis, we asked three annotators to segment the Physics lecture corpus.These annotators had taken the class in the past and were familiar with the subject matter under consideration.We wrote a detailed instruction manual for the task, with annotation guidelines for the most part following the model used by Gruenstein et al. (2005).The annotators were instructed to segment at a level of granularity that would identify most of the prominent topical transitions necessary for a summary of the lecture.The annotators used the NOMOS annotation software toolkit, developed for meeting segmentation (Gruenstein et al., 2005).They were provided with recorded audio of the lectures and the corresponding text transcriptions.We intentionally did not provide the subjects with the target number of boundaries, since we wanted to see if the annotators would converge on a common segmentation granularity.Table 2 presents the annotator segmentation statistics.We see two classes of segmentation granularities.The original reference (O) and annotator A segmented at a coarse level with an average of 6.6 and 8.9 segments per lecture, respectively.Annotators B and C operated at much finer levels of discrimination with 18.4 and 13.8 segments per lecture on average.We conclude that multiple levels of granularity are acceptable in spoken lecture segmentation.This is expected given the length of the lectures and varying human judgments in selecting relevant topical content.Following previous studies, we quantify the level of annotator agreement with the Pk measure (Gruenstein et al., 2005).3 Table 3 shows the annotator agreement scores between different pairs of annotators.Pk measures ranged from 0.24 and 0.42.We observe greater consistency at similar levels of granularity, and less so across the two classes.Note that annotator A operated at a level of granularity consistent with the original reference segmentation.Hence, the 0.24 Pk measure score serves as the benchmark with which we can compare the results attained by segmentation algorithms on the Physics lecture data.As an additional point of reference we note that the uniform and random baseline segmentations attain 0.469 and 0.493 Pk measure, respectively, on the Physics lecture set.Figure 3: ROC plot for the Minimum Cut Segmenter on thirty Physics Lectures, with edge cutoffs set at five and hundred sentences.Benefits of global analysis We first determine the impact of long-range pairwise similarity dependencies on segmentation performance.Our rithms using the Pk and WindowDiff measures, with three lectures heldout for development. key hypothesis is that considering long-distance lexical relations contributes to the effectiveness of the algorithm.To test this hypothesis, we discard edges between nodes that are more than a certain number of sentences apart.We test the system on a range of data sets, including the Physics and AI lectures and the synthetic corpus created by Choi (2000).We also include segmentation results on Physics ASR transcripts.The results in Table 4 confirm our hypothesis — taking into account non-local lexical dependencies helps across different domains.On manually transcribed Physics lecture data, for example, the algorithm yields 0.394 Pk measure when taking into account edges separated by up to ten sentences.When dependencies up to a hundred sentences are considered, the algorithm yields a 25% reduction in Pk measure.Figure 3 shows the ROC plot for the segmentation of the Physics lecture data with different cutoff parameters, again demonstrating clear gains attained by employing longrange dependencies.As Table 4 shows, the improvement is consistent across all lecture datasets.We note, however, that after some point increasing the threshold degrades performance, because it introduces too many spurious dependencies (see the last column of Table 4).The speaker will occasionally return to a topic described at the beginning of the lecture, and this will bias the algorithm to put the segment boundary closer to the end of the lecture.Long-range dependencies do not improve the performance on the synthetic dataset.This is expected since the segments in the synthetic dataset are randomly selected from widely-varying documents in the Brown corpus, even spanning different genres of written language.So, effectively, there are no genuine long-range dependencies that can be exploited by the algorithm.Comparison with local dependency models We compare our system with the state-of-the-art similarity-based segmentation system developed by Choi (2000).We use the publicly available implementation of the system and optimize the system on a range of mask-sizes and different parameter settings described in (Choi, 2000) on a heldout development set of three lectures.To control for segmentation granularity, we specify the number of segments in the reference (“O”) segmentation for both our system and the baseline.Table 5 shows that the Minimum Cut algorithm consistently outperforms the similarity-based baseline on all the lecture datasets.We attribute this gain to the presence of more attenuated topic transitions in spoken language.Since spoken language is more spontaneous and less structured than written language, the speaker needs to keep the listener abreast of the changes in topic content by introducing subtle cues and references to prior topics in the course of topical transitions.Non-local dependencies help to elucidate shifts in focus, because the strength of a particular transition is measured with respect to other local and long-distance contextual discourse relationships.Our system does not outperform Choi’s algorithm on the synthetic data.This again can be attributed to the discrepancy in distributional properties of the synthetic corpus which lacks coherence in its thematic shifts and the lecture corpus of spontaneous speech with smooth distributional variations.We also note that we did not try to adjust our model to optimize its performance on the synthetic data.The smoothing method developed for lecture segmentation may not be appropriate for short segments ranging from three to eleven sentences that constitute the synthetic set.We also compared our method with another state-of-the-art algorithm which does not explicitly rely on pairwise similarity analysis.This algorithm (Utiyama and Isahara, 2001) (UI) computes the optimal segmentation by estimating changes in the language model predictions over different partitions.We used the publicly available implementation of the system that does not require parameter tuning on a heldout development set.Again, our method achieves favorable performance on a range of lecture data sets (See Table 5), and both algorithms attain results close to the range of human agreement scores.The attractive feature of our algorithm, however, is robustness to recognition errors — testing it on the ASR transcripts caused only 7.8% relative increase in Pk measure (from 0.298 to 0.322), compared to a 13.5% relative increase for the UI system.We attribute this feature to the fact that the model is less dependent on individual recognition errors, which have a detrimental effect on the local segment language modeling probability estimates for the UI system.The block-level similarity function is not as sensitive to individual word errors, because the partition volume normalization factor dampens their overall effect on the derived models.In this paper we studied the impact of long-range dependencies on the accuracy of text segmentation.We modeled text segmentation as a graphpartitioning task aiming to simultaneously optimize the total similarity within each segment and dissimilarity across various segments.We showed that global analysis of lexical distribution improves the segmentation accuracy and is robust in the presence of recognition errors.Combining global analysis with advanced methods for smoothing (Ji and Zha, 2003) and weighting could further boost the segmentation performance.Our current implementation does not automatically determine the granularity of a resulting segmentation.This issue has been explored in the past (Ji and Zha, 2003; Utiyama and Isahara, 2001), and we will explore the existing strategies in our framework.We believe that the algorithm has to produce segmentations for various levels of granularity, depending on the needs of the application that employs it.Our ultimate goal is to automatically generate tables of content for lectures.We plan to investigate strategies for generating titles that will succinctly describe the content of each segment.We will explore how the interaction between the generation and segmentation components can improve the performance of such a system as a whole.The authors acknowledge the support of the National Science Foundation (CAREER grant IIS-0448168, grant IIS0415865, and the NSF Graduate Fellowship).Any opinions, findings, conclusions or recommendations expressed in this publication are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.We would like to thank Masao Utiyama for providing us with an implementation of his segmentation system and Alex Gruenstein for assisting us with the NOMOS toolkit.We are grateful to David Karger for an illuminating discussion on the Minimum Cut algorithm.We also would like to acknowledge the MIT NLP and Speech Groups, the three annotators, and the three anonymous reviewers for valuable comments, suggestions, and help.
Learning to Extract Relations from the Web using Minimal SupervisionWe present a new approach to relation extraction that requires only a handful of training examples.Given a few pairs of named entities known to exhibit or not exhibit a particular relation, bags of sentences containing the pairs are extracted from the web.We extend an existing relation extraction method to handle this weaker form of supervision, and present experimental results demonstrating that our approach can reliably extract relations from web documents.A growing body of recent work in information extraction has addressed the problem of relation extraction (RE), identifying relationships between entities stated in text, such as LivesIn(Person, Location) or EmployedBy(Person, Company).Supervised learning has been shown to be effective for RE (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2006); however, annotating large corpora with examples of the relations to be extracted is expensive and tedious.In this paper, we introduce a supervised learning approach to RE that requires only a handful of training examples and uses the web as a corpus.Given a few pairs of well-known entities that clearly exhibit or do not exhibit a particular relation, such as CorpAcquired(Google, YouTube) and not(CorpAcquired(Yahoo, Microsoft)), a search engine is used to find sentences on the web that mention both of the entities in each of the pairs.Although not all of the sentences for positive pairs will state the desired relationship, many of them will.Presumably, none of the sentences for negative pairs state the targeted relation.Multiple instance learning (MIL) is a machine learning framework that exploits this sort of weak supervision, in which a positive bag is a set of instances which is guaranteed to contain at least one positive example, and a negative bag is a set of instances all of which are negative.MIL was originally introduced to solve a problem in biochemistry (Dietterich et al., 1997); however, it has since been applied to problems in other areas such as classifying image regions in computer vision (Zhang et al., 2002), and text categorization (Andrews et al., 2003; Ray and Craven, 2005).We have extended an existing approach to relation extraction using support vector machines and string kernels (Bunescu and Mooney, 2006) to handle this weaker form of MIL supervision.This approach can sometimes be misled by textual features correlated with the specific entities in the few training pairs provided.Therefore, we also describe a method for weighting features in order to focus on those correlated with the target relation rather than with the individual entities.We present experimental results demonstrating that our approach is able to accurately extract relations from the web by learning from such weak supervision.We address the task of learning a relation extraction system targeted to a fixed binary relationship R. The only supervision given to the learning algorithm is a small set of pairs of named entities that are known to belong (positive) or not belong (negative) to the given relationship.Table 1 shows four positive and two negative example pairs for the corporate acquisition relationship.For each pair, a bag of sentences containing the two arguments can be extracted from a corpus of text documents.The corpus is assumed to be sufficiently large and diverse such that, if the pair is positive, it is highly likely that the corresponding bag contains at least one sentence that explicitly asserts the relationship R between the two arguments.In Section 6 we describe a method for extracting bags of relevant sentences from the web.Using a limited set of entity pairs (e.g. those in Table 1) and their associated bags as training data, the aim is to induce a relation extraction system that can reliably decide whether two entities mentioned in the same sentence exhibit the target relationship or not.In particular, when tested on the example sentences from Figure 1, the system should classify S1, S3,and S4 as positive, and S2 and S5 as negative.+/S1: Search engine giant Google has bought videosharing website YouTube in a controversial $1.6 billion deal.−/S2: The companies will merge Google’s search expertise with YouTube’s video expertise, pushing what executives believe is a hot emerging market of video offered over the Internet.+/S3: Google has acquired social media company, YouTube for $1.65 billion in a stock-for-stock transaction as announced by Google Inc. on October 9, 2006.+/S4: Drug giant Pfizer Inc. has reached an agreement to buy the private biotechnology firm Rinat Neuroscience Corp., the companies announced Thursday.−/S5: He has also received consulting fees from Alpharma, Eli Lilly and Company, Pfizer, Wyeth Pharmaceuticals, Rinat Neuroscience, Elan Pharmaceuticals, and Forest Laboratories.As formulated above, the learning task can be seen as an instance of multiple instance learning.However, there are important properties that set it apart from problems previously considered in MIL.The most distinguishing characteristic is that the number of bags is very small, while the average size of the bags is very large.Since its introduction by Dietterich (1997), an extensive and quite diverse set of methods have been proposed for solving the MIL problem.For the task of relation extraction, we consider only MIL methods where the decision function can be expressed in terms of kernels computed between bag instances.This choice was motivated by the comparatively high accuracy obtained by kernel-based SVMs when applied to various natural language tasks, and in particular to relation extraction.Through the use of kernels, SVMs (Vapnik, 1998; Sch¨olkopf and Smola, 2002) can work efficiently with instances that implicitly belong to a high dimensional feature space.When used for classification, the decision function computed by the learning algorithm is equivalent to a hyperplane in this feature space.Overfitting is avoided in the SVM formulation by requiring that positive and negative training instances be maximally separated by the decision hyperplane.Gartner et al. (2002) adapted SVMs to the MIL setting using various multi-instance kernels.Two of these – the normalized set kernel, and the statistic kernel – have been experimentally compared to other methods by Ray and Craven (2005), with competitive results.Alternatively, a simple approach to MIL is to transform it into a standard supervised learning problem by labeling all instances from positive bags as positive.An interesting outcome of the study conducted by Ray and Craven (2005) was that, despite the class noise in the resulting positive examples, such a simple approach often obtains competitive results when compared against other more sophisticated MIL methods.We believe that an MIL method based on multiinstance kernels is not appropriate for training datasets that contain just a few, very large bags.In a multi-instance kernel approach, only bags (and not instances) are considered as training examples, which means that the number of support vectors is going to be upper bounded by the number of training bags.Taking the bags from Table 1 as a sample training set, the decision function is going to be specified by at most seven parameters: the coefficients for at most six support vectors, plus an optional bias parameter.A hypothesis space characterized by such a small number of parameters is likely to have insufficient capacity.Based on these observations, we decided to transform the MIL problem into a standard supervised problem as described above.The use of this approach is further motivated by its simplicity and its observed competitive performance on very diverse datasets (Ray and Craven, 2005).Let X be the set of bags used for training, Xp C X the set of positive bags, and Xn C X the set of negative bags.For any instance x E X from a bag X E X, let φ(x) be the (implicit) feature vector representation of x.Then the corresponding SVM optimization problem can be formulated as in Figure 2: minimize: The capacity control parameter C is normalized by the total number of instances L = Lp + Ln = EXEXp |X |+ EXEXn |X|, so that it remains independent of the size of the dataset.The additional non-negative parameter cp (cn = 1−cp) controls the relative influence that false negative vs. false positive errors have on the value of the objective function.Because not all instances from positive bags are real positive instances, it makes sense to have false negative errors be penalized less than false positive errors (i.e. cp < 0.5).In the dual formulation of the optimization problem from Figure 2, bag instances appear only inside dot products of the form K(x1, x2) = φ(x1)φ(x2).The kernel K is instantiated to a subsequence kernel, as described in the next section.The training bags consist of sentences extracted from online documents, using the methodology described in Section 6.Parsing web documents in order to obtain a syntactic analysis often gives unreliable results – the type of narrative can vary greatly from one web document to another, and sentences with grammatical errors are frequent.Therefore, for the initial experiments, we used a modified version of the subsequence kernel of Bunescu and Mooney (2006), which does not require syntactic information.This kernel computes the number of common subsequences of tokens between two sentences.The subsequences are constrained to be “anchored” at the two entity names, and there is a maximum number of tokens that can appear in a sequence.For example, a subsequence feature for the sentence S1 in Figure 1 is s� = “(e1) ... bought ... (e2) ... in ... billion ... deal”, where (e1) and (e2) are generic placeholders for the two entity names.The subsequence kernel induces a feature space where each dimension corresponds to a sequence of words.Any such sequence that matches a subsequence of words in a sentence example is down-weighted as a function of the total length of the gaps between every two consecutive words.More exactly, let s = w1w2 ... wk be a sequence of k words, and s� = w1 g1 w2 g2 ... wk−1 gk−1 wk a matching subsequence in a relation example, where gi stands for any sequence of words between wi and wi+1.Then the sequence s will be represented in the relation example as a feature with weight computed as τ(s) = λgP).The parameter λ controls the magnitude of the gap penalty, where g(s) = Ei |gi |is the total gap.Many relations, like the ones that we explore in the experimental evaluation, cannot be expressed without using at least one content word.We therefore modified the kernel computation to optionally ignore subsequence patterns formed exclusively of stop words and punctuation signs.In Section 5.1, FrameNet terminology (Baker et al., 1998), these we introduce a new weighting scheme, wherein a correspond to instantiated frame elements.For exweight is assigned to every token.Correspondingly, ample, the corporate acquisition frame can be seen every sequence feature will have an additional mul- as a subtype of the “Getting” frame in FrameNet. tiplicative weight, computed as the product of the The core elements in this frame are the Recipiweights of all the tokens in the sequence.The aim ent (e.g.Google) and the Theme (e.g.YouTube), of this new weighting scheme, as detailed in the next which for the acquisition relationship coincide with section, is to eliminate the bias caused by the special the two arguments.They do not contribute any structure of the relation extraction MIL problem. bias, since they are replaced with the generic tags 5 Two Types of Bias (e1) and (e2) in all sentences from the bag.There As already hinted at the end of Section 2, there is are however other frame elements – peripheral, or one important property that distinguishes the cur- extra-thematic – that can be instantiated with the rent MIL setting for relation extraction from other same value in many sentences.In Figure 1, for inMIL problems: the training dataset contains very stance, sentence 53 contains two non-core frame elefew bags, and each bag can be very large.Con- ments: the Means element (e.g “in a stock-for-stock sequently, an application of the learning model de- transaction”) and the Time element (e.g.“on Ocscribed in Sections 3 & 4 is bound to be affected by tober 9, 2006”).Words from these elements, like the following two types of bias: “stock”, or “October”, are likely to occur very often ■ [Type I Bias] By definition, all sentences inside in the Google-YouTube bag, and because the traina bag are constrained to contain the same two ar- ing dataset contains only a few other bags, subseguments.Words that are semantically correlated quence patterns containing these words will be given with either of the two arguments are likely to oc- too much weight in the learned model.This is probcur in many sentences.For example, consider the lematic, since these words can appear in many other sentences 51 and 52 from the bag associated with frames, and thus the learned model is likely to make “Google” and “YouTube” (as shown in Figure 1). errors.Instead, we would like the model to foThey both contain the words “search” – highly cor- cus on words that trigger the target relationship (in related with “Google”, and “video” – highly corre- FrameNet, these are the lexical units associated with lated with “YouTube”, and it is likely that a signifi- the target frame). cant percentage of sentences in this bag contain one 5.1 A Solution for Type I Bias of the two words (or both).The two entities can be In order to account for how strongly the words in a mentioned in the same sentence for reasons other sequence are correlated with either of the individual than the target relation R, and these noisy training arguments of the relation, we modify the formula for sentences are likely to contain words that are corre- the sequence weight T(s) by factoring in a weight lated with the two entities, without any relationship T(w) for each word in the sequence, as illustrated in to R. A learning model where the features are based Equation 1. on words, or word sequences, is going to give too 11 T(s) � A���s) · T(w) (1) much weight to words or combinations of words that wEs are correlated with either of individual arguments.Given a predefined set of weights T(w), it is straightThis overweighting will adversely affect extraction forward to update the recursive computation of performance through an increased number of errors. the subsequence kernel so that it reflects the new A method for eliminating this type of bias is intro- weighting scheme. duced in Section 5.1.If all the word weights are set to 1, then the new ■ [Type II Bias] While Type I bias is due to words kernel is equivalent to the old one.What we want, that are correlated with the arguments of a relation however, is a set of weights where words that are instance, the Type II bias is caused by words that correlated with either of the two arguments are given are specific to the relation instance itself.Using lower weights.For any word, the decrease in weight 579 should reflect the degree of correlation between that word and the two arguments.Before showing the formula used for computing the word weights, we first introduce some notation: The quantity P(wjX.a1 V X.a2) • C(X) represents the expected number of sentences in which w would occur, if the only causes were X.a1 or X.a2, independent of each other.We want to discard this quantity from the total number of occurrences C(X, w), so that the effect of correlations with X.a1 or X.a2 is eliminated.We still need to compute P(wjX.a1 V X.a2).Because in the definition of P(wjX.a1 V X.a2), the arguments X.a1 and X.a2 were considered independent causes, P(wjX.a1 V X.a2) can be computed with the noisy-or operator (Pearl, 1986): The quantity P(wja) represents the probability that the word w appears in a sentence due only to the presence of a, and it could be estimated using counts on a sufficiently large corpus.For our experimental evaluation, we used the following approximation: given an argument a, a set of sentences containing a are extracted from web documents (details in Section 6).Then P(wja) is simply approximated with the ratio of the number of sentences containing w over the total number of sentences, i.e.P(wja) = C(w, a)/C(a).Because this may be an overestimate (w may appear in a sentence containing a due to causes other than a), and also because of data sparsity, the quantity T(w) may sometimes result in a negative value – in these cases it is set to 0, which is equivalent to ignoring the word w in all subsequence patterns.For the purpose of evaluation, we created two datasets: one for corporate acquisitions, as shown in Table 2, and one for the person-birthplace relation, with the example pairs from Table 3.In both tables, the top part shows the training pairs, while the bottom part shows the test pairs.Given a pair of arguments (a1, a2), the corresponding bag of sentences is created as follows: ■ A query string “a1 * * * * * * * a2” containing seven wildcard symbols between the two arguments is submitted to Google.The preferences are set to search only for pages written in English, with Safesearch turned on.This type of query will match documents where an occurrence of a1 is separated from an occurrence of a2 by at most seven content words.This is an approximation of our actual information ber of sentences in the bag), and C(X, w) the number of sentences in the bag X that contain the word w. Let P(wjX) = C(X, w)/C(X).• Let P(wjX.a1 V X.a2) be the probability that the word w appears in a sentence due only to the presence of X.a1 or X.a2, assuming X.a1 and X.a2 are independent causes for w. The word weights are computed as follows: C(X, w) − P(wjX.a1 V X.a2) • C(X) 580 need: “return all documents containing a1 and a2 in itive bags in the person-birthplace dataset are sigthe same sentence”. nificantly sparser in real positive instances than the ■ The returned documents (limited by Google to positive bags in the corporate acquisition dataset. the first 1000) are downloaded, and then the text The subsequence kernel described in Section 4 is extracted using the HTML parser from the Java was used as a custom kernel for the LibSVM2 Java Swing package.Whenever possible, the appropriate package.When run with the default parameters, HTML tags (e.g.BR, DD, P, etc.) are used as hard the results were extremely poor – too much weight end-of-sentence indicators.The text is further seg- was given to the slack term in the objective funcmented into sentences with the OpenNLP1 package. tion.Minimizing the regularization term is essen■ Sentences that do not contain both arguments a1 tial in order to capture subsequence patterns shared and a2 are discarded.For every remaining sentence, among positive bags.Therefore LibSVM was modwe find the occurrences of a1 and a2 that are clos- ified to solve the optimization problem from Figest to each other, and create a relation example by ure 2, where the capacity parameter C is normalreplacing a1 with (e1) and a2 with (e2).All other ized by the size of the transformed dataset.In this occurrences of a1 and a2 are replaced with a null new formulation, C is set to its default value of 1.0 token ignored by the subsequence kernel.– changing it to other values did not result in signifiThe number of sentences in every bag is shown in cant improvement.The trade-off between false posthe last column of Tables 2 & 3.Because Google itive and false negative errors is controlled by the also counts pages that are deemed too similar in the parameter cp.When set to its default value of 0.5, first 1000, some of the bags can be relatively small. false-negative errors and false positive errors have As described in Section 5.1, the word-argument the same impact on the objective function.As excorrelations are modeled through the quantity pected, setting cp to a smaller value (0.1) resulted P(wla) = C(w, a)/C(a), estimated as the ratio be- in better performance.Tests with even lower values tween the number of sentences containing w and a, did not improve the results. and the number of sentences containing a.These We compare the following four systems: counts are computed over a bag of sentences con- ■ SSK–MIL: This corresponds to the MIL formutaining a, which is created by querying Google for lation from Section 3, with the original subsequence the argument a, and then by processing the results kernel described in Section 4. as described above.■ SSK–T1: This is the SSK–MIL system aug7 Experimental Evaluation mented with word weights, so that the Type I bias Each dataset is split into two sets of bags: one is reduced, as described in Section 5.1. for training and one for testing.The test dataset ■ BW-MIL: This is a bag-of-words kernel, in was purposefully made difficult by including neg- which the relation examples are classified based on ative bags with arguments that during training were the unordered words contained in the sentence.This used in positive bags, and vice-versa.In order to baseline shows the performance of a standard textevaluate the relation extraction performance at the classification approach to the problem using a statesentence level, we manually annotated all instances of-the art algorithm (SVM). from the positive test bags.The last column in Ta- ■ SSK–SIL: This corresponds to the original subbles 2 & 3 shows, between parentheses, how many sequence kernel trained with traditional, single ininstances from the positive test bags are real pos- stance learning (SIL) supervision.For evaluation, itive instances.The corporate acquisition test set we train on the manually labeled instances from the has a total of 995 instances, out of which 156 are test bags.We use a combination of one positive bag positive.The person-birthplace test set has a total and one negative bag for training, while the other of 601 instances, and only 45 of them are positive. two bags are used for testing.The results are averExtrapolating from the test set distribution, the pos- aged over all four possible combinations.Note that the supervision provided to SSK–SIL requires significantly more annotation effort, therefore, given a sufficient amount of training examples, we expect this system to perform at least as well as its MIL counterpart.In Figure 3, precision is plotted against recall by varying a threshold on the value of the SVM decision function.To avoid clutter, we show only the graphs for the first three systems.In Table 4 we show the area under the precision recall curves of all four systems.Overall, the learned relation extractors are able to identify the relationship in novel sentences quite accurately and significantly out-perform a bag-of-words baseline.The new version of the subsequence kernel SSK–T1 is significantly more accurate in the MIL setting than the original subsequence kernel SSK–MIL, and is also competitive with SSK–SIL, which was trained using a reasonable amount of manually labeled sentence examples.An interesting potential application of our approach is a web relation-extraction system similar to Google Sets, in which the user provides only a handful of pairs of entities known to exhibit or not to exhibit a particular relation, and the system is used to find other pairs of entities exhibiting the same relation.Ideally, the user would only need to provide positive pairs.Sentences containing one of the relation arguments could be extracted from the web, and likely negative sentence examples automatically created by pairing this entity with other named entities mentioned in the sentence.In this scenario, the training set can contain both false positive and false negative noise.One useful side effect is that Type I bias is partially removed – some bias still remains due to combinations of at least two words, each correlated with a different argument of the relation.We are also investigating methods for reducing Type II bias, either by modifying the word weights, or by integrating an appropriate measure of word distribution across positive bags directly in the objective function for the MIL problem.Alternatively, implicit negative evidence can be extracted from sentences in positive bags by exploiting the fact that, besides the two relation arguments, a sentence from a positive bag may contain other entity mentions.Any pair of entities different from the relation pair is very likely to be a negative example for that relation.This is similar to the concept of negative neighborhoods introduced by Smith and Eisner (2005), and has the potential of eliminating both Type I and Type II bias.One of the earliest IE methods designed to work with a reduced amount of supervision is that of Hearst (1992), where a small set of seed patterns is used in a bootstrapping fashion to mine pairs of hypernym-hyponym nouns.Bootstrapping is actually orthogonal to our method, which could be used as the pattern learner in every bootstrapping iteration.A more recent IE system that works by bootstrapping relation extraction patterns from the web is KNOWITALL (Etzioni et al., 2005).For a given target relation, supervision in KNOWITALL is provided as a rule template containing words that describe the class of the arguments (e.g.“company”), and a small set of seed extraction patterns (e.g.“has acquired”).In our approach, the type of supervision is different – we ask only for pairs of entities known to exhibit the target relation or not.Also, KNOWITALL requires large numbers of search engine queries in order to collect and validate extraction patterns, therefore experiments can take weeks to complete.Comparatively, the approach presented in this paper requires only a small number of queries: one query per relation pair, and one query for each relation argument.Craven and Kumlien (1999) create a noisy training set for the subcellular-localization relation by mining Medline for sentences that contain tuples extracted from relevant medical databases.To our knowledge, this is the first approach that is using a “weakly” labeled dataset for relation extraction.The resulting bags however are very dense in positive examples, and they are also many and small – consequently, the two types of bias are not likely to have significant impact on their system’s performance.We have presented a new approach to relation extraction that leverages the vast amount of information available on the web.The new RE system is trained using only a handful of entity pairs known to exhibit and not exhibit the target relationship.We have extended an existing relation extraction kernel to learn in this setting and to resolve problems caused by the minimal supervision provided.Experimental results demonstrate that the new approach can reliably extract relations from web documents.We would like to thank the anonymous reviewers for their helpful suggestions.This work was supported by grant IIS-0325116 from the NSF, and a gift from Google Inc.
Learning Semantic Correspondences with Less SupervisionA central problem in grounded language acquisition is learning the correspondences between a rich world state and a stream of text which references that world state.To deal with the high degree of ambiguity present in this setting, we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state.We show that our model generalizes across three domains of increasing difficulty—Robocup sportscasting, weather forecasts (a new domain), and NFL recaps.Recent work in learning semantics has focused on mapping sentences to meaning representations (e.g., some logical form) given aligned sentence/meaning pairs as training data (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Lu et al., 2008).However, this degree of supervision is unrealistic for modeling human language acquisition and can be costly to obtain for building large-scale, broadcoverage language understanding systems.A more flexible direction is grounded language acquisition: learning the meaning of sentences in the context of an observed world state.The grounded approach has gained interest in various disciplines (Siskind, 1996; Yu and Ballard, 2004; Feldman and Narayanan, 2004; Gorniak and Roy, 2007).Some recent work in the NLP community has also moved in this direction by relaxing the amount of supervision to the setting where each sentence is paired with a small set of candidate meanings (Kate and Mooney, 2007; Chen and Mooney, 2008).The goal of this paper is to reduce the amount of supervision even further.We assume that we are given a world state represented by a set of records along with a text, an unsegmented sequence of words.For example, in the weather forecast domain (Section 2.2), the text is the weather report, and the records provide a structured representation of the temperature, sky conditions, etc.In this less restricted data setting, we must resolve multiple ambiguities: (1) the segmentation of the text into utterances; (2) the identification of relevant facts, i.e., the choice of records and aspects of those records; and (3) the alignment of utterances to facts (facts are the meaning representations of the utterances).Furthermore, in some of our examples, much of the world state is not referenced at all in the text, and, conversely, the text references things which are not represented in our world state.This increased amount of ambiguity and noise presents serious challenges for learning.To cope with these challenges, we propose a probabilistic generative model that treats text segmentation, fact identification, and alignment in a single unified framework.The parameters of this hierarchical hidden semi-Markov model can be estimated efficiently using EM.We tested our model on the task of aligning text to records in three different domains.The first domain is Robocup sportscasting (Chen and Mooney, 2008).Their best approach (KRISPER) obtains 67% F1; our method achieves 76.5%.This domain is simplified in that the segmentation is known.The second domain is weather forecasts, for which we created a new dataset.Here, the full complexity of joint segmentation and alignment arises.Nonetheless, we were able to obtain reasonable results on this task.The third domain we considered is NFL recaps (Barzilay and Lapata, 2005; Snyder and Barzilay, 2007).The language used in this domain is richer by orders of magnitude, and much of it does not reference the world state.Nonetheless, taking the first unsupervised approach to this problem, we were able to make substantial progress: We achieve an F1 of 53.2%, which closes over half of the gap between a heuristic baseline (26%) and supervised systems (68%–80%).Our goal is to learn the correspondence between a text w and the world state s it describes.We use the term scenario to refer to such a (w, s) pair.The text is simply a sequence of words w = (wi, ... , w|w|).We represent the world state s as a set of records, where each record r E s is described by a record type r.t E T and a tuple of field values r.v = (r.vi, ... , r.vm).1 For example, temperature is a record type in the weather domain, and it has four fields: time, min, mean, and max.The record type r.t E T specifies the field type r.tf E {INT, STR, CAT} of each field value r.vf, f = 1, ... , m. There are three possible field types—integer (INT), string (STR), and categorical (CAT)—which are assumed to be known and fixed.Integer fields represent numeric properties of the world such as temperature, string fields represent surface-level identifiers such as names of people, and categorical fields represent discrete concepts such as score types in football (touchdown, field goal, and safety).The field type determines the way we expect the field value to be rendered in words: integer fields can be numerically perturbed, string fields can be spliced, and categorical fields are represented by open-ended word distributions, which are to be learned.See Section 3.3 for details.In this domain, a Robocup simulator generates the state of a soccer game, which is represented by a set of event records.For example, the record pass(arg1=pink1,arg2=pink5) denotes a passing event; this type of record has two fields: arg1 (the actor) and arg2 (the recipient).As the game is progressing, humans interject commentaries about notable events in the game, e.g., pink] passes back to pink5 near the middle of the field.All of the fields in this domain are categorical, which means there is no a priori association between the field value pink1 and the word pink].This degree of flexibility is desirable because pink] is sometimes referred to as pink goalie, a mapping which does not arise from string operations but must instead be learned.We used the dataset created by Chen and Mooney (2008), which contains 1919 scenarios from the 2001–2004 Robocup finals.Each scenario consists of a single sentence representing a fragment of a commentary on the game, paired with a set of candidate records.In the annotation, each sentence corresponds to at most one record (possibly one not in the candidate set, in which case we automatically get that sentence wrong).See Figure 1(a) for an example and Table 1 for summary statistics on the dataset.In this domain, the world state contains detailed information about a local weather forecast and the text is a short forecast report (see Figure 1(b) for an example).To create the dataset, we collected local weather forecasts for 3,753 cities in the US (those with population at least 10,000) over three days (February 7–9, 2009) from www.weather.gov.For each city and date, we created two scenarios, one for the day forecast and one for the night forecast.The forecasts consist of hour-by-hour measurements of temperature, wind speed, sky cover, chance of rain, etc., which represent the underlying world state.This world state is summarized by records which aggregate measurements over selected time intervals.For example, one of the records states the minimum, average, and maximum temperature from 5pm to 6am.This aggregation process produced 22,146 scenarios, each containing |s |= 36 multi-field records.There are 12 record types, each consisting of only integer and categorical fields.To annotate the data, we split the text by punctuation into lines and labeled each line with the records to which the line refers.These lines are used only for evaluation and are not part of the model (see Section 5.1 for further discussion).The weather domain is more complex than the Robocup domain in several ways: The text w is longer, there are more candidate records, and most notably, w references multiple records (5.8 on average), so the segmentation of w is unknown.See Table 1 for a comparison of the two datasets.In this domain, each scenario represents a single NFL football game (see Figure 1(c) for an example).The world state (the things that happened during the game) is represented by database tables, e.g., scoring summary, team comparison, drive chart, play-by-play, etc.Each record is a database entry, for instance, the receiving statistics for a certain player.The text is the recap of the game— an article summarizing the game highlights.The dataset we used was collected by Barzilay and Lapata (2005).The data includes 466 games during the 2003–2004 NFL season.78 of these games were annotated by Snyder and Barzilay (2007), who aligned each sentence to a set of records.This domain is by far the most complicated of the three.Many records corresponding to inconsequential game statistics are not mentioned.Conversely, the text contains many general remarks (e.g., it was just that type of game) which are not present in any of the records.Furthermore, the complexity of the language used in the recap is far greater than what we can represent using our simple model.Fortunately, most of the fields are integer fields or string fields (generally names or brief descriptions), which provide important anchor points for learning the correspondences.Nonetheless, the same names and numbers occur in multiple records, so there is still uncertainty about which record is referenced by a given sentence.To learn the correspondence between a text w and a world state s, we propose a generative model p(w I s) with latent variables specifying this correspondence.Our model combines segmentation with alignment.The segmentation aspect of our model is similar to that of Grenager et al. (2005) and Eisenstein and Barzilay (2008), but in those two models, the segments are clustered into topics rather than grounded to a world state.The alignment aspect of our model is similar to the HMM model for word alignment (Ney and Vogel, 1996).DeNero et al. (2008) perform joint segmentation and word alignment for machine translation, but the nature of that task is different from ours.The model is defined by a generative process, which proceeds in three stages (Figure 2 shows the corresponding graphical model): 1.Record choice: choose a sequence of records r = (r1, ... , r|r|) to describe, where each ri E s. 2.Field choice: for each chosen record ri, select a sequence of fields fi = (fi1, ..., fi|fi|), where each fij E 11, ... , m}.3.Word choice: for each chosen field fij, choose a number cij > 0 and generate a sequence of cij words.The observed text w is the terminal yield formed by concatenating the sequences of words of all fields generated; note that the segmentation of w provided by c = 1cij} is latent.Think of the words spanned by a record as constituting an utterance with a meaning representation given by the record and subset of fields chosen.Formally, our probabilistic model places a distribution over (r, f, c, w) and factorizes according to the three stages as follows: p(r, f, c, w  |s) = p(r  |s)p(f  |r)p(c, w  |r, f, s) The following three sections describe each of these stages in more detail.The record choice model specifies a distribution over an ordered sequence of records r = (r1, ... , r|r|), where each record ri E s. This model is intended to capture two types of regularities in the discourse structure of language.The first is salience, that is, some record types are simply more prominent than others.For example, in the NFL domain, 70% of scoring records are mentioned whereas only 1% of punting records are mentioned.The second is the idea of local coherence, that is, the order in which one mentions records tend to follow certain patterns.For example, in the weather domain, the sky conditions are generally mentioned first, followed by temperature, and then wind speed.To capture these two phenomena, we define a Markov model on the record types (and given the record type, a record is chosen uniformly from the set of records with that type): where s(t) def = 1r E s : r.t = t} and r0.t is a dedicated START record type.2 We also model the transition of the final record type to a designated STOP record type in order to capture regularities about the types of records which are described last.More sophisticated models of coherence could also be employed here (Barzilay and Lapata, 2008).We assume that s includes a special null record whose type is NULL, responsible for generating parts of our text which do not refer to any real records.Each record type t E T has a separate field choice model, which specifies a distribution over a sequence of fields.We want to capture salience and coherence at the field level like we did at the record level.For instance, in the weather domain, the minimum and maximum fields of a temperature record are mentioned whereas the average is not.In the Robocup domain, the actor typically precedes the recipient in passing event records.Formally, we have a Markov model over the fields:3 Each record type has a dedicated null field with its own multinomial distribution over words, intended to model words which refer to that record type in general (e.g., the word passes for passing records).We also model transitions into the first field and transitions out of the final field with special START and STOP fields.This Markov structure allows us to capture a few elements of rudimentary syntax.We arrive at the final component of our model, which governs how the information about a particular field of a record is rendered into words.For each field fij, we generate the number of words cij from a uniform distribution over 11, 2,... , Cmax}, where Cmax is set larger than the length of the longest text we expect to see.Conditioned on the fields f, the words w are generated independently:4 where r(k) and f(k) are the record and field responsible for generating word wk, as determined by the segmentation c. The word choice model pw(w I t, v) specifies a distribution over words given the field type t and field value v. This distribution is a mixture of a global backoff distribution over words and a field-specific distribution which depends on the field type t. Although we designed our word choice model to be relatively general, it is undoubtedly influenced by the three domains.However, we can readily extend or replace it with an alternative if desired; this modularity is one principal benefit of probabilistic modeling.Integer Fields (t = INT) For integer fields, we want to capture the intuition that a numeric quantity v is rendered in the text as a word which is possibly some other numerical value w due to stylistic factors.Sometimes the exact value v is used (e.g., in reporting football statistics).Other times, it might be customary to round v (e.g., wind speeds are typically rounded to a multiple of 5).In other cases, there might just be some unexplained error, where w deviates from v by some noise c+ = w − v > 0 or c− = v − w > 0.We model c+ and c− as geometric distributions.5 In summary, we allow six possible ways of generating the word w given v: v rv15 LvI5 round5(v) v − c− v + c+ Separate probabilities for choosing among these possibilities are learned for each field type (see Figure 3 for an example).String Fields (t = STR) Strings fields are intended to represent values which we expect to be realized in the text via a simple surface-level transformation.For example, a name field with value v = Moe Williams is sometimes referenced in the text by just Williams.We used a simple generic model of rendering string fields: Let w be a word chosen uniformly from those in v. Categorical Fields (t = CAT) Unlike string fields, categorical fields are not tied down to any lexical representation; in fact, the identities of the categorical field values are irrelevant.For each categorical field f and possible value v, we have a , clear mostly sunny partly, cloudy increasing mostly cloudy, partly of inch an possible new a rainfall herence structure at both the record and field levels.To quantify the benefits of incorporating these two aspects, we compare our full model with two simpler variants. skyCover.mode in the weather domain.It is interesting to note that skyCover=75-100 is so highly correlated with rain that the model learns to connect an overcast sky in the world to the indication of rain in the text. separate multinomial distribution over words from which w is drawn.An example of a categorical field is skyCover.mode in the weather domain, which has four values: 0-25, 25-50, 50-75, and 75-100.Table 2 shows the top words for each of these field values learned by our model.Our learning and inference methodology is a fairly conventional application of Expectation Maximization (EM) and dynamic programming.The input is a set of scenarios D, each of which is a text w paired with a world state s. We maximize the marginal likelihood of our data, summing out the latent variables (r, f, c): where 0 are the parameters of the model (all the multinomial probabilities).We use the EM algorithm to maximize (3), which alternates between the E-step and the M-step.In the E-step, we compute expected counts according to the posterior p(r, f, c  |w, s; 0).In the M-step, we optimize the parameters 0 by normalizing the expected counts computed in the E-step.In our experiments, we initialized EM with a uniform distribution for each multinomial and applied add-0.1 smoothing to each multinomial in the M-step.As with most complex discrete models, the bulk of the work is in computing expected counts under p(r, f, c  |w, s; 0).Formally, our model is a hierarchical hidden semi-Markov model conditioned on s. Inference in the E-step can be done using a dynamic program similar to the inside-outside algorithm.Two important aspects of our model are the segmentation of the text and the modeling of the coIn the annotated data, each text w has been divided into a set of lines.These lines correspond to clauses in the weather domain and sentences in the Robocup and NFL domains.Each line is annotated with a (possibly empty) set of records.Let A be the gold set of these line-record alignment pairs.To evaluate a learned model, we compute the Viterbi segmentation and alignment (argmaxr,f,c p(r, f, c  |w, s)).We produce a predicted set of line-record pairs A' by aligning a line to a record ri if the span of (the utterance corresponding to) ri overlaps the line.The reason we evaluate indirectly using lines rather than using utterances is that it is difficult to annotate the segmentation of text into utterances in a simple and consistent manner.We compute standard precision, recall, and F1 of A' with respect to A.Unless otherwise specified, performance is reported on all scenarios, which were also used for training.However, we did not tune any hyperparameters, but rather used generic values which worked well enough across all three domains.We ran 10 iterations of EM on Models 1–3.Table 3 shows that performance improves with increased model sophistication.We also compare our model to the results of Chen and Mooney (2008) in Table 4.Figure 4 provides a closer look at the predictions made by each of our three models for a particular example.Model 1 easily mistakes pink10 for the recipient of a pass record because decisions are made independently for each word.Model 2 chooses the correct record, but having no model of the field structure inside a record, it proposes an incorrect field segmentation (although our evaluation is insensitive to this).Equipped with the ability to prefer a coherent field sequence, Model 3 fixes these errors.Many of the remaining errors are due to the garbage collection phenomenon familiar from word alignment models (Moore, 2004; Liang et al., 2006).For example, the ballstopped record occurs frequently but is never mentioned in the text.At the same time, there is a correlation between ballstopped and utterances such as pink2 holds onto the ball, which are not aligned to any record in the annotation.As a result, our model incorrectly chooses to align the two.For the weather domain, staged training was necessary to get good results.For Model 1, we ran 15 iterations of EM.For Model 2, we ran 5 iterations of EM on Model 1, followed by 10 iterations on Model 2.For Model 3, we ran 5 iterations of Model 1, 5 iterations of a simplified variant of Model 3 where records were chosen independently, and finally, 5 iterations of Model 3.When going from one model to another, we used the final posterior distributions of the former to initialize the parameters of the latter.6 We also prohibited utterances in Models 2 and 3 from crossing punctuation during inference.Table 5 shows that performance improves substantially in the more sophisticated models, the gains being greater than in the Robocup domain.Figure 5 shows the predictions of the three models on an example.Model 1 is only able to form isolated (but not completely inaccurate) associations.By modeling segmentation, Model 2 accounts for the intermediate words, but errors are still made due to the lack of Markov structure.Model 3 remedies this.However, unexpected structures are sometimes learned.For example, the temperature.time=6-21 field indicates daytime, which happens to be perfectly correlated with the word high, although high intuitively should be associated with the temperature.max field.In these cases of high correlation (Table 2 provides another example), it is very difficult to recover the proper alignment without additional supervision.In order to scale up our models to the NFL domain, we first pruned for each sentence the records which have either no numerical values (e.g., 23, 23-10, 2/4) nor name-like words (e.g., those that appear only capitalized in the text) in common.This eliminated all but 1.5% of the record candidates per sentence, while maintaining an oracle alignment F1 score of 88.7.Guessing a single random record for each sentence yields an F1 of 12.0.A reasonable heuristic which uses weighted number- and string-matching achieves 26.7.Due to the much greater complexity of this domain, Model 2 was easily misled as it tried without success to find a coherent segmentation of the fields.We therefore created a variant, Model 2’, where we constrained each field to generate exactly one word.To train Model 2’, we ran 5 iterations of EM where each sentence is assumed to have exactly one record, followed by 5 iterations where the constraint was relaxed to also allow record boundaries at punctuation and the word and.We did not experiment with Model 3 since the discourse structure on records in this domain is not at all governed by a simple Markov model on record types—indeed, most regions do not refer to any records at all.We also fixed the backoff probability to 0.1 instead of learning it and enforced zero numerical deviation on integer field values.Model 2’ achieved an F1 of 39.9, an improvement over Model 1, which attained 32.8.Inspection of the errors revealed the following problem: The alignment task requires us to sometimes align a sentence to multiple redundant records (e.g., play and score) referenced by the same part of the text.However, our model generates each part of text from only one record, and thus it can only allow an alignment to one record.7 To cope with this incompatibility between the data and our notion of semantics, we used the following solution: We divided the records into three groups by type: play, score, and other.Each group has a copy of the model, but we enforce that they share the same segmentation.We also introduce a potential that couples the presence or absence of records across groups on the same segment to capture regular cooccurrences between redundant records.Table 6 shows our results.With groups, we achieve an F1 of 53.2.Though we still trail supervised techniques, which attain numbers in the 68–80 range, we have made substantial progress over our baseline using an unsupervised method.Furthermore, our model provides a more detailed analysis of the correspondence between the world state and text, rather than just producing a single alignment decision.Most of the remaining errors made by our model are due to a lack of calibration.Sometimes, our false positives are close calls where a sentence indirectly references a record, and our model predicts the alignment whereas the annotation standard does not.We believe that further progress is possible with a richer model.We have presented a generative model of correspondences between a world state and an unsegmented stream of text.By having a joint model of salience, coherence, and segmentation, as well as a detailed rendering of the values in the world state into words in the text, we are able to cope with the increased ambiguity that arises in this new data setting, successfully pushing the limits of unsupervision.
Application-driven Statistical Paraphrase GenerationParaphrase generation (PG) is important in plenty of NLP applications.However, the research of PG is far from enough.In this paper, we propose a novel method for statistical paraphrase generation (SPG), which can (1) achieve various applications based on a uniform statistical model, and (2) naturally combine multiple resources to enhance the PG performance.In our experiments, we use the proposed method to generate paraphrases for three different applications.The results show that the method can be easily transformed from one application to another and generate valuable and interesting paraphrases.Paraphrases are alternative ways that convey the same meaning.There are two main threads in the research of paraphrasing, i.e., paraphrase recognition and paraphrase generation (PG).Paraphrase generation aims to generate a paraphrase for a source sentence in a certain application.PG shows its importance in many areas, such as question expansion in question answering (QA) (Duboue and Chu-Carroll, 2006), text polishing in natural language generation (NLG) (Iordanskaja et al., 1991), text simplification in computer-aided reading (Carroll et al., 1999), and sentence similarity computation in the automatic evaluation of machine translation (MT) (Kauchak and Barzilay, 2006) and summarization (Zhou et al., 2006).This paper presents a method for statistical paraphrase generation (SPG).As far as we know, this is the first statistical model specially designed for paraphrase generation.It’s distinguishing feature is that it achieves various applications with a uniform model.In addition, it exploits multiple resources, including paraphrase phrases, patterns, and collocations, to resolve the data shortage problem and generate more varied paraphrases.We consider three paraphrase applications in our experiments, including sentence compression, sentence simplification, and sentence similarity computation.The proposed method generates paraphrases for the input sentences in each application.The generated paraphrases are then manually scored based on adequacy, fluency, and usability.The results show that the proposed method is promising, which generates useful paraphrases for the given applications.In addition, comparison experiments show that our method outperforms a conventional SMT-based PG method.Conventional methods for paraphrase generation can be classified as follows: Rule-based methods: Rule-based PG methods build on a set of paraphrase rules or patterns, which are either hand crafted or automatically collected.In the early rule-based PG research, the paraphrase rules are generally manually written (McKeown, 1979; Zong et al., 2001), which is expensive and arduous.Some researchers then tried to automatically extract paraphrase rules (Lin and Pantel, 2001; Barzilay and Lee, 2003; Zhao et al., 2008b), which facilitates the rule-based PG methods.However, it has been shown that the coverage of the paraphrase patterns is not high enough, especially when the used paraphrase patterns are long or complicated (Quirk et al., 2004).Thesaurus-based methods: The thesaurus-based methods generate a paraphrase t for a source sentence s by substituting some words in s with their synonyms (Bolshakov and Gelbukh, 2004; Kauchak and Barzilay, 2006).This kind of method usually involves two phases, i.e., candidate extraction and paraphrase validation.In the first phase, it extracts all synonyms from a thesaurus, such as WordNet, for the words to be substituted.In the second phase, it selects an optimal substitute for each given word from the synonyms according to the context in s. This kind of method is simple, since the thesaurus synonyms are easy to access.However, it cannot generate other types of paraphrases but only synonym substitution.NLG-based methods: NLG-based methods (Kozlowski et al., 2003; Power and Scott, 2005) generally involve two stages.In the first one, the source sentence s is transformed into its semantic representation r by undertaking a series of NLP processing, including morphology analyzing, syntactic parsing, semantic role labeling, etc.In the second stage, a NLG system is employed to generate a sentence t from r. s and t are paraphrases as they are both derived from r. The NLG-based methods simulate human paraphrasing behavior, i.e., understanding a sentence and presenting the meaning in another way.However, deep analysis of sentences is a big challenge.Moreover, developing a NLG system is also not trivial.SMT-based methods: SMT-based methods viewed PG as monolingual MT, i.e., translating s into t that are in the same language.Researchers employ the existing SMT models for PG (Quirk et al., 2004).Similar to typical SMT, a large parallel corpus is needed as training data in the SMT-based PG.However, such data are difficult to acquire compared with the SMT data.Therefore, data shortage becomes the major limitation of the method.To address this problem, we have tried combining multiple resources to improve the SMT-based PG model (Zhao et al., 2008a).There have been researchers trying to propose uniform PG methods for multiple applications.But they are either rule-based (Murata and Isahara, 2001; Takahashi et al., 2001) or thesaurusbased (Bolshakov and Gelbukh, 2004), thus they have some limitations as stated above.Furthermore, few of them conducted formal experiments to evaluate the proposed methods.Despite the similarity between PG and MT, the statistical model used in SMT cannot be directly The SPG method proposed in this work contains three components, i.e., sentence preprocessing, paraphrase planning, and paraphrase generation (Figure 1).Sentence preprocessing mainly includes POS tagging and dependency parsing for the input sentences, as POS tags and dependency information are necessary for matching the paraphrase pattern and collocation resources in the following stages.Paraphrase planning (Section 3.3) aims to select the units to be paraphrased (called source units henceforth) in an input sentence and the candidate paraphrases for the source units (called target units) from multiple resources according to the given application A.Paraphrase generation (Section 3.4) is designed to generate paraphrases for the input sentences by selecting the optimal target units with a statistical model.In this work, the multiple paraphrase resources are stored in paraphrase tables (PTs).A paraphrase table is similar to a phrase table in SMT, which contains fine-grained paraphrases, such as paraphrase phrases, patterns, or collocations.The PTs used in this work are constructed using different corpora and different score functions (Section 3.5).If the applications are not considered, all units of an input sentence that can be paraphrased using the PTs will be extracted as source units.Accordingly, all paraphrases for the source units will be extracted as target units.However, when a certain application is given, only the source and target units that can achieve the application will be kept.We call this process paraphrase planning, which is formally defined as in Figure 2.An example is depicted in Figure 3.The application in this example is sentence compression.All source and target units are listed below the input sentence, in which the first two source units are phrases, while the third and fourth are a pattern and a collocation, respectively.As can be seen, the first and fourth source units are filtered in paraphrase planning, since none of their paraphrases achieve the application (i.e., shorter in bytes than the source).The second and third source units are kept, but some of their paraphrases are filtered.Our SPG model contains three sub-models: a paraphrase model, a language model, and a usability model, which control the adequacy, fluency, and usability of the paraphrases, respectively1.Paraphrase Model: Paraphrase generation is a decoding process.The input sentence s is first segmented into a sequence of I units sI1, which are then paraphrased to a sequence of units �tI1.Let (si, ti) be a pair of paraphrase units, their paraphrase likelihood is computed using a score function Opm(�si, ti).Thus the paraphrase score ppm(9I1, �tI1) between s and t is decomposed into: where Apm is the weight of the paraphrase model.Actually, it is defined similarly to the translation model in SMT (Koehn et al., 2003).In practice, the units of a sentence may be paraphrased using different PTs.Suppose we have K PTs, (ski, tki) is a pair of paraphrase units from the k-th PT with the score function Ok(ski, �tki), then Equation (1) can be rewritten as: where Ak is the weight for Ok(Ski, tki).Equation (2) assumes that a pair of paraphrase units is from only one paraphrase table.However, 1The SPG model applies monotone decoding, which does not contain a reordering sub-model that is often used in SMT.Instead, we use the paraphrase patterns to achieve word reordering in paraphrase generation. we find that about 2% of the paraphrase units appear in two or more PTs.In this case, we only count the PT that provides the largest paraphrase score, i.e., kˆ = arg maxk{φk(¯si, ¯ti)λk}.In addition, note that there may be some units that cannot be paraphrased or prefer to keep unchanged during paraphrasing.Therefore, we have a self-paraphrase table in the K PTs, which paraphrases any separate word w into itself with a constant score c: φself(w, w) = c (we set c = e−1).Language Model: We use a tri-gram language model in this work.The language model based score for the paraphrase t is computed as: where J is the length of t, tj is the j-th word of t, and λlm is the weight for the language model.Usability Model: The usability model prefers paraphrase units that can better achieve the application.The usability of t depends on paraphrase units it contains.Hence the usability model where λum is the weight for the usability model and pum(¯si, ¯ti) is defined as follows: We consider three applications, including sentence compression, simplification, and similarity computation. µ(¯si, ¯ti) is defined separately for each: only the target units that can enhance the similarity to the reference sentence are kept in planning.We define µ(si, ti) = sim(�ti, s')− sim(si, s'), where sim(·, ·) is simply computed as the count of overlapping words.We combine the three sub-models based on a log-linear framework and get the SPG model: We use five PTs in this work (except the selfparaphrase table), in which each pair of paraphrase units has a score assigned by the score function of the corresponding method.Paraphrase phrases (PT-1 to PT-3): Paraphrase phrases are extracted from three corpora: lel translations of the same foreign novel).The details of the corpora, methods, and score functions are presented in (Zhao et al., 2008a).In our experiments, PT-1 is the largest, which contains 3,041,822 pairs of paraphrase phrases.PT-2 and PT-3 contain 92,358, and 17,668 pairs of paraphrase phrases, respectively.Paraphrase patterns (PT-4): Paraphrase patterns are also extracted from Corp-1.We applied the approach proposed in (Zhao et al., 2008b).Its basic assumption is that if two English patterns e1 and e2 are aligned with the same foreign pattern f, then e1 and e2 are possible paraphrases.One can refer to (Zhao et al., 2008b) for the details.PT-4 contains 1,018,371 pairs of paraphrase patterns.Paraphrase collocations (PT-5): Collocations4 can cover long distance dependencies in sentences.Thus paraphrase collocations are useful for SPG.We extract collocations from a monolingual corpus and use a binary classifier to recognize if any two collocations are paraphrases.Due to the space limit, we cannot introduce the detail of the approach.We assign the score “1” for any pair of paraphrase collocations.PT-5 contains 238,882 pairs of paraphrase collocations.To estimate parameters λk(1 < k < K), λlm, and λum, we adopt the approach of minimum error rate training (MERT) that is popular in SMT (Och, 2003).In SMT, however, the optimization objective function in MERT is the MT evaluation criteria, such as BLEU.As we analyzed above, the BLEU-style criteria cannot be adapted in SPG.We therefore introduce a new optimization objective function in this paper.The basic assumption is that a paraphrase should contain as many correct unit replacements as possible.Accordingly, we design the following criteria: Replacement precision (rp): rp assesses the precision of the unit replacements, which is defined as rp = cdev(+r)/cdev(r), where cdev(r) is the total number of unit replacements in the generated paraphrases on the development set. cdev(+r) is the number of the correct replacements.Replacement rate (rr): rr measures the paraphrase degree on the development set, i.e., the percentage of words that are paraphrased.We define rr as: rr = wdev(r)/wdev(s), where wdev(r) is the total number of words in the replaced units on the development set, and wdev(s) is the number of words of all sentences on the development set.Replacement f-measure (rf): We use rf as the optimization objective function in MERT, which is similar to the conventional f-measure and leverages rp and rr: rf = (2 x rp x rr)/(rp + rr).We estimate parameters for each paraphrase application separately.For each application, we first ask two raters to manually label all possible unit replacements on the development set as correct or incorrect, so that rp, rr, and rf can be automatically computed under each set of parameters.The parameters that result in the highest rf on the development set are finally selected.Our SPG decoder is developed by remodeling Moses that is widely used in SMT (Hoang and Koehn, 2008).The POS tagger and dependency parser for sentence preprocessing are SVMTool (Gimenez and Marquez, 2004) and MSTParser (McDonald et al., 2006).The language model is trained using a 9 GB English corpus.Our method is not restricted in domain or sentence style.Thus any sentence can be used in development and test.However, for the sentence similarity computation purpose in our experiments, we want to evaluate if the method can enhance the stringlevel similarity between two paraphrase sentences.Therefore, for each input sentence s, we need a reference sentence s' for similarity computation.Based on the above consideration, we acquire experiment data from the human references of the MT evaluation, which provide several human translations for each foreign sentence.In detail, we use the first translation of a foreign sentence as the source s and the second translation as the reference s' for similarity computation.In our experiments, the development set contains 200 sentences and the test set contains 500 sentences, both of which are randomly selected from the human translations of 2008 NIST Open Machine Translation Evaluation: Chinese to English Task.The evaluation metrics for SPG are similar to the human evaluation for MT (Callison-Burch et al., 2007).The generated paraphrases are manually evaluated based on three criteria, i.e., adequacy, fluency, and usability, each of which has three scales from 1 to 3.Here is a brief description of the different scales for the criteria:We use our method to generate paraphrases for the three applications.Results show that the percentages of test sentences that can be paraphrased are 97.2%, 95.4%, and 56.8% for the applications of sentence compression, simplification, and similarity computation, respectively.The reason why the last percentage is much lower than the first two is that, for sentence similarity computation, many sentences cannot find unit replacements from the PTs that improve the similarity to the reference sentences.For the other applications, only some very short sentences cannot be paraphrased.Further results show that the average number of unit replacements in each sentence is 5.36, 4.47, and 1.87 for sentence compression, simplification, and similarity computation.It also indicates that sentence similarity computation is more difficult than the other two applications.We ask two raters to label the paraphrases based on the criteria defined in Section 4.2.The labeling results are shown in the upper part of Table 1.We can see that for adequacy and fluency, the paraphrases in sentence similarity computation get the highest scores.About 70% of the paraphrases are labeled “3”.This is because in sentence similarity computation, only the target units appearing in the reference sentences are kept in paraphrase planning.This constraint filters most of the noise.The adequacy and fluency scores of the other two applications are not high.The percentages of label “3” are around 30%.The main reason is that the average numbers of unit replacements for these two applications are much larger than sentence similarity computation.It is thus more likely to bring in incorrect unit replacements, which influence the quality of the generated paraphrases.The usability is needed to be manually labeled only for sentence simplification, since it can be automatically labeled in the other two applications.As shown in Table 1, for sentence simplification, most paraphrases are labeled “2” in usability, while merely less than 20% are labeled “3”.We conjecture that it is because the raters are not sensitive to the slight change of the simplification degree.Thus they labeled “2” in most cases.We compute the kappa statistic between the raters.Kappa is defined as K = P(A)−P(E) (Car1−P(E)letta, 1996), where P(A) is the proportion of times that the labels agree, and P(E) is the proportion of times that they may agree by chance.We define P(E) = 13 , as the labeling is based on three point scales.The results show that the kappa statistics for adequacy and fluency are 0.6560 and 0.6500, which indicates a substantial agreement (K: 0.610.8) according to (Landis and Koch, 1977).The kappa statistic for usability is 0.5849, which is only moderate (K: 0.41-0.6).Table 2 shows an example of the generated paraphrases.A source sentence s is paraphrased in each application and we can see that: (1) for sentence compression, the paraphrase t is 8 bytes shorter than s; (2) for sentence simplification, the words wealth and part in t are easier than their sources asset and proportion, especially for the non-native speakers; (3) for sentence similarity computation, the reference sentence s' is listed below t, in which the words appearing in t but not in s are highlighted in blue.In our experiments, we implement two baseline methods for comparison: Baseline-1: Baseline-1 follows the method proposed in (Quirk et al., 2004), which generates paraphrases using typical SMT tools.Similar to Quirk et al.’s method, we extract a paraphrase table for the SMT model from a monolingual comparable corpus (PT-2 described above).The SMT decoder used in Baseline-1 is Moses.Baseline-2: Baseline-2 extends Baseline-1 by combining multiple resources.It exploits all PTs introduced above in the same way as our proposed method.The difference from our method is that Baseline-2 does not take different applications into consideration.Thus it contains no paraphrase planning stage or the usability sub-model.We tune the parameters for the two baselines using the development data as described in Section 3.6 and evaluate them with the test data.Since paraphrase applications are not considered by the baselines, each baseline method outputs a single best paraphrase for each test sentence.The generation results show that 93% and 97.8% of the test sentences can be paraphrased by Baseline-1 and Baseline-2.The average number of unit replacements per sentence is 4.23 and 5.95, respectively.This result suggests that Baseline-1 is less capable than Baseline-2, which is mainly because its paraphrase resource is limited.The generated paraphrases are also labeled by our two raters and the labeling results can be found in the lower part of Table 1.As can be seen, Baseline-1 performs poorly compared with our method and Baseline-2, as the percentage of label “1” is the highest for both adequacy and fluency.This result demonstrates that it is necessary to combine multiple paraphrase resources to improve the paraphrase generation performance.Table 1 also shows that Baseline-2 performs comparably with our method except that it does not consider paraphrase applications.However, we are interested how many paraphrases generated by Baseline-2 can achieve the given applications by chance.After analyzing the results, we find that 24.95%, 8.79%, and 7.16% of the paraphrases achieve sentence compression, simplification, and similarity computation, respectively, which are much lower than our method.Previous research regarded sentence compression, simplification, and similarity computation as totally different problems and proposed distinct method for each one.Therefore, it is interesting to compare our method to the application-specific methods.However, it is really difficult for us to reimplement the methods purposely designed for these applications.Thus here we just conduct an informal comparison with these methods.Sentence compression: Sentence compression is widely studied, which is mostly reviewed as a word deletion task.Different from prior research, Cohn and Lapata (2008) achieved sentence compression using a combination of several operations including word deletion, substitution, insertion, and reordering based on a statistical model, which is similar to our paraphrase generation process.Besides, they also used paraphrase patterns extracted from bilingual parallel corpora (like our PT-4) as a kind of rewriting resource.However, as most other sentence compression methods, their method allows information loss after compression, which means that the generated sentences are not necessarily paraphrases of the source sentences.Sentence Simplification: Carroll et al. (1999) has proposed an automatic text simplification method for language-impaired readers.Their method contains two main parts, namely the lexical simplifier and syntactic simplifier.The former one focuses on replacing words with simpler synonyms, while the latter is designed to transfer complex syntactic structures into easy ones (e.g., replacing passive sentences with active forms).Our method is, to some extent, simpler than Carroll et al.’s, since our method does not contain syntactic simplification strategies.We will try to address sentence restructuring in our future work.Sentence Similarity computation: Kauchak and Barzilay (2006) have tried paraphrasing-based sentence similarity computation.They paraphrase a sentence s by replacing its words with WordNet synonyms, so that s can be more similar in wording to another sentence s'.A similar method has also been proposed in (Zhou et al., 2006), which uses paraphrase phrases like our PT-1 instead of WordNet synonyms.These methods can be roughly viewed as special cases of ours, which only focus on the sentence similarity computation application and only use one kind of paraphrase resource.This paper proposes a method for statistical paraphrase generation.The contributions are as follows.(1) It is the first statistical model specially designed for paraphrase generation, which is based on the analysis of the differences between paraphrase generation and other researches, especially machine translation.(2) It generates paraphrases for different applications with a uniform model, rather than presenting distinct methods for each application.(3) It uses multiple resources, including paraphrase phrases, patterns, and collocations, to relieve data shortage and generate more varied and interesting paraphrases.Our future work will be carried out along two directions.First, we will improve the components of the method, especially the paraphrase planning algorithm.The algorithm currently used is simple but greedy, which may miss some useful paraphrase units.Second, we will extend the method to other applications, We hope it can serve as a universal framework for most if not all applications.The research was supported by NSFC (60803093, 60675034) and 863 Program (2008AA01Z144).Special thanks to Wanxiang Che, Ruifang He, Yanyan Zhao, Yuhang Guo and the anonymous reviewers for insightful comments and suggestions.
Distant supervision for relation extraction without labeled dataModern models of relation extraction for tasks like ACE are based on supervised learning of relations from small hand-labeled corpora.We investigate an alternative paradigm that does not require labeled corpora, avoiding the domain dependence of ACEstyle algorithms, and allowing the use of corpora of any size.Our experiments use Freebase, a large semantic database of several thousand relations, to For each pair of entities that appears in some Freebase relation, we find all sentences containing those entities in a large unlabeled corpus and extract textual features to train a relation classifier.Our algorithm combines the advantages of supervised IE (combining 400,000 noisy pattern features in a probabilistic classifier) and unsupervised IE (extracting large numbers of relations from large corpora of any domain).Our model is able to extract 10,000 instances of 102 relations at a precision of 67.6%.We also analyze feature performance, showing that syntactic parse features are particularly helpful for relations that are ambiguous or lexically distant in their expression.At least three learning paradigms have been applied to the task of extracting relational facts from text (for example, learning that a person is employed by a particular organization, or that a geographic entity is located in a particular region).In supervised approaches, sentences in a corpus are first hand-labeled for the presence of entities and the relations between them.The NIST Automatic Content Extraction (ACE) RDC 2003 and 2004 corpora, for example, include over 1,000 documents in which pairs of entities have been labeled with 5 to 7 major relation types and 23 to 24 subrelations, totaling 16,771 relation instances.ACE systems then extract a wide variety of lexical, syntactic, and semantic features, and use supervised classifiers to label the relation mention holding between a given pair of entities in a test set sentence, optionally combining relation mentions (Zhou et al., 2005; Zhou et al., 2007; Surdeanu and Ciaramita, 2007).Supervised relation extraction suffers from a number of problems, however.Labeled training data is expensive to produce and thus limited in quantity.Also, because the relations are labeled on a particular corpus, the resulting classifiers tend to be biased toward that text domain.An alternative approach, purely unsupervised information extraction, extracts strings of words between entities in large amounts of text, and clusters and simplifies these word strings to produce relation-strings (Shinyama and Sekine, 2006; Banko et al., 2007).Unsupervised approaches can use very large amounts of data and extract very large numbers of relations, but the resulting relations may not be easy to map to relations needed for a particular knowledge base.A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008).These seeds are used with a large corpus to extract a new set of patterns, which are used to extract more instances, which are used to extract more patterns, in an iterative fashion.The resulting patterns often suffer from low precision and semantic drift.We propose an alternative paradigm, distant supervision, that combines some of the advantages of each of these approaches.Distant supervision is an extension of the paradigm used by Snow et al. (2005) for exploiting WordNet to extract hypernym (is-a) relations between entities, and is similar to the use of weakly labeled data in bioinformatics (Craven and Kumlien, 1999; Morgan et al., 2004).Our algorithm uses Freebase (Bollacker et al., 2008), a large semantic database, to provide distant supervision for relation extraction.Freebase contains 116 million instances of 7,300 relations between 9 million entities.The intuition of distant supervision is that any sentence that contains a pair of entities that participate in a known Freebase relation is likely to express that relation in some way.Since there may be many sentences containing a given entity pair, we can extract very large numbers of (potentially noisy) features that are combined in a logistic regression classifier.Thus whereas the supervised training paradigm uses a small labeled corpus of only 17,000 relation instances as training data, our algorithm can use much larger amounts of data: more text, more relations, and more instances.We use 1.2 million Wikipedia articles and 1.8 million instances of 102 relations connecting 940,000 entities.In addition, combining vast numbers of features in a large classifier helps obviate problems with bad features.Because our algorithm is supervised by a database, rather than by labeled text, it does not suffer from the problems of overfitting and domain-dependence that plague supervised systems.Supervision by a database also means that, unlike in unsupervised approaches, the output of our classifier uses canonical names for relations.Our paradigm offers a natural way of integrating data from multiple sentences to decide if a relation holds between two entities.Because our algorithm can use large amounts of unlabeled data, a pair of entities may occur multiple times in the test set.For each pair of entities, we aggregate the features from the many different sentences in which that pair appeared into a single feature vector, allowing us to provide our classifier with more information, resulting in more accurate labels.Table 1 shows examples of relation instances extracted by our system.We also use this system to investigate the value of syntactic versus lexical (word sequence) features in relation extraction.While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.Most previous research in bootstrapping or unsupervised IE has used only simple lexical features, thereby avoiding the computational expense of parsing (Brin, 1998; Agichtein and Gravano, 2000; Etzioni et al., 2005), and the few systems that have used unsupervised IE have not compared the performance of these two types of feature.Except for the unsupervised algorithms discussed above, previous supervised or bootstrapping approaches to relation extraction have typically relied on relatively small datasets, or on only a small number of distinct relations.Approaches based on WordNet have often only looked at the hypernym (is-a) or meronym (part-of) relation (Girju et al., 2003; Snow et al., 2005), while those based on the ACE program (Doddington et al., 2004) have been restricted in their evaluation to a small number of relation instances and corpora of less than a million words.Many early algorithms for relation extraction used little or no syntactic information.For example, the DIPRE algorithm by Brin (1998) used string-based regular expressions in order to recognize relations such as author-book, while the SNOWBALL algorithm by Agichtein and Gravano (2000) learned similar regular expression patterns over words and named entity tags.Hearst (1992) used a small number of regular expressions over words and part-of-speech tags to find examples of the hypernym relation.The use of these patterns has been widely replicated in successful systems, for example by Etzioni et al. (2005).Other work such as Ravichandran and Hovy (2002) and Pantel and Pennacchiotti (2006) use the same formalism of learning regular expressions over words and part-of-speech tags to discover patterns indicating a variety of relations.More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al. (2005), and work in the ACE paradigm such as Zhou et al.(2005) and Zhou et al. (2007).Perhaps most similar to our distant supervision algorithm is the effective method of Wu and Weld (2007) who extract relations from a Wikipedia page by using supervision from the page’s infobox.Unlike their corpus-specific method, which is specific to a (single) Wikipedia page, our algorithm allows us to extract evidence for a relation from many different documents, and from any genre.Following the literature, we use the term ‘relation’ to refer to an ordered, binary relation between entities.We refer to individual ordered pairs in this relation as ‘relation instances’.For example, the person-nationality relation holds between the entities named ‘John Steinbeck’ and ‘United States’, so it has (John Steinbeck, United States) as an instance.We use relations and relation instances from Freebase, a freely available online database of structured semantic data.Data in Freebase is collected from a variety of sources.One major source is text boxes and other tabular data from Wikipedia.Data is also taken from NNDB (biographical information), MusicBrainz (music), the SEC (financial and corporate data), as well as direct, wiki-style user editing.After some basic processing of the July 2008 link export to convert Freebase’s data representation into binary relations, we have 116 million instances of 7,300 relations between 9 million entities.We next filter out nameless and uninteresting entities such as user profiles and music tracks.Freebase also contains the reverses of many of its relations (bookauthor v. author-book), and these are merged.Filtering and removing all but the largest relations leaves us with 1.8 million instances of 102 relations connecting 940,000 entities.Examples are shown in Table 2.The intuition of our distant supervision approach is to use Freebase to give us a training set of relations and entity pairs that participate in those relations.In the training step, all entities are identified in sentences using a named entity tagger that labels persons, organizations and locations.If a sentence contains two entities and those entities are an instance of one of our Freebase relations, features are extracted from that sentence and are added to the feature vector for the relation.The distant supervision assumption is that if two entities participate in a relation, any sentence that contain those two entities might express that relation.Because any individual sentence may give an incorrect cue, our algorithm trains a multiclass logistic regression classifier, learning weights for each noisy feature.In training, the features for identical tuples (relation, entity1, entity2) from different sentences are combined, creating a richer feature vector.In the testing step, entities are again identified using the named entity tagger.This time, every pair of entities appearing together in a sentence is considered a potential relation instance, and whenever those entities appear together, features are extracted on the sentence and added to a feature vector for that entity pair.For example, if a pair of entities occurs in 10 sentences in the test set, and each sentence has 3 features extracted from it, the entity pair will have 30 associated features.Each entity pair in each sentence in the test corpus is run through feature extraction, and the regression classifier predicts a relation name for each entity pair based on the features from all of the sentences in which it appeared.Consider the location-contains relation, imagining that in Freebase we had two instances of this relation: (Virginia, Richmond) and (France, Nantes).As we encountered sentences like ‘Richmond, the capital of Virginia’ and ‘Henry’s Edict of Nantes helped the Protestants of France’ we would extract features from these sentences.Some features would be very useful, such as the features from the Richmond sentence, and some would be less useful, like those from the Nantes sentence.In testing, if we came across a sentence like ‘Vienna, the capital of Austria’, one or more of its features would match those of the Richmond sentence, providing evidence that (Austria, Vienna) belongs to the locationcontains relation.Note that one of the main advantages of our architecture is its ability to combine information from many different mentions of the same relation.Consider the entity pair (Steven Spielberg, Saving Private Ryan) from the following two sentences, as evidence for the film-director relation.The first sentence, while providing evidence for film-director, could instead be evidence for filmwriter orfilm-producer.The second sentence does not mention that Saving Private Ryan is a film, and so could instead be evidence for the CEO relation (consider ‘Robert Mueller directed the FBI’).In isolation, neither of these features is conclusive, but in combination, they are.Our features are based on standard lexical and syntactic features from the literature.Each feature describes how two entities are related in a sentence, using either syntactic or non-syntactic information.Our lexical features describe specific words between and surrounding the two entities in the sentence in which they appear: Each lexical feature consists of the conjunction of all these components.We generate a conjunctive feature for each k E 10, 1, 21.Thus each lexical row in Table 3 represents a single lexical feature.Part-of-speech tags were assigned by a maximum entropy tagger trained on the Penn Treebank, and then simplified into seven categories: nouns, verbs, adverbs, adjectives, numbers, foreign words, and everything else.In an attempt to approximate syntactic features, we also tested variations on our lexical features: (1) omitting all words that are not verbs and (2) omitting all function words.In combination with the other lexical features, they gave a small boost to precision, but not large enough to justify the increased demand on our computational resources.In addition to lexical features we extract a number of features based on syntax.In order to generate these features we parse each sentence with the broad-coverage dependency parser MINIPAR (Lin, 1998).A dependency parse consists of a set of words and chunks (e.g.‘Edwin Hubble’, ‘Missouri’, ‘born’), linked by directional dependencies (e.g.‘pred’, ‘lex-mod’), as in Figure 1.For each sentence we extract a dependency path between each pair of entities.A dependency path consists of a series of dependencies, directions and words/chunks representing a traversal of the parse.Part-of-speech tags are not included in the dependency path.Our syntactic features are similar to those used in Snow et al. (2005).They consist of the conjunction of: A window node is a node connected to one of the two entities and not part of the dependency path.We generate one conjunctive feature for each pair of left and right window nodes, as well as features which omit one or both of them.Thus each syntactic row in Table 3 represents a single syntactic feature.Every feature contains, in addition to the content described above, named entity tags for the two entities.We perform named entity tagging using the Stanford four-class named entity tagger (Finkel et al., 2005).The tagger provides each word with a label from {person, location, organization, miscellaneous, none}.Rather than use each of the above features in the classifier independently, we use only conjunctive features.Each feature consists of the conjunction of several attributes of the sentence, plus the named entity tags.For two features to match, all of their conjuncts must match exactly.This yields low-recall but high-precision features.With a small amount of data, this approach would be problematic, since most features would only be seen once, rendering them useless to the classifier.Since we use large amounts of data, even complex features appear multiple times, allowing our highprecision features to work as intended.Features for a sample sentence are shown in Table 3.For unstructured text we use the Freebase Wikipedia Extraction, a dump of the full text of all Wikipedia articles (not including discussion and user pages) which has been sentence-tokenized by Metaweb Technologies, the developers of Freebase (Metaweb, 2008).This dump consists of approximately 1.8 million articles, with an average of 14.3 sentences per article.The total number of words (counting punctuation marks) is 601,600,703.For our experiments we use about half of the articles: 800,000 for training and 400,000 for testing.We use Wikipedia because it is relatively upto-date, and because its sentences tend to make explicit many facts that might be omitted in newswire.Much of the information in Freebase is derived from tabular data from Wikipedia, meaning that Freebase relations are more likely to appear in sentences in Wikipedia.Each sentence of this unstructured text is dependency parsed by MINIPAR to produce a dependency graph.In preprocessing, consecutive words with the same named entity tag are ‘chunked’, so that Edwin/PERSON Hubble/PERSON becomes [Edwin Hubble]/PERSON.This chunking is restricted by the dependency parse of the sentence, however, in that chunks must be contiguous in the parse (i.e., no chunks across subtrees).This ensures that parse tree structure is preserved, since the parses must be updated to reflect the chunking.For held-out evaluation experiments (see section 7.1), half of the instances of each relation are not used in training, and are later used to compare against newly discovered instances.This means that 900,000 Freebase relation instances are used in training, and 900,000 are held out.These experiments used 800,000 Wikipedia articles in the training phase and 400,000 different articles in the testing phase.For human evaluation experiments, all 1.8 million relation instances are used in training.Again, we use 800,000 Wikipedia articles in the training phase and 400,000 different articles in the testing phase.For all our experiments, we only extract relation instances that do not appear in our training data, i.e., instances that are not already in Freebase.Our system needs negative training data for the purposes of constructing the classifier.Towards this end, we build a feature vector in the training phase for an ‘unrelated’ relation by randomly selecting entity pairs that do not appear in any Freebase relation and extracting features for them.While it is possible that some of these entity pairs on the 102 largest relations we use.Precision for three different feature sets (lexical features, syntactic features, and both) is reported at recall levels from 10 to 100,000.At the 100,000 recall level, we classify most of the instances into three relations: 60% as location-contains, 13% as person-place-of-birth, and 10% as person-nationality. are in fact related but are wrongly omitted from the Freebase data, we expect that on average these false negatives will have a small effect on the performance of the classifier.For performance reasons, we randomly sample 1% of such entity pairs for use as negative training examples.By contrast, in the actual test data, 98.7% of the entity pairs we extract do not possess any of the top 102 relations we consider in Freebase.We use a multi-class logistic classifier optimized using L-BFGS with Gaussian regularization.Our classifier takes as input an entity pair and a feature vector, and returns a relation name and a confidence score based on the probability of the entity pair belonging to that relation.Once all of the entity pairs discovered during testing have been classified, they can be ranked by confidence score and used to generate a list of the n most likely new relation instances.Table 4 shows some high-weight features learned by our system.We discuss the results in the next section.We evaluate labels in two ways: automatically, by holding out part of the Freebase relation data during training, and comparing newly discovered relation instances against this held-out data, and manually, having humans who look at each positively labeled entity pair and mark whether the relation indeed holds between the participants.Both evaluations allow us to calculate the precision of the system for the best N instances.Figure 2 shows the performance of our classifier on held-out Freebase relation data.While held-out evaluation suffers from false negatives, it gives a rough measure of precision without requiring expensive human evaluation, making it useful for parameter setting.At most recall levels, the combination of syntactic and lexical features offers a substantial improvement in precision over either of these feature sets on its own.Human evaluation was performed by evaluators on Amazon’s Mechanical Turk service, shown to be effective for natural language annotation in Snow et al. (2008).We ran three experiments: one using only syntactic features; one using only lexical features; and one using both syntactic and lexical features.For each of the 10 relations that appeared most frequently in our test data (according to our classifier), we took samples from the first 100 and 1000 instances of this relation generated in each experiment, and sent these to Mechanical Turk for results per relation, using stratified samples.‘Average’ gives the mean precision of the 10 relations.Key: Syn = syntactic features only.Lex = lexical features only.We use stratified samples because of the overabundance of location-contains instances among our high-confidence results. human evaluation.Our sample size was 100.Each predicted relation instance was labeled as true or false by between 1 and 3 labelers on Mechanical Turk.We assigned the truth or falsehood of each relation according to the majority vote of the labels; in the case of a tie (one vote each way) we assigned the relation as true or false with equal probability.The evaluation of the syntactic, lexical, and combination of features at a recall of 100 and 1000 instances is presented in Table 5.At a recall of 100 instances, the combination of lexical and syntactic features has the best performance for a majority of the relations, while at a recall level of 1000 instances the results are mixed.No feature set strongly outperforms any of the others across all relations.Our results show that the distant supervision algorithm is able to extract high-precision patterns for a reasonably large number of relations.The held-out results in Figure 2 suggest that the combination of syntactic and lexical features provides better performance than either feature set on its own.In order to understand the role of syntactic features, we examine Table 5, the human evaluation of the most frequent 10 relations.For the topranking 100 instances of each relation, most of the best results use syntactic features, either alone or in combination with lexical features.For the topranking 1000 instances of each relation, the results are more mixed, but syntactic features still helped in most classifications.We then examine those relations for which syntactic features seem to help.For example, syntactic features consistently outperform lexical features for the director-film and writer-film relations.As discussed in section 4, these two relations are particularly ambiguous, suggesting that syntactic features may help tease apart difficult relations.Perhaps more telling, we noticed many examples with a long string of words between the director and the film: Back Street is a 1932 film made by Universal Pictures, directed by John M. Stahl, and produced by Carl Laemmle Jr. Sentences like this have very long (and thus rare) lexical features, but relatively short dependency paths.Syntactic features can more easily abstract from the syntactic modifiers that comprise the extraneous parts of these strings.Our results thus suggest that syntactic features are indeed useful in distantly supervised information extraction, and that the benefit of syntax occurs in cases where the individual patterns are particularly ambiguous, and where they are nearby in the dependency structure but distant in terms of words.It remains for future work to see whether simpler, chunk-based syntactic features might be able to capture enough of this gain without the overhead of full parsing, and whether coreference resolution could improve performance.We would like to acknowledge Sarah Spikes for her help in developing the relation extraction system, Christopher Manning and Mihai Surdeanu for their invaluable advice, and Fuliang Weng and Baoshi Yan for their guidance.Our research was partially funded by the NSF via award IIS0811974 and by Robert Bosch LLC.
Intelligent Selection of Language Model Training DataWe address the problem of selecting nondomain-specific language model training data to build auxiliary language models for use in tasks such as machine translation.Our approach is based on comparing the cross-entropy, according to domainspecific and non-domain-specifc language models, for each sentence of the text source used to produce the latter language model.We show that this produces better language models, trained on less data, than both random data selection and two other previously proposed methods.Statistical N-gram language models are widely used in applications that produce natural-language text as output, particularly speech recognition and machine translation.It seems to be a universal truth that output quality can always be improved by using more language model training data, but only if the training data is reasonably well-matched to the desired output.This presents a problem, because in virtually any particular application the amount of in-domain data is limited.Thus it has become standard practice to combine in-domain data with other data, either by combining N-gram counts from in-domain and other data (usually weighting the counts in some way), or building separate language models from different data sources, interpolating the language model probabilities either linearly or log-linearly.Log-linear interpolation is particularly popular in statistical machine translation (e.g., Brants et al., 2007), because the interpolation weights can easily be discriminatively trained to optimize an end-to-end translation objective function (such as BLEU) by making the log probability according to each language model a separate feature function in the overall translation model.The normal practice when using multiple languages models in machine translation seems to be to train models on as much data as feasible from each source, and to depend on feature weight optimization to down-weight the impact of data that is less well-matched to the translation application.In this paper, however, we show that for a data source that is not entirely in-domain, we can improve the match between the language model from that data source and the desired application output by intelligently selecting a subset of the available data as language model training data.This not only produces a language model better matched to the domain of interest (as measured in terms of perplexity on held-out in-domain data), but it reduces the computational resources needed to exploit a large amount of non-domain-specific data, since the resources needed to filter a large amount of data are much less (especially in terms of memory) than those required to build a language model from all the data.Our approach to the problem assumes that we have enough in-domain data to train a reasonable indomain language model, which we then use to help score text segments from other data sources, and we select segments based on a score cutoff optimized on held-out in-domain data.We are aware of two comparable previous approaches.Lin et al. (1997) and Gao et al.(2002) both used a method similar to ours, in which the metric used to score text segments is their perplexity according to the in-domain language model.The candidate text segments with perplexity less than some threshold are selected.The second previous approach does not explicitly make use of an in-domain language model, but is still applicable to our scenario.Klakow (2000) estimates a unigram language model from the entire non-domain-specific corpus to be selected from, and scores each candidate text segment from that corpus by the change in the log likelihood of the in-domain data according to the unigram model, if that segment were removed from the corpus used to estimate the unigram model.Those segments whose removal would decrease the log likelihood of the in-domain data more than some threshold are selected.Our method is a fairly simple variant of scoring by perplexity according to an in-domain language model.First, note that selecting segments based on a perplexity threshold is equivalent to selecting based on a cross-entropy threshold.Perplexity and cross-entropy are monotonically related, since the perplexity of a string s according to a model M is simply bHmW, where HM(s) is the cross-entropy of s according to M and b is the base with respect to which the cross-entropy is measured (e.g., bits or nats).However, instead of scoring text segments by perplexity or cross-entropy according to the in-domain language model, we score them by the difference of the cross-entropy of a text segment according to the in-domain language model and the cross-entropy of the text segment according to a language model trained on a random sample of the data source from which the text segment is drawn.To state this formally, let I be an in-domain data set and N be a non-domain-specific (or otherwise not entirely in-domain) data set.Let HI(s) be the per-word cross-entropy, according to a language model trained on I, of a text segment s drawn from N. Let HN(s) be the per-word cross-entropy of s according to a language model trained on a random sample of N. We partition N into text segments (e.g., sentences), and score the segments according to HI(s) − HN(s), selecting all text segments whose score is less than a threshold T. This method can be justified by reasoning simliar to that used to derive methods for training binary text classifiers without labeled negative examples (Denis et al., 2002; Elkin and Noto, 2008).Let us imagine that our non-domainspecific corpus N contains an in-domain subcorpus NI, drawn from the same distribution as our in-domain corpus I.Since NI is statistically just like our in-domain data I, it would seem to be a good candidate for the data that we want to extract from N. By a simple variant of Bayes rule, the probability P(NI|s, N) of a text segment s, drawn randomly from N, being in NI is given by Since NI is a subset of N, P(s|NI, N) = P(s|NI), and by our assumption about the relationship of I and NI, P(s|NI) = P(s|I).Hence, If we could estimate all the probabilities in the right-hand side of this equation, we could use it to select text segments that have a high probability of being in NI.We can estimate P(s|I) and P(s|N) by training language models on I and a sample of N, respectively.That leaves us only P(NI|N), to estimate, but we really don’t care what P(NI|N) is, because knowing that would still leave us wondering what threshold to set on P (NI|s, N).We don’t care about classification accuracy; we care only about the quality of the resulting language model, so we might as well just attempt to find a threshold on P(s|I)/P(s|N) that optimizes the fit of the resulting language model to held-out indomain data.Equivalently, we can work in the log domain with the quantity log(P(s|I)) − log(P(s|N)).This gets us very close to working with the difference in cross-entropies, because HI(s)−HN(s) is just a length-normalized version of log(P(s|I)) − log(P(s|N)), with the sign reversed.The reason that we need to normalize for length is that the value of log(P(s|I)) − log(P(s|N)) tends to correlate very strongly with text segment length.If the candidate text segments vary greatly in length—e.g., if we partition N into sentences— this correlation can be a serious problem.We estimated this effect on a 1000-sentence sample of our experimental data described below, and found the correlation between sentence log probability difference and sentence length to be r = −0.92, while the cross-entropy difference was almost uncorrelated with sentence length (r = 0.04).Hence, using sentence probability ratios or log probability differences as our scoring function would result in selecting disproportionately very short sentences.We tested this in an experiment not described here in detail, and found it not to be significantly better as a selection criterion than random selection.We have empirically evaluated our proposed method for selecting data from a non-domainspecific source to model text in a specific domain.For the in-domain corpus, we chose the English side of the English-French parallel text from release v5 of the Europarl corpus (Koehn, 2005).This consists of proceedings of the European Parliament from 1999 through 2009.We used the text from 1999 through 2008 as in-domain training data, and we used the first 2000 sentences from January 2009 as test data.For the nondomain-specific corpus, we used the LDC English Gigaword Third Edition (LDC Catalog No.: LDC2007T07).We used a simple tokenization scheme on all data, splitting on white space and on boundaries between alphanumeric and nonalphanumeric (e.g., punctuation) characters.With this tokenization, the sizes of our data sets in terms of sentences and tokens are shown in Table 1.The token counts include added end-of-sentence tokens.To implement our data selection method we required one language model trained on the Europarl training data and one trained on the Gigaword data.To make these language models comparable, and to show the feasibility of optimizing the fit to the in-domain data without training a model on the entire Gigaword corpus, we trained the Gigaword language model for data selection on a random sample of the Gigaword corpus of a similar size to that of the Europarl training data: 1,874,051 sentences, 48,459,945 tokens.To further increase the comparability of these Europarl and Gigaword language models, we restricted the vocabulary of both models to the tokens appearing at least twice in the Europarl training data, treating all other tokens as instances of <UNK>.With this vocabulary, 4-gram language models were trained on both the Europarl training data and the Gigaword random sample using backoff absolute discounting (Ney et al. 1994), with a discount of 0.7 used for all N-gram lengths.The discounted probability mass at the unigram level was added to the probability of <UNK>.A count cutoff of 2 occurrences was applied to the trigrams and 4-grams in estimating these models.We computed the cross-entropy of each sentence in the Gigaword corpus according to both models, and scored each sentence by the difference in cross-entropy, HEp(s)−HG,,,(s).We then selected subsets of the Gigaword data corresponding to 8 cutoff points in the cross-entropy difference scores, and trained 4-gram models (again using absolute discounting with a discount of 0.7) on each of these subsets and on the full Gigaword corpus.These language models were estimated without restricting the vocabulary or applying count cutoffs, but the only parameters computed were those needed to determine the perplexity of the held-out Europarl test set, which saves a substantial amount of computation in determining the optimal selection threshold.We compared our selection method to three other methods.As a baseline, we trained language models on random subsets of the Gigaword corpus of approximately equal size to the data sets produced by the cutoffs we selected for the cross-entropy difference scores.Next, we scored all the Gigaword sentences by the cross-entropy according to the Europarl-trained model alone.As we noted above, this is equivalent to the indomain perplexity scoring method used by Lin et al. (1997) and Gao et al.(2002).Finally, we implemented Klakow’s (2000) method, scoring each Gigaword sentence by removing it from the Gigaword corpus and computing the difference in the log likelihood of the Europarl corpus according to unigram models trained on the Gigaword corpus with and without that sentence.With the latter two methods, we chose cutoff points in the resulting scores to produce data sets approximately equal in size to those obtained using our selection method.For all four selection methods, plots of test set perplexity vs. the number of training data tokens selected are displayed in Figure 1.(Note that the training data token counts are displayed on a logarithmic scale.)The test set perplexity for the language model trained on the full Gigaword corpus is 135.As we might expect, reducing training data by random sampling always increases perplexity.Selecting Gigaword sentences by their cross-entropy according to the Europarl-trained model is effective in reducing both test set perplexity and training corpus size, with an optimum perplexity of 124, obtained with a model built from 36% of the Gigaword corpus.Klakow’s method is even more effective, with an optimum perplexity of 111, obtained with a model built from 21% of the Gigaword corpus.The cross-entropy difference selection method, however, is yet more effective, with an optimum perplexity of 101, obtained with a model built from less than 7% of the Gigaword corpus.The comparisons implied by Figure 1, however, are only approximate, because each perplexity (even along the same curve) is computed with respect to a different vocabulary, resulting in a different out-of-vocabulary (OOV) rate.OOV tokens in the test data are excluded from the perplexity computation, so the perplexity measurements are not strictly comparable.Out of the 55566 test set tokens, the number of OOV tokens ranges from 418 (0.75%), for the smallest training set based on in-domain crossentropy scoring, to 20 (0.03%), for training on the full Gigaword corpus.If we consider only the training sets that appear to produce the lowest perplexity for each selection method, however, the spread of OOV counts is much narrower, ranging 53 (0.10%) for best training set based on crossentropy difference scoring, to 20 (0.03%), for random selection.To control for the difference in vocabulary, we estimated a modified 4-gram language model for each selection method (other than random selection) using the training set that appeared to produce the lowest perplexity for that selection method in our initial experiments.In the modified language models, the unigram model based on the selected training set is smoothed by absolute discounting, and backed-off to an unsmoothed unigram model based on the full Gigaword corpus.This produces language models that are normalized over the same vocabulary as a model trained on the full Gigaword corpus; thus the test set has the same OOVs for each model.Test set perplexity for each of these modifed language models is compared to that of the original version of the model in Table 2.It can be seen that adjusting the vocabulary in this way, so that all models are based on the same vocabulary, yields only very small changes in the measured test-set perplexity, and these differences are much smaller than the differences between the different selection methods, whichever way the vocabulary of the language models is determined.The cross-entropy difference selection method introduced here seems to produce language models that are both a better match to texts in a restricted domain, and require less data for training, than any of the other data selection methods tested.This study is preliminary, however, in that we have not yet shown improved end-to-end task performance applying this approach, such as improved BLEU scores in a machine translation task.However, we believe there is reason to be optimistic about this.When a language model trained on non-domain-specific data is used in a statistical translation model as a separate feature function (as is often the case), lower perplexity on indomain target language test data derived from reference translations corresponds directly to assigning higher language model feature scores to those reference translations, which should in turn lead to translation system output that matches reference translations better.
A Centering Approach To PronounsIn this paper we present a formalization of the centering approach to modeling attentional structure in discourse and use it as the basis for an algorithm to track discourse context and bind pronouns.As described in [GJW86], the process of centering attention on entities in the discourse gives rise to the intersentential states of, retaining shiftpropose an extension to these states which handles some additional cases of multiple ambiguous pronouns.The algorithm has been implemented in an HPSG natural language system which serves as the interface to a database query application.In the approach to discourse structure developed in [Sid83] and [GJW86], a discourse exhibits both global and local coherence.On this view, a key element of local coherence is centering, a system of rules and constraints that govern the relationship between what the discourse is about and some of the linguistic choices made by the discourse participants, e.g. choice of grammatical function, syntactic structure, and type of referring expression (proper noun, definite or indefinite description, reflexive or personal pronoun, etc.).Pronominalization in particular serves to focus attention on what is being talked about; inappropriate use or failure to use pronouns causes communication to be less fluent.For instance, it takes longer for hearers to process a pronominalized noun phrase that is not in focus than one that is, while it takes longer to process a non-pronominalized noun phrase that is in focus than one that is not [G ui85].The [G3W86] centering model is based on the following assumptions.A discourse segment consists of a sequence of utterances U1, , Um.With each utterance Un is associated a list of forward-looking centers, C f(Un), consisting of those discourse entities that are directly realized or realized' by linguistic expressions in the utterance.Ranking of an entity on this list corresponds roughly to the likelihood that it will be the primary focus of subsequent discourse; the first entity on this list is the preferred center, Cp(Un).Un actually centers, or is &quot;about&quot;, only one entity at a time, the backward-looking center, Cb(Un).The backward center is a confirmation of an entity that has already been introduced into the discourse; more specifically, it must be realized in the immediately preceding utterance, Un_.1.There are several distinct types of transitions from one utterance to the next.The typology of transitions is based on two factors: whether or not the center of attention, Cb, is the same from Un_1 to Un, and whether or not this entity coincides with the preferred center of Un.Definitions of these transition types appear in figure 1.These transitions describe how utterances are linked together in a coherent local segment of discourse.If a speaker has a number of propositions to express, one very simple way to do this coherently is to express all the propositions about a given entity (continuing) before introducing a related entity As is evident in constraint 3, ranking of the items on the forward center list, Cf, is crucial.We rank the items in Cf by obliqueness of grammatical relation of the subcategorized functions of the main verb: that is, first the subject, object, and object2, followed by other subcategorized functions, and finally, adjuncts.This captures the idea in [GJW86] that subjecthood contributes strongly to the priority of an item on the Cf list.(retaining) and then shifting the center to this new entity.See figure 2.Retaining may be a way to signal an intention to shift.While we do not claim that speakers really behave in such an orderly fashion, an algorithm that expects this kind of behavior is more successful than those which depend solely on recency or parallelism of grammatical function.The interaction of centering with global focusing mechanisms and with other factors such as intentional structure, semantic selectional restrictions, verb tense and aspect, modality, intonation and pitch accent are topics for further research.Note that these transitions are more specific than focus movement as described in [Sid83].The extension we propose makes them more specific still.Note also that the Cb of [GJW86] corresponds roughly to Sidner's discourse focus and the Cf to her potential foci.The formal system of constraints and rules for centering, as we have interpreted them from [GJW86], are as follows.For each Un in U1, , CONTINUING... Un+i: Carl works at HP on the Natural Language We are aware that this ranking usually coincides with surface constituent order in English.It would be of interest to examine data from languages with relatively freer constituent order (e.g.German) to determine the influence of constituent order upon centering when the grammatical functions are held constant.In addition, languages that provide an identifiable topic function (e.g.Japanese) suggest that topic takes precedence over subject.The part of the HPSG system that uses the centering algorithm for pronoun binding is called the pragmatics processor.It interacts with another module called the semantics processor, which computes representations of intrasentential anaphoric relations, (among other things).The semantics processor has access to information such as the surface syntactic structure of the utterance.It provides the pragmatics processor with representations which include of a set of reference markers.Each reference marker is contraindexed2 with expressions with which it cannot co-specify3.Reference markers also carry information about agreement and grammatical function.Each pronominal reference marker has a unique index from A1, , An and is displayed in the figures in the form [POLLARD:Al], where POLLARD is the semantic representation of the co-specifier.For non-pronominal reference markers the surface string is used as the index.Indices for indefinites are generated from X1, .. •The constraints proposed by [GJW86] fail in certain examples like the following (read with pronouns destressed): Brennan drives an Alfa Romeo.She drives too fast.Friedman races her on weekends.She often beats her.This example is characterized by its multiple ambiguous pronouns and by the fact that the final utterance achieves a shift (see figure 4).A shift is inevitable because of constraint 3, which states that the Cb(Un) must equal the Cp(Un_i) (since the Cp(U,i) is directly realized by the subject of Un, &quot;Friedman&quot;).However the constraints and rules from [GJW86] would fail to make a choice here between the co-specification possibilities for the pronouns in Un.Given that the transition is a shift, there seem to be more and less coherent ways to shift.Note that the three items being examined in order to characterize the transition between each pair of anchor?' are theCb of Un_1, the Cb of Un, and the Cp of U.By [GJW86] a shift occurs whenever successive Cb's are not the same.This definition of shifting does not consider whether the Cb of Un and the Cp of Un are equal.It seems that the status of the Cp of Un should be as important in this case as it is in determining the retaining/continuing distinction.Therefore, we propose the following extension which handles some additional cases containing multiple ambiguous pronouns: we have extended rule 2 so that there are two kinds of shifts.A transition for Un is ranked more highly if Cb(Un) = CP(U); this state we call shifting-1 and it represents a more coherent way to shift.The preferred ranking is continuing >- retaining shifting-1 >- shifting (see figure 3).This extension enables us to successfully bind the &quot;she&quot; in the final utterance of the example in figure 4 to &quot;Friedman.&quot; The appendix illustrates the application of the algorithm to figure 4.Kameyama [Kam86] has proposed another extension to the [G3W86] theory — a property-sharing constraint which attempts to enforce a parallellism between entities in successive utterances.She considers two properties: SUB/ and IDENT .With her extension, subject pronouns prefer subject antecedents and non-subject pronouns prefer non-subject antecedents.However, structural parallelism is a consequence of our ordering the Cf list by grammatical function and the preference for continuing over retaining.Furthermore, the constraints suggested in [GJW86] succeed in many cases without invoking an independent structural parallelism constraint, due to the distinction between continuing and retaining, which Kameyama fails to consider.Her example which we reproduce in figure 5 can also be accounted for using the continuing/retaining distinctions.The third utterance in this example has two interpretations which are both consistent with the centering rules and constraints.Because of rule 2, the interpretation in figure 5 is preferred over the one in figure 6.There are three basic phases to this algorithm.First the proposed anchors are constructed, then they are filtered, and finally, they are classified and ranked.The proposed anchors represent all the cospecification relationships available for this utterance.Each step is discussed and illustrated in figure 7.It would be possible to classify and rank the proposed anchors before filtering them without any other changes to the algorithm.In fact, using this strategy This filter doesn't eliminate any of the proposed anchors in this example.Even though [A4] and [A5] are contraindexed we have not proposed the same co-specifier due to agreement.This filter eliminates proposed anchors ii, iii, iv.This filter doesn't eliminate any of the proposed anchors.The proposed Cb was realized as a pronoun.Anchor i is classified as a retention based on the transition state definition. one could see if the highest ranked proposal passed all the filters, or if the next highest did, etc.The three filters in the filtering phase may be done in parallel.The example we use to illustrate the algorithm is in figure 2.The goal of the current algorithm design was conceptual clarity rather than efficiency.The hope is that the structure provided will allow easy addition of further constraints and preferences.It would be simple to change the control structure of the algorithm so that it first proposed all the continuing or retaining anchors and then the shifting ones, thus avoiding a precomputation of all possible anchors.[GJW86] states that a realization may contribute more than one entity to the Cf(U).This is true in cases when a partially specified semantic description is consistent with more than one interpretation.There is no need to enumerate explicitly all the possible interpretations when constructing possible Cf(U)'s6, as long as the associated semantic theory allows partially specified interpretations.This also holds for entities not directly realized in an utterance.On our view, after referring to &quot;a house&quot; in If, a reference to &quot;the door&quot; in Un4.1 might be gotten via inference from the representation for &quot;a house&quot; in Cf( U,,).Thus when the proposed anchors are constructed there is no possibility of having an infinite number of potential Cf's for an utterance of finite length.Another question is whether the preference ordering of transitions in constraint 3 should always be the same.For some examples, particularly where Un contains a single pronoun and Un...1 is a retention, some informants seem to have a preference for shifting, whereas the centering algorithm chooses a continuation (see figure 8).Many of our informants have no strong preference as to the co-specification of the unstressed &quot;She&quot; in Un+4.Speakers can avoid ambiguity by stressing a pronoun with respect to its phonological environment.A computational system for understanding may need to explicitly acknowledge this ambiguity.A computational system for generation would try to plan a retention as a signal of an impending shift, so that after a retention, a shift would be preferred rather than a continuation.Of course the local approach described here does not provide all the necessary information for interpreting pronouns; constraints are also imposed by world knowledge, pragmatics, semantics and phonology.There are other interesting questions concerning the centering algorithm.How should the centering algorithm interact with an inferencing mechanism?Should it make choices when there is more than one proposed anchor with the same ranking?In a database query system, how should answers be incorporated into the discourse model?How does centering interact with a treatment of definite/indefinite NP's and quantifiers?We are exploring ideas for these and other extensions to the centering approach for modeling reference in local discourse.We would like to thank the following people for their help and insight: Hewlett Packard Lab's Natural Language group, CSLI's DIA group, Candy Sidner, Dan Flickinger, Mark Gawron, John Nerbonne, Tom Wasow, Barry Arons, Martha Pollack, Aravind Joshi, two anonymous referees, and especially Barbara Grosz.This illustrates the extension in the same detail as the example we used in the algorithm.The numbering here corresponds to the numbered steps in the algorithm figure 7.The example is the last utterance from figure 4.EXAMPLE: She often beats her.(a) Filter by contraindices.Anchors i, iv, v, viii, ix, xii, xiii, xvi are eliminated since [A9] and [A10] are contraindexed.
Automatic Acquisition Of Subcategorization Frames From Untagged TextThis paper describes an implemented program that takes a raw, untagged text corpus as its only input (no open-class dictionary) and generates a partial list of verbs occurring in the text and the subcategorization frames (SFs) in which they occur.Verbs are detected by a novel technique based on the Case Filter of Rouvret and Vergnaud (1980).The completeness of the output list increases monotonically with the total number of occurrences of each verb in the corpus.False positive rates are one to three percent of observations.Five SFs are currently detected and more are planned.Ultimately, I expect to provide a large SF dictionary to the NLP community and to train dictionaries for specific corpora.This paper describes an implemented program that takes an untagged text corpus and generates a partial list of verbs occurring in it and the subcategorization frames (SFs) in which they occur.So far, it detects the five SFs shown in Table 1.SF Good Example Bad Example Description direct object greet them *arrive them direct object tell him he's a *hope him he's a Si clause fool fool direct object want him to *hope him to Si infinitive attend attend clause know I'll attend *want I'll attend infinitive hope to attend *greet to attend The SF acquisition program has been tested on a corpus of 2.6 million words of the Wall Street Journal (kindly provided by the Penn Tree Bank project).On this corpus, it makes 5101 observations about 2258 orthographically distinct verbs.False positive rates vary from one to three percent of observations, depending on the SF.Accurate parsing requires knowing the subcategorization frames of verbs, as shown by (1).(I) a. I expected [Np the man who smoked Nil to eat ice-cream b. I doubted [NP the man who liked to eat ice-cream NI.]Current high-coverage parsers tend to use either custom, hand-generated lists of subcategorization frames (e.g., Hindle, 1983), or published, handgenerated lists like the Oxford Advanced Learner's Dictionary of Contemporary English, Hornby and Covey (1973) (e.g., DeMarcken, 1990).In either case, such lists are expensive to build and to maintain in the face of evolving usage.In addition, they tend not to include rare usages or specialized vocabularies like financial or military jargon.Further, they are often incomplete in arbitrary ways.For example, Webster's Ninth New Collegiate Dictionary lists the sense of strike meaning &quot;to occur to&quot;, as in &quot;it struck him that... &quot; , but it does not list that same sense of hit.(My program discovered both.)The initial priorities in this research were: Efficient use of the available text was not a high priority, since it was felt that plenty of text was available even for an inefficient learner, assuming sufficient speed to make use of it.These priorities had a substantial influence on the approach taken.They are evaluated in retrospect in Section 4.The first step in finding a subcategorization frame is finding a verb.Because of widespread and productive noun/verb ambiguity, dictionaries are not much use — they do not reliably exclude the possibility of lexical ambiguity.Even if they did, a program that could only learn SFs for unambiguous verbs would be of limited value.Statistical disambiguators make dictionaries more useful, but they have a fairly high error rate, and degrade in the presence of many unfamiliar words.Further, it is often difficult to understand where the error is coming from or how to correct it.So finding verbs poses a serious challenge for the design of an accurate, general-purpose algorithm for detecting SFs.In fact, finding main verbs is more difficult than it might seem.One problem is distinguishing participles from adjectives and nouns, as shown below.In each case the main verb is have or be in a context where most parsers (and statistical disambiguators) would mistake it for an auxiliary and mistake the following word for a participial main verb.A second challenge to accuracy is determining which verb to associate a given complement with.Paradoxically, example (1) shows that in general it isn't possible to do this without already knowing the SF.One obvious strategy would be to wait for sentences where there is only one candidate verb; unfortunately, it is very difficult to know for certain how many verbs occur in a sentence.Finding some of the verbs in a text reliably is hard enough; finding all of them reliably is well beyond the scope of this work.Finally, any system applied to real input, no matter how carefully designed, will occasionally make errors in finding the verb and determining its subcategorization frame.The more times a given verb appears in the corpus, the more likely it is that one of those occurrences will cause an erroneous judgment.For that reason any learning system that gets only positive examples and makes a permanent judgment on a single example will always degrade as the number of occurrences increases.In fact, making a judgment based on any fixed number of examples with any finite error rate will always lead to degradation with corpussize.A better approach is to require a fixed percentage of the total occurrences of any given verb to appear with a given SF before concluding that random error is not responsible for these observations.Unfortunately, determining the cutoff percentage requires human intervention and sampling error makes classification unstable for verbs with few occurrences in the input.The sampling error can be dealt with (Brent, 1991) but predetermined cutoff percentages still require eye-balling the data.Thus robust, unsupervised judgments in the face of error pose the third challenge to developing an accurate learning system.The architecture of the system, and that of this paper, directly reflects the three challenges described above.The system consists of three modules: genuinely associated with a given SF, or whether instead its apparent occurrences in that SF are due to error.This is done using statistical models of the frequency distributions.The following two sections describe and evaluate the verb detection module and the SF detection module, respectively; the decision module, which is still being refined, will be described in a subsequent paper.The final two sections provide a brief comparison to related work and draw conclusions.The technique I developed for finding verbs is based on the Case Filter of Rouvret and Vergnaud (1980).The Case Filter is a proposed rule of granimar which, as it applies to English, says that every noun-phrase must appear either immediately to the left of a tensed verb, immediately to the right of a preposition, or immediately to the right of a main verb.Adverbs and adverbial phrases (including days and dates) are ignored for the purposes of case adjacency.A noun-phrase that satisfies the Case Filter is said to &quot;get case&quot; or &quot;have case&quot;, while one that violates it is said to &quot;lack case&quot;.The program judges an open-class word to be a main verb if it is adjacent to a pronoun or proper name that would otherwise lack case.Such a pronoun or proper name is either the subject or the direct object of the verb.Other noun phrases are not used because it is too difficult to determine their right boundaries accurately.The two criteria for evaluating the performance of the main-verb detection technique are efficiency and accuracy.Both were measured using a 2.6 million word corpus for which the Penn Treebank project provides hand-verified tags.Efficiency of verb detection was assessed by running the SF detection module in the normal mode, where verbs were detected using the Case Filter technique, and then running it again with the Penn Tags substituted for the verb detection module.The results are shown in Table 2.Note the five SFs, as tested on 2.6 million words of the Wall Street Journal and controlled by the Penn Treebank's hand-verified tagging the substantial variation among the SFs: for the SFs &quot;direct object&quot; and &quot;direct object & clause&quot; efficiency is roughly 40% and 25%, respectively; for &quot;direct object & infinitive&quot; it drops to about 8%; and for the intransitive SFs it is under 5%.The reason that the transitive SFs fare better is that the direct object gets case from the preceding verb and hence reveals its presence — intransitive verbs are harder to find.Likewise, clauses fare better than infinitives because their subjects get case from the main verb and hence reveal it, whereas infinitives lack overt subjects.Another obvious factor is that, for every SF listed above except &quot;direct object&quot; two verbs need to be found — the matrix verb and the complement verb — if either one is not detected then no observation is recorded.Accuracy was measured by looking at the Penn tag for every word that the system judged to be a verb.Of approximately 5000 verb tokens found by the Case Filter technique, there were 28 disagreements with the hand-verified tags.My program was right in 8 of these cases and wrong in 20, for a 0.24% error-rate beyond the rate using hand-verified tags.Typical disagreements in which my system was right involved verbs that are ambiguous with much more frequent nouns, like mold in &quot;The Soviet Communist Party has the power to shape corporate development and mold it into a body dependent upon it .&quot; There were several systematic constructions in which the Penn tags were right and my system was wrong, including constructions like &quot;We consumers are...&quot; and pseudo-clefts like &quot;what you then do is you make them think.... (These examples are actual text from the Penn corpus.)The extraordinary accuracy of verb detection — within a tiny fraction of the rate achieved by trained human taggers — and it's relatively low efficiency are consistent with the priorities laid out in Section 1.2.The obvious approach to finding SFs like &quot;V NP to V&quot; and &quot;V to V&quot; is to look for occurrences of just those patterns in the training corpus; but the obvious approach fails to address the attachment problem illustrated by example (1) above.The solution is based on the following insights: Rather than take the obvious approach of looking for &quot;V NP to V&quot;, my approach is to wait for clear cases like &quot;V PRONOUN to V&quot;.The advantages can be seen by contrasting (3) with (1).More generally, the system recognizes linguistic structure using a small finite-state grammar that describes only that fragment of English that is most useful for recognizing SFs.The grammar relies exclusively on closed-class lexical items such as pronouns, prepositions, determiners, and auxiliary verbs.The grammar for detecting SFs needs to distinguish three types of complements: direct objects, infinitives, and clauses.The grammars for each of these are presented in Figure 1.Any open-class word judged to be a verb (see Section 2) and followed immediately by matches for <DO>, <clause>, <infinitive>, <DO> <clause>, or <D0><inf> is assigned the corresponding SF.Any word ending in &quot;Iy&quot; or belonging to a list of 25 irregular adverbs is ignored for purposes of adjacency.The notation &quot;?&quot; follows optional expressions.The category previously-noted-uninflected-verb is special in that it is not fixed in advance — open-class nonadverbs are added to it when they occur following an unambiguous modal.'This is the only case in which the program makes use of earlier decisions — literally bootstrapping.Note, however, that ambiguity is possible between mass nouns and uninflected verbs, as in to fish.Like the verb detection algorithm, the SF detection algorithm is evaluated in terms of efficiency and accuracy.The most useful estimate of efficiency is simply the density of observations in the corpus, shown in the first column of Table 3.The 'If there were room to store an unlimited number of uninflected verbs for later reference then the grammar formalism would not be finite-state.In fact, a fixed amount of storage, sufficient to store all the verbs in the language, is allocated.This question is purely academic, however — a hash-table gives constant-time average performance. column of Table 3.2 The most common source of error was purpose adjuncts, as in &quot;John quit to pursue a career in finance,&quot; which comes from omitting the in order from &quot;John quit in order to pursue a career in finance.&quot; These purpose adjuncts were mistaken for infinitival complements.The other errors were more sporadic in nature, many coming from unusual extrapositions or other relatively rare phenomena.Once again, the high accuracy and low efficiency are consistent with the priorities of Section 1.2.The throughput rate is currently about ten-thousand words per second on a Sparcstation 2, which is also consistent with the initial priorities.Furthermore, at ten-thousand words per second the current density of observations is not problematic.Interest in extracting lexical and especially collocational information from text has risen dramatically in the last two years, as sufficiently large corpora and sufficiently cheap computation have become available.Three recent papers in this area are Church and Hanks (1990), Hindle (1990), and Smadja and McKeown (1990).The latter two are concerned exclusively with collocation relations between open-class words and not with grammatical properties.Church is also interested primarily in open-class collocations, but he does discuss verbs that tend to be followed by infinitives within his mutual information framework.Mutual information, as applied by Church, is a measure of the tendency of two items to appear near one-another — their observed frequency in nearby positions is divided by the expectation of that frequency if their positions were random and independent.To measure the tendency of a verb to be followed within a few words by an infinitive, Church uses his statistical disambiguator 'Error rates computed by hand verification of 200 examples for each SF using the tagged mode.These are estimated independently of the error rates for verb detection.(Church, 1988) to distinguish between to as an infinitive marker and to as a preposition.Then he measures the mutual information between occurrences of the verb and occurrences of infinitives following within a certain number of words.Unlike our system, Church's approach does not aim to decide whether or not a verb occurs with an infinitival complement — example (1) showed that being followed by an infinitive is not the same as taking an infinitival complement.It might be interesting to try building a verb categorization scheme based on Church's mutual information measure, but to the best of our knowledge no such work has been reported.The ultimate goal of this work is to provide the NLP community with a substantially complete, automatically updated dictionary of sub categorization frames.The methods described above solve several important problems that had stood in the way of that goal.Moreover, the results obtained with those methods are quite encouraging.Nonetheless, two obvious barriers still stand on the path to a fully automated SF dictionary: a decision algorithm that can handle random error, and techniques for detecting many more types of SFs.Algorithms are currently being developed to resolve raw SF observations into genuine lexical properties and random error.The idea is to automatically generate statistical models of the sources of error.For example, purpose adjuncts like &quot;John quit to pursue a career in finance&quot; are quite rare, accounting for only two percent of the apparent infinitival complements.Furthermore, they are distributed across a much larger set of matrix verbs than the true infinitival complements, so any given verb should occur with a purpose adjunct extremely rarely.In a histogram sorting verbs by their apparent frequency of occurrence with infinitival complements, those that in fact have appeared with purpose adjuncts and not true subcategorized infinitives will be clustered at the low frequencies.The distributions of such clusters can be modeled automatically and the models used for identifying false positives.The second requirement for automatically generating a full-scale dictionary is the ability to detect many more types of SFs.SFs involving certain prepositional phrases are particularly challenging.For example, while purpose adjuncts (mistaken for infinitival complements) are relatively rare, instrumental adjuncts as in &quot;John hit the naiL with a hammer&quot; are more common.The problem, of course, is how to distinguish them from genuine, subcategorized PPs headed by with, as in &quot;John sprayed the lawn with distilled water&quot;.The hope is that a frequency analysis like the one planned for purpose adjuncts will work here as well, but how successful it will be, and if successful how large a sample size it will require, remain to be seen.The question of sample size leads back to an evaluation of the initial priorities, which favored simplicity, speed, and accuracy, over efficient use of the corpus.There are various ways in which the high-priority criteria can be traded off against efficiency.For example, consider (2c): one might expect that the overwhelming majority of occurrences of &quot;is V-ing&quot; are genuine progressives, while a tiny minority are cases copula.One might also expect that the occasional copula constructions are not concentrated around any one present participle but rather distributed randomly among a large population.If those expectations are true then a frequency-modeling mechanism like the one being developed for adjuncts ought to prevent the mistaken copula from doing any harm.In that case it might be worthwhile to admit &quot;is V-ing&quot;, where V is known to be a (possibly ambiguous) verb root, as a verb, independent of the Case Filter mechanism.Thanks to Don Hindle, Lila Gleitman, and Jane Grimshaw for useful and encouraging conversations.Thanks also to Mark Liberman, Mitch Marcus and the Penn Treebank project at the University of Pennsylvania for supplying tagged text.This work was supported in part by National Science Foundation grant DCR-85552543 under a Presidential Young Investigator Award to Professor Robert C. Berwick.
Towards The Automatic Identification Of Adjectival Scales: Clustering Adjectives According To MeaningIn this paper we present a method to group adjectives according to their meaning, as a first step towards the automatic identification of adjectival scales.We discuss the properties of adjectival scales and of groups of semantically related adjectives and how they imply sources of linguistic knowledge in text corpora.We describe how our system exploits this linguistic knowledge to compute a measure of similarity between two adjectives, using statistical techniques and without having access to any semantic information about the adjectives.We also show how a clustering algorithm can use these similarities to produce the groups of adjectives, and we present results produced by our system for a sample set of adjectives.We conclude by presenting evaluation methods for the task at hand, and analyzing the significance of the results obtained.As natural language processing systems become more oriented towards solving real-world problems like machine translation or spoken language understanding in a limited domain, their need for access to vast amounts of knowledge increases.While a model of the general rules of the language at various levels (morphological, syntactic, etc.) can be hand-encoded, knowledge which pertains to each specific word is harder to encode manually, if only because of the size of the lexicon.Most systems currently rely on human linguists or lexicographers who compile lexicon entries by hand.This approach requires significant amounts of time and effort for expanding the system's lexicon.Furthermore, if the compiled information depends in any way on the domain of the application, the acquisition of lexical knowledge must be repeated whenever the system is transported to another domain.For systems which need access to large lexicons, some form of at least partial automation of the lexical knowledge acquisition phase is needed.One type of lexical knowledge which is useful for many natural language (NL) tasks is the semantic relatedness between words of the same or different syntactic categories.Semantic relatedness subsumes hyponymy, synonymy, and antonymyincompatibility.Special forms of relatedness are represented in the lexical entries of the WordNet lexical database (Miller et al., 1990).Paradigmatic semantic relations in WordNet have been used for diverse NL problems, including disambiguation of syntactic structure (Resnik, 1993) and semiautomatic construction of a large-scale ontology for machine translation (Knight, 1993).In this paper, we focus on a particular case of semantic relatedness: relatedness between adjectives which describe the same property.We describe a technique for automatically grouping adjectives according to their meaning based on a given text corpus, so that all adjectives placed in one group describe different values of the same property.Our method is based on statistical techniques, augmented with linguistic information derived from the corpus, and is completely domain independent.It demonstrates how high-level semantic knowledge can be computed from large amounts of low-level knowledge (essentially plain text, part-of-speech rules, and optionally syntactic relations).The problem of identifying semantically related words has received considerable attention, both in computational linguistics (e.g. in connection with thesaurus or dictionary construction (Sparck-Jones, 1986)) and in psychology (Osgood et al., 1957).However, only recently has work been done on the automatic computation of such relationships from text, quantifying similarity between words and clustering them ( (Brown et al., 1992), (Pereira et al., 1993)).In comparison, our work emphasizes the use of shallow linguistic knowledge in addition to a statistical model and is original in the use of negative knowledge to constrain the search space.Furthermore, we use a flexible architecture which will allow us to easily incorporate additional knowledge sources for computing similarity.While our current system does not distinguish between scalar and non-scalar adjectives, it is a first step in the automatic identification of adjectival scales, since the scales can be subsequently ordered and the non-scalar adjectives filtered on the basis of independent tests, done in part automatically and in part by hand in a post-editing phase.The result is a semi-automated system for the compilation of adjectival scales.In the following sections, we first provide background on scales, then describe our algorithm in detail, present the results obtained, and finally provide a formal evaluation of the results.A linguistic scale is a set of words, of the same grammatical category, which can be ordered by their semantic strength or degree of informativeness (Levinson, 1983).For example, lukewarm, warm, and hot fall along a single adjectival scale since they indicate a variation in the intensity of temperature of the modified noun (at least when used in their nonmetaphorical senses; metaphorical usage of scalar words normally also follows the order of the scale by analogy).Scales are not limited to adjectives; for example, (may, should, must) and (sometimes, often, always) (Horn, 1972) are linguistic scales consisting of auxiliary verbs expressing obligation and of adverbs expressing frequency respectively.In the case of adjectives, the above definition is commonly relaxed to replace the total order among the elements of the scale by a partial one, so that the elements of the scale may be partitioned into two groups (sub-scales), within each of which the order is total.The two sub-groups correspond to positive and negative degrees of the common property that the scale describes.For example, the set of adjectives (cold, lukewarm, warm, hot) are normally considered part of one scale, even though no direct ordering of semantic strength exists between cold and hot.Linguistic scales are known to possess interesting properties, derived from conventional logical entailment on the linear ordering of their elements and from Gricean scalar implicature (Levinson, 1983).Despite these properties and their potential usefulness in both understanding and generating natural language text, dictionary entries are largely incomplete for adjectives in this regard.Yet, if systems are to use the information encoded in adjectival scales for generation or interpretation (e.g. for selecting an adjective with a particular degree of semantic strength (Elhadad, 1991, Elhadad, 1993), or for handling negation), they must have access to the sets of words comprising a scale.Linguists have presented various tests for accepting or rejecting a particular scalar relationship between any two adjectives.For example, Horn (1969) proposed a test using the phrase &quot;x even y&quot; for two elements x and y of a totally ordered scale.More refined tests locate the position of an adjective in a scale relative to the neutral element or to the extremes of the scale (Bolinger, 1977).The common problem with these methods is that they are designed to be applied by a human who incorporates the two adjectives in specific sentential frames (e.g.&quot;X is warm, even hot&quot;) and assesses the semantic validity of the resulting sentences.Such tests cannot be used computationally to identify scales in a domain, since the specific sentences do not occur frequently enough in a corpus to produce an adequate description of the adjectival scales in the domain (Smadja, 1991).As scales vary across domains, the task of compiling such information is compounded.Our algorithm, whose overall architecture is depicted in Figure 1, operates in four stages.First, we extract linguistic data from the parsed corpus in the form of syntactically related word pairs, or, more generally, sequences of syntactically related words; this co-occurrence information is processed by a morphology component and tabulated.In the second stage, the various types of co-occurrence relations which have been identified in the text are forwarded to a set of independent similarity modules, which operate in parallel.Each similarity module uses some linguistic criterion to judge the similarity or dissimilarity between any two adjectives, producing a real number between 0 and 1; a module may also refrain from making any judgement.The third stage combines the opinions of the various similarity modules in a single dissimilarity measure for any pair of adjectives.Finally, the fourth stage clusters the adjectives into groups according to the dissimilarity measure, so that adjectives with a high degree of pairwise similarity fall in the same cluster (and, consequently, adjectives with a low degree of similarity fall in different clusters).The algorithm currently uses two similarity modules based on two sources of linguistic data: data that help establish that two adjectives are related, and data that indicate that two adjectives are unrelated.First, we extract adjective-noun pairs that occur in a modification relation in order to identify the distribution of nouns an adjective modifies and, ultimately, determine which adjectives it is related to.This is based on the expectation that adjectives describing the same property tend to modify approximately the same set of nouns.For example, temperature is normally defined for physical objects and we can expect to find that adjectives conveying different values of temperature will all modify physical objects.Therefore, our algorithm finds the distribution of nouns that each adjective modifies and categorizes adjectives as similar if they have similar distributions.Second, we use adjective-adjective pairs occurring as pre-modifiers within the same NP as a strong indication that the two adjectives do not belong in the same group.There are three cases: The use of multiple types of linguistic data, in Note that sequences such as blue-green are usually hyphenated and thus better considered as a compound. addition to statistical measures, is a unique property of our work and significantly improves the accuracy of our results.One other published model for grouping semantically related words (Brown et al., 1992), is based on a statistical model of bigrams and trigrams and produces word groups using no linguistic knowledge, but no evaluation of the results is reported.During the first stage, the system extracts adjective-noun and adjective-adjective pairs from the corpus.To determine the syntactic category of each word, and identify the NP boundaries and the syntactic relations among the words, we used the Fidditch parser (Hindle, 1989).For each NP, we then determine its minimal NP, that part of an NP consisting of the head noun and its adjectival pre-modifiers2.We match a set of regular expressions, consisting of syntactic categories and representing the different forms a minimal NP can take, against the NPs.From the minimal NP, we produce the different pairs of adjectives and nouns, assuming that all adjectives modify the head noun3.This assumption is rarely invalid, because a minimal NP with multiple adjectives all modifying the head noun is far more common than a minimal NP with multiple adjectives where one of them modifies another.Furthermore, minimal NPs with multiple adjectives are relatively rare in the first place; most minimal NPs consist simply of a noun or an adjective and a noun.The resulting adjective-adjective and adjectivenoun pairs are filtered by a morphology component, which removes pairs that contain erroneous information (such as mistyped words, proper names, and closed-class words which may be mistakenly classified as adjectives (e.g. possessive pronouns)).This component also reduces the number of different pairs without losing information by transforming words to an equivalent, base form (e.g. plural nouns are converted to singular) so that the expected and actual frequencies of each pair are higher.Stage one then produces as output a simple list of adjective-adjective pairs that occurred within the same minimal NP and a table with the observed frequencies of every adjective-noun combination.Each row in the table contains the frequencies of modified nouns for a given adjective.This stage currently employs two similarity modules, each of which processes a part of the output of stage one and produces a measure of similarity for each possible pair of adjectives.The first module processes the adjective-noun frequency table; for each possible pair in the table we compare the two distributions of nouns.We use a robust non-parametric method to compute the similarity between the modified noun distributions for any two adjectives, namely Kendall's coefficient (Kendall, 1938) for two random variables with paired observations.In our case, the two random variables are the two adjectives we are comparing, and each paired observation is their frequency of cooccurrence with a given noun.Kendall's T coefficient compares the two variables by repeatedly comparing two pairs of their corresponding observations.Formally, if (Xi,Yi) and (XJ,Yi) are two pairs of observations for the adjectives X' and Y on the nouns i and j respectively, we call these pairs concordant if Xi>XJ. and Y.>Y. or if X<X. and Y t <11, otherwise these pairs are discordant.We discard ties, that is pairs of observations where Xi=Xf or Y/.YJ.'For example, Table 1 shows the frequencies observed for the co-occurrences of the nouns coordination and market and the adjectives global and international in the test corpus which is described in Section 4.From the table we observe that for i=coordination, j=market, X=global, and Y=intemational, we have Xi=16 < 24=Xi and Y1=19 <33=Y so this particular pair of paired' observations is concordant and contributes positively to the similarity between global and international.In general, if the distributions for the two adjectives are similar, we expect a large number of concordances, and a small number of discordances.Kendall's T is defined as where pc and pd are the probabilities of observing a concordance or discordance respectively.T ranges from -1 to +1, with +1 indicating complete concordance, -1 complete discordance, and 0 no correlation between X and Y.An unbiased estimator oft is the statistic where n is the number of paired observations in the sample and C and Q are the numbers of observed concordances and discordances respectively (Wayne, 1990).We compute T for each pair of adjectives, adjusting for possible ties in the values of each variable, so that our statistic remains an unbiased estimator of T. We determine concordances and discordances by sorting the pairs of observations (noun frequencies) on one of the variables (adjectives), and computing how many of the (3) pairs of paired observations agree or disagree with the expected order on the other adjective.We normalize the result to the range 0 to 1 using a simple linear transformation.The second similarity module utilizes the knowledge offered by the observed adjectiveadjective pairs.We know that the adjectives which appear in any such pair cannot be part of the same group, so the module produces zero similarity for all such pairs.The module does not output any similarity value for pairs of adjectives which have not been observed together in the same minimal NP.The two modules produce results of a significantly different character.The adjective-noun module always outputs a similarity value for any pair of adjectives, but these values tend to be around the middle of the range of possible values; rarely will the pattern of similarity or dissimilarity be strong enough to produce a value which has a large deviation from 0.5.This compression of the range of the similarity values can be attributed to the existence of many ties and many adjective-noun pairs with low frequencies, as would be expected by Zipf s law (Zipf, 1949).However, the expected number of concordances and discordances which can be attributed to chance will be the same (a random pair can produce a concordance or discordance with probability 0.5 for each), so the effect of chance fluctuations on T is not very significant.Furthermore, the robustness of the method guarantees that it will not be significantly influenced by any outliers (this is true for all rank based methods).Therefore, although we cannot have complete confidence in a statistical estimate like T, we expect the module to produce useful estimates of similarity.On the other hand, the adjective-adjective module produces similarity values with absolute certainty, since once two adjectives have been seen in the same NP even once, we can deduce that they do not belong in the same group.However, this negative knowledge is computed only for a few of the possible pairs of adjectives, and it cannot be propagated to more pairs as dissimilarity is not a transitive relation.As a result we can make some inferences with very high confidence, but we cannot make very many of them.In stage three we combine the values produced by the various similarity modules in stage two using a pre-specified algorithm.The output of this stage is a single table of dissimilarity values (as required by the next stage) having one entry for each adjective pair.Currently we have only the two similarity modules described in the previous subsection, so we employ the following simple algorithm: for any pair of adjectives (x,y) do if the adjective-adjective module has no opinion on (x,y) then dissimilarity = 1 - (the similarity reported by the adjective-noun module) else dissimilarity = some constant As can be easily seen, the algorithm has complete confidence in the results of the adjective-adjective module whenever that module has an opinion; when it does not, the algorithm uses the similarity value produced by the adjective-noun module, after a simple linear transformation is applied to convert it to a dissimilarity.The choice of the constant k reflects how undesirable it is to place in the same group two adjectives which have been observed in the same minimal NP.Since we consider the results of the adjective-adjective module more reliable than the adjective-noun module, we use a high value for k, k=10; this practically guarantees that a suggestion by the adjective-adjective module will be respected by the clustering algorithm unless the evidence for the contrary is overwhelming.Note that by placing complete confidence in the output of the adjective-adjective module, the algorithm of stage three is sensitive to small errors that this module may perform.An incorrect suggestion would make possibly related adjectives be kept separate.However, this problem looks more severe than it really is.An erroneous opinion produced by that module must correspond to a violation of one of the three linguistic principles listed at the start of this section; such violations do not occur in carefully written English (as is our test corpus of Associated Press news reports).In fact, during the analysis of the corpus for our test set of adjectives we found no erroneously identified pairs of adjectives; however, if the system is used with a less well written, or even spoken, corpus, the complete confidence in the adjective-adjective module may need to be reduced.This can be accomplished by taking into account the frequency of an adjective-adjective pair, and making our confidence an increasing function of this frequency.When new similarity modules, such as the ones discussed in Section 6, are added to the system, the above algorithm will be inadequate for combining their suggestions.We plan to extend the algorithm to compute an extended weighted average of the similarities and/or dissimilarities produced by these modules, and add a separate training component which will determine the appropriate value for the weight of each module.In stage four we form groups of adjectives (a partition) according to the combined dissimilarity values computed in the previous stage.We want to find a partition which is optimal, in the sense that adjectives with high dissimilarity are placed in different groups.We use a non-hierarchical clustering algorithm, since such algorithms are in general stronger than hierarchical methods (Kaufman and Rousseeuw, 1990).The number of clusters produced is an input parameter.The algorithm uses the exchange method (Spath, 1985) since the more commonly used Kmeans method (Kaufman and Rousseeuw, 1990) is not applicable; the K-means method, like all centroid methods, requires the measure d between the clustered objects to be a distance; this means, among other conditions, that for any three objects x, y, and z the triangle inequality applies.However, this inequality does not necessarily hold for our dissimilarity measure.If the adjectives x and y were observed in the same minimal NP, their dissimilarity is quite large.If neither z and x nor z and y were found in the same minimal NP, then it is quite possible that the sum of their dissimilarities could be less than the dissimilarity between x and y.The algorithm tries to produce a partition of the set of adjectives as close as possible to the optimal one.This is accomplished by minimizing an objective function (Ico which scores a partition P. The objective function we use is The algorithm starts by producing a random partition of the adjectives, computing its 0:1) value and then for each adjective computing the improvement in (1) for every cluster where it can be moved; the adjective is moved to the cluster that yields the best improvement of (13 if there is such a cluster and the next adjective is considered.This procedure is repeated until no more moves lead to an improvement of O.This is a hill-climbing method and therefore is guaranteed to converge, but it may lead to a local minimum of (1), inferior to the global minimum that corresponds to the optimal solution.To alleviate this problem, the partitioning algorithm is called repeatedly with different random starting partitions and the best solution in these runs is kept.As with many practical optimization problems, computing the optimal solution is NP-complete (Brucker, 1978).Note that if the problem's search space had been relatively small, then we could have computed the optimal partition by enumerating all possible solutions and keeping the best one.However, again as with many other practical optimization problems, the search space turns out to be intractably large.The number of possible partitions of n objects to m nonempty subsets with mtt is equal to the corresponding Stirling number of the second kind (Knuth, 1973), and this number grows exponentially with n for all but trivial values of m. For example, for our test set of adjectives presented in the next section, we have n=21 and m=9; the corresponding number of possible partitions is roughly 1.23 x 1014.We tested our system on a 8.2 million word corpus of stock market reports from the Associated Press news wire.A subset of 21 of the adjectives in the corpus (Figure 2) was selected for practical reasons (mainly for keeping the evaluation task tractable).We selected adjectives that have one modified noun in common (problem) to ensure some semantic relatedness, and we included only adjectives that occurred frequently so that our similarity measure would be meaningful.The partition produced by the system for 9 clusters appears in Figure 3.Before presenting a formal evaluation of the results, we note that this partition contains interesting data.First, the results contain two clusters of gradable adjectives which fall in the same scale.Groups 5 and 8 contain adjectives that indicate the size, or scope, of a problem; by augmenting the system with tests to identify when an adjective is gradable, we could separate out these two groups from other potential scales, and perhaps consider combining them.Second, groups 1 and 6 clearly identify separate sets of non-gradable adjectives.The first contains adjectives that describe the geographical scope of the problem.Although at first sight we would classify these adjectives as non-scalar, we observed that the phrase international, even global, problem is acceptable while the phrase *global, even international, problem is not.These patterns seem to suggest at least some degree of scalability.On the other hand, group 6 contains non-scalar relational adjectives that specify the nature of the problem.It is interesting to note here that the clustering algorithm discourages long groups, with the expected number of adjectives per cluster being -§- ---- 2.33; nevertheless, the evidence for the adjectives in group 6 is strong enough to allow the creation of a group with more than twice the expected number of members.Finally, note that even in group 4 which is the weakest group produced, there is a positive semantic correlation between the adjectives new and unexpected.To summarize, the system seems to be able to identify many of the existent semantic relationships among the adjectives, while its mistakes are limited to creating singleton groups containing adjectives that are related to other adjectives in the test set (e.g., missing the semantic associations between new-old and potential-real) and &quot;recognizing&quot; a non-significant relationship between real and new-unexpected in group 4.We produced good results with a relatively small corpus of 8.2 million words4, out of which only 34,359 total / 3,073 distinct adjective-noun pairs involving 1,509 distinct nouns were relevant to our test set of 21 adjectives (Figure 2).The accuracy of the results can be improved if a larger, homogeneous corpus is used to provide the raw data.Also, we can increase the size of the adjective-noun and adjectiveadjective data that we are using if we introduce more syntactic patterns in stage one to extract more complex cases of pairs.Furthermore, some of the associations between adjectives that the system reports appear to be more stable than others; these associations remain in the same group when we vary the number of clusters in the partition.We have noticed that adjectives with a higher degree of semantic content (e.g. international or severe) appear to form more stable associations than relatively semantically empty adjectives (e.g. little or real).This observation can be used to filter out adjectives which are too general to be meaningfully clustered in groups.To evaluate the performance of our system we compared its output to a model solution for the problem designed by humans.Nine human judges were presented with the set of adjectives to be partitioned, a description of the domain, and a simple example.They were told that clusters should not overlap but they could select any number of clusters (the judges used from 6 to 11 clusters, with an average of 8.565 and a sample standard deviation of 1.74).Note that this evaluation method differs significantly from the alternative method of asking the humans to directly estimate the goodness of the system's results (e.g.(Matsukawa, 1993)).It requires an explicit construction of a model from the human judge and places the burden of the comparison between the model and the system's output on the system instead of the judge.It has been repeatedly demonstrated that in complex evaluation tasks humans can easily find arguments to support observed data, leading to biased results and to an inflation of the evaluation scores.To score our results, we converted the comparison of two partitions to a series of yes-no questions, each of which has a correct answer (as dictated by the model) and an answer assigned by the system.For each pair of adjectives, we asked if they fell in the same cluster (&quot;yes&quot;) or not (&quot;no&quot;).Since human judges did not always agree, we used fractional values for the correctness of each answer instead of 0 (&quot;incorrect&quot;) and 1 (&quot;correct&quot;).We defined the correctness of each answer as the relative frequency of the association between the two adjectives among the human models and the incorrectness of each answer as 1 - correctness; in this way, associations receive a correctness value proportional to their popularity among the human judges.For example, in the sample set of adjectives discussed in the previous section, the association (foreign, international) received a correctness value of 1, since all the humans placed these two adjectives in the same group, while the association (legal, severe) received a correctness value of 0.The pair (economic, political) on the other hand received a correctness value of 0.67, since two thirds of the judges placed the two adjectives in the same group.Once correctness and incorrectness values have been defined, we can generalize measures such as &quot;the number of correct associations retrieved by the system&quot; by using summation of those values instead of counting.Then the contingency table model (Swets, 1969), widely used in Information Retrieval and Psychology, is applicable.Referring to the classification of the yes-no answers in Table 2, the following measures are defined: a 7— i.d In other words, recall is the percentage of correct &quot;yes&quot; answers that the system found among the model &quot;yes&quot; answers, precision is the percentage of correct &quot;yes&quot; answers among the total of &quot;yes&quot; answers that the system reported, and fallout is the percentage of incorrect &quot;yes&quot; answers relative to the total number of &quot;no&quot; answers6.Note that in our generalized contingency table model, the symbols a, b, c, and d do not represent numbers of observed associations but rather sums of correctness or incorrectness values.These sums use correctness values for the quantities in the first column of Table 2 and incorrectness values for the quantities in the second column of Table 2.Furthermore, the summation is performed over all pairs reported or not reported by the system for quantities in the first or second row of Table 2 respectively.Consequently, the information theoretic measures represent the generalized counterparts of their original definitions.In the case of perfect agreement between the models, or of only one model, the generalized measures reduce to their original definitions.We also compute a combined measure for recall and precision, the F-measure (Van Rijsbergen, 1979), which always takes a value between the values of recall and precision, and is higher when recall and precision are closer; it is defined as where 3 is the weight of recall relative to precision; we use 13.1.0, which corresponds to equal weighting of the two measures.The results of applying our evaluation method to the system output (Figure 3) are shown in Table 3, which also includes the scores obtained for several other sub-optimal choices of the number of clusters.We have made these observations related to the evaluation mechanism: there has been increasing concern that the scoring methods used for evaluating the goodness of parsers are producing values which seem extremely good (in the >90% range), while in fact the parse trees produced are not so satisfactory; the blame for this inflation of the scores can be assigned to an inadequate comparison technique, which essentially considers a tree fragment correct when it is a part of (although not exactly matching) the corresponding fragment in the model.For other tasks, such as part-of-speech assignment to free text, the comparison techniques are sound, but very high levels of performance (e.g.90%) can be obtained by a zeroparameter model which operates at random; clearly this makes the assessment of the significance of an improvement over the base line of the random algorithm much harder.As a consequence of point (3) made above, we need to understand the significance of the scores produced by our evaluation methods (for example, the limits of their ranges) before trying to interpret them.There are theoretical principles which indicate that the evaluation metrics will produce lower values much more easily than higher ones.Because of the multiple models used, perfect scores are not attainable.Also, because each pair of adjectives in a cluster is considered an observed association, the relationship between the number of associations produced by a cluster and the number of adjectives in the cluster is not linear (a cluster with k adjectives will produce () k 0(k2) associations).This leads to lower values 2/ of recall, since moving a single adjective out of a cluster with k elements in the model will cause the system to miss k-1 associations.As an example of this phenomenon, consider the hypothetical (single) model and partition of Figure 4; while the partition differs from the model only in that the first cluster has been split into two, the recall score abruptly falls to 50%.In order to provide empirical evidence in addition to the theoretical discussion above, and be able to estimate an upper bound on the values of the evaluation metrics, we evaluated each human model against all the other human models, using the same evaluation method which was used for the system; the results ranged from 38 to 72% for recall, 1 to 12% for fallout, 38 to 81% for precision, and, covering a remarkably short range, 49 to 59% for F-measure7, indicating that the performance of the system is not far behind human performance.In order to provide a lower bound for the evaluation metrics and thus show that the system's scores are not close to the scores of the human judges simply by chance, we performed a Monte Carlo analysis (Rubinstein, 1981) for the evaluation metrics, by repeatedly creating random partitions of the sample adjectives and evaluating the results.Then we estimated a smoothed probability density function for each metric from the resulting histograms; the results obtained are shown in Figure 5 for F-measure and fallout using 9 clusters.We observed that the system's performance (indicated by a square in the diagrams) was significantly better than what we would expect under the null hypothesis of random performance; the probability of getting a better partition than the system's is extremely small for all metrics (no occurrence in 20,000 trials) except for fallout, for which a random system may be better 4.9% of the time.The estimated density functions also show that the metrics are severely constrained by the structure imposed by the clustering as they tend to peak at some point and then fall rapidly.Finally, we performed another study to quantify the impact of using negative knowledge obtained from adjective-adjective pairs.We ran our system in a mode where the suggestions of the adjectiveadjective module were ignored (i.e. stage three simply passed to the output the similarities computed by the adjective-noun module, after converting them to dissimilarities), and evaluated the results produced.The values of the metrics for the partition with 9 clusters appear in Table 4, alongside the corresponding values produced when the system uses both modules.When both modules are used, we can see a significant improvement of about 15 points, which is a 43% to 50% improvement for all metrics (except for fallout where the improvement is about 17%).This represents a definite improvement even though for our test set of 21 adjectives (Figure 2) we observed in our corpus only 41 distinct adjectiveadjective pairs, out of a possible (221)=210 pairs.Al7Thus indicating that human models which fared well on the precision metric tended to perform badly on recall, and vice versa; remember that the values of the metrics are related to the number of clusters used, and that the human judges were allowed to select the number of clusters they considered most appropriate; consequently, the models with high recall/low precision are the ones with a small number of clusters, while the opposite pattern of scores characterizes the models with a large number of clusters. though the observed pairs represent only 19.52% of the possible pairs, their importance is considerable.Note that the sparsity of the adjective-adjective pairs does not allow us to perform a comparable study for the partition produced using the adjectiveadjective module alone, since such a partition would be largely determined by chance.We have described a system for extracting groups of semantically related adjectives from large text corpora, with a flexible architecture which allows for multiple knowledge sources influencing similarity to be easily incorporated into the system.Our evaluation reveals that it has significantly high performance levels, comparable to humans, using only a relatively small amount of input data; in addition, it shows the usefulness of negative knowledge, an original feature of our approach.The system's results can be filtered to produce scalar adjectives that are applicable in any given domain.Furthermore, while we have demonstrated the algorithm on adjectives, it can be directly applied to other word classes once sources of linguistic information for judging their similarity have been identified.Our immediate plans are to incorporate more similarity modules into stage two of the system and add a training component to stage three so that the relative weights of the various modules can be estimated.We have identified several additional sources of linguistic knowledge which look promising, namely pairs of adjectives separated by connectives and adverb-adjective pairs.We also plan to extend the adjective-noun module to cover adjectives in predicative positions, in addition to our current use of attributive adjectives.These extensions not only will provide us with a better way of exploiting the information in the corpus but may also help us categorize the adjectives as relational or attributive (Levi, 1978); such a categorization may be useful in classifying them as either scalar or non-scalar.For determining whether a group of adjectives is scalar, we also plan to use the gradability of the adjectives as observed in the corpus.In addition, we are exploring tests for determining whether two adjectives are antonymous, essentially in the opposite direction of the work by Justeson and Katz (1991) , and tests for comparing the relative semantic strength of two adjectives.Furthermore, we plan to consider alternative evaluation methods and test our system on a much larger set of adjectives.That was not done for the current evaluation because of the difficulty for humans of constructing large models.We are considering an evaluation method which would use a thesaurus to judge similarity, as well as a supplementary method based on mathematical properties of the clustering.Neither of these methods would access any human models.The mathematical method, which uses cluster silhouettes and the silhouette coefficient (Kaufman and Rousseeuw, 1990), can also be used to automatically determine the proper number of clusters, one of the hardest problems in cluster analysis.We also plan a formal study to evaluate the appropriateness of the clustering method used, by computing and evaluating the results when a hierarchical algorithm is employed instead in stage four.Eventually, we plan to evaluate the system's output by using it to augment adjective entries in a lexicon and test the augmented lexicon in an application such as language generation.This work was supported jointly by DARPA and ONR under contract N00014-894-1782, by NSF GER-90-24069, and by New York State Center for Advanced Technology Contract NYSSTFCAT(91)-053.We wish to thank Diane Litman and Donald Hindle for providing us with access to the Fidditch parser at AT&T Bell Labs, and Karen Kulcich and Frank Smadja for providing us with access to the Associated Press news wire corpus.Finally, we thank Rebecca Passonneau and the anonymous reviewers for providing us with useful comments on earlier versions of the paper.
A Trainable Rule-Based Algorithm For Word SegmentationThis paper presents a trainable rule-based algorithm for performing word segmentation.The algorithm provides a simple, language-independent alternative to large-scale lexical-based segmenters requiring large amounts of knowledge engineering.As a stand-alone segmenter, we show our algorithm to produce high performance Chinese segmentation.In addition, we show the transformation-based algorithm to be effective in improving the output of several existing word segmentation algorithms in three different languages.This paper presents a trainable rule-based algorithm for performing word segmentation.Our algorithm is effective both as a high-accuracy stand-alone segmenter and as a postprocessor that improves the output of existing word segmentation algorithms.In the writing systems of many languages, including Chinese, Japanese, and Thai, words are not delimited by spaces.Determining the word boundaries, thus tokenizing the text, is usually one of the first necessary processing steps, making tasks such as part-of-speech tagging and parsing possible.A variety of methods have recently been developed to perform word segmentation and the results have been published widely.'A major difficulty in evaluating segmentation algorithms is that there are no widely-accepted guidelines as to what constitutes a word, and there is therefore no agreement on how to &quot;correctly&quot; segment a text in an unsegmented language.It is 1Most published segmentation work has been done for Chinese.For a discussion of recent Chinese segmentation work, see Sproat et al. (1996). frequently mentioned in segmentation papers that native speakers of a language do not always agree about the &quot;correct&quot; segmentation and that the same text could be segmented into several very different (and equally correct) sets of words by different native speakers.Sproat et al. (1996) and Wu and Fung (1994) give empirical results showing that an agreement rate between native speakers as low as 75% is common.Consequently, an algorithm which scores extremely well compared to one native segmentation may score dismally compared to other, equally &quot;correct&quot; segmentations.We will discuss some other issues in evaluating word segmentation in Section 3.1.One solution to the problem of multiple correct segmentations might be to establish specific guidelines for what is and is not a word in unsegmented languages.Given these guidelines, all corpora could theoretically be uniformly segmented according to the same conventions, and we could directly compare existing methods on the same corpora.While this approach has been successful in driving progress in NLP tasks such as part-of-speech tagging and parsing, there are valid arguments against adopting it for word segmentation.For example, since word segmentation is merely a preprocessing task for a wide variety of further tasks such as parsing, information extraction, and information retrieval, different segmentations can be useful or even essential for the different tasks.In this sense, word segmentation is similar to speech recognition, in which a system must be robust enough to adapt to and recognize the multiple speaker-dependent &quot;correct&quot; pronunciations of words.In some cases, it may also be necessary to allow multiple &quot;correct&quot; segmentations of the same text, depending on the requirements of further processing steps.However, many algorithms use extensive domain-specific word lists and intricate name recognition routines as well as hard-coded morphological analysis modules to produce a predetermined segmentation output.Modifying or retargeting an existing segmentation algorithm to produce a different segmentation can be difficult, especially if it is not clear what and where the systematic differences in segmentation are.It is widely reported in word segmentation papers,2 that the greatest barrier to accurate word segmentation is in recognizing words that are not in the lexicon of the segmenter.Such a problem is dependent both on the source of the lexicon as well as the correspondence (in vocabulary) between the text in question and the lexicon.Wu and Fung (1994) demonstrate that segmentation accuracy is significantly higher when the lexicon is constructed using the same type of corpus as the corpus on which it is tested.We argue that rather than attempting to construct a single exhaustive lexicon or even a series of domain-specific lexica, it is more practical to develop a robust trainable means of compensating for lexicon inadequacies.Furthermore, developing such an algorithm will allow us to perform segmentation in many different languages without requiring extensive morphological resources and domain-specific lexica in any single language.For these reasons, we address the problem of word segmentation from a different direction.We introduce a rule-based algorithm which can produce an accurate segmentation of a text, given a rudimentary initial approximation to the segmentation.Recognizing the utility of multiple correct segmentations of the same text, our algorithm also allows the output of a wide variety of existing segmentation algorithms to be adapted to different segmentation schemes.In addition, our rule-based algorithm can also be used to supplement the segmentation of an existing algorithm in order to compensate for an incomplete lexicon.Our algorithm is trainable and language independent, so it can be used with any unsegmented h.nguage.The key component of our trainable segmentation algorithm is Transformation-based Error-driven Learning, the corpus-based language processing method introduced by Brill (1993a).This technique provides a simple algorithm for learning a sequence of rules that can be applied to various NLP tasks.It differs from other common corpus-based methods in several ways.For one, it is weakly statistical, but not probabilistic; transformation-based approaches conseo,--iitly require far less training data than most ..stical approaches.It is rule-based, but relies on 'See, for example, Sproat et al. (1996). machine learning to acquire the rules, rather than expensive manual knowledge engineering.The rules produced can be inspected, which is useful for gaining insight into the nature of the rule sequence and for manual improvement and debugging of the sequence.The learning algorithm also considers the entire training set at all learning steps, rather than decreasing the size of the training data as learning progresses, such as is the case in decision-tree induction (Quinlan, 1986).For a thorough discussion of transformation-based learning, see Ramshaw and Marcus (1996).Brill's work provides a proof of viability of transformation-based techniques in the form of a number of processors, including a (widelydistributed) part-of-speech tagger (Brill, 1994), a procedure for prepositional phrase attachment (Brill and Resnik, 1994), and a bracketing parser (Brill, 1993b).All of these provided performance comparable to or better than previous attempts.Transformation-based learning has also been successfully applied to text chunking (Ramshaw and Marcus, 1995), morphological disambiguation (Oflazer and Tur, 1996), and phrase parsing (Vilain and Day, 1996).Word segmentation can easily be cast as a transformation-based problem, which requires an initial model, a goal state into which we wish to transform the initial model (the &quot;gold standard&quot;), and a series of transformations to effect this improvement.The transformation-based algorithm involves applying and scoring all the possible rules to training data and determining which rule improves the model the most.This rule is then applied to all applicable sentences, and the process is repeated until no rule improves the score of the training data.In this manner a sequence of rules is built for iteratively improving the initial model.Evaluation of the rule sequence is carried out on a test set of data which is independent of the training data.If we treat the output of an existing segmentation algorithm' as the initial state and the desired segmentation as the goal state, we can perform a series of transformations on the initial state - removing extraneous boundaries and inserting new boundaries to obtain a more accurate approximation of the goal state.We therefore need only define an appropriate rule syntax for transforming this initial approxima'The &quot;existing&quot; algorithm does not need to be a large or even accurate system; the algorithm can be arbitrarily simple as long as it assigns some form of initial segmentation. tion and prepare appropriate training data.For our experiments, we obtained corpora which had been manually segmented by native or nearnative speakers of Chinese and Thai.We divided the hand-segmented data randomly into training and test sets.Roughly 80% of the data was used to train the segmentation algorithm, and 20% was used as a blind test set to score the rules learned from the training data.In addition to Chinese and Thai, we also performed segmentation experiments using a large corpus of English in which all the spaces had been removed from the texts.Most of our English experiments were performed using training and test sets with roughly the same 80-20 ratio, but in Section 3.4.3 we discuss results of English experiments with different amounts of training data.Unfortunately, we could not repeat these experiments with Chinese and Thai due to the small amount of handsegmented data available.There are three main types of transformations which can act on the current state of an imperfect segmentation: In our syntax, Insert and Delete transformations can be triggered by any two adjacent characters (a bigram) and one character to the left or right of the bigram.Slide transformations can be triggered by a sequence of one, two, or three characters over which the boundary is to be moved.Figure 1 enumerates the 22 segmentation transformations we define.With the above algorithm in place, we can use the training data to produce a rule sequence to augment an initial segmentation approximation in order to obtain a better approximation of the desired segmentation.Furthermore, since all the rules are purely character-based, a sequence can be learned for any character set and thus any language.We used our rule-based algorithm to improve the word segmentation rate for several segmentation algorithms in three languages.Despite the number of papers on the topic, the evaluation and comparison of existing segmentation algorithms is virtually impossible.In addition to the problem of multiple correct segmentations of the same texts, the comparison of algorithms is difficult because of the lack of a single metric for reporting scores.Two common measures of performance are recall and precision, where recall is defined as the percent of words in the hand-segmented text identified by the segmentation algorithm, and precision is defined as the percentage of words returned by the algorithm that also occurred in the hand-segmented text in the same position.The component recall and precision scores are then used to calculate an F-measure (Rijsbergen, 1979), where F = (1 + 0)PRI(13P + R).In this paper we will report all scores as a balanced F-measure (precision and recall weighted equally) with /3 = 1, such that For our Chinese experiments, the training set consisted of 2000 sentences (60187 words) from a Xinhua news agency corpus; the test set was a separate set of 560 sentences (18783 words) from the same corpus.5 We ran four experiments using this corpus, with four different algorithms providing the starting point for the learning of the segmentation transformations.In each case, the rule sequence learned from the training set resulted in a significant improvement in the segmentation of the test set.A very simple initial segmentation for Chinese is to consider each character a distinct word.Since the average word length is quite short in Chinese, with most words containing only 1 or 2 characters,6 this character-as-word segmentation correctly identified many one-character words and produced an initial segmentation score of F=40.3.While this is a low segmentation score, this segmentation algorithm identifies enough words to provide a reasonable initial segmentation approximation.In fact, the CAW algorithm alone has been shown (Buckley et al., 1996; Broglio et al., 1996) to be adequate to be used successfully in Chinese information retrieval.Our algorithm learned 5903 transformations from the 2000 sentence training set.The 5903 transformations applied to the test set improved the score from F=40.3 to 78.1, a 63.3% reduction in the error rate.This is a very surprising and encouraging result, in that, from a very naive initial approximation using no lexicon except that implicit from the training data, our rule-based algorithm is able to produce a series of transformations with a high segmentation accuracy.A common approach to word segmentation is to use a variation of the maximum matching algorithm, frequently referred to as the &quot;greedy algorithm.&quot; The greedy algorithm starts at the first character in a text and, using a word list for the language being segmented, attempts to find the longest word in the list starting with that character.If a word is found, the maximum-matching algorithm marks a boundary at the end of the longest word, then begins the same longest match search starting at the character following the match.If no match is found in the word list, the greedy algorithm simply skips that character and begins the search starting at the next character.In this manner, an initial segmentation can be obtained that is more informed than a simple character-as-word approach.We applied the maximum matching algorithm to the test set using a list of 57472 Chinese words from the NMSU CHSEG segmenter (described in the next section).This greedy algorithm produced an initial score of F=64.4.A sequence of 2897 transformations was learned from the training set; applied to the test set, they improved the score from F=64.4 to 84.9, a 57.8% error reduction.From a simple Chinese word list, the rule-based algorithm was thus able to produce asegmentation score comparable to segmentation algorithms developed with a large amount of domain knowledge (as we will see in the next section).This score was improved further when combining the character-as-word (CAW) and the maximum matching algorithms.In the maximum matching algorithm described above, when a sequence of characters occurred in the text, and no subset of the sequence was present in the word list, the entire sequence was treated as a single word.This often resulted in words containing 10 or more characters, which is very unlikely in Chinese.In this experiment, when such a sequence of characters was encountered, each of the characters was treated as a separate word, as in the CAW algorithm above.This variation of the greedy algorithm, using the same list of 57472 words, produced an initial score of F=82.9.A sequence of 2450 transformations was learned from the training set; applied to the test set, they improved the score from F=82.9 to 87.7, a 28.1% error reduction.The score produced using this variation of the maximum matching algorithm combined with a rule sequence (87.7) is nearly equal to the score produced by the NMSU segmenter segmenter (87.9) discussed in the next section.The previous three experiments showed that our rule sequence algorithm can produce excellent segmentation results given very simple initial segmentation algorithms.However, assisting in the adaptation of an existing algorithm to different segmentation schemes, as discussed in Section 1, would most likely be performed with an already accurate, fullydeveloped algorithm.In this experiment we demonstrate that our algorithm can also improve the output of such a system.The Chinese segmenter CHSEG developed at the Computing Research Laboratory at New Mexico State University is a complete system for highaccuracy Chinese segmentation (Jin, 1994).In addition to an initial segmentation module that finds words in a text based on a list of Chinese words, CHSEG additionally contains specific modules for recognizing idiomatic expressions, derived words, Chinese person names, and foreign proper names.The accuracy of CHSEG on an 8.6MB corpus has been independently reported as F=84.0 (Ponte and Croft, 1996).(For reference, Ponte and Croft report scores of F=86.1 and 83.6 for their probabilistic Chinese segmentation algorithms trained on over 100MB of data.)On our test set, CHSEG produced a segmentation score of F=87.9.Our rule-based algorithm learned a sequence of 1755 transformations from the training set; applied to the test set, they improved the score from 87.9 to 89.6, a 14.0% reduction in the error rate.Our rule-based algorithm is thus able to produce an improvement to an existing high-performance system.Table 1 shows a summary of the four Chinese experiments.While Thai is also an unsegmented language, the Thai writing system is alphabetic and the average word length is greater than Chinese.7 We would therefore expect that our character-based transformations would not work as well with Thai, since a context of more than one character is necessary in many cases to make many segmentation decisions in alphabetic languages.The Thai corpus consisted of texts' from the Thai News Agency via NECTEC in Thailand.For our experiment, the training set consisted of 3367 sentences (40937 words); the test set was a separate set of 1245 sentences (13724 words) from the same corpus.The initial segmentation was performed using the maximum matching algorithm, with a lexicon of 9933 Thai words from the word separation filter in cite; a Thai language Latex package.This greedy algorithm gave an initial segmentation score of F=48.2 on the test set.Our rule-based algorithm learned a sequence of 731 transformations which improved the score from 48.2 to 63.6, a 29.7% error reduction.While the alphabetic system is obviously harder to segment, we still see a significant reduction in the segmenter error rate using the transformation-based algorithm.Nevertheless, it is doubtful that a segmentation with a score of 63.6 would be useful in too many applications, and this result will need to be significantly improved.Although English is not an unsegmented language, the writing system is alphabetic like Thai and the average word length is similar.'Since English language resources (e.g. word lists and morphological analyzers) are more readily available, it is instructive to experiment with a de-segmented English corpus, that is, English texts in which the spaces have been removed and word boundaries are not explicitly indicated.The following shows an example of an English sentence and its de-segmented version: About 20,000 years ago the last ice age ended.About20,000yearsagothelasticeageended The results of such experiments can help us determine which resources need to be compiled in order to develop a high-accuracy segmentation algorithm in unsegmented alphabetic languages such as Thai.In addition, we are also able to provide a more detailed error analysis of the English segmentation (since the author can read English but not Thai).Our English experiments were performed using a corpus of texts from the Wall Street Journal (WSJ).The training set consisted of 2675 sentences (64632 words) in which all the spaces had been removed; the test set was a separate set of 700 sentences (16318 words) from the same corpus (also with all spaces removed).For an initial experiment, segmentation was performed using the maximum matching algorithm, with a large lexicon of 34272 English words compiled from the WSJ.&quot; In contrast to the low initial Thai score, the greedy algorithm gave an initial English segmentation score of F=73.2.Our rule-based algorithm learned a sequence of 800 transformations, 'The average length of a word in our English data was 4.46 characters, compared to 5.01 for Thai and 1.60 for Chinese.10Note that the portion of the WSJ corpus used to compile the word list was independent of both the training and test sets used in the segmentation experiments. which improved the score from 73.2 to 79.0, a 21.6% error reduction.The difference in the greedy scores for English and Thai demonstrates the dependence on the word list in the greedy algorithm.For example, an experiment in which we randomly removed half of the words from the English list reduced the performance of the greedy algorithm from 73.2 to 32.3; although this reduced English word list was nearly twice the size of the Thai word list (17136 vs. 9939), the longest match segmentation utilizing the list was much lower (32.3 vs. 48.2).Successive experiments in which we removed different random sets of half the words from the original list resulted in greedy algorithm performance of 39.2, 35.1, and 35.5.Yet, despite the disparity in initial segmentation scores, the transformation sequences effect a significant error reduction in all cases, which indicates that the transformation sequences are effectively able to compensate (to some extent) for weaknesses in the lexicon.Table 2 provides a summary of the results using the greedy algorithm for each of the three languages.As mentioned above, lexical resources are more readily available for English than for Thai.We can use these resources to provide an informed initial segmentation approximation separate from the greedy algorithm.Using our native knowledge of English as well as a short list of common English prefixes and suffixes, we developed a simple algorithm for initial segmentation of English which placed boundaries after any of the suffixes and before any of the prefixes, as well as segmenting punctuation characters.In most cases, this simple approach was able to locate only one of the two necessary boundaries for recognizing full words, and the initial score was understandably low, F=29.8.Nevertheless, even from this flawed initial approximation, our rule-based algorithm learned a sequence of 632 transformations which nearly doubled the word recall, improving the score from 29.8 to 53.3, a 33.5% error reduction.Since we had a large amount of English data, we also performed a classic experiment to determine the effect the amount of training data had on the ability of the rule sequences to improve segmentation.We started with a training set only slightly larger than the test set, 872 sentences, and repeated the maximum matching experiment described in Section 3.4.1.We then incrementally increased the amount of training data and repeated the experiment.The results, summarized in Table 3, clearly indicate (not surprisingly) that more training sentences produce both a longer rule sequence and a larger error reduction in the test data.Upon inspection of the English segmentation errors produced by both the maximum matching algorithm and the learned transformation sequences, one major category of errors became clear.Most apparent was the fact that the limited context transformations were unable to recover from many errors introduced by the naive maximum matching algorithm.For example, because the greedy algorithm always looks for the longest string of characters which can be a word, given the character sequence &quot;economicsituation&quot; , the greedy algorithm first recognized &quot;economics&quot; and several shorter words, segmenting the sequence as &quot;economics it u at io n&quot;.Since our transformations consider only a single character of context, the learning algorithm was unable to patch the smaller segments back together to produce the desired output &quot;economic situation&quot;.In some cases, the transformations were able to recover some of the word, but were rarely able to produce the full desired output.For example, in one case the greedy algorithm segmented &quot;humanactivity&quot; as &quot;humana c ti vi ty&quot;.The rule sequence was able to transform this into &quot;humana ctivity&quot;, but was not able to produce the desired &quot;human activity&quot;.This suggests that both the greedy algorithm and the transformation learning algorithm need to have a more global word model, with the ability to recognize the impact of placing a boundary on the longer sequences of characters surrounding that point.The results of these experiments demonstrate that a transformation-based rule sequence, supplementing a rudimentary initial approximation, can produce accurate segmentation.In addition, they are able to improve the performance of a wide range of segmentation algorithms, without requiring expensive knowledge engineering.Learning the rule sequences can be achieved in a few hours and requires no language-specific knowledge.As discussed in Section 1, this simple algorithm could be used to adapt the output of an existing segmentation algorithm to different segmentation schemes as well as compensating for incomplete segmenter lexica, without requiring modifications to segmenters themselves.The rule-based algorithm we developed to improve word segmentation is very effective for segmenting Chinese; in fact, the rule sequences combined with a very simple initial segmentation, such as that from a maximum matching algorithm, produce performance comparable to manually-developed segmenters.As demonstrated by the experiment with the NMSU segmenter, the rule sequence algorithm can also be used to improve the output of an already highly-accurate segmenter, thus producing one of the best segmentation results reported in the literature.In addition to the excellent overall results in Chinese segmentation, we also showed the rule sequence algorithm to be very effective in improving segmentation in Thai, an alphabetic language.While the scores themselves were not as high as the Chinese performance, the error reduction was nevertheless very high, which is encouraging considering the simple rule syntax used.The current state of our algorithm, in which only three characters are considered at a time, will understandably perform better with a language like Chinese than with an alphabetic language like Thai, where average word length is much greater.The simple syntax described in Section 2.2 can, however, be easily extended to consider larger contexts to the left and the right of boundaries; this extension would necessarily come at a corresponding cost in learning speed since the size of the rule space searched during training would grow accordingly.In the future, we plan to further investigate the application of our rule-based algorithm to alphabetic languages.Acknowledgements This work would not have been possible without the assistance and encouragement of all the members of the MITRE Natural Language Group.This paper benefited greatly from discussions with and comments from Marc Vilain, Lynette Hirschman, Sam Bayer, and the anonymous reviewers.
Classifying The Semantic Relations In Noun Compounds Via A Domain-Specific Lexical HierarchyWe are developing corpus-based techniques for identifying semantic relations at an intermediate level of description (more specific than those used in case frames, but more general than those used in traditional knowledge representation systems).In this paper we describe a classification algorithm for identifying relationships between two-word noun compounds.We find that a very simple approach using a machine learning algorithm and a domain-specific lexical hierarchy successfully generalizes from training instances, performing better on previously unseen words than a baseline consisting of training on the words themselves.We are exploring empirical methods of determining semantic relationships between constituents in natural language.Our current project focuses on biomedical text, both because it poses interesting challenges, and because it should be possible to make inferences about propositions that hold between scientific concepts within biomedical texts (Swanson and Smalheiser, 1994).One of the important challenges of biomedical text, along with most other technical text, is the proliferation of noun compounds.A typical article title is shown below; it consists a cascade of four noun phrases linked by prepositions: Open-labeled long-term study of the efficacy, safety, and tolerability of subcutaneous sumatriptan in acute migraine treatment.The real concern in analyzing such a title is in determining the relationships that hold between different concepts, rather than on finding the appropriate attachments (which is especially difficult given the lack of a verb).And before we tackle the prepositional phrase attachment problem, we must find a way to analyze the meanings of the noun compounds.Our goal is to extract propositional information from text, and as a step towards this goal, we classify constituents according to which semantic relationships hold between them.For example, we want to characterize the treatment-for-disease relationship between the words of migraine treatment versus the method-of-treatment relationship between the words of aerosol treatment.These relations are intended to be combined to produce larger propositions that can then be used in a variety of interpretation paradigms, such as abductive reasoning (Hobbs et al., 1993) or inductive logic programming (Ng and Zelle, 1997).Note that because we are concerned with the semantic relations that hold between the concepts, as opposed to the more standard, syntax-driven computational goal of determining left versus right association, this has the fortuitous effect of changing the problem into one of classification, amenable to standard machine learning classification techniques.We have found that we can use such algorithms to classify relationships between two-word noun compounds with a surprising degree of accuracy.A one-out-of-eighteen classification using a neural net achieves accuracies as high as 62%.By taking advantage of lexical ontologies, we achieve strong results on noun compounds for which neither word is present in the training set.Thus, we think this is a promising approach for a variety of semantic labeling tasks.The reminder of this paper is organized as follows: Section 2 describes related work, Section 3 describes the semantic relations and how they were chosen, and Section 4 describes the data collection and ontologies.In Section 5 we describe the method for automatically assigning semantic relations to noun compounds, and report the results of experiments using this method.Section 6 concludes the paper and discusses future work.Several approaches have been proposed for empirical noun compound interpretation.Lauer and Dras (1994) point out that there are three components to the problem: identification of the compound from within the text, syntactic analysis of the compound (left versus right association), and the interpretation of the underlying semantics.Several researchers have tackled the syntactic analysis (Lauer, 1995; Pustejovsky et al., 1993; Liberman and Sproat, 1992), usually using a variation of the idea of finding the subconstituents elsewhere in the corpus and using those to predict how the larger compounds are structured.We are interested in the third task, interpretation of the underlying semantics.Most related work relies on hand-written rules of one kind or another.Finin (1980) examines the problem of noun compound interpretation in detail, and constructs a complex set of rules.Vanderwende (1994) uses a sophisticated system to extract semantic information automatically from an on-line dictionary, and then manipulates a set of hand-written rules with handassigned weights to create an interpretation.Rindflesch et al. (2000) use hand-coded rule based systems to extract the factual assertions from biomedical text.Lapata (2000) classifies nominalizations according to whether the modifier is the subject or the object of the underlying verb expressed by the head noun.1 In the related sub-area of information extraction (Cardie, 1997; Riloff, 1996), the main goal is to find every instance of particular entities or events of interest.These systems use empirical techniques to learn which terms signal entities of interest, in order to fill in pre-defined templates.Our goals are more general than those of information extraction, and so this work should be helpful for that task.However, our approach will not solve issues surrounding previously unseen proper nouns, which are often important for information extraction tasks.There have been several efforts to incorporate lexical hierarchies into statistical processing, primarily for the problem of prepositional phrase (PP) attachment.The current standard formulation is: given a verb followed by a noun and a prepositional phrase, represented by the tuple v, n1, p, n2, determine which of v or n1 the PP consisting of p and n2 attaches to, or is most closely associated with.Because the data is sparse, empirical methods that train on word occurrences alone (Hindle and Rooth, 1993) have been supplanted by algorithms that generalize one or both of the nouns according to classmembership measures (Resnik, 1993; Resnik and Hearst, 1993; Brill and Resnik, 1994; Li and Abe, 1998), but the statistics are computed for the particular preposition and verb.It is not clear how to use the results of such analysis after they are found; the semantics of the relationship between the terms must still be determined.In our framework we would cast this problem as finding the relationship R(p, n2) that best characterizes the preposition and the NP that follows it, and then seeing if the categorization algorithm determines their exists any relationship R'(n1, R(p, n2)) or R'(v,R(p,n2)).The algorithms used in the related work reflect the fact that they condition probabilities on a particular verb and noun.Resnik (1993; 1995) use classes in Wordnet (Fellbaum, 1998) and a measure of conceptual association to generalize over the nouns.Brill and Resnik (1994) use Brill’s transformation-based algorithm along with simple counts within a lexical hierarchy in order to generalize over individual words.Li and Abe (1998) use a minimum description length-based algorithm to find an optimal tree cut over WordNet for each classification problem, finding improvements over both lexical association (Hindle and Rooth, 1993) and conceptual association, and equaling the transformation-based results.Our approach differs from these in that we are using machine learning techniques to determine which level of the lexical hierarchy is appropriate for generalizing across nouns.In this work we aim for a representation that is intermediate in generality between standard case roles (such as Agent, Patient, Topic, Instrument), and the specificity required for information extraction.We have created a set of relations that are sufficiently general to cover a significant number of noun compounds, but that can be domain specific enough to be useful in analysis.We want to support relationships between entities that are shown to be important in cognitive linguistics, in particular we intend to support the kinds of inferences that arise from Talmy’s force dynamics (Talmy, 1985).It has been shown that relations of this kind can be combined in order to determine the “directionality” of a sentence (e.g., whether or not a politician is in favor of, or opposed to, a proposal) (Hearst, 1990).In the medical domain this translates to, for example, mapping a sentence into a representation showing that a chemical removes an entity that is blocking the passage of a fluid through a channel.The problem remains of determining what the appropriate kinds of relations are.In theoretical linguistics, there are contradictory views regarding the semantic properties of noun compounds (NCs).Levi (1978) argues that there exists a small set of semantic relationships that NCs may imply.Downing (1977) argues that the semantics of NCs cannot be exhausted by any finite listing of relationships.Between these two extremes lies Warren’s (1978) taxonomy of six major semantic relations organized into a hierarchical structure.We have identified the 38 relations shown in Table 1.We tried to produce relations that correspond to the linguistic theories such as those of Levi and Warren, but in many cases these are inappropriate.Levi’s classes are too general for our purposes; for example, she collapses the “location” and “time” relationships into one single class “In” and therefore field mouse and autumnal rain belong to the same class.Warren’s classification schema is much more detailed, and there is some overlap between the top levels of Warren’s hierarchy and our set of relations.For example, our “Cause (2-1)” for flu virus corresponds to her “Causer-Result” of hay fever, and our “Person Afflicted” (migraine patient) can be thought as Warren’s “Belonging-Possessor” of gunman.Warren differentiates some classes also on the basis of the semantics of the constituents, so that, for example, the “Time” relationship is divided up into “Time-Animate Entity” of weekend guests and “Time-Inanimate Entity” of Sunday paper.Our classification is based on the kind of relationships that hold between the constituent nouns rather than on the semantics of the head nouns.For the automatic classification task, we used only the 18 relations (indicated in bold in Table 1) for which an adequate number of examples were found in the current collection.Many NCs were ambiguous, in that they could be described by more than one semantic relationship.In these cases, we simply multi-labeled them: for example, cell growth is both “Activity” and “Change”, tumor regression is “Ending/reduction” and “Change” and bladder dysfunction is “Location” and “Defect”.Our approach handles this kind of multi-labeled classification.Two relation types are especially problematic.Some compounds are non-compositional or lexicalized, such as vitamin k and e2 protein; others defy classification because the nouns are subtypes of one another.This group includes migraine headache, guinea pig, and hbv carrier.We placed all these NCs in a catch-all category.We also included a “wrong” category containing word pairs that were incorrectly labeled as NCs.2 The relations were found by iterative refinement based on looking at 2245 extracted compounds (described in the next section) and finding commonalities among them.Labeling was done by the authors of this paper and a biology student; the NCs were classified out of context.We expect to continue development and refinement of these relationship types, based on what ends up clearly being use2The percentage of the word pairs extracted that were not true NCs was about 6%; some examples are: treat migraine, ten patient, headache more.We do not know, however, how many NCs we missed.The errors occurred when the wrong label was assigned by the tagger (see Section 4). ful “downstream” in the analysis.The end goal is to combine these relationships in NCs with more that two constituent nouns, like in the example intranasal migraine treatment of Section 1.To create a collection of noun compounds, we performed searches from MedLine, which contains references and abstracts from 4300 biomedical journals.We used several query terms, intended to span across different subfields.We retained only the titles and the abstracts of the retrieved documents.On these titles and abstracts we ran a part-of-speech tagger (Cutting et al., 1991) and a program that extracts only sequences of units tagged as nouns.We extracted NCs with up to 6 constituents, but for this paper we consider only NCs with 2 constituents.The Unified Medical Language System (UMLS) is a biomedical lexical resource produced and maintained by the National Library of Medicine (Humphreys et al., 1998).We use the MetaThesaurus component to map lexical items into unique concept IDs (CUIs).3 The UMLS also has a mapping from these CUIs into the MeSH lexical hierarchy (Lowe and Barnett, 1994); we mapped the CUIs into MeSH terms.There are about 19,000 unique main terms in MeSH, as well as additional modifiers.There are 15 main subhierarchies (trees) in MeSH, each corresponding to a major branch of medical ontology.For example, tree A corresponds to Anatomy, tree B to Organisms, and so on.The longer the name of the MeSH term, the longer the path from the root and the more precise the description.For example migraine is C10.228.140.546.800.525, that is, C (a disease), C10 (Nervous System Diseases), C10.228 (Central Nervous System Diseases) and so on.We use the MeSH hierarchy for generalization across classes of nouns; we use it instead of the other resources in the UMLS primarily because of MeSH’s hierarchical structure.For these experiments, we considered only those noun compounds for which both nouns can be mapped into MeSH terms, resulting in a total of 2245 NCs.Because we have defined noun compound relation determination as a classification problem, we can make use of standard classification algorithms.In particular, we used neural networks to classify across all relations simultaneously. shown in boldface are those used in the experiments reported on here.Relation ID numbers are shown in parentheses by the relation names.The second column shows the number of labeled examples for each class; the last row shows a class consisting of compounds that exhibit more than one relation.The notation (1-2) and (2-1) indicates the directionality of the relations.For example, Cause (1-2) indicates that the first noun causes the second, and Cause (2-1) indicates the converse.We ran the experiments creating models that used different levels of the MeSH hierarchy.For example, for the NC flu vaccination, flu maps to the MeSH term D4.808.54.79.429.154.349 and vaccination to G3.770.670.310.890.Flu vaccination for Model 4 would be represented by a vector consisting of the concatenation of the two descriptors showing only the first four levels: D4.808.54.79 G3.770.670.310 (see Table 2).When a word maps to a general MeSH term (like treatment, Y11) zeros are appended to the end of the descriptor to stand in place of the missing values (so, for example, treatment in Model 3 is Y 11 0, and in Model 4 is Y 11 0 0, etc.).The numbers in the MeSH descriptors are categorical values; we represented them with indicator variables.That is, for each variable we calculated the number of possible categories c and then represented an observation of the variable as a sequence of c binary variables in which one binary variable was one and the remaining c − 1 binary variables were zero.We also used a representation in which the words themselves were used as categorical input variables (we call this representation “lexical”).For this collection of NCs there were 1184 unique nouns and therefore the feature vector for each noun had 1184 components.In Table 3 we report the length of the feature vectors for one noun for each model.The entire NC was described by concatenating the feature vectors for the two nouns in sequence.The NCs represented in this fashion were used as input to a neural network.We used a feed-forward network trained with conjugate gradient descent. number corresponds to the level of the MeSH hierarchy used for classification.Lexical NN is Neural Network on Lexical and Lexical: Log Reg is Logistic Regression on NN.Acc1 refers to how often the correct relation is the top-scoring relation, Acc2 refers to how often the correct relation is one of the top two according to the neural net, and so on.Guessing would yield a result of 0.077.The network had one hidden layer, in which a hyperbolic tangent function was used, and an output layer representing the 18 relations.A logistic sigmoid function was used in the output layer to map the outputs into the interval (0, 1).The number of units of the output layer was the number of relations (18) and therefore fixed.The network was trained for several choices of numbers of hidden units; we chose the best-performing networks based on training set error for each of the models.We subsequently tested these networks on held-out testing data.We compared the results with a baseline in which logistic regression was used on the lexical features.Given the indicator variable representation of these features, this logistic regression essentially forms a table of log-odds for each lexical item.We also compared to a method in which the lexical indicator variables were used as input to a neural network.This approach is of interest to see to what extent, if any, the MeSH-based features affect performance.Note also that this lexical neural-network approach is feasible in this setting because the number of unique words is limited (1184) – such an approach would not scale to larger problems.In Table 4 and in Figure 1 we report the results from these experiments.Neural network using lexical features only yields 62% accuracy on average across all 18 relations.A neural net trained on Model 6 using the MeSH terms to represent the nouns yields an accuracy of 61% on average across all 18 relations.Note that reasonable performance is also obtained for Model 2, which is a much more general representation.Table 4 shows that both methods achieve up to 78% accuracy at including the correct relation among the top three hypothesized.Multi-class classification is a difficult problem (Vapnik, 1998).In this problem, a baseline in which Testing set performance on the best models for each MeSH level Levels of the MeSH Hierarchy the algorithm guesses yields about 5% accuracy.We see that our method is a significant improvement over the tabular logistic-regression-based approach, which yields an accuracy of only 31 percent.Additionally, despite the significant reduction in raw information content as compared to the lexical representation, the MeSH-based neural network performs as well as the lexical-based neural network.(And we again stress that the lexical-based neural network is not a viable option for larger domains.)Figure 2 shows the results for each relation.MeSH-based generalization does better on some relations (for example 14 and 15) and Lexical on others (7, 22).It turns out that the test set for relationship 7 (“Produces on a genetic level”) is dominated by NCs containing the words alleles and mrna and that all the NCs in the training set containing these words are assigned relation label 7.A similar situation is seen for relation 22, “Time(2-1)”.In the test set examples the second noun is either recurrence, season or time.In the training set, these nouns appear only in NCs that have been labeled as belonging to relation 22.On the other hand, if we look at relations 14 and 15, we find a wider range of words, and in some cases the words in the test set are not present in the training set.In relationship 14 (“Purpose”), for example, vaccine appears 6 times in the test set (e.g., varicella vaccine).In the training set, NCs with vaccine in it have also been classified as “Instrument” (antigen vaccine, polysaccharide vaccine), as “Object” (vaccine development), as “Subtype of” (opv vaccine) and as “Wrong” (vaccines using).Other words in the test set for 14 are varicella which is present in the trainig set only in varicella serology labeled as “Attribute of clinical study”, drainage which is in the training set only as “Location” (gallbladder drainage and tract drainage) and “Activity” (bile drainage).Other test set words such as immunisation and carcinogen do not appear in the training set at all.In other words, it seems that the MeSHk-based categorization does better when generalization is required.Additionally, this data set is “dense” in the sense that very few testing words are not present in the training data.This is of course an unrealistic situation and we wanted to test the robustness of the method in a more realistic setting.The results reported in Table 4 and in Figure 1 were obtained splitting the data into 50% training and 50% testing for each relation and we had a total of 855 training points and 805 test points.Of these, only 75 examples in the testing set consisted of NCs in which both words were not present in the training set.We decided to test the robustness of the MeSHbased model versus the lexical model in the case of unseen words; we are also interested in seeing the relative importance of the first versus the second noun.Therefore, we split the data into 5% training (73 data points) and 95% testing (1587 data points) and partitioned the testing set into 4 subsets as follows (the numbers in parentheses are the numbers of points for each case): Table 5 and Figures 3 and 4 present the accuracies for these test set partitions.Figure 3 shows that the MeSH-based models are more robust than the lexical when the number of unseen words is high and when the size of training set is (very) small.In this more realistic situation, the MeSH models are able to generalize over previously unseen words.For unseen words, lexical reduces to guessing.4 Figure 4 shows the accuracy for the MeSH basedmodel for the the four cases of Table 5.It is interesting to note that the accuracy for Case 1 (first noun not present in the training set) is much higher than the accuracy for Case 2 (second noun not present in the training set).This seems to indicate that the second noun is more important for the classification that the first one.We have presented a simple approach to corpusbased assignment of semantic relations for noun compounds.The main idea is to define a set of relations that can hold between the terms and use standard machine learning techniques and a lexical hierarchy to generalize from training instances to new examples.The initial results are quite promising.In this task of multi-class classification (with 18 classes) we achieved an accuracy of about 60%.These results can be compared with Vanderwende ¢Note that for unseen words, the baseline lexical-based logistic regression approach, which essentially builds a tabular representation of the log-odds for each class, also reduces to random guessing.Testing set performances for different partitions on the test set Levels of the MeSH Hierarchy els accuracies (for the entire test set and for case 4) and the dashed lines represent the corresponding lexical accuracies.The accuracies are smaller than the previous case of Table 4 because the training set is much smaller, but the point of interest is the difference in the performance of MeSH vs. lexical in this more difficult setting.Note that lexical for case 4 reduces to random guessing.Testing set performances for different partitions on the test set for the MeSH−based model Levels of the MeSH Hierarchy (1994) who reports an accuracy of 52% with 13 classes and Lapata (2000) whose algorithm achieves about 80% accuracy for a much simpler binary classification.We have shown that a class-based representation performes as well as a lexical-based model despite the reduction of raw information content and despite a somewhat errorful mapping from terms to concepts.We have also shown that representing the nouns of the compound by a very general representation (Model 2) achieves a reasonable performance of aout 52% accuracy on average.This is particularly important in the case of larger collections with a much bigger number of unique words for which the lexical-based model is not a viable option.Our results seem to indicate that we do not lose much in terms of accuracy using the more compact MeSH representation.We have also shown how MeSH-besed models out perform a lexical-based approach when the number of training points is small and when the test set consists of words unseen in the training data.This indicates that the MeSH models can generalize successfully over unseen words.Our approach handles “mixed-class” relations naturally.For the mixed class Defect in Location, the algorithm achieved an accuracy around 95% for both “Defect” and “Location” simultaneously.Our results also indicate that the second noun (the head) is more important in determining the relationships than the first one.In future we plan to train the algorithm to allow different levels for each noun in the compound.We also plan to compare the results to the tree cut algorithm reported in (Li and Abe, 1998), which allows different levels to be identified for different subtrees.We also plan to tackle the problem of noun compounds containing more than two terms.We would like to thank Nu Lai for help with the classification of the noun compound relations.This work was supported in part by NSF award number IIS-9817353.
Tuning Support Vector Machines For Biomedical Named Entity RecognitionWe explore the use of Support Vector Machines (SVMs) for biomedical named entity recognition.To make the SVM training with the available largest corpus – the GENIA corpus – tractable, we propose to split the non-entity class into sub-classes, using part-of-speech information.In addition, we explore new features such as word cache and the states of an HMM trained by unsupervised learning.Experiments on the GENIA corpus show that our class splitting technique not only enables the training with the GENIA corpus but also improves the accuracy.The proposed new features also contribute to improve the accuracy.We compare our SVMbased recognition system with a system using Maximum Entropy tagging method.Application of natural language processing (NLP) is now a key research topic in bioinformatics.Since it is practically impossible for a researcher to grasp all of the huge amount of knowledge provided in the form of natural language, e.g., journal papers, there is a strong demand for biomedical information extraction (IE), which extracts knowledge automatically from biomedical papers using NLP techniques (Ohta et al., 1997; Proux et al., 2000; Yakushiji et al., 2001).The process called named entity recognition, which finds entities that fill the information slots, e.g., proteins, DNAs, RNAs, cells etc., in the biomedical context, is an important building block in such biomedical IE systems.Conceptually, named entity recognition consists of two tasks: identification, which finds the region of a named entity in a text, and classification, which determines the semantic class of that named entity.The following illustrates biomedical named entity recognition.“Thus, CIITAPROTEZN not only activates the expression of class II genesDNA but recruits another B cell-specific coactivator to increase transcriptional activity of class II promotersDNA in Machine learning approach has been applied to biomedical named entity recognition (Nobata et al., 1999; Collier et al., 2000; Yamada et al., 2000; Shimpuku, 2002).However, no work has achieved sufficient recognition accuracy.One reason is the lack of annotated corpora for training as is often the case of a new domain.Nobata et al. (1999) and Collier et al.(2000) trained their model with only 100 annotated paper abstracts from the MEDLINE database (National Library of Medicine, 1999), and Yamada et al. (2000) used only 77 annotated paper abstracts.In addition, it is difficult to compare the techniques used in each study because they used a closed and different corpus.To overcome such a situation, the GENIA corpus (Ohta et al., 2002) has been developed, and at this time it is the largest biomedical annotated corpus available to public, containing 670 annotated abstracts of the MEDLINE database.Another reason for low accuracies is that biomedical named entities are essentially hard to recognize using standard feature sets compared with the named entities in newswire articles (Nobata et al., 2000).Thus, we need to employ powerful machine learning techniques which can incorporate various and complex features in a consistent way.Support Vector Machines (SVMs) (Vapnik, 1995) and Maximum Entropy (ME) method (Berger et al., 1996) are powerful learning methods that satisfy such requirements, and are applied successfully to other NLP tasks (Kudo and Matsumoto, 2000; Nakagawa et al., 2001; Ratnaparkhi, 1996).In this paper, we apply Support Vector Machines to biomedical named entity recognition and train them with the GENIA corpus.We formulate the named entity recognition as the classification of each word with context to one of the classes that represent region and named entity’s semantic class.Although there is a previous work that applied SVMs to biomedical named entity task in this formulation (Yamada et al., 2000), their method to construct a classifier using SVMs, one-vs-rest, fails to train a classifier with entire GENIA corpus, since the cost of SVM training is super-linear to the size of training samples.Even with a more feasible method, pairwise (Kreßel, 1998), which is employed in (Kudo and Matsumoto, 2000), we cannot train a classifier in a reasonable time, because we have a large number of samples that belong to the non-entity class in this formulation.To solve this problem, we propose to split the non-entity class to several sub-classes, using part-ofspeech information.We show that this technique not only enables the training feasible but also improves the accuracy.In addition, we explore new features such as word cache and the states of an unsupervised HMM for named entity recognition using SVMs.In the experiments, we show the effect of using these features and compare the overall performance of our SVMbased recognition system with a system using the Maximum Entropy method, which is an alternative to the SVM method.The GENIA corpus is an annotated corpus of paper abstracts taken from the MEDLINE database.Currently, 670 abstracts are annotated with named entity tags by biomedical experts and made available to public (Ver.1.1).1 These 670 abstracts are a subset of more than 5,000 abstracts obtained by the query “human AND blood cell AND transcription factor“ to the MEDLINE database.Table 1 shows basic statistics of the GENIA corpus.Since the GENIA corpus is intended to be extensive, there exist 24 distinct named entity classes in the corpus.2 Our task is to find a named entity region in a paper abstract and correctly select its class out of these 24 classes.This number of classes is relatively large compared with other corpora used in previous studies, and compared with the named entity task for newswire articles.This indicates that the task with the GENIA corpus is hard, apart from the difficulty of the biomedical domain itself.We formulate the named entity task as the classification of each word with context to one of the classes that represent region information and named entity’s semantic class.Several representations to encode region information are proposed and examined (Ramshaw and Marcus, 1995; Uchimoto et al., 2000; Kudo and Matsumoto, 2001).In this paper, we employ the simplest BIO representation, which is also used in (Yamada et al., 2000).We modify this representation in Section 5.1 in order to accelerate the SVM training.In the BIO representation, the region information is represented as the class prefixes “B-” and “I-”, and a class “O”.B- means that the current word is at the beginning of a named entity, I- means that the current word is in a named entity (but not at the beginning), and O means the word is not in a named entity.For each named entity class C, class B-C and I-C are produced.Therefore, if we have N named entity classes, the BIO representation yields 2N + 1 classes, which will be the targets of a classifier.For instance, the following corresponds to the annotation “Number of glucocorticoid receptorsPROTEIN in Support Vector Machines (SVMs) (Cortes and Vapnik, 1995) are powerful methods for learning a classifier, which have been applied successfully to many NLP tasks such as base phrase chunking (Kudo and Matsumoto, 2000) and part-of-speech tagging (Nakagawa et al., 2001).The SVM constructs a binary classifier that outputs +1 or −1 given a sample vector x E Rn.The decision is based on the separating hyperplane as follows.The class for an input x, c(x), is determined by seeing which side of the space separated by the hyperplane, w · x + b = 0, the input lies on.Given a set of labeled training samples {(y1, x1), ··· , (yL, xL)}, xi ∈ Rn, yi ∈ {+1, −1}, the SVM training tries to find the optimal hyperplane, i.e., the hyperplane with the maximum margin.Margin is defined as the distance between the hyperplane and the training samples nearest to the hyperplane.Maximizing the margin insists that these nearest samples (support vectors) exist on both sides of the separating hyperplane and the hyperplane lies exactly at the midpoint of these support vectors.This margin maximization tightly relates to the fine generalization power of SVMs.Assuming that |w·xi+b |= 1 at the support vectors without loss of generality, the SVM training can be formulated as the following optimization problem.3 The solution of this problem is known to be written as follows, using only support vectors and weights for them.In the SVM learning, we can use a function k(xi, xj) called a kernel function instead of the inner product in the above equation.Introducing a kernel function means mapping an original input x using (D(x), s.t.(D(xi)·(D(xj) = k(xi, xj) to another, usually a higher dimensional, feature space.We construct the optimal hyperplane in that space.By using kernel functions, we can construct a non-linear separating surface in the original feature space.Fortunately, such non-linear training does not increase the computational cost if the calculation of the kernel function is as cheap as the inner product.A polynomial function defined as (sxi · xj + r)d is popular in applications of SVMs to NLPs (Kudo and Matsumoto, 2000; Yamada et al., 2000; Kudo and Matsumoto, 2001), because it has an intuitively sound interpretation that each dimension of the mapped space is a 3For many real-world problems where the samples may be inseparable, we allow the constraints are broken with some penalty.In the experiments, we use so-called 1-norm soft margin formulation described as: subject to yi(w · xi + b) ≥ 1 − ei, i = 1, · · · , L, ei ≥ 0, i = 1,··· , L. (weighted) conjunction of d features in the original sample.As described above, the standard SVM learning constructs a binary classifier.To make a named entity recognition system based on the BIO representation, we require a multi-class classifier.Among several methods for constructing a multi-class SVM (Hsu and Lin, 2002), we use a pairwise method proposed by Kre13el (1998) instead of the one-vs-rest method used in (Yamada et al., 2000), and extend the BIO representation to enable the training with the entire GENIA corpus.Here we describe the one-vs-rest method and the pairwise method to show the necessity of our extension.Both one-vs-rest and pairwise methods construct a multi-class classifier by combining many binary SVMs.In the following explanation, K denotes the number of the target classes. one-vs-rest Construct K binary SVMs, each of which determines whether the sample should be classified as class i or as the other classes.The output is the class with the maximum f(x) in Equation 1. pairwise Construct K(K − 1)/2 binary SVMs, each of which determines whether the sample should be classified as class i or as class j.Each binary SVM has one vote, and the output is the class with the maximum votes.Because the SVM training is a quadratic optimization program, its cost is super-linear to the size of the training samples even with the tailored techniques such as SMO (Platt, 1998) and kernel evaluation caching (Joachims, 1998).Let L be the number of the training samples, then the one-vs-rest method takes time in K × OSVM(L).The BIO formulation produces one training sample per word, and the training with the GENIA corpus involves over 100,000 training samples as can be seen from Table 1.Therefore, it is apparent that the one-vsrest method is impractical with the GENIA corpus.On the other hand, if target classes are equally distributed, the pairwise method will take time in K(K− 1)/2 × OS VM(2L/K).This method is worthwhile because each training is much faster, though it requires the training of (K − 1)/2 times more classifiers.It is also reported that the pairwise method achieves higher accuracy than other methods in some benchmarks (Kre13el, 1998; Hsu and Lin, 2002).An input x to an SVM classifier is a feature representation of the word to be classified and its context.We use a bit-vector representation, each dimension of which indicates whether the input matches with 4 Named Entity Recognition Using ME a certain feature.The following illustrates the well- Model used features for the named entity recognition task.The Maximum Entropy method, with which we compare our SVM-based method, defines the probability that the class is c given an input vector x as follows. where Z(x) is a normalization constant, and fi(c, x) is a feature function.A feature function is defined in the same way as the features in the SVM learning, except that it includes c in it like f(c, x) = (c is the jth class) ∧ wi,k(x).If x contains previously assigned classes, then the most probable searched by using the Viterbi-type algorithm.We use the maximum entropy tagging method described in (Kazama et al., 2001) for the experiments, which is a variant of (Ratnaparkhi, 1996) modified to use HMM state features.In the above definitions, k is a relative word position from the word to be classified.A negative value represents a preceding word’s position, and a positive value represents a following word’s position.Note that we assume that the classification proceeds left to right as can be seen in the definition of the preceding class feature.For the SVM classification, we does not use a dynamic argmax-type classification such as the Viterbi algorithm, since it is difficult to define a good comparable value for the confidence of a prediction such as probability.The consequences of this limitation will be discussed with the experimental results.Features usually form a group with some variables such as the position unspecified.In this paper, we instantiate all features, i.e., instantiate for all i, for a group and a position.Then, it is convenient to denote a set of features for a group g and a position k as gk (e.g., wk and posk).Using this notation, we write a feature set as {w−1, w0, pre−1, pre0, pc−1}.4 This feature description derives the following input vector.5 In Section 3.3, we described that if target classes are equally distributed, the pairwise method will reduce the training cost.In our case, however, we have a very unbalanced class distribution with a large number of samples belonging to the class “O” (see Table 1).This leads to the same situation with the one-vsrest method, i.e., if LO is the number of the samples belonging to the class “O”, then the most dominant part of the training takes time in K × OSVM(LO).One solution to this unbalanced class distribution problem is to split the class “O” into several subclasses effectively.This will reduce the training cost for the same reason that the pairwise method works.In this paper, we propose to split the non-entity class according to part-of-speech (POS) information of the word.That is, given a part-of-speech tag set POS, we produce new |POS |classes, “Op” p ∈ POS.Since we use a POS tagger that outputs 45 Penn Treebank’s POS tags in this paper, we have new 45 sub-classes which correspond to nonentity regions such as “O-NNS” (plural nouns), “OJJ” (adjectives), and “O-DT” (determiners).Splitting by POS information seems useful for improving the system accuracy as well, because in the named entity recognition we must discriminate between nouns in named entities and nouns in ordinal noun phrases.In the experiments, we show this class splitting technique not only enables the feasible training but also improves the accuracy.In addition to the standard features, we explore word cache feature and HMM state feature, mainly to solve the data sparseness problem.Although the GENIA corpus is the largest annotated corpus for the biomedical domain, it is still small compared with other linguistic annotated corpora such as the Penn Treebank.Thus, the data sparseness problem is severe, and must be treated carefully.Usually, the data sparseness is prevented by using more general features that apply to a broader set of instances (e.g., disjunctions).While polynomial kernels in the SVM learning can effectively generate feature conjunctions, kernel functions that can effectively generate feature disjunctions are not known.Thus, we should explicitly add dimensions for such general features.The word cache feature is defined as the disjunction of several word features as: We intend that the word cache feature captures the similarities of the patterns with a common key word such as follows.We use a left word cache defined as lwck,i � wc{_k,···,0},i, and a right word cache defined as rwck,i - wc{1,···,k},i for patterns like (a) and (b) in the above example respectively.Kazama et al. (2001) proposed to use as features the Viterbi state sequence of a hidden Markov model (HMM) to prevent the data sparseness problem in the maximum entropy tagging model.An HMM is trained with a large number of unannotated texts by using an unsupervised learning method.Because the number of states of the HMM is usually made smaller than |V|, the Viterbi states give smoothed but maximally informative representations of word patterns tuned for the domain, from which the raw texts are taken.The HMM feature is defined in the same way as the word feature as follows. hmmk,i = { 1 if the Viterbi state for Wk is the ith state in the HMM’s states W 0 otherwise (HMMfeature) In the experiments, we train an HMM using raw MEDLINE abstracts in the GENIA corpus, and show that the HMM state feature can improve the accuracy.Towards practical named entity recognition using SVMs, we have tackled the following implementation issues.It would be impossible to carry out the experiments in a reasonable time without such efforts.Parallel Training: The training of pairwise SVMs has trivial parallelism, i.e., each SVM can be trained separately.Since computers with two or more CPUs are not expensive these days, parallelization is very practical solution to accelerate the training of pairwise SVMs.Fast Winner Finding: Although the pairwise method reduces the cost of training, it greatly increases the number of classifications needed to determine the class of one sample.For example, for our experiments using the GENIA corpus, the BIO representation with class splitting yields more than 4,000 classification pairs.Fortunately, we can stop classifications when a class gets K —1 votes and this stopping greatly saves classification time (Kreßel, 1998).Moreover, we can stop classifications when the current votes of a class is greater than the others’ possible votes.Support Vector Caching: In the pairwise method, though we have a large number of classifiers, each classifier shares some support vectors with other classifiers.By storing the bodies of all support vectors together and letting each classifier have only the weights, we can greatly reduce the size of the classifier.The sharing of support vectors also can be exploited to accelerate the classification by caching the value of the kernel function between a support vector and a classifiee sample.To conduct experiments, we divided 670 abstracts of the GENIA corpus (Ver.1.1) into the training part (590 abstracts; 4,487 sentences; 133,915 words) and the test part (80 abstracts; 622 sentences; 18,211 words).6 Texts are tokenized by using Penn Treebank’s tokenizer.An HMM for the HMM state features was trained with raw abstracts of the GENIA corpus (39,116 sentences).7 The number of states is 160.The vocabulary for the word feature is constructed by taking the most frequent 10,000 words from the above raw abstracts, the prefix/suffix/prefix list by taking the most frequent 10,000 prefixes/suffixes/substrings.8 The performance is measured by precision, recall, and F-score, which are the standard measures for the named entity recognition.Systems based on the BIO representation may produce an inconsistent class sequence such as “O B-DNA I-RNA O”.We interpret such outputs as follows: once a named entity starts with “B-C” then we interpret that the named entity with class “C” ends only when we see another “B-” or “O-” tag.We have implemented SMO algorithm (Platt, 1998) and techniques described in (Joachims, 1998) for soft margin SVMs in C++ programming language, and implemented support codes for pairwise classification and parallel training in Java programming language.To obtain POS information required for features and class splitting, we used an English POS tagger described in (Kazama et al., 2001).First, we show the effect of the class splitting described in Section 5.1.Varying the size of training data, we compared the change in the training time and the accuracy with and without the class splitting.We used a feature set {hw, pre, suf, sub, posi[−2,···,2],pc[−2,−1]} and the inner product kernel.9 The training time was measured on a machine with four 700MHz PentiumIIIs and 16GB RAM.Table 2 shows the results of the experiments.Figure 1 shows the results graphically.We can see that without splitting we soon suffer from super-linearity of the SVM training, while with splitting we can handle the training with over 100,000 samples in a reasonable time.It is very important that the splitting technique does not sacrifice the accuracy for speed, rather improves the accuracy.In this experiment, we see the effect of the word cache feature and the HMM state feature described in Section 3.4.The effect is assessed by the accuracy gain observed by adding each feature set to a base feature set and the accuracy degradation observed by subtracting it from a (complete) base set.The first column (A) in Table 3 shows an adding case where the base feature set is {w[−2,···,2]}.The columns (B) and (C) show subtracting cases where the base feature set is {hw, pre, suf, sub, pos, hmmi[−k,··· ,k], lwck, rwck, pc[−2,−1]} with k = 2 and k = 3 respectively.The kernel function is the inner product.We can see that word cache and HMM state features surely improve the recognition accuracy.In the table, we also included the accuracy change for other standard features.Preceeding classes and suffixes are definitely helpful.On the other hand, the substring feature is not effective in our setting.Although the effects of part-of-speech tags and prefixes are not so definite, it can be said that they are practically effective since they show positive effects in the case of the maximum performance.In this set of experiments, we compare our SVM-based system with a named entity recognition system based on the Maximum Entropy method.For the SVM system, we used the feature set {hw, pre, suf, pos, hmmi[−3,··· ,3], lwc3, rwc3, pc[−2,−1]}, which is shown to be the best in the previous experiment.The compared system is a maximum entropy tagging model described in (Kazama et al., 2001).Though it supports several character type features such as number and hyphen and some conjunctive features such as word n-gram, we do not use these features to compare the performance under as close a condition as possible.The feature set used in the maximum entropy system is expressed as {hw,pre,suf,pos,hmmi[−2,···,2], pc[−2,−1]}.10 Both systems use the BIO representation with splitting.Table 4 shows the accuracies of both systems.For the SVM system, we show the results with the inner product kernel and several polynomial kernels.The row “All (id)” shows the accuracy from the view10When the width becomes [−3,··· , 3], the accuracy degrades (53.72 to 51.73 in F-score). point of the identification task, which only finds the named entity regions.The accuracies for several major entity classes are also shown.The SVM system with the 2-dimensional polynomial kernel achieves the highest accuracy.This comparison may be unfair since a polynomial kernel has the effect of using conjunctive features, while the ME system does not use such conjunctive features.Nevertheless, the facts: we can introduce the polynomial kernel very easily; there are very few parameters to be tuned;11 we could achieve the higher accuracy; show an advantage of the SVM system.It will be interesting to discuss why the SVM systems with the inner product kernel (and the polynomial kernel with d = 1) are outperformed by the ME system.We here discuss two possible reasons.The first is that the SVM system does not use a dynamic decision such as the Viterbi algorithm, while the ME system uses it.To see this, we degrade the ME system so that it predicts the classes deterministically without using the Viterbi algorithm.We found that this system only marks 51.54 in F-score.Thus, it can be said that a dynamic decision is important for this named entity task.However, although a method to convert the outputs of a binary SVM to probabilistic values is proposed (Platt, 1999), the way to obtain meaningful probabilistic values needed in Viterbitype algorithms from the outputs of a multi-class SVM is unknown.Solving this problem is certainly a part of the future work.The second possible reason is that the SVM system in this paper does not use any cut-off or feature truncation method to remove data noise, while the ME system uses a simple feature cut-off method.12 We observed that the ME system without the cut-off only marks 49.11 in 11C, s, r, and d 12Features that occur less than 10 times are removed.F-score.Thus, such a noise reduction method is also important.However, the cut-off method for the ME method cannot be applied without modification since, as described in Section 3.4, the definition of the features are different in the two approaches.It can be said the features in the ME method is “finer” than those in SVMs.In this sense, the ME method allows us more flexible feature selection.This is an advantage of the ME method.The accuracies achieved by both systems can be said high compared with those of the previous methods if we consider that we have 24 named entity classes.However, the accuracies are not sufficient for a practical use.Though higher accuracy will be achieved with a larger annotated corpus, we should also explore more effective features and find effective feature combination methods to exploit such a large corpus maximally.We have described the use of Support Vector Machines for the biomedical named entity recognition task.To make the training of SVMs with the GENIA corpus practical, we proposed to split the nonentity class by using POS information.In addition, we explored the new types of features, word cache and HMM states, to avoid the data sparseness problem.In the experiments, we have shown that the class splitting technique not only makes training feasible but also improves the accuracy.We have also shown that the proposed new features also improve the accuracy and the SVM system with the polynomial kernel function outperforms the ME-based system.We would like to thank Dr. Jin-Dong Kim for providing us easy-to-use preprocessed training data.
Discriminative Training Methods For Hidden Markov Models: Theory And Experiments With Perceptron AlgorithmsWe describe new algorithms for train ing tagging models, as an alternativeto maximum-entropy models or conditional random elds (CRFs).The al gorithms rely on Viterbi decoding oftraining examples, combined with simple additive updates.We describe the ory justifying the algorithms througha modi cation of the proof of conver gence of the perceptron algorithm forclassi cation problems.We give experimental results on part-of-speech tag ging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger.Maximum-entropy (ME) models are justi ably a very popular choice for tagging problems in Natural Language Processing: for example see (Ratnaparkhi 96) for their use on part-of-speech tagging, and (McCallum et al 2000) for their use on a FAQ segmentation task.ME models have the advantage of being quite exible in the features that can be incorporated in the model.However, recent theoretical and experimental re sults in (Laerty et al 2001) have highlighted problems with the parameter estimation method for ME models.In response to these problems, they describe alternative parameter estimation methods based on Conditional Markov RandomFields (CRFs).(Laerty et al 2001) give experimental results suggesting that CRFs can per form signi cantly better than ME models.In this paper we describe parameter estima tion algorithms which are natural alternatives toCRFs.The algorithms are based on the percep tron algorithm (Rosenblatt 58), and the voted or averaged versions of the perceptron described in (Freund  Schapire 99).These algorithms have been shown by (Freund  Schapire 99) to be competitive with modern learning algorithms such as support vector machines; however, theyhave previously been applied mainly to classi cation tasks, and it is not entirely clear how the algorithms can be carried across to NLP tasks such as tagging or parsing.This paper describes variants of the perceptron algorithm for tagging problems.The al gorithms rely on Viterbi decoding of trainingexamples, combined with simple additive updates.We describe theory justifying the algorithm through a modi cation of the proof of convergence of the perceptron algorithm for classi cation problems.We give experimental results on part-of-speech tagging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger (a11.9% relative reduction in error for POS tag ging, a 5.1% relative reduction in error for NP chunking).Although we concentrate on taggingproblems in this paper, the theoretical frame work and algorithm described in section 3 ofthis paper should be applicable to a wide va riety of models where Viterbi-style algorithmscan be used for decoding: examples are Proba bilistic Context-Free Grammars, or ME models for parsing.See (Collins and Duy 2001; Collinsand Duy 2002; Collins 2002) for other applica tions of the voted perceptron to NLP problems.12.1 HMM Taggers.In this section, as a motivating example, we de scribe a special case of the algorithm in thispaper: the algorithm applied to a trigram tag ger.In a trigram HMM tagger, each trigram 1The theorems in section 3, and the proofs in sec tion 5, apply directly to the work in these other papers.Association for Computational Linguistics.Language Processing (EMNLP), Philadelphia, July 2002, pp.1-8.Proceedings of the Conference on Empirical Methods in Natural of tags and each tag/word pair have associated parameters.We write the parameter associated with a trigram hx; y; zi as  x;y;z, and the param eter associated with a tag/word pair (t; w) as  t;w. A common approach is to take the param eters to be estimates of conditional probabilities:  x;y;z = logP (z j x; y),  t;w = logP (w j t).For convenience we will use w [1:n]as short hand for a sequence of words [w 1 ; w 2 : : : w n ], and t [1:n] as shorthand for a taq sequence [t 1 ; t 2 : : : t n ].In a trigram tagger the score for a tagged sequence t [1:n]paired with a word se quence w [1:n] is 2 P n i=1  t i
A Phrase-Based Joint Probability Model For Statistical Machine TranslationWe present a joint probability model for statistical machine translation, which automatically learns word and phrase equivalents from bilingual corpora.Translations produced with parameters estimated using the joint model are more accurate than translations produced using IBM Model 4.1 Motivation Most of the noisy-channel-based models used in statistical machine translation (MT) (Brown et al., 1993) are conditional probability models.In the noisy-channel framework, each source sentence e in a parallel corpus is assumed to “generate” a target sentence f by means of a stochastic process, whose parameters are estimated using traditional EM techniques (Dempster et al., 1977).The generative model explains how source words are mapped into target words and how target words are re-ordered to yield well-formed target sentences.A variety of methods are used to account for the re-ordering stage: word-based (Brown et al., 1993), templatebased (Och et al., 1999), and syntax-based (Yamada and Knight, 2001), to name just a few.Although these models use different generative processes to explain how translated words are re-ordered in a target language, at the lexical level they are quite similar; all these models assume that source words are into target individual words may contain a non-existent element, called NULL.We suspect that MT researchers have so far chosen to automatically learn translation lexicons defined only over words for primarily pragmatic reasons.Large scale bilingual corpora with vocabularies in the range of hundreds of thousands yield very large translation lexicons.Tuning the probabilities associated with these large lexicons is a difficult enough task to deter one from trying to scale up to learning phrase-based lexicons.Unfortunately, trading space requirements and efficiency for explanatory power often yields non-intuitive results.Consider, for example, the parallel corpus of three sentence pairs shown in Figure 1.Intuitively, if we allow any Source words to be aligned to any Target words, the best alignment that we can come up with is the one in Figure 1.c.Sentence pair (S2, T2) offers strong evidence that “b c” in language S means the same thing as “x” in language T. On the basis of this evidence, we expect the system to also learn from sentence pair (S1, T1) that “a” in language S means the same thing as “y” in language T. Unfortunately, if one works with translation models that do not allow Target words to be aligned to more than one Source word — as it is the case in the IBM models (Brown et al., 1993) — it is impossible to learn that the phrase “b c” in language S means the same thing as word “x” in language T. The IBM Model 4 (Brown et al., 1993), for example, converges to the word alignments shown in Figure 1.b and learns the probabilities shown in Figure Since in the IBM model one cannot link a Target word to more than a Source word, the training procedure train the IBM-4 model, we used Giza (Al-Onaizan et al., 1999).IBM−4 T−Table IBM−4 Intuitive Joint Joint T−Table p(y  |a) = 1 p(x  |c) = 1 p(z  |b) = 0.98 p(x  |b) = 0.02 S1: a b c T1: x y S2: b c S1: a b c T1: x y S2: b c S1: a b c T1: x y S2: b c p(x y, a b c) = 0.32 p(x, b c) = 0.34 p(y, a) = 0.01 p(z, b) = 0.33 Corresponding Conditional Table T2: x T2: x T2: x p(x y  |a b c ) = 1 p(x  |b c) = 1 p(y  |a) = 1 p(z  |b) = 1 S3: b S3: b S3: b T3: z T3: z T3: z a) b) c) d) e) Figure 1: Alignments and probability distributions in IBM Model 4 and our joint phrase-based model. yields unintuitive translation probabilities.(Note that another good word-for-word model is one that assigns high probability to p(xb) and p(zb) and low probability to p(xc).)In this paper, we describe a translation model that assumes that lexical correspondences can be established not only at the word level, but at the phrase level as well.In constrast with many previous approaches (Brown et al., 1993; Och et al., 1999; Yamada and Knight, 2001), our model does not try to capture how Source sentences can be mapped into Target sentences, but rather how Source and Target sentences can be generated simultaneously.In other words, in the style of Melamed (2001), we estimate a joint probability model that can be easily marginalized in order to yield conditional probability models for both source-to-target and target-tosource machine translation applications.The main difference between our work and that of Melamed is that we learn joint probability models of translation equivalence not only between words but also between phrases and we show that these models can be used not only for the extraction of bilingual lexicons but also for the automatic translation of unseen sentences.In the rest of the paper, we first describe our model (Section 2) and explain how it can be implemented/trained (Section 3).We briefly describe a decoding algorithm that works in conjunction with our model (Section 4) and evaluate the performance of a translation system that uses the joint-probability model (Section 5).We end with a discussion of the strengths and weaknesses of our model as compared to other models proposed in the literature.2 A Phrase-Based Joint Probability Model 2.1 Model 1 In developing our joint probability model, we started out with a very simple generative story.We assume that each sentence pair in our corpus is generated by the following stochastic process: 1.Generate a bag of concepts.2.For each concept , generate a pair of phrases , according to the distribution contain at least one word.3.Order the phrases generated in each language so as to create two linear sequences of phrases; these sequences correspond to the sentence pairs in a bilingual corpus.For simplicity, we initially assume that the bag of concepts and the ordering of the generated phrases are modeled by uniform distributions.We do not assume that is a hidden variable that generates pair , but rather that .Under these assumptions, it follows that the probability of generating a sentence pair (E, F) using concepts given by the product of all phrase-tophrase translation probabilities, yield bags of phrases that can be ordered linearly so as to obtain the sentences E and F. For example, the sentence pair “a b c” — “x y” can be generated using two concepts, (“a b” : “y”) and (“c” : “x”); or one concept, (“a b c” : “x y”), because in both cases the phrases in each language can be arranged in a sequence that would yield the original sentence pair.However, the same sentence pair cannot be generated using the concepts (“a b” : “y”) and (“c” : “y”) because the sequence “x y” cannot be recreated from the two phrases “y” and “y”.Similarly, the pair cannot be generated using concepts (“a c” : “x”) and (“b” : “y”) because the sequence “a b c” cannot be created by catenating the phrases “a c” and “b”.We say that a set of concepts can be linearized into a sentence pair (E, F) if E and F can be obtained by permuting the phrasesandthat characterize concepts .We denote this property using the predicate .Under this model, the probability of a given sentence pair (E, F) can then be obtained by summing up over all possible ways of generating bags of concepts that can linearized to (E, F).(1) 2.2 Model 2 Although Model 1 is fairly unsophisticated, we have found that it produces in practice fairly good alignments.However, this model is clearly unsuited for translating unseen sentences as it imposes no constraints on the ordering of the phrases associated with a given concept.In order to account for this, we modify slightly the generative process in Model 1 so as to account for distortions.The generative story of Model 2 is this: 1.Generate a bag of concepts.2.Initialize E and F to empty sequences.3.Randomly take a concept and generate a pair of phrases , according to the distribution , whereandeach contain at least one word.Remove then from.4.Append phraseat the end of F. Letbe the start position ofin F. 5.Insert phraseat positionin E provided that no other phrase occupies any of the positions betweenand , wheregives length of the phrase.We hence create the alignment between the two phrasesand with probability is a position-based distortion distribution.6.Repeat steps 3 to 5 untilis empty.In Model 2, the probability to generate a sentence pair (E, F) is given by formula (2), where the position of wordof phrasein sen- F and denotes the position in tence E of the center of mass of phrase.(2) Model 2 implements an absolute position-based distortion model, in the style of IBM Model 3.We have tried many types of distortion models.We eventually settled for the model discussed here because it produces better translations during decoding.Since the number of factors involved in computing the probability of an alignment does not vary with the size of the Target phrases into which Source phrases are translated, this model is not predisposed to produce translations that are shorter than the Source sentences given as input.3 Training Training the models described in Section 2 is computationally challenging.Since there is an exponential number of alignments that can generate a sentence pair (E, F), it is clear that we cannot apply the 1.Determine high-frequency ngrams in the bilingual corpus.2.Initialize the t-distribution table.3.Apply EM training on the Viterbi alignments, while using smoothing.4.Generate conditional model probabilities.Figure 2: Training algorithm for the phrase-based joint probability model.EM training algorithm exhaustively.To estimate the parameters of our model, we apply the algorithm in Figure 2, whose steps are motivated and described below.3.1 Determine high-frequency n-grams in E and F If one assumes from the outset that any phrases can be generated from a cept , one would need a supercomputer in order to store in the memory a table that models the distribution.Since we don’t have access to computers with unlimited memory, we initially learn t distribution entries only for the phrases that occur often in the corpus and for unigrams.Then, through smoothing, we learn t distribution entries for the phrases that occur rarely as well.In order to be considered in step 2 of the algorithm, a phrase has to occur at least five times in the corpus.3.2 Initialize the t-distribution table Before the EM training procedure starts, one has no idea what word/phrase pairs are likely to share the same meaning.In other words, all alignments that can generate a sentence pair (E, F) can be assumed to have the same probability.Under these conditions, the evidence that a sentence pair (E, F) contributes to fact that are generated by the same cept is given by the number of alignments that can be built between (E, F) that have a concept that is linked to phrasein sentence E and phrase sentence F divided by the total number of alignments that can be built between the two sentences.Both these numbers can be easily approximated.Given a sentence E ofwords, there are ways in which thewords can be partitioned into sets/concepts, where is the ling number of second kind.There are also ways in which the words a sentence F can be partitioned into nonempty sets.Given that any words in E can be mapped to any words in F, it follows that there are alignments that can be built between two sentences (E, F) of lengthsand , respectively.When a concept generates two phrases of lengthand, respectively, there are only and words left to link.Hence, the absence of any other information, the probability that phrasesandare generated by the same concept is given by formula (4).Note that the fractional counts returned by equation (4) are only an approximation of the t distribution that we are interested in because the Stirling numbers of the second kind do not impose any restriction on the words that are associated with a given concept be consecutive.However, since formula (4) overestimates the numerator and denominator equally, the approximation works well in practice.In the second step of the algorithm, we apply equation (4) to collect fractional counts for all unigram and high-frequency n-gram pairs in the cartesian product defined over the phrases in each sentence pair (E, F) in a corpus.We sum over all these t-counts and we normalize to obtain an initial joint distribution.This step amounts to running the EM algorithm for one step over all possible alignments in the corpus.3.3 EM training on Viterbi alignments Given a non-uniform t distribution, phrase-to-phrase alignments have different weights and there are no other tricks one can apply to collect fractional counts over all possible alignments in polynomial time.Starting with step 3 of the algorithm in Figure 2, for each sentence pair in a corpus, we greedily produce an initial alignment by linking together phrases so as to create concepts that have high t probabilities.We then hillclimb towards the Viterbi alignment of highest probability by breaking and merging concepts, swapping words between concepts, and moving words across concepts.We compute the probabilities associated with all the alignments we generate during the hillclimbing process and collect t counts over all concepts in these alignments.We apply this Viterbi-based EM training procedure for a few iterations.The first iterations estimate the alignment probabilities using Model 1.The rest of the iterations estimate the alignment probabilities using Model 2.During training, we apply smoothing so we can associate non-zero values to phrase-pairs that do not occur often in the corpus.3.4 Derivation of conditional probability model At the end of the training procedure, we take marginals on the joint probability distributionsand .This yields conditional probability distributions and which we use for decoding.3.5 Discussion When we run the training procedure in Figure 2 on the corpus in Figure 1, after four Model 1 iterations we obtain the alignments in Figure 1.d and the joint and conditional probability distributions shown in Figure 1.e.At prima facie, the Viterbi alignment for the first sentence pair appears incorrect because we, as humans, have a natural tendency to build alignments between the smallest phrases possible.However, note that the choice made by our model is quite reasonable.After all, in the absence of additional information, the model can either assume that “a” and “y” mean the same thing or that phrases “a b c” and “x y” mean the same thing.The model chose to give more weight to the second hypothesis, while preserving some probability mass for the first one.Also note that although the joint distribution puts the second hypothesis at an advantage, the conditional distribution does not.The conditional distribution in Figure 1.e is consistent with our intuitions that tell us that it is reasonable both to translate “a b c” into “x y”, as well as “a” into “y”.The conditional distribution mirrors perfectly our intuitions.4 Decoding For decoding, we have implemented a greedy procedure similar to that proposed by Germann et al. (2001).Given a Foreign sentence F, we first produce a gloss of it by selecting phrases inthat the probability .We then tively hillclimb by modifying E and the alignment between E and F so as to maximize the formula .We hillclimb by modifying an existing alignment/translation through a set of operations that modify locally the aligment/translation built until a given time.These operations replace the English side of an alignment with phrases of different probabilities, merge and break existing concepts, and swap words across concepts.The probability p(E) is computed using a simple trigram language model that was trained using the CMU Language Modeling Toolkit (Clarkson and Rosenfeld, 1997).The language model is estimated at the word (not phrase) level.Figure 3 shows the steps taken by our decoder in order to find the translation of sentence “je vais me arrˆeter l`a .” Each intermediate translation in Figure 3 is preceded by its probability and succeded by the operation that changes it to yield a translation of higher probability.5 Evaluation To evaluate our system, we trained both Giza (IBM Model 4) (Al-Onaizan et al., 1999) and our joint probability model on a French-English parallel corpus of 100,000 sentence pairs from the Hansard corpus.The sentences in the corpus were at most 20 words long.The English side had a total of 1,073,480 words (21,484 unique tokens).The French side had a total of 1,177,143 words (28,132 unique tokens).We translated 500 unseen sentences, which were uniformly distributed across lengths 6, 8, 10, 15, and 20.For each group of 100 sentences, we manually determined the number of sentences translated perfectly by the IBM model decoder of Germann et (2001) and the decoder that uses the joint prob- Model Percent perfect translations IBM Bleu score Sentence length Sentence length 6 8 10 15 20 Avg.6 8 10 15 20 Avg.IBM 36 26 35 11 2 22 0.2076 0.2040 0.2414 0.2248 0.2011 0.2158 Phrase-based 43 37 33 19 6 28 0.2574 0.2181 0.2435 0.2407 0.2028 0.2325 Table 1: Comparison of IBM and Phrase-Based, Joint Probability Models on a translation task. je vais me arreter la . je vais me arreter la .9.46e−08 i am going to stop there .Figure 3: Example of phrase-based greedy decoding. ability model.We also evaluated the translations automatically, using the IBM-Bleu metric (Papineni et al., 2002).The results in Table 1 show that the phrased-based translation model proposed in this paper significantly outperforms IBM Model 4 on both the subjective and objective metrics.6 Discussion 6.1 Limitations The main shortcoming of the phrase-based model in this paper concerns the size of the t-table and the cost of the training procedure we currently apply.To keep the memory requirements manageable, we arbitrarily restricted the system to learning phrase translations of at most six words on each side.Also, the swap, break, and merge operations used during the Viterbi training are computationally expensive.We are currently investigating the applicability of dynamic programming techniques to increase the speed of the training procedure.Clearly, there are language pairs for which it would be helpful to allow concepts to be realized as non-contiguous phrases.The English word “not”, for example, is often translated into two French words, “ne” and “pas”.But “ne” and “pas” almost never occur in adjacent positions in French texts.At the outset of this work, we attempted to develop a translation model that enables concepts to be mapped into non-contiguous phrases.But we were not able to scale and train it on large amounts of data.The model described in this paper cannot learn that the English word “not” corresponds to the French words “ne” and “pas”.However, our model learns to deal with negation by memorizing longer phrase translation equivalents, such as (“ne est pas”, “is not”); (“est inadmissible”, “is not good enough”); and (“ne est pas ici”, “is not here”).6.2 Comparison with other work A number of researchers have already gone beyond word-level translations in various MT settings.For example, Melamed (2001) uses wordlevel alignments in order to learn translations of noncompositional compounds.Och and Ney (1999) learn phrase-to-phrase mappings involving word classes, which they call “templates”, and exploit them in a statistical machine translation system.And Marcu (2001) extracts phrase translations from automatically aligned corpora and uses them in conjunction with a word-for-word statistical translation system.However, none of these approaches learn simultaneously the translation of phrases/templates and the translation of words.As a consequence, there is a chance that the learning procedure will not discover phrase-level patterns that occur often in the je vais me arreter la .7.50e−11 FuseAndChangeTrans(&quot;la .&quot;, &quot;there .&quot;) i want me to that . je vais me arreter la .2.97e−10 ChangeWordTrans(&quot;arreter&quot;,&quot;stop&quot;) 7.75e−10 1.09e−09 i want me to there . je vais me arreter la . i want me stop there . je vais me arreter la . let me to stop there .FuseAndChange(&quot;je vais&quot;,&quot;let me&quot;) FuseAndChange(&quot;je vais me&quot;, &quot;i am going to&quot;) 1.28e−14 changeWordTrans(&quot;vais&quot;, &quot;want&quot;) i . me to that . data.In our approach, phrases are not treated differently from individual words, and as a consequence the likelihood of the EM algorithm converging to a better local maximum is increased.Working with phrase translations that are learned independent of a translation model can also affect the decoder performance.For example, in our previous work (Marcu, 2001), we have used a statistical translation memory of phrases in conjunction with a statistical translation model (Brown et al., 1993).The phrases in the translation memory were automatically extracted from the Viterbi alignments produced by Giza (Al-Onaizan et al., 1999) and reused in decoding.The decoder described in (Marcu, 2001) starts from a gloss that uses the translations in the translation memory and then tries to improve on the gloss translation by modifying it incrementally, in the style described in Section 4.However, because the decoder hill-climbs on a word-forword translation model probability, it often discards good phrasal translations in favour of word-for-word translations of higher probability.The decoder in Section 4 does not have this problem because it hillclimbs on translation model probabilities in which phrases play a crucial role.Most of the noisy-channel-based models used in statistical machine translation (MT) (Brown et al., 1993) are conditional probability models.In the noisy-channel framework, each source sentence e in a parallel corpus is assumed to “generate” a target sentence f by means of a stochastic process, whose parameters are estimated using traditional EM techniques (Dempster et al., 1977).The generative model explains how source words are mapped into target words and how target words are re-ordered to yield well-formed target sentences.A variety of methods are used to account for the re-ordering stage: word-based (Brown et al., 1993), templatebased (Och et al., 1999), and syntax-based (Yamada and Knight, 2001), to name just a few.Although these models use different generative processes to explain how translated words are re-ordered in a target language, at the lexical level they are quite similar; all these models assume that source words are individually translated into target words.1 We suspect that MT researchers have so far chosen to automatically learn translation lexicons defined only over words for primarily pragmatic reasons.Large scale bilingual corpora with vocabularies in the range of hundreds of thousands yield very large translation lexicons.Tuning the probabilities associated with these large lexicons is a difficult enough task to deter one from trying to scale up to learning phrase-based lexicons.Unfortunately, trading space requirements and efficiency for explanatory power often yields non-intuitive results.Consider, for example, the parallel corpus of three sentence pairs shown in Figure 1.Intuitively, if we allow any Source words to be aligned to any Target words, the best alignment that we can come up with is the one in Figure 1.c.Sentence pair (S2, T2) offers strong evidence that “b c” in language S means the same thing as “x” in language T. On the basis of this evidence, we expect the system to also learn from sentence pair (S1, T1) that “a” in language S means the same thing as “y” in language T. Unfortunately, if one works with translation models that do not allow Target words to be aligned to more than one Source word — as it is the case in the IBM models (Brown et al., 1993) — it is impossible to learn that the phrase “b c” in language S means the same thing as word “x” in language T. The IBM Model 4 (Brown et al., 1993), for example, converges to the word alignments shown in Figure 1.b and learns the translation probabilities shown in Figure 1.a.2 Since in the IBM model one cannot link a Target word to more than a Source word, the training procedure yields unintuitive translation probabilities.(Note that another good word-for-word model is one that assigns high probability to p(xb) and p(zb) and low probability to p(xc).)In this paper, we describe a translation model that assumes that lexical correspondences can be established not only at the word level, but at the phrase level as well.In constrast with many previous approaches (Brown et al., 1993; Och et al., 1999; Yamada and Knight, 2001), our model does not try to capture how Source sentences can be mapped into Target sentences, but rather how Source and Target sentences can be generated simultaneously.In other words, in the style of Melamed (2001), we estimate a joint probability model that can be easily marginalized in order to yield conditional probability models for both source-to-target and target-tosource machine translation applications.The main difference between our work and that of Melamed is that we learn joint probability models of translation equivalence not only between words but also between phrases and we show that these models can be used not only for the extraction of bilingual lexicons but also for the automatic translation of unseen sentences.In the rest of the paper, we first describe our model (Section 2) and explain how it can be implemented/trained (Section 3).We briefly describe a decoding algorithm that works in conjunction with our model (Section 4) and evaluate the performance of a translation system that uses the joint-probability model (Section 5).We end with a discussion of the strengths and weaknesses of our model as compared to other models proposed in the literature.In developing our joint probability model, we started out with a very simple generative story.We assume that each sentence pair in our corpus is generated by the following stochastic process: phrases , according to the distribution , whereandeach contain at least one word.For simplicity, we initially assume that the bag of concepts and the ordering of the generated phrases are modeled by uniform distributions.We do not assume that is a hidden variable that generates the pair , but rather that .Under these assumptions, it follows that the probability of generating a sentence pair (E, F) using concepts is given by the product of all phrase-tophrase translation probabilities, that yield bags of phrases that can be ordered linearly so as to obtain the sentences E and F. For example, the sentence pair “a b c” — “x y” can be generated using two concepts, (“a b” : “y”) and (“c” : “x”); or one concept, (“a b c” : “x y”), because in both cases the phrases in each language can be arranged in a sequence that would yield the original sentence pair.However, the same sentence pair cannot be generated using the concepts (“a b” : “y”) and (“c” : “y”) because the sequence “x y” cannot be recreated from the two phrases “y” and “y”.Similarly, the pair cannot be generated using concepts (“a c” : “x”) and (“b” : “y”) because the sequence “a b c” cannot be created by catenating the phrases “a c” and “b”.We say that a set of concepts can be linearized into a sentence pair (E, F) if E and F can be obtained by permuting the phrasesandthat characterize all concepts .We denote this property using the predicate .Under this model, the probability of a given sentence pair (E, F) can then be obtained by summing up over all possible ways of generating bags of concepts that can be linearized to (E, F).Although Model 1 is fairly unsophisticated, we have found that it produces in practice fairly good alignments.However, this model is clearly unsuited for translating unseen sentences as it imposes no constraints on the ordering of the phrases associated with a given concept.In order to account for this, we modify slightly the generative process in Model 1 so as to account for distortions.The generative story of Model 2 is this: a pair of phrases , according to the distribution , whereandeach contain at least one word.Remove then from. betweenand , wheregives the length of the phrase.We hence create the alignment between the two phrasesand with probability where is a position-based distortion distribution.Model 2 implements an absolute position-based distortion model, in the style of IBM Model 3.We have tried many types of distortion models.We eventually settled for the model discussed here because it produces better translations during decoding.Since the number of factors involved in computing the probability of an alignment does not vary with the size of the Target phrases into which Source phrases are translated, this model is not predisposed to produce translations that are shorter than the Source sentences given as input.Training the models described in Section 2 is computationally challenging.Since there is an exponential number of alignments that can generate a sentence pair (E, F), it is clear that we cannot apply the If one assumes from the outset that any phrases and can be generated from a concept , one would need a supercomputer in order to store in the memory a table that models the distribution.Since we don’t have access to computers with unlimited memory, we initially learn t distribution entries only for the phrases that occur often in the corpus and for unigrams.Then, through smoothing, we learn t distribution entries for the phrases that occur rarely as well.In order to be considered in step 2 of the algorithm, a phrase has to occur at least five times in the corpus.Before the EM training procedure starts, one has no idea what word/phrase pairs are likely to share the same meaning.In other words, all alignments that can generate a sentence pair (E, F) can be assumed to have the same probability.Under these conditions, the evidence that a sentence pair (E, F) contributes to the fact that are generated by the same concept is given by the number of alignments that can be built between (E, F) that have a concept that is linked to phrasein sentence E and phrase in sentence F divided by the total number of alignments that can be built between the two sentences.Both these numbers can be easily approximated.Given a sentence E ofwords, there are ways in which thewords can be partitioned into non-empty sets/concepts, where is the Stirling number of second kind.There are also ways in which the words of a sentence F can be partitioned into nonempty sets.Given that any words in E can be mapped to any words in F, it follows that there are alignments that can be built between two sentences (E, F) of lengthsand , respectively.When a concept generates two phrases of lengthand, respectively, there are only and words left to link.Hence, in the absence of any other information, the probability that phrasesandare generated by the same concept is given by formula (4).Note that the fractional counts returned by equation (4) are only an approximation of the t distribution that we are interested in because the Stirling numbers of the second kind do not impose any restriction on the words that are associated with a given concept be consecutive.However, since formula (4) overestimates the numerator and denominator equally, the approximation works well in practice.In the second step of the algorithm, we apply equation (4) to collect fractional counts for all unigram and high-frequency n-gram pairs in the cartesian product defined over the phrases in each sentence pair (E, F) in a corpus.We sum over all these t-counts and we normalize to obtain an initial joint distribution.This step amounts to running the EM algorithm for one step over all possible alignments in the corpus.Given a non-uniform t distribution, phrase-to-phrase alignments have different weights and there are no other tricks one can apply to collect fractional counts over all possible alignments in polynomial time.Starting with step 3 of the algorithm in Figure 2, for each sentence pair in a corpus, we greedily produce an initial alignment by linking together phrases so as to create concepts that have high t probabilities.We then hillclimb towards the Viterbi alignment of highest probability by breaking and merging concepts, swapping words between concepts, and moving words across concepts.We compute the probabilities associated with all the alignments we generate during the hillclimbing process and collect t counts over all concepts in these alignments.We apply this Viterbi-based EM training procedure for a few iterations.The first iterations estimate the alignment probabilities using Model 1.The rest of the iterations estimate the alignment probabilities using Model 2.During training, we apply smoothing so we can associate non-zero values to phrase-pairs that do not occur often in the corpus.At the end of the training procedure, we take marginals on the joint probability distributionsand .This yields conditional probability distributions and which we use for decoding.When we run the training procedure in Figure 2 on the corpus in Figure 1, after four Model 1 iterations we obtain the alignments in Figure 1.d and the joint and conditional probability distributions shown in Figure 1.e.At prima facie, the Viterbi alignment for the first sentence pair appears incorrect because we, as humans, have a natural tendency to build alignments between the smallest phrases possible.However, note that the choice made by our model is quite reasonable.After all, in the absence of additional information, the model can either assume that “a” and “y” mean the same thing or that phrases “a b c” and “x y” mean the same thing.The model chose to give more weight to the second hypothesis, while preserving some probability mass for the first one.Also note that although the joint distribution puts the second hypothesis at an advantage, the conditional distribution does not.The conditional distribution in Figure 1.e is consistent with our intuitions that tell us that it is reasonable both to translate “a b c” into “x y”, as well as “a” into “y”.The conditional distribution mirrors perfectly our intuitions.For decoding, we have implemented a greedy procedure similar to that proposed by Germann et al. (2001).Given a Foreign sentence F, we first produce a gloss of it by selecting phrases inthat maximize the probability .We then iteratively hillclimb by modifying E and the alignment between E and F so as to maximize the formula .We hillclimb by modifying an existing alignment/translation through a set of operations that modify locally the aligment/translation built until a given time.These operations replace the English side of an alignment with phrases of different probabilities, merge and break existing concepts, and swap words across concepts.The probability p(E) is computed using a simple trigram language model that was trained using the CMU Language Modeling Toolkit (Clarkson and Rosenfeld, 1997).The language model is estimated at the word (not phrase) level.Figure 3 shows the steps taken by our decoder in order to find the translation of sentence “je vais me arrˆeter l`a .” Each intermediate translation in Figure 3 is preceded by its probability and succeded by the operation that changes it to yield a translation of higher probability.To evaluate our system, we trained both Giza (IBM Model 4) (Al-Onaizan et al., 1999) and our joint probability model on a French-English parallel corpus of 100,000 sentence pairs from the Hansard corpus.The sentences in the corpus were at most 20 words long.The English side had a total of 1,073,480 words (21,484 unique tokens).The French side had a total of 1,177,143 words (28,132 unique tokens).We translated 500 unseen sentences, which were uniformly distributed across lengths 6, 8, 10, 15, and 20.For each group of 100 sentences, we manually determined the number of sentences translated perfectly by the IBM model decoder of Germann et al. (2001) and the decoder that uses the joint probje vais me arreter la . je vais me arreter la .9.46e−08 i am going to stop there . ability model.We also evaluated the translations automatically, using the IBM-Bleu metric (Papineni et al., 2002).The results in Table 1 show that the phrased-based translation model proposed in this paper significantly outperforms IBM Model 4 on both the subjective and objective metrics.The main shortcoming of the phrase-based model in this paper concerns the size of the t-table and the cost of the training procedure we currently apply.To keep the memory requirements manageable, we arbitrarily restricted the system to learning phrase translations of at most six words on each side.Also, the swap, break, and merge operations used during the Viterbi training are computationally expensive.We are currently investigating the applicability of dynamic programming techniques to increase the speed of the training procedure.Clearly, there are language pairs for which it would be helpful to allow concepts to be realized as non-contiguous phrases.The English word “not”, for example, is often translated into two French words, “ne” and “pas”.But “ne” and “pas” almost never occur in adjacent positions in French texts.At the outset of this work, we attempted to develop a translation model that enables concepts to be mapped into non-contiguous phrases.But we were not able to scale and train it on large amounts of data.The model described in this paper cannot learn that the English word “not” corresponds to the French words “ne” and “pas”.However, our model learns to deal with negation by memorizing longer phrase translation equivalents, such as (“ne est pas”, “is not”); (“est inadmissible”, “is not good enough”); and (“ne est pas ici”, “is not here”).A number of researchers have already gone beyond word-level translations in various MT settings.For example, Melamed (2001) uses wordlevel alignments in order to learn translations of noncompositional compounds.Och and Ney (1999) learn phrase-to-phrase mappings involving word classes, which they call “templates”, and exploit them in a statistical machine translation system.And Marcu (2001) extracts phrase translations from automatically aligned corpora and uses them in conjunction with a word-for-word statistical translation system.However, none of these approaches learn simultaneously the translation of phrases/templates and the translation of words.As a consequence, there is a chance that the learning procedure will not discover phrase-level patterns that occur often in the data.In our approach, phrases are not treated differently from individual words, and as a consequence the likelihood of the EM algorithm converging to a better local maximum is increased.Working with phrase translations that are learned independent of a translation model can also affect the decoder performance.For example, in our previous work (Marcu, 2001), we have used a statistical translation memory of phrases in conjunction with a statistical translation model (Brown et al., 1993).The phrases in the translation memory were automatically extracted from the Viterbi alignments produced by Giza (Al-Onaizan et al., 1999) and reused in decoding.The decoder described in (Marcu, 2001) starts from a gloss that uses the translations in the translation memory and then tries to improve on the gloss translation by modifying it incrementally, in the style described in Section 4.However, because the decoder hill-climbs on a word-forword translation model probability, it often discards good phrasal translations in favour of word-for-word translations of higher probability.The decoder in Section 4 does not have this problem because it hillclimbs on translation model probabilities in which phrases play a crucial role.Acknowledgments.This work was supported by DARPA-ITO grant N66001-00-1-9814 and by NSFSTTR grant 0128379.
A Bootstrapping Method For Learning Semantic Lexicons Using Extraction Pattern ContextsThis paper describes a bootstrapping algorithm called Basilisk that learns highquality semantic lexicons for multiple categories.Basilisk begins with an unannotated corpus and seed words for each semantic category, which are then bootstrapped to learn new words for each category.Basilisk hypothesizes the semantic class of a word based on collective information over a large body of extraction pattern contexts.We evaluate Basilisk on six semantic categories.The semantic lexicons produced by Basilisk have higher precision than those produced by previous techniques, with several categories showing substantial improvement.In recent years, several algorithms have been developed to acquire semantic lexicons automatically or semi-automatically using corpus-based techniques.For our purposes, the term semantic lexicon will refer to a dictionary of words labeled with semantic classes (e.g., “bird” is an ANIMAL and “truck” is a VEHICLE).Semantic class information has proven to be useful for many natural language processing tasks, including information extraction (Riloff and Schmelzenbach, 1998; Soderland et al., 1995), anaphora resolution (Aone and Bennett, 1996), question answering (Moldovan et al., 1999; Hirschman et al., 1999), and prepositional phrase attachment (Brill and Resnik, 1994).Although some semantic dictionaries do exist (e.g., WordNet (Miller, 1990)), these resources often do not contain the specialized vocabulary and jargon that is needed for specific domains.Even for relatively general texts, such as the Wall Street Journal (Marcus et al., 1993) or terrorism articles (MUC4 Proceedings, 1992), Roark and Charniak (Roark and Charniak, 1998) reported that 3 of every 5 terms generated by their semantic lexicon learner were not present in WordNet.These results suggest that automatic semantic lexicon acquisition could be used to enhance existing resources such as WordNet, or to produce semantic lexicons for specialized domains.We have developed a weakly supervised bootstrapping algorithm called Basilisk that automatically generates semantic lexicons.Basilisk hypothesizes the semantic class of a word by gathering collective evidence about semantic associations from extraction pattern contexts.Basilisk also learns multiple semantic classes simultaneously, which helps constrain the bootstrapping process.First, we present Basilisk’s bootstrapping algorithm and explain how it differs from previous work on semantic lexicon induction.Second, we present empirical results showing that Basilisk outperforms a previous algorithm.Third, we explore the idea of learning multiple semantic categories simultaneously by adding this capability to Basilisk as well as another bootstrapping algorithm.Finally, we present results showing that learning multiple semantic categories simultaneously improves performance.Basilisk (Bootstrapping Approach to SemantIc Lexicon Induction using Semantic Knowledge) is a weakly supervised bootstrapping algorithm that automatically generates semantic lexicons.Figure 1 shows the high-level view of Basilisk’s bootstrapping process.The input to Basilisk is an unannotated text corpus and a few manually defined seed words for each semantic category.Before bootstrapping begins, we run an extraction pattern learner over the corpus which generates patterns to extract every noun phrase in the corpus.The bootstrapping process begins by selecting a subset of the extraction patterns that tend to extract the seed words.We call this the pattern pool.The nouns extracted by these patterns become candidates for the lexicon and are placed in a candidate word pool.Basilisk scores each candidate word by gathering all patterns that extract it and measuring how strongly those contexts are associated with words that belong to the semantic category.The five best candidate words are added to the lexicon, and the process starts over again.In this section, we describe Basilisk’s bootstrapping algorithm in more detail and discuss related work.The input to Basilisk is a text corpus and a set of seed words.We generated seed words by sorting the words in the corpus by frequency and manually identifying the 10 most frequent nouns that belong to each category.These seed words form the initial semantic lexicon.In this section we describe the learning process for a single semantic category.In Section 3 we will explain how the process is adapted to handle multiple categories simultaneously.To identify new lexicon entries, Basilisk relies on extraction patterns to provide contextual evidence that a word belongs to a semantic class.As our representation for extraction patterns, we used the AutoSlog system (Riloff, 1996).AutoSlog’s extraction patterns represent linguistic expressions that extract a noun phrase in one of three syntactic roles: subject, direct object, or prepositional phrase object.For example, three patterns that would extract people are: “<subject> was arrested”, “murdered <direct object>”, and “collaborated with <pp object>”.Extraction patterns represent linguistic contexts that often reveal the meaning of a word by virtue of syntax and lexical semantics.Extraction patterns are typically designed to capture role relationships.For example, consider the verb “robbed” when it occurs in the active voice.The subject of “robbed” identifies the perpetrator, while the direct object of “robbed” identifies the victim or target.Before bootstrapping begins, we run AutoSlog exhaustively over the corpus to generate an extraction Generate all extraction patterns in the corpus and record their extractions. pattern for every noun phrase that appears.The patterns are then applied to the corpus and all of their extracted noun phrases are recorded.Figure 2 shows the bootstrapping process that follows, which we explain in the following sections.The first step in the bootstrapping process is to score the extraction patterns based on their tendency to extract known category members.All words that are currently defined in the semantic lexicon are considered to be category members.Basilisk scores each pattern using the RlogF metric that has been used for extraction pattern learning (Riloff, 1996).The score for each pattern is computed as: where Fi is the number of category members extracted by patterni and Ni is the total number of nouns extracted by patterni.Intuitively, the RlogF metric is a weighted conditional probability; a pattern receives a high score if a high percentage of its extractions are category members, or if a moderate percentage of its extractions are category members and it extracts a lot of them.The top N extraction patterns are put into a pattern pool.Basilisk uses a value of N=20 for the first iteration, which allows a variety of patterns to be considered, yet is small enough that all of the patterns are strongly associated with the category.1 The purpose of the pattern pool is to narrow down the field of candidates for the lexicon.Basilisk collects all noun phrases (NPs) extracted by patterns in the pattern pool and puts the head noun of each NP into the candidate word pool.Only these nouns are considered for addition to the lexicon.As the bootstrapping progresses, using the same value N=20 causes the candidate pool to become stagnant.For example, let’s assume that Basilisk performs perfectly, adding only valid category words to the lexicon.After some number of iterations, all of the valid category members extracted by the top 20 patterns will have been added to the lexicon, leaving only non-category words left to consider.For this reason, the pattern pool needs to be infused with new patterns so that more nouns (extractions) become available for consideration.To achieve this effect, we increment the value of N by one after each bootstrapping iteration.This ensures that there is always at least one new pattern contributing words to the candidate word pool on each successive iteration.The next step is to score the candidate words.For each word, Basilisk collects every pattern that extracted the word.All extraction patterns are used during this step, not just the patterns in the pattern pool.Initially, we used a scoring function that computes the average number of category members extracted by the patterns.The formula is: where Pi is the number of patterns that extract wordi, and Fj is the number of distinct category members extracted by pattern j.A word receives a high score if it is extracted by patterns that also have a tendency to extract known category members.As an example, suppose the word “Peru” is in the candidate word pool as a possible location.Basilisk finds all patterns that extract “Peru” and computes the average number of known locations extracted by those patterns.Let’s assume that the three patterns shown below extract “Peru” and that the underlined words are known locations.“Peru” would receive a score of (2+3+2)/3 = 2.3.Intuitively, this means that patterns that extract “Peru” also extract, on average, 2.3 known location words.Unfortunately, this scoring function has a problem.The average can be heavily skewed by one pattern that extracts a large number of category members.For example, suppose word w is extracted by 10 patterns, 9 which do not extract any category members but the tenth extracts 50 category members.The average number of category members extracted by these patterns will be 5.This is misleading because the only evidence linking word w with the semantic category is a single, high-frequency extraction pattern (which may extract words that belong to other categories as well).To alleviate this problem, we modified the scoring function to compute the average logarithm of the number of category members extracted by each pattern.The logarithm reduces the influence of any single pattern.We will refer to this scoring metric as the AvgLog function, which is defined below.Since log2(1) = 0, we add one to each frequency count so that patterns which extract a single category member contribute a positive value.Using this scoring metric, all words in the candidate word pool are scored and the top five words are added to the semantic lexicon.The pattern pool and the candidate word pool are then emptied, and the bootstrapping process starts over again.Several weakly supervised learning algorithms have previously been developed to generate semantic lexicons from text corpora.Riloff and Shepherd (Riloff and Shepherd, 1997) developed a bootstrapping algorithm that exploits lexical co-occurrence statistics, and Roark and Charniak (Roark and Charniak, 1998) refined this algorithm to focus more explicitly on certain syntactic structures.Hale, Ge, and Charniak (Ge et al., 1998) devised a technique to learn the gender of words.Caraballo (Caraballo, 1999) and Hearst (Hearst, 1992) created techniques to learn hypernym/hyponym relationships.None of these previous algorithms used extraction patterns or similar contexts to infer semantic class associations.Several learning algorithms have also been developed for named entity recognition (e.g., (Collins and Singer, 1999; Cucerzan and Yarowsky, 1999)).(Collins and Singer, 1999) used contextual information of a different sort than we do.Furthermore, our research aims to learn general nouns (e.g., “artist”) rather than proper nouns, so many of the features commonly used to great advantage for named entity recognition (e.g., capitalization and title words) are not applicable to our task.The algorithm most closely related to Basilisk is meta-bootstrapping (Riloff and Jones, 1999), which also uses extraction pattern contexts for semantic lexicon induction.Meta-bootstrapping identifies a single extraction pattern that is highly correlated with a semantic category and then assumes that all of its extracted noun phrases belong to the same category.However, this assumption is often violated, which allows incorrect terms to enter the lexicon.Riloff and Jones acknowledged this issue and used a second level of bootstrapping (the “Meta” bootstrapping level) to alleviate this problem.While meta-bootstrapping trusts individual extraction patterns to make unilateral decisions, Basilisk gathers collective evidence from a large set of extraction patterns.As we will demonstrate in Section 2.2, Basilisk’s approach produces better results than meta-bootstrapping and is also considerably more efficient because it uses only a single bootstrapping loop (meta-bootstrapping uses nested bootstrapping).However, meta-bootstrapping produces category-specific extraction patterns in addition to a semantic lexicon, while Basilisk focuses exclusively on semantic lexicon induction.To evaluate Basilisk’s performance, we ran experiments with the MUC-4 corpus (MUC-4 Proceedings, 1992), which contains 1700 texts associated with terrorism.We used Basilisk to learn semantic lexicons for six semantic categories: BUILDING, EVENT, HUMAN, LOCATION, TIME, and WEAPON.Before we ran these experiments, one of the authors manually labeled every head noun in the corpus that was found by an extraction pattern.These manual annotations were the gold standard.Table 1 shows the breakdown of semantic categories for the head nouns.These numbers represent a baseline: an algorithm that randomly selects words would be expected to get accuracies consistent with these numbers.Three semantic lexicon learners have previously been evaluated on the MUC-4 corpus (Riloff and Shepherd, 1997; Roark and Charniak, 1998; Riloff and Jones, 1999), and of these meta-bootstrapping achieved the best results.So we implemented the meta-bootstrapping algorithm ourselves to directly compare its performance with that of Basilisk.A difference between the original implementation and ours is that our version learns individual nouns (as does Basilisk) instead of noun phrases.We believe that learning individual nouns is a more conservative approach because noun phrases often overlap (e.g., “high-power bombs” and “incendiary bombs” would count as two different lexicon entries in the original meta-bootstrapping algorithm).Consequently, our meta-bootstrapping results differ from those reported in (Riloff and Jones, 1999).Figure 3 shows the results for Basilisk (ba-1) and meta-bootstrapping (mb-1).We ran both algorithms for 200 iterations, so that 1000 words were added to the lexicon (5 words per iteration).The X axis shows the number of words learned, and the Y axis shows how many were correct.The Y axes have different ranges because some categories are more prolific than others.Basilisk outperforms meta-bootstrapping for every category, often substantially.For the human and location categories, Basilisk learned hundreds of words, with accuracies in the 80-89% range through much of the bootstrapping.It is worth noting that Basilisk’s performance held up well on the human and location categories even at the end, achieving 79.5% (795/1000) accuracy for humans and 53.2% (532/1000) accuracy for locations.We also explored the idea of bootstrapping multiple semantic classes simultaneously.Our hypothesis was that errors of confusion2 between semantic categories can be lessened by using information about multiple categories.This hypothesis makes sense only if a word cannot belong to more than one semantic class.In general, this is not true because words are often polysemous.But within a limited domain, a word usually has a dominant word sense.Therefore we make a “one sense per domain” assumption (similar Figure 4 illustrates what happens when a semantic lexicon is generated for a single category.The seed words for the category (in this case, category C) are represented by the solid black area in category C’s territory.The hypothesized words in the growing lexicon are represented by a shaded area.The goal of the bootstrapping algorithm is to expand the area of hypothesized words so that it exactly matches the category’s true territory.If the shaded area expands beyond the category’s true territory, then incorrect words have been added to the lexicon.In Figure 4, category C has claimed a significant number of words that belong to categories B and E. When generating a lexicon for one category at a time, these confusion errors are impossible to detect because the learner has no knowledge of the other categories.Figure 5 shows the same search space when lexicons are generated for six categories simultaneously.If the lexicons cannot overlap, then we constrain the ability of a category to overstep its bounds.Category C is stopped when it begins to encroach upon the territories of categories B and E because words in those areas have already been claimed.The easiest way to take advantage of multiple categories is to add simple conflict resolution that enforces the “one sense per domain” constraint.If more than one category tries to claim a word, then we use conflict resolution to decide which category should win.We incorporated a simple conflict resolution procedure into Basilisk, as well as the metabootstrapping algorithm.For both algorithms, the conflict resolution procedure works as follows.(1) If a word is hypothesized for category A but has already been assigned to category B during a previous iteration, then the category A hypothesis is discarded.(2) If a word is hypothesized for both category A and category B during the same iteration, then it to the “one sense per discourse” observation (Gale et al., 1992)) that a word belongs to a single semantic category within a limited domain.All of our experiments involve the MUC-4 terrorism domain and corpus, for which this assumption seems appropriate.Figure 4 shows one way of viewing the task of semantic lexicon induction.The set of all words in the corpus is visualized as a search space.Each category owns a certain territory within the space (demarcated with a dashed line), representing the words that are true members of that category.Not all territories are the same size, since some categories have more members than others.���������� ���������� ���������� ���������� ���������� ���������� ���������� ���������� ��������� ��������� ��������� ��������� ��������� ��������� ��������� ��������� ��������� ��������� �������� �������� ������� ������� ������� ������� ������ ������ E......... .....r,:: ����� ����� ����� ����� ���� ���� is assigned to the category for which it receives the highest score.In Section 3.4, we will present empirical results showing how this simple conflict resolution scheme affects performance.Simple conflict resolution helps the algorithm recognize when it has encroached on another category’s territory, but it does not actively steer the bootstrapping in a more promising direction.A more intelligent way to handle multiple categories is to incorporate knowledge about other categories directly into the scoring function.We modified Basilisk’s scoring function to prefer words that have strong evidence for one category but little or no evidence for competing categories.Each word wi in the candidate word pool receives a score for category ca based on the following formula: where AvgLog is the candidate scoring function used previously by Basilisk (see Equation 3) and the max function returns the maximum AvgLog value over all competing categories.For example, the score for each candidate LOCATION word will be its AvgLog score for the LOCATION category minus its maximum AvgLog score for all other categories.A word is ranked highly only if it has a high score for the targeted category and there is little evidence that it belongs to a different category.This has the effect of steering the bootstrapping process away from ambiguous parts of the search space.We will use the abbreviation 1CAT to indicate that only one semantic category was bootstrapped, and MCAT to indicate that multiple semantic categories were simultaneously bootstrapped.Figure 6 compares the performance of Basilisk-MCAT with conflict resolution (ba-M) against Basilisk-1CAT (ba-1).Most categories show small performance gains, with the BUILDING, LOCATION, and WEAPON categories benefitting the most.However, the improvement usually doesn’t kick in until many bootstrapping iterations have passed.This phenomenon is consistent with the visualization of the search space in Figure 5.Since the seed words for each category are not generally located near each other in the search space, the bootstrapping process is unaffected by conflict resolution until the categories begin to encroach on each other’s territories.1).Learning multiple categories improves the performance of meta-bootstrapping dramatically for most categories.We were surprised that the improvement for meta-bootstrapping was much We also measured the recall of Basilisk’s lexicons after 1000 words had been learned, based on the gold standard data shown in Table 1.The recall results range from 40-60%, which indicates that a good percentage of the category words are being found, although there are clearly more category words lurking in the corpus.Basilisk’s bootstrapping algorithm exploits two ideas: (1) collective evidence from extraction patterns can be used to infer semantic category associations, and (2) learning multiple semantic categories simultaneously can help constrain the bootstrapping process.The accuracy achieved by Basilisk is substantially higher than that of previous techniques for semantic lexicon induction on the MUC-4 corpus, and empirical results show that both of Basilisk’s ideas contribute to its performance.We also demonBuilding: theatre store cathedral temple palace penitentiary academy houses school mansions Event: ambush assassination uprisings sabotage takeover incursion kidnappings clash shoot-out Human: boys snipers detainees commandoes extremists deserter narcoterrorists demonstrators cronies missionaries Location: suburb Soyapango capital Oslo regions cities neighborhoods Quito corregimiento Time: afternoon evening decade hour March weeks Saturday eve anniversary Wednesday Weapon: cannon grenade launchers firebomb car-bomb rifle pistol machineguns firearms strated that learning multiple semantic categories simultaneously improves the meta-bootstrapping algorithm, which suggests that this is a general observation which may improve other bootstrapping algorithms as well.This research was supported by the National Science Foundation under award IRI-9704240.
Japanese Dependency Analysis Using Cascaded ChunkingIn this paper, we propose a new statistical Japanese dependency parser using a cascaded chunking model.Conventional Japanese statistical dependency parsers are mainly based on a probabilistic model, which is not always efficient or scalable.We propose a new method that is simple and efficient, since it parses a sentence deterministically only deciding whether the current segment modifies the segment on its immediate right hand side.Experiments using the Kyoto University Corpus show that the method outperforms previous systems as well as improves the parsing and training efficiency.Dependency analysis has been recognized as a basic process in Japanese sentence analysis, and a number of studies have been proposed.Japanese dependency structure is usually defined in terms of the relationship between phrasal units called bunsetsu segments (hereafter “segments”).Most of the previous statistical approaches for Japanese dependency analysis (Fujio and Matsumoto, 1998; Haruno et al., 1999; Uchimoto et al., 1999; Kanayama et al., 2000; Uchimoto et al., 2000; Kudo and Matsumoto, 2000) are based on a probabilistic model consisting of the following two steps.First, they estimate modification probabilities, in other words, how probable one segment tends to modify another.Second the optimal combination of dependencies is searched from the all candidates dependencies.Such a probabilistic model is not always efficient since it needs to calculate the probabilities for all possible dependencies and creates n˙(n−1)/2 (where n is the number of segments in a sentence) training examples per sentence.In addition, the probabilistic model assumes that each pairs of dependency structure is independent.In this paper, we propose a new Japanese dependency parser which is more efficient and simpler than the probabilistic model, yet performs better in training and testing on the Kyoto University Corpus.The method parses a sentence deterministically only deciding whether the current segment modifies segment on its immediate right hand side.Moreover, it does not assume the independence constraint between dependenciesThis section describes the general formulation of the probabilistic model for parsing which has been applied to Japanese statistical dependency analysis.First of all, we define a sentence as a sequence of segments B = (b1, b2 ..., bm) and its syntactic structure as a sequence of dependency patterns D = (Dep(1), Dep(2), ... , Dep(m−1)) , where Dep(i) = j means that the segment bi depends on (modifies) segment bj.In this framework, we assume that the dependency sequence D satisfies the following two constraints.Statistical dependency analysis is defined as a searching problem for the dependency pattern D that maximizes the conditional probability P(D|B) of the input sequence under the above-mentioned constraints.If we assume that the dependency probabilities are mutually independent, P(D|B) can be rewritten as: modifies bj. fzj is an n dimensional feature vector that represents various kinds of linguistic features related to the segments bz and bj.We obtain Dbest = argmaxD P(D|B) taking into all the combination of these probabilities.Generally, the optimal solution Dbest can be identified by using bottom-up parsing algorithm such as CYK algorithm.The problem in the dependency structure analysis is how to estimate the dependency probabilities accurately.A number of statistical and machine learning approaches, such as Maximum Likelihood estimation (Fujio and Matsumoto, 1998), Decision Trees (Haruno et al., 1999), Maximum Entropy models (Uchimoto et al., 1999; Uchimoto et al., 2000; Kanayama et al., 2000), and Support Vector Machines (Kudo and Matsumoto, 2000), have been applied to estimate these probabilities.In order to apply a machine learning algorithm to dependency analysis, we have to prepare the positive and negative examples.Usually, in a probabilistic model, all possible pairs of segments that are in a dependency relation are used as positive examples, and two segments that appear in a sentence but are not in a dependency relation are used as negative examples.Thus, a total of n˙(n − 1)/2 training examples (where n is the number of segments in a sentence) must be produced per sentence.In the probabilistic model, we have to estimate the probabilities of each dependency relation.However, some machine learning algorithms, such as SVMs, cannot estimate these probabilities directly.Kudo and Matsumoto (2000) used the sigmoid function to obtain pseudo probabilities in SVMs.However, there is no theoretical endorsement for this heuristics.Moreover, the probabilistic model is not good in its scalability since it usually requires a total of n˙(n − 1)/2 training examples per sentence.It will be hard to combine the probabilistic model with some machine learning algorithms, such as SVMs, which require a polynomial computational cost on the number of given training examples.In this paper, we introduce a new method for Japanese dependency analysis, which does not require the probabilities of dependencies and parses a sentence deterministically.The proposed method can be combined with any type of machine learning algorithm that has classification ability.The original idea of our method stems from the cascaded chucking method which has been applied in English parsing (Abney, 1991).Let us introduce the basic framework of the cascaded chunking parsing method: We apply this cascaded chunking parsing technique to Japanese dependency analysis.Since Japanese is a head-final language, and the chunking can be regarded as the creation of a dependency between two segments, we can simplify the process of Japanese dependency analysis as follows: Figure 1 shows an example of the parsing process with the cascaded chunking model.The input for the model is the linguistic features related to the modifier and modifiee, and the output from the model is either of the tags (D or O).In training, the model simulates the parsing algorithm by consulting the correct answer from the training annotated corpus.During the training, positive (D) and negative (O) examples are collected.In testing, the model consults the trained system and parses the input with the cascaded chunking algorithm.We think this proposed cascaded chunking model has the following advantages compared with the traditional probabilistic models.If we use the CYK algorithm, the probabilistic model requires O(n3) parsing time, (where n is the number of segments in a sentence.).On the other hand, the cascaded chunking model requires O(n2) in the worst case when all segments modify the rightmost segment.The actual parsing time is usually lower than O(n2), since most of segments modify segment on its immediate right hand side.Furthermore, in the cascaded chunking model, the training examples are extracted using the parsing algorithm itself.The training examples required for the cascaded chunking model is much smaller than that for the probabilistic model.The model reduces the training cost significantly and enables training using larger amounts of annotated corpus.• No assumption on the independence between dependency relations The probabilistic model assumes that dependency relations are independent.However, there are some cases in which one cannot parse a sentence correctly with this assumption.For example, coordinate structures cannot be always parsed with the independence constraint.The cascaded chunking model parses and estimates relations simultaneously.This means that one can use all dependency relations, which have narrower scope than that of the current focusing relation being considered, as feature sets.We describe the details in the next section.The cascaded chunking model can be combined with any machine learning algorithm that works as a binary classifier, since the cascaded chunking model parses a sentence deterministically only deciding whether or not the current segment modifies the segment on its immediate right hand side.Probabilities of dependencies are not always necessary for the cascaded chunking model.Linguistic features that are supposed to be effective in Japanese dependency analysis are: head words and their parts-of-speech tags, functional words and inflection forms of the words that appear at the end of segments, distance between two segments, existence of punctuation marks.As those are solely defined by the pair of segments, we refer to them as the static features.Japanese dependency relations are heavily constrained by such static features since the inflection forms and postpositional particles constrain the dependency relation.However, when a sentence is long and there are more than one possible dependency, static features, by themselves cannot determine the correct dependency.To cope with this problem, Kudo and Matsumoto (2000) introduced a new type of features called dynamic features, which are created dynamically during the parsing process.For example, if some relation is determined, this modification relation may have some influence on other dependency relation.Therefore, once a segment has been determined to modify another segment, such information is kept in both of the segments and is added to them as a new feature.Specifically, we take the following three types of dynamic features in our experiments.He her warm heart be moved A.The segments which modify the current candidate modifiee.(boxes marked with A in Figure 2) B.The segments which modify the current candidate modifier.(boxes marked with B in Figure 2) C. The segment which is modified by the current candidate modifiee.(boxes marked with C in Figure 2)Although any kind of machine learning algorithm can be applied to the cascaded chunking model, we use Support Vector Machines (Vapnik,1998) for our experiments because of their state-of-the-art performance and generalization ability.SVM is a binary linear classifier trained from the samples, each of which belongs either to positive or negative class as follows: (x1, y1), ... , (xl, yl) (xi E Rn, yi E {+1, −1}), where xi is a feature vector of the i-th sample represented by an n dimensional vector, and yi is the class (positive(+1) or negative(−1) class) label of the i-th sample.SVMs find the optimal separating hyperplane (w • x + b) based on the maximal margin strategy.The margin can be seen as the distance between the critical examples and the separating hyperplane.We omit the details here, the maximal margin strategy can be realized by the following optimization problem:Furthermore, SVMs have the potential to carry out non-linear classifications.Though we leave the details to (Vapnik, 1998), the optimization problem can be rewritten into a dual form, where all feature vectors appear as their dot products.By simply substituting every dot product of xi and xj in dual form with a Kernel function K(xi, xj), SVMs can handle non-linear hypotheses.Among many kinds of Kernel functions available, we will focus on the dth polynomial kernel: K(xi, xj) = (xi • xj + 1)d. Use of d-th polynomial kernel functions allows us to build an optimal separating hyperplane which takes into account all combinations of features up to d.We used the following two annotated corpora for our experiments.This data set consists of the Kyoto University text corpus Version 2.0 (Kurohashi and Nagao, 1997).We used 7,958 sentences from the articles on January 1st to January 7th as training examples, and 1,246 sentences from the articles on January 9th as the test data.This data set was used in (Uchimoto et al., 1999; Uchimoto et al., 2000) and (Kudo and Matsumoto, 2000).In order to investigate the scalability of the cascaded chunking model, we prepared larger data set.We used all 38,383 sentences of the Kyoto University text corpus Version 3.0.The training and test data were generated by a two-fold cross validation.The feature sets used in our experiments are shown in Table 1.The static features are basically taken from Uchimoto’s list (Uchimoto et al., 1999).Head Word (HW) is the rightmost content word in the segment.Functional Word (FW) is set as follows: - FW = the rightmost functional word, if there is a functional word in the segment - FW = the rightmost inflection form, if there is a predicate in the segment - FW = same as the HW, otherwise.The static features include the information on existence of brackets, question marks and punctuation marks, etc.Besides, there are features that show the relative relation of two segments, such as distance, and existence of brackets, quotation marks and punctuation marks between them.For a segment X and its dynamic feature Y (where Y is of type A or B), we set the Functional Representation (FR) feature of X based on the FW of X (X-FW) as follows: - FR = lexical form of X-FW if POS of X-FW is particle, adverb, adnominal or conjunction - FR = inflectional form ofX-FW ifX-FW has an inflectional form.- FR = the POS tag ofX-FW, otherwise.For a segment X and its dynamic feature C, we set POS tag and POS-subcategory of the HW of X.All our experiments are carried out on AlphaSever 8400 (21164A 500Mhz) for training and Linux (PentiumIII 1GHz) for testing.We used a third degree polynomial kernel function, which is exactly the same setting in (Kudo and Matsumoto, 2000).Performance on the test data is measured using dependency accuracy and sentence accuracy.Dependency accuracy is the percentage of correct dependencies out of all dependency relations.Sentence accuracy is the percentage of sentences in which all dependencies are determined correctly.The results for the new cascaded chunking model as well as for the previous probabilistic model based on SVMs (Kudo and Matsumoto, 2000) are summarized in Table 2.We cannot employ the experiments for the probabilistic model using large dataset, since the data size is too large for our current SVMs learning program to terminate in a realistic time period.Even though the number of training examples used for the cascaded chunking model is less than a quarter of that for the probabilistic model, and the used feature set is the same, dependency accuracy and sentence accuracy are improved using the cascaded chunking model (89.09% → 89.29%, 46.17% → 47.53%).The time required for training and parsing are significantly reduced by applying the cascaded chunking model (336h.→8h, 2.1sec.→ 0.5sec.).As can be seen Table 2, the cascaded chunking model is more accurate, efficient and scalable than the probabilistic model.It is difficult to apply the probabilistic model to the large data set, since it takes no less than 336 hours (2 weeks) to carry out the experiments even with the standard data set, and SVMs require quadratic or more computational cost on the number of training examples.For the first impression, it may seems natural that higher accuracy is achieved with the probabilistic model, since all candidate dependency relations are used as training examples.However, the experimental results show that the cascaded chunking model performs better.Here we list what the most significant contributions are and how well the cascaded chunking model behaves compared with the probabilistic model.The probabilistic model is trained with all candidate pairs of segments in the training corpus.The problem of this training is that exceptional dependency relations may be used as training examples.For example, suppose a segment which appears to right hand side of the correct modifiee and has a similar content word, the pair with this segment becomes a negative example.However, this is negative because there is a better and correct candidate at a different point in the sentence.Therefore, this may not be a true negative example, meaning that this can be positive in other sentences.In addition, if a segment is not modified by a modifier because of cross dependency constraints but has a similar content word with correct modifiee, this relation also becomes an exception.Actually, we cannot ignore these exceptions, since most segments modify a segment on its immediate right hand side.By using all candidates of dependency relation as the training examples, we have committed to a number of exceptions which are hard to be trained upon.Looking in particular on a powerful heuristics for dependency structure analysis: “A segment tends to modify a nearer segment if possible,” it will be most important to train whether the current segment modifies the segment on its immediate right hand side.The cascaded chunking model is designed along with this heuristics and can remove the exceptional relations which has less potential to improve performance.Figure 3 shows the relationship between the size of the training data and the parsing accuracy.This figure also shows the accuracy with and without the dynamic features.Generally, the results with the dynamic feature set is better than the results without it.The dynamic features constantly outperform static features when the size of the training data is large.In most cases, the improvements is considerable.Table 3 summarizes the performance without some dynamic features.From these results, we can conclude that all dynamic features are effective in improving the performance.Table 4 summarizes recent results on Japanese dependency analysis.Uchimoto et al. (2000) report that using the Kyoto University Corpus for their training and testing, they achieve around 87.93% accuracy by building statistical model based on the Maximum Entropy framework.They extend the original probabilistic model, which learns only two class; ‘modify‘ and ‘not modify‘, to the one that learns three classes; ‘between‘, ‘modify‘ and ‘beyond‘.Their model can also avoid the influence of the exceptional dependency relations.Using same training and test data, we can achieve accuracy of 89.29%.The difference is considerable.Kanayama et al. (2000) use an HPSG-based Japanese grammar to restrict the candidate dependencies.Their model uses at most three candidates restricted by the grammar as features; the nearest, the second nearest, and the farthest from the modifier.Thus, their model can take longer context into account, and disambiguate complex dependency relations.However, the features are still static, and dynamic features are not used in their model.We cannot directly compare their model with ours because they use a different corpus, EDR corpus, which is ten times as large as the corpus we used.Nevertheless, they reported an accuracy 88.55%, which is worse than our model.Haruno et al. (99) report that using the EDR Corpus for their training and testing, they achieve around 85.03% accuracy with Decision Tree and Boosting.Although Decision Tree can take combinations of features as SVMs, it easily overfits on its own.To avoid overfitting, Decision Tree is usually used as an weak learner for Boosting.Combining Boosting technique with Decision Tree, the performance may be improved.However, Haruno et al. (99) report that the performance with Decision Tree falls down when they added lexical entries with lower frequencies as features even using Boosting.We think that Decision Tree requires a careful feature selection for achieving higher accuracy.We presented a new Japanese dependency parser using a cascaded chunking model which achieves 90.46% accuracy using the Kyoto University Corpus.Our model parses a sentence deterministically only deciding whether the current segment modifies the segment on its immediate right hand side.Our model outperforms the previous probabilistic model with respect to accuracy and efficiency.In addition, we showed that dynamic features significantly contribute to improve the performance.
Identifying Semantic Roles Using Combinatory Categorial GrammarWe present a system for automatically identifying PropBank-style semantic roles based on the output of a statistical parser for Combinatory Categorial Grammar.This system performs at least as well as a system based on a traditional Treebank parser, and outperforms it on core argument roles.Correctly identifying the semantic roles of sentence constituents is a crucial part of interpreting text, and in addition to forming an important part of the information extraction problem, can serve as an intermediate step in machine translation or automatic summarization.Even for a single predicate, semantic arguments can have multiple syntactic realizations, as shown by the following paraphrases: Recently, attention has turned to creating corpora annotated with argument structures.The PropBank (Kingsbury and Palmer, 2002) and the FrameNet (Baker et al., 1998) projects both document the variation in syntactic realization of the arguments of predicates in general English text.Gildea and Palmer (2002) developed a system to predict semantic roles (as defined in PropBank) from sentences and their parse trees as determined by the statistical parser of Collins (1999).In this paper, we examine how the syntactic representations used by different statistical parsers affect the performance of such a system.We compare a parser based on Combinatory Categorial Grammar (CCG) (Hockenmaier and Steedman, 2002b) with the Collins parser.As the CCG parser is trained and tested on a corpus of CCG derivations that have been obtained by automatic conversion from the Penn Treebank, we are able to compare performance using both goldstandard and automatic parses for both CCG and the traditional Treebank representation.The Treebankparser returns skeletal phrase-structure trees without the traces or functional tags in the original Penn Treebank, whereas the CCG parser returns wordword dependencies that correspond to the underlying predicate-argument structure, including longrange dependencies arising through control, raising, extraction and coordination.The Proposition Bank (Kingsbury and Palmer, 2002) provides a human-annotated corpus of semantic verb-argument relations.For each verb appearing in the corpus, a set of semantic roles is defined.Roles for each verb are simply numbered Arg0, Arg1, Arg2, etc.As an example, the entryspecific roles for the verb offer are given below: These roles are then annotated for every instance of the verb appearing in the corpus, including the following examples: A variety of additional roles are assumed to apply across all verbs.These secondary roles can be thought of as being adjuncts, rather than arguments, although no claims are made as to optionality or other traditional argument/adjunct tests.The secondary roles include: Location in Tokyo, outside Time last week, on Tuesday, never Manner easily, dramatically Direction south, into the wind Cause due to pressure from Washington Discourse however, also, on the other hand Extent 15%, 289 points Purpose to satisfy requirements Negation not, n’t Modal can, might, should, will Adverbial (none of the above) and are represented in PropBank as “ArgM” with an additional function tag, for example ArgM-TMP for temporal.We refer to PropBank’s numbered arguments as “core” arguments.Core arguments represent 75% of the total labeled roles in the PropBank data.Our system predicts all the roles, including core arguments as well as the ArgM labels and their function tags.Combinatory Categorial Grammar (CCG) (Steedman, 2000), is a grammatical theory which provides a completely transparent interface between surface syntax and underlying semantics, such that each syntactic derivation corresponds directly to an interpretable semantic representation which includes long-range dependencies that arise through control, raising, coordination and extraction.In CCG, words are assigned atomic categories such as NP, or functor categories like (S[dcl]\NP)/NP (transitive declarative verb) or S/S (sentential modifier).Adjuncts are represented as functor categories such as S/S which expect and return the same type.We use indices to number the arguments of functor categories, eg.(S[dcl]\NP1)/NP2, or S/S1, and indicate the wordword dependencies in the predicate-argument structure as tuples (wh, ch, i, wa), where ch is the lexical category of the head word wh, and wa is the head word of the constituent that fills the ith argument of ch.Long-range dependencies can be projected through certain types of lexical categories or through rules such as coordination of functor categories.For example, in the lexical category of a relative pronoun, (NP\NP;)/(S[dcl]/NP;), the head of the NP that is missing from the relative clause is unified with (as indicated by the indices i) the head of the NP that is modified by the entire relative clause.Figure 1 shows the derivations of an ordinary sentence, a relative clause and a right-node-raising construction.In all three sentences, the predicateargument relations between London and denied and plans and denied are the same, which in CCG is expressed by the fact that London fills the first (ie. subject) argument slot of the lexical category of denied, (S[dcl]\NP1)/NP2, and plans fills the second (object) slot.The relations extracted from the CCG derivation for the sentence “London denied plans on Monday” are shown in Table 1.The CCG parser returns the local and long-range word-word dependencies that express the predicateargument structure corresponding to the derivation.These relations are recovered with an accuracy of around 83% (labeled recovery) or 91% (unlabeled recovery) (Hockenmaier, 2003).By contrast, standard Treebank parsers such as (Collins, 1999) only return phrase-structure trees, from which non-local dependencies are difficult to recover.The CCG parser has been trained and tested on CCGbank (Hockenmaier and Steedman, 2002a), a treebank of CCG derivations obtained from the Penn Treebank, from which we also obtain our training data.Our aim is to use CCG derivations as input to a system for automatically producing the argument labels of PropBank.In order to do this, we wish to correlate the CCG relations above with PropBank arguments.PropBank argument labels are assigned to nodes in the syntactic trees from the Penn Treebank.While the CCGbank is derived from the Penn Treebank, in many cases the constituent structures do not correspond.That is, there may be no constituent in the CCG derivation corresponding to the same sequence of words as a particular constituent in the Treebank tree.For this reason, we compute the correspondence between the CCG derivation and the PropBank labels at the level of head words.For each role label for a verb’s argument in PropBank, we first find the head word for its constituent according to the the head rules of (Collins, 1999).We then look for the label of the CCG relation between this head word and the verb itself.In previous work using the PropBank corpus, Gildea and Palmer (2002) developed a system to predict semantic roles from sentences and their parse trees as determined by the statistical parser of Collins (1999).We will briefly review their probability model before adapting the system to incorporate features from the CCG derivations.For the Treebank-based system, we use the probability model of Gildea and Palmer (2002).Probabilities of a parse constituent belonging to a given semantic role are calculated from the following features: The phrase type feature indicates the syntactic type of the phrase expressing the semantic roles: examples include noun phrase (NP), verb phrase (VP) , and clause (S).The parse tree path feature is designed to capture the syntactic relation of a constituent to the predicate.It is defined as the path from the predicate through the parse tree to the constituent in question, represented as a string of parse tree nonterminals linked by symbols indicating upward or downward movement through the tree, as shown in Figure 2.Although the path is composed as a string of symbols, our systems will treat the string as an atomic value.The path includes, as the first element of the string, the part of speech of the predicate, and, as the last element, the phrase type or syntactic category of the sentence constituent marked as an argument.He ate some pancakes Figure 2: In this example, the path from the predicate ate to the argument NP He can be represented as VBTVPTS1NP, with T indicating upward movement in the parse tree and 1 downward movement.The position feature simply indicates whether the constituent to be labeled occurs before or after the predicate.This feature is highly correlated with grammatical function, since subjects will generally appear before a verb, and objects after.This feature may overcome the shortcomings of reading grammatical function from the parse tree, as well as errors in the parser output.The voice feature distinguishes between active and passive verbs, and is important in predicting semantic roles because direct objects of active verbs correspond to subjects of passive verbs.An instance of a verb was considered passive if it is tagged as a past participle (e.g. taken), unless it occurs as a descendent verb phrase headed by any form of have (e.g. has taken) without an intervening verb phrase headed by any form of be (e.g. has been taken).The head word is a lexical feature, and provides information about the semantic type of the role filler.Head words of nodes in the parse tree are determined using the same deterministic set of head word rules used by Collins (1999).The system attempts to predict argument roles in new data, looking for the highest probability assignment of roles ri to all constituents i in the sentence, given the set of features Fi = {pti, pathi, posi, vi, hi} at each constituent in the parse tree, and the predicate p: We break the probability estimation into two parts, the first being the probability P(riIFi, p) of a constituent’s role given our five features for the consituent, and the predicate p. Due to the sparsity of the data, it is not possible to estimate this probability from the counts in the training data.Instead, probabilities are estimated from various subsets of the features, and interpolated as a linear combination of the resulting distributions.The interpolation is performed over the most specific distributions for which data are available, which can be thought of as choosing the topmost distributions available from a backoff lattice, shown in Figure 3.The probabilities P(ri|Fi, p) are combined with the probabilities P({r1..n}|p) for a set of roles appearing in a sentence given a predicate, using the following formula: This approach, described in more detail in Gildea and Jurafsky (2002), allows interaction between the role assignments for individual constituents while making certain independence assumptions necessary for efficient probability estimation.In particular, we assume that sets of roles appear independent of their linear order, and that the features F of a constituents are independent of other constituents’ features given the constituent’s role.In the CCG version, we replace the features above with corresponding features based on both the sentence’s CCG derivation tree (shown in Figure 1) and the CCG predicate-argument relations extracted from it (shown in Table 1).The parse tree path feature, designed to capture grammatical relations between constituents, is replaced with a feature defined as follows: If there is a dependency in the predicate-argument structure of the CCG derivation between two words w and w', the path feature from w to w' is defined as the lexical category of the functor, the argument slot i occupied by the argument, plus an arrow (← or →) to indicate whether w or w' is the categorial functor.For example, in our sentence “London denied plans on Monday”, the relation connecting the verb denied with plans is (S[dcl]\NP)/NP.2.←, with the left arrow indicating the lexical category included in the relation is that of the verb, while the relation connecting denied with on is ((S\NP)\(S\NP))/NP.2.→, with the right arrow indicating the the lexical category included in the relation is that of the modifier.If the CCG derivation does not define a predicateargument relation between the two words, we use the parse tree path feature described above, defined over the CCG derivation tree.In our training data, 77% of PropBank arguments corresponded directly to a relation in the CCG predicate-argument representation, and the path feature was used for the remaining 23%.Most of these mismatches arise because the CCG parser and PropBank differ in their definition of head words.For instance, the CCG parser always assumes that the head of a PP is the preposition, whereas PropBank roles can be assigned to the entire PP (7), or only to the NP argument of the preposition (8), in which case the head word comes from the NP: In embedded clauses, CCG assumes that the head is the complementizer, whereas in PropBank, the head comes from the embedded sentence itself.In complex verb phrases (eg.“might not have gone”), the CCG parser assumes that the first auxiliary (might) is head, whereas PropBank assumes it is the main verb (gone).Therefore, CCG assumes that not modifies might, whereas PropBank assumes it modifies gone.Although the head rules of the parser could in principle be changed to reflect more directly the dependencies in PropBank, we have not attempted to do so yet.Further mismatches occur because the predicate-argument structure returned by the CCG parser only contains syntactic dependencies, whereas the PropBank data also contain some anaphoric dependencies, eg.: Such dependencies also do not correspond to a relation in the predicate-argument structure of the CCG derivation, and cause the path feature to be used.The phrase type feature is replaced with the lexical category of the maximal projection of the PropBank argument’s head word in the CCG derivation tree.For example, the category of plans is N, and the category of denied is (S[dcl]\NP)/NP.The voice feature can be read off the CCG categories, since the CCG categories of past participles carry different features in active and passive voice (eg. sold can be (S[pt]\NP)/NP or S[pss]\NP).The head word of a constituent is indicated in the derivations returned by the CCG parser.We use data from the November 2002 release of PropBank.The dataset contains annotations for 72,109 predicate-argument structures with 190,815 individual arguments (of which 75% are core, or numbered, arguments) and has includes examples from 2462 lexical predicates (types).Annotations from Sections 2 through 21 of the Treebank were used for training; Section 23 was the test set.Both parsers were trained on Sections 2 through 21.Because of the mismatch between the constituent structures of CCG and the Treebank, we score both systems according to how well they identify the head words of PropBank’s arguments.Table 2 gives the performance of the system on both PropBank’s core, or numbered, arguments, and on all PropBank roles including the adjunct-like ArgM roles.In order to analyze the impact of errors in the syntactic parses, we present results using features extracted from both automatic parser output and the gold standard parses in the Penn Treebank (without functional tags) and in CCGbank.Using the gold standard parses provides an upper bound on the performance of the system based on automatic parses.Since the Collins parser does not provide trace information, its upper bound is given by the system tested on the gold-standard Treebank representation with traces removed.In Table 2, “core” indicates results on PropBank’s numbered arguments (ARG0...ARG5) only, and “all” includes numbered arguments as well as the ArgM roles.Most of the numbered arguments (in particular ARG0 and ARG1) correspond to arguments that the CCG category of the verb directly subcategorizes for.The CCG-based system outperforms the system based on the Collins parser on these core arguments, and has comparable performance when all PropBank labels are considered.We believe that the superior performance of the CCG system on this core arguments is due to its ability to recover long-distance dependencies, whereas we attribute its lower performance on non-core arguments mainly to the mismatches between PropBank and CCGbank.The importance of long-range dependencies for our task is indicated by the fact that the performance on the Penn Treebank gold standard without traces is significantly lower than that on the Penn Treebank with trace information.Long-range dependencies are especially important for core arguments, shown by the fact that removing trace information from the Treebank parses results in a bigger drop for core arguments (83.5 to 76.3 F-score) than for all roles (74.1 to 70.2).The ability of the CCG parser to recover these long-range dependencies accounts for its higher performance, and in particular its higher recall, on core arguments.The CCG gold standard performance is below that of the Penn Treebank gold standard with traces.We believe this performance gap to be caused by the mismatches between the CCG analyses and the PropBank annotations described in Section 5.2.For the reasons described, the head words of the constituents that have PropBank roles are not necessarily the head words that stand in a predicate-argument relation in CCGbank.If two words do not stand in a predicate-argument relation, the CCG system takes recourse to the path feature.This feature is much sparser in CCG: since CCG categories encode subcategorization information, the number of categories in CCGbank is much larger than that of Penn Treebank labels.Analysis of our system’s output shows that the system trained on the Penn Treebank gold standard obtains 55.5% recall on those relations that require the CCG path feature, whereas the system using CCGbank only achieves 36.9% recall on these.Also, in CCG, the complement-adjunct distinction is represented in the categories for the complement (eg.PP) or adjunct (eg.(S\NP)\(S\NP) and in the categories for the head (eg.(S[dcl]\NP)/PP or S[dcl]\NP).In generating the CCGbank, various heuristics were used to make this distinction.In particular, for PPs, it depends on the “closely-related” (CLR) function tag, which is known to be unreliable.The decisions made in deriving the CCGbank often do not match the hand-annotated complementadjunct distinctions in PropBank, and this inconsistency is likely to make our CCGbank-based features less predictive.A possible solution is to regenerate the CCGbank using the Propbank annotations.The impact of our head-word based scoring is analyzed in Table 3, which compares results when only the head word must be correctly identified (as in Table 2) and to results when both the beginning and end of the argument must be correctly identified in the sentence (as in Gildea and Palmer (2002)).Even if the head word is given the correct label, the boundaries of the entire argument may be different from those given in the PropBank annotation.Since constituents in CCGbank do not always match those in PropBank, even the CCG gold standard parses obtain comparatively low scores according to this metric.This is exacerbated when automatic parses are considered.Our CCG-based system for automatically labeling verb arguments with PropBank-style semantic roles outperforms a system using a traditional Treebankbased parser for core arguments, which comprise 75% of the role labels, but scores lower on adjunctlike roles such as temporals and locatives.The CCG parser returns predicate-argument structures that include long-range dependencies; therefore, it seems inherently better suited for this task.However, the performance of our CCG system is lowered by the fact that the syntactic analyses in its training corpus differ from those that underlie PropBank in important ways (in particular in the notion of heads and the complement-adjunct distinction).We would expect a higher performance for the CCG-based system if the analyses in CCGbank resembled more closely those in PropBank.Our results also indicate the importance of recovering long-range dependencies, either through the trace information in the Penn Treebank, or directly, as in the predicate-argument structures returned by the CCG parser.We speculate that much of the performance improvement we show could be obtained with traditional (ie. non-CCG-based) parsers if they were designed to recover more of the information present in the Penn Treebank, in particular the trace co-indexation.An interesting experiment would be the application of our role-labeling system to the output of the trace recovery system of Johnson (2002).Our results also have implications for parser evaluation, as the most frequently used constituent-based precision and recall measures do not evaluate how well long-range dependencies can be recovered from the output of a parser.Measures based on dependencies, such as those of Lin (1995) and Carroll et al. (1998), are likely to be more relevant to real-world applications of parsing.Acknowledgments This work was supported by the Institute for Research in Cognitive Science at the University of Pennsylvania, the Propbank project (DoD Grant MDA904-00C2136), an EPSRC studentship and grant GR/M96889, and NSF ITR grant 0205 456.We thank Mark Steedman, Martha Palmer and Alexandra Kinyon for their comments on this work.
Seeing Stars When There Aren’t Many Stars: Graph-Based Semi-Supervised Learning For Sentiment CategorizationWe present a graph-based semi-supervised learning algorithm to address the sentiment analysis task of rating inference.Given a set of documents (e.g., movie reviews) and accompanying ratings (e.g., “4 stars”), the task calls for inferring numerical ratings for unlabeled documents based on the perceived sentiment expressed by their text.In particular, we are interested in the situation where labeled data is scarce.We place this task in the semi-supervised setting and demonstrate that considering unlabeled reviews in the learning process can improve ratinginference performance.We do so by creating a graph on both labeled and unlabeled data to encode certain assumptions for this task.We then solve an optimization problem to obtain a smooth rating function over the whole graph.When only limited labeled data is available, this method achieves significantly better predictive accuracy over other methods that ignore the unlabeled examples during training.Sentiment analysis of text documents has received considerable attention recently (Shanahan et al., 2005; Turney, 2002; Dave et al., 2003; Hu and Liu, 2004; Chaovalit and Zhou, 2005).Unlike traditional text categorization based on topics, sentiment analysis attempts to identify the subjective sentiment expressed (or implied) in documents, such as consumer product or movie reviews.In particular Pang and Lee proposed the rating-inference problem (2005).Rating inference is harder than binary positive / negative opinion classification.The goal is to infer a numerical rating from reviews, for example the number of “stars” that a critic gave to a movie.Pang and Lee showed that supervised machine learning techniques (classification and regression) work well for rating inference with large amounts of training data.However, review documents often do not come with numerical ratings.We call such documents unlabeled data.Standard supervised machine learning algorithms cannot learn from unlabeled data.Assigning labels can be a slow and expensive process because manual inspection and domain expertise are needed.Often only a small portion of the documents can be labeled within resource constraints, so most documents remain unlabeled.Supervised learning algorithms trained on small labeled sets suffer in performance.Can one use the unlabeled reviews to improve rating-inference?Pang and Lee (2005) suggested that doing so should be useful.We demonstrate that the answer is ‘Yes.’ Our approach is graph-based semi-supervised learning.Semi-supervised learning is an active research area in machine learning.It builds better classifiers or regressors using both labeled and unlabeled data, under appropriate assumptions (Zhu, 2005; Seeger, 2001).This paper contains three contributions: Workshop on TextGraphs, at HLT-NAACL 2006, pages 45–52, New York City, June 2006. c�2006 Association for Computational Linguistics to the sentiment analysis domain, extending past supervised learning work by Pang and Lee (2005);The semi-supervised rating-inference problem is formalized as follows.There are n review documents x1 ... xn, each represented by some standard feature representation (e.g., word-presence vectors).Without loss of generality, let the first l < n documents be labeled with ratings y1 ... yl E C. The remaining documents are unlabeled.In our experiments, the unlabeled documents are also the test documents, a setting known as transduction.The set of numerical ratings are C = {c1, ... , cC}, with c1 < ... < cC E R. For example, a one-star to four-star movie rating system has C = {0, 1, 2, 3}.We seek a function f : x H R that gives a continuous rating f(x) to a document x.Classification is done by mapping f(x) to the nearest discrete rating in C. Note this is ordinal classification, which differs from standard multi-class classification in that C is endowed with an order.In the following we use ‘review’ and ‘document,’ ‘rating’ and ‘label’ interchangeably.We make two assumptions: 1.We are given a similarity measure wij > 0 between documents xi and xj. wij should be computable from features, so that we can measure similarities between any documents, including unlabeled ones.A large wij implies that the two documents tend to express the same sentiment (i.e., rating).We experiment with positive-sentence percentage (PSP) based similarity which is proposed in (Pang and Lee, 2005), and mutual-information modulated word-vector cosine similarity.Details can be found in section 4.2.Optionally, we are given numerical rating predictions yl+1, ... , yn on the unlabeled documents from a separate learner, for instance E-insensitive support vector regression (Joachims, 1999; Smola and Sch¨olkopf, 2004) used by (Pang and Lee, 2005).This acts as an extra knowledge source for our semisupervised learning framework to improve upon.We note our framework is general and works without the separate learner, too.(For this to work in practice, a reliable similarity measure is required.)We now describe our graph for the semisupervised rating-inference problem.We do this piece by piece with reference to Figure 1.Our undirected graph G = (V, E) has 2n nodes V , and weighted edges E among some of the nodes.Summing over all edges in the graph, we obtain the (un)smoothness L(f) over the whole graph.We call L(f) the energy or loss, which should be minimized.Let L = 1... l and U = l + 1... n be labeled and unlabeled review indices, respectively.With the graph in Figure 1, the loss L(f) can be written as are set by cross validation in experiments.The last two kinds of edges are the key to semisupervised learning: They connect unobserved nodes and force ratings to be smooth throughout the graph, as we discuss in the next section.With the graph defined, there are several algorithms one can use to carry out semi-supervised learning (Zhu et al., 2003; Delalleau et al., 2005; Joachims, 2003; Blum and Chawla, 2001; Belkin et al., 2005).The basic idea is the same and is what we use in this paper.That is, our rating function f(x) should be smooth with respect to the graph. f(x) is not smooth if there is an edge with large weight w between nodes xi and xj, and the difference between f(xi) and f(xj) is large.The (un)smoothness over the particular edge can be defined as w(f(xi) − f(xj))2.A small loss implies that the rating of an unlabeled review is close to its labeled peers as well as its unlabeled peers.This is how unlabeled data can participate in learning.The optimization problem is minf L(f).To understand the role of the parameters, we define α = ak + bk' and Q = a, so that L(f) can be written as Thus Q controls the relative weight between labeled neighbors and unlabeled neighbors; α is roughly the relative weight given to semi-supervised (nondongle) edges.We can find the closed-form solution to the optimization problem.Defining an n x n matrix W, Let W = max(W, WT) be a symmetrized version of this matrix.Let D be a diagonal degree matrix with Note that we define a node’s degree to be the sum of its edge weights.Let A = D − W be the combinatorial Laplacian matrix.Let C be a diagonal dongle This is a quadratic function in f. Setting the gradient to zero, aL(f)/af = 0 , we find the minimum loss function Because C has strictly positive eigenvalues, the inverse is well defined.All our semi-supervised learning experiments use (7) in what follows.Before moving on to experiments, we note an interesting connection to the supervised learning method in (Pang and Lee, 2005), which formulates rating inference as a metric labeling problem (Kleinberg and Tardos, 2002).Consider a special case of our loss function (1) when b = 0 and M —* oc.It is easy to show for labeled nodes j E L, the optimal value is the given label: f(xj) = yj.Then the optimization problem decouples into a set of onedimensional problems, one for each unlabeled node The above problem is easy to solve.It corresponds exactly to the supervised, non-transductive version of metric labeling, except we use squared difference while (Pang and Lee, 2005) used absolute difference.Indeed in experiments comparing the two (not reported here), their differences are not statistically significant.From this perspective, our semisupervised learning method is an extension with interacting terms among unlabeled data.We performed experiments using the movie review documents and accompanying 4-class (C = 10, 1, 2,31) labels found in the “scale dataset v1.0” available at http://www.cs.cornell.edu/people/pabo/ movie-review-data/ and first used in (Pang and Lee, 2005).We chose 4-class instead of 3-class labeling because it is harder.The dataset is divided into four author-specific corpora, containing 1770, 902, 1307, and 1027 documents.We ran experiments individually for each author.Each document is represented as a 10, 11 word-presence vector, normalized to sum to 1.We systematically vary labeled set size |L |E 10.9n, 800, 400, 200,100, 50, 25,12, 6} to observe the effect of semi-supervised learning.|L |= 0.9n is included to match 10-fold cross validation used by (Pang and Lee, 2005).For each |L |we run 20 trials where we randomly split the corpus into labeled and test (unlabeled) sets.We ensure that all four classes are represented in each labeled set.The same random splits are used for all methods, allowing paired t-tests for statistical significance.All reported results are average test set accuracy.We compare our graph-based semi-supervised method with two previously studied methods: regression and metric labeling as in (Pang and Lee, 2005).We ran linear E-insensitive support vector regression using Joachims’ SVM&quot;ght package (1999) with all default parameters.The continuous prediction on a test document is discretized for classification.Regression results are reported under the heading ‘reg.’ Note this method does not use unlabeled data for training.We ran Pang and Lee’s method based on metric labeling, using SVM regression as the initial label preference function.The method requires an itemsimilarity function, which is equivalent to our similarity measure wij.Among others, we experimented with PSP-based similarity.For consistency with (Pang and Lee, 2005), supervised metric labeling results with this measure are reported under ‘reg+PSP.’ Note this method does not use unlabeled data for training either.PSPi is defined in (Pang and Lee, 2005) as the percentage of positive sentences in review xi.The similarity between reviews xi, xj is the cosine angle between the vectors (PSPZ,1−PSPZ) and (PSPj, 1− PSPj).Positive sentences are identified using a binary classifier trained on a separate “snippet data set” located at the same URL as above.The snippet data set contains 10662 short quotations taken from movie reviews appearing on the rottentomatoes.com Web site.Each snippet is labeled positive or negative based on the rating of the originating review.Pang and Lee (2005) trained a Naive Bayes classifier.They showed that PSP is a (noisy) measure for comparing reviews—reviews with low ratings tend to receive low PSP scores, and those with higher ratings tend to get high PSP scores.Thus, two reviews with a high PSP-based similarity are expected to have similar ratings.For our experiments we derived PSP measurements in a similar manner, but using a linear SVM classifier.We observed the same relationship between PSP and ratings (Figure 2).The metric labeling method has parameters (the equivalent of k, α in our model).Pang and Lee tuned them on a per-author basis using cross validation but did not report the optimal parameters.We were interested in learning a single set of parameters for use with all authors.In addition, since we varied labeled set size, it is convenient to tune c = k/|L|, the fraction of labeled reviews used as neighbors, instead of k. We then used the same c, α for all authors at all labeled set sizes in experiments involving PSP.Because c is fixed, k varies directly with |L |(i.e., when less labeled data is available, our algorithm considers fewer nearby labeled examples).In an attempt to reproduce the findings in (Pang and Lee, 2005), we tuned c, α with cross validation.Tuning ranges are c ∈ {0.05, 0.1, 0.15, 0.2, 0.25, 0.3} and α ∈ {0.01, 0.1, 0.5,1.0,1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 5.0}.The optimal parameters we found are c = 0.2 and α = 1.5.(In section 4.4, we discuss an alternative similarity measure, for which we re-tuned these parameters.)Note that we learned a single set of shared parameters for all authors, whereas (Pang and Lee, 2005) tuned k and α on a per-author basis.To demonstrate that our implementation of metric labeling produces comparable results, we also determined the optimal author-specific parameters.Table 1 shows the accuracy obtained over 20 trials with |L |= 0.9n for each author, using SVM regression, reg+PSP using shared c, α parameters, and reg+PSP using authorspecific c, α parameters (listed in parentheses).The best result in each row of the table is highlighted in bold.We also show in bold any results that cannot be distinguished from the best result using a paired t-test at the 0.05 level.(Pang and Lee, 2005) found that their metric labeling method, when applied to the 4-class data we are using, was not statistically better than regression, though they observed some improvement for authors (c) and (d).Using author-specific parameters, we obtained the same qualitative result, but the improvement for (c) and (d) appears even less significant in our results.Possible explanations for this difference are the fact that we derived our PSP measurements using an SVM classifier instead of an NB classifier, and that we did not use the same range of parameters for tuning.The optimal shared parameters produced almost the same results as the optimal author-specific parameters, and were used in subsequent experiments.We used the same PSP-based similarity measure and the same shared parameters c = 0.2, α = 1.5 from our metric labeling experiments to perform graph-based semi-supervised learning.The results are reported as ‘SSL+PSP.’ SSL has three additional parameters k', Q, and M. Again we tuned k', Q with cross validation.Tuning ranges are k' E 12, 3, 5,10, 20} and Q E 10.001, 0.01, 0.1,1.0,10.01.The optimal parameters are k' = 5 and Q = 1.0.These were used for all authors and for all labeled set sizes.Note that unlike k = c|L|, which decreases as the labeled set size decreases, we let k' remain fixed for all |L|.We set M arbitrarily to a large number 108 to ensure that the ratings of labeled reviews are respected.In addition to using PSP as a similarity measure between reviews, we investigated several alternative similarity measures based on the cosine of word vectors.Among these options were the cosine between the word vectors used to train the SVM regressor, and the cosine between word vectors containing only words with high (top 1000 or top 5000) mutual information values.The mutual information is computed with respect to the positive and negative classes in the 10662-document “snippet data set.” Finally, we experimented with using as a similarity measure the cosine between word vectors containing all words, each weighted by its mutual information.We found this measure to be the best among the options tested in pilot trial runs using the metric labeling algorithm.Specifically, we scaled the mutual information values such that the maximum value was one.Then, we used these values as weights for the corresponding words in the word vectors.For words in the movie review data set that did not appear in the snippet data set, we used a default weight of zero (i.e., we excluded them.We experimented with setting the default weight to one, but found this led to inferior performance.)We repeated the experiments described in sections 4.2 and 4.3 with the only difference being that we used the mutual-information weighted word vector similarity instead of PSP whenever a similarity measure was required.We repeated the tuning procedures described in the previous sections.Using this new similarity measure led to the optimal parameters c = 0.1, α = 1.5, k' = 5, and Q = 10.0.The results are reported under ‘reg+WV’ and ‘SSL+WV,’ respectively.We tested the five algorithms for all four authors using each of the nine labeled set sizes.The results are presented in table 2.Each entry in the table represents the average accuracy across 20 trials for an author, a labeled set size, and an algorithm.The best result in each row is highlighted in bold.Any results on the same row that cannot be distinguished from the best result using a paired t-test at the 0.05 level are also bold.The results indicate that the graph-based semisupervised learning algorithm based on PSP similarity (SSL+PSP) achieved better performance than all other methods in all four author corpora when only 200, 100, 50, 25, or 12 labeled documents were available.In 19 out of these 20 learning scenarios, the unlabeled set accuracy by the SSL+PSP algorithm was significantly higher than all other methods.While accuracy generally degraded as we trained on less labeled data, the decrease for the SSL approach was less severe through the mid-range labeled set sizes.SSL+PSP remains among the best methods with only 6 labeled examples.Note that the SSL algorithm appears to be quite sensitive to the similarity measure used to form the graph on which it is based.In the experiments where we used mutual-information weighted word vector similarity (reg+WV and SSL+WV), we notice that reg+WV remained on par with reg+PSP at high labeled set sizes, whereas SSL+WV appears significantly worse in most of these cases.It is clear that PSP is the more reliable similarity measure.SSL uses the similarity measure in more ways than the metric labeling approaches (i.e., SSL’s graph is denser), so it is not surprising that SSL’s accuracy would suffer more with an inferior similarity measure.Unfortunately, our SSL approach did not do as well with large labeled set sizes.We believe this is due to two factors: a) the baseline SVM regressor trained on a large labeled set can achieve fairly high accuracy for this difficult task without considering pairwise relationships between examples; b) PSP similarity is not accurate enough.Gain in variance reduction achieved by the SSL graph is offset by its bias when labeled data is abundant.We have demonstrated the benefit of using unlabeled data for rating inference.There are several directions to improve the work: 1.We will investigate better document representations and similarity measures based on parsing and other linguistic knowledge, as well as reviews’ sentiment patterns.For example, several positive sentences followed by a few concluding negative sentences could indicate an overall negative review, as observed in prior work (Pang and Lee, 2005).2.Our method is transductive: new reviews must be added to the graph before they can be classified.We will extend it to the inductive learning setting based on (Sindhwani et al., 2005).3.We plan to experiment with cross-reviewer and cross-domain analysis, such as using a model learned on movie reviews to help classify product reviews.We thank Bo Pang, Lillian Lee and anonymous reviewers for helpful comments.
Detection of Grammatical Errors Involving PrepositionsThis paper presents ongoing work on the detection of preposition errors of non-native speakers of English.Since prepositions account for a substantial proportion of all grammatical errors by ESL (English as a Second Language) learners, developing an NLP application that can reliably detect these types of errors will provide an invaluable learning resource to ESL students.To address this problem, we use a maximum entropy classifier combined with rule-based filters to detect preposition errors in a corpus of student essays.Although our work is preliminary, we achieve a precision of 0.8 with a recall of 0.3.The National Clearinghouse for English Language Acquisition (2002) estimates that 9.6% of the students in the US public school population speak a language other than English and have limited English proficiency.Clearly, there is a substantial and increasing need for tools for instruction in English as a Second Language (ESL).In particular, preposition usage is one of the most difficult aspects of English grammar for non-native speakers to master.Preposition errors account for a significant proportion of all ESL grammar errors.They represented the largest category, about 29%, of all the errors by 53 intermediate to advanced ESL students (Bitchener et al., 2005), and 18% of all errors reported in an intensive analysis of one Japanese writer (Murata and Ishara, 2004).Preposition errors are not only prominent among error types, they are also quite frequent in ESL writing.Dalgish (1985) analyzed the essays of 350 ESL college students representing 15 different native languages and reported that preposition errors were present in 18% of sentences in a sample of text produced by writers from first languages as diverse as Korean, Greek, and Spanish.The goal of the research described here is to provide software for detecting common grammar and usage errors in the English writing of non-native English speakers.Our work targets errors involving prepositions, specifically those of incorrect preposition selection, such as arrive to the town, and those of extraneous prepositions, as in most ofpeople.We present an approach that combines machine learning with rule-based filters to detect preposition errors in a corpus of ESL essays.Even though this is work in progress, we achieve precision of 0.8 with a recall of 0.3.The paper is structured as follows: in the next section, we describe the difficulty in learning English preposition usage; in Section 3, we discuss related work; in Sections 4-7 we discuss our methodology and evaluation.Why are prepositions so difficult to master?Perhaps it is because they perform so many complex roles.In English, prepositions appear in adjuncts, they mark the arguments of predicates, and they combine with other parts of speech to express new meanings.The choice of preposition in an adjunct is largely constrained by its object (in the summer, on Friday, at noon) and the intended meaning (at the beach, on the beach, near the beach, by the beach).Since adjuncts are optional and tend to be flexible in their position in a sentence, the task facing the learner is quite complex.Prepositions are also used to mark the arguments of a predicate.Usually, the predicate is expressed by a verb, but sometimes it takes the form of an adjective (He was fond of beer), a noun (They have a thirst for knowledge), or a nominalization (The child’s removal from the classroom).The choice of the preposition as an argument marker depends on the type of argument it marks, the word that fills the argument role, the particular word used as the predicate, and whether the predicate is a nominalization.Even with these constraints, there are still variations in the ways in which arguments can be expressed.Levin (1993) catalogs verb alternations such as They loaded hay on the wagon vs.They loaded the wagon with hay, which show that, depending on the verb, an argument may sometimes be marked by a preposition and sometimes not.English has hundreds of phrasal verbs, consisting of a verb and a particle (some of which are also prepositions).To complicate matters, phrasal verbs are often used with prepositions (i.e., give up on someone; give in to their demands).Phrasal verbs are particularly difficult for non-native speakers to master because of their non-compositionality of meaning, which forces the learner to commit them to rote memory.If mastering English prepositions is a daunting task for the second language learner, it is even more so for a computer.To our knowledge, only three other groups have attempted to automatically detect errors in preposition usage.Eeg-Olofsson et al. (2003) used 31 handcrafted matching rules to detect extraneous, omitted, and incorrect prepositions in Swedish text written by native speakers of English, Arabic, and Japanese.The rules, which were based on the kinds of errors that were found in a training set of text produced by non-native Swedish writers, targeted spelling errors involving prepositions and some particularly problematic Swedish verbs.In a test of the system, 11 of 40 preposition errors were correctly detected.Izumi et al. (2003) and (2004) used errorannotated transcripts of Japanese speakers in an interview-based test of spoken English to train a maximum entropy classifier (Ratnaparkhi, 1998) to recognize 13 different types of grammatical and lexical errors, including errors involving prepositions.The classifier relied on lexical and syntactic features.Overall performance for the 13 error types reached 25.1% precision with 7.1% recall on an independent test set of sentences from the same source, but the researchers do not separately report the results for preposition error detection.The approach taken by Izumi and colleagues is most similar to the one we have used, which is described in the next section.More recently, (Lee and Seneff, 2006) used a language model and stochastic grammar to replace prepositions removed from a dialogue corpus.Even though they reported a precision of 0.88 and recall of 0.78, their evaluation was on a very restricted domain with only a limited number of prepositions, nouns and verbs.A preposition error can be a case of incorrect preposition selection (They arrived to the town), use of a preposition in a context where it is prohibited (They came to inside), or failure to use a preposition in a context where it is obligatory (e.g., He is fond this book).To detect the first type of error, incorrect selection, we have employed a maximum entropy (ME) model to estimate the probability of each of 34 prepositions, based on the features in their local contexts.The ME Principle says that the best model will satisfy the constraints found in the training, and for those situations not covered in the training, the best model will assume a distribution of maximum entropy.This approach has been shown to perform well in combining heterogeneous forms of evidence, as in word sense disambiguation (Ratnaparkhi, 1998).It also has the desirable property of handling interactions among features without having to rely on the assumption of feature independence, as in a Naive Bayesian model.Our ME model was trained on 7 million “events” consisting of an outcome (the preposition that appeared in the training text) and its associated context (the set of feature-value pairs that accompanied it).These 7 million prepositions and their contexts were extracted from the MetaMetrics corpus of 1100 and 1200 Lexile text (11th and 12th grade) and newspaper text from the San Jose Mercury News.The sentences were then POS-tagged (Ratnaparkhi, 1998) and then chunked into noun phrases and verb phrases by a heuristic chunker.The maximum entropy model was trained with 25 contextual features.Some of the features represented the words and tags found at specific locations adjacent to the preposition; others represented the head words and tags of phrases that preceded or followed the preposition.Table 1 shows a subset of the feature list.Some features had only a few values while others had many.PHR pre is the “preceding phrase” feature that indicates whether the preposition was preceded by a noun phrase (NP) or a verb phrase (VP).In the example in Table 2, the preposition into is preceded by an NP.In a sentence that begins After the crowd was whipped up into a frenzy of anticipation, the preposition into is preceded by a VP.There were only two feature#value pairs for this feature: PHR pre#NP and PHR pre#VP.Other features had hundreds or even thousands of different values because they represented the occurrence of specific words that preceded or followed a preposition.Any feature#value pairs which occurred with very low frequency in the training (less than 10 times in the 7 million contexts) were eliminated to avoid the need for smoothing their probabilities.Lemma forms of words were used as feature values to further reduce the total number and to allow the model to generalize across inflectional variants.Even after incorporating these reductions, the number of values was still very large.As Table 1 indicates, TGR, the word sequence including the preposition and the two words to its right, had 54,906 different values.Summing across all features, the model contained a total of about 388,000 feature#value pairs.Table 2 shows an example of where some of the features are derived from.The model was tested on 18,157 preposition contexts extracted from 12 files randomly selected from a portion of 1100 Lexile text (11th grade) that had not been used for training.For each context, the model predicted the probability of each preposition given the contextual representation.The highest probability preposition was then compared to the preposition that had actually been used by the writer.Because the test corpus consisted of published, edited text, we assumed that this material contained few, if any, errors.In this and subsequent tests, the model was used to classify each context as one of 34 classes (prepositions).Results of the comparison between the classifier and the test set showed that the overall proportion of agreement between the text and the classifier was 0.69.The value of kappa was 0.64.When we examined the errors, we discovered that, frequently, the classifier’s most probable preposition (the one it assigned) differed from the second most probable by just a few percentage points.This corresponded to a situation in which two or more prepositions were likely to be found in a given context.Consider the context They thanked him for his consideration this matter, where either of or in could fill the blank.Because the classifier was forced to make a choice in this and other close cases, it incurred a high probability of making an error.To avoid this situation, we re-ran the test allowing the classifier to skip any preposition if its top ranked and second ranked choices differed by less than a specified amount.In other words, we permitted it to respond only when it was confident of its decision.When the difference between the first and second ranked choices was 0.60 or greater, 50% of the cases received no decision, but for the remaining half of the test cases, the proportion of agreement was 0.90 and kappa was 0.88.This suggests that a considerable improvement in performance can be achieved by using a more conservative approach based on a higher confidence level for the classifier.To evaluate the ME model’s suitability for analyzing ungrammatical text, 2,000 preposition contexts were extracted from randomly selected essays written on ESL tests by native speakers of Chinese, Japanese, and Russian.This set of materials was used to look for problems that were likely to arise as a consequence of the mismatch between the training corpus (edited, grammatical text) and the testing corpus (ESL essays with errors of various kinds).When the model was used to classify prepositions in the ESL essays, it became obvious, almost immediately, that a number of new performance issues would have to be addressed.The student essays contained many misspelled words.Because misspellings were not in the training, the model was unable to use the features associated with them (e.g., FHword#frinzy) in its decision making.The tagger was also affected by spelling errors, so to avoid these problems, the classifier was allowed to skip any context that contained misspelled words in positions adjacent to the preposition or in its adjacent phrasal heads.A second problem resulted from punctuation errors in the student writing.This usually took the form of missing commas, as in I disagree because from my point of view there is no evidence.In the training corpus, commas generally separated parenthetical expressions, such as from my point of view, from the rest of the sentence.Without the comma, the model selected of as the most probable preposition following because, instead of from.A set of heuristics was used to locate common sites of comma errors and skip these contexts.There were two other common sources of classification error: antonyms and benefactives.The model very often confused prepositions with opposite meanings (like with/without and from/to), so when the highest probability preposition was an antonym of the one produced by the writer, we blocked the classifier from marking the usage as an error.Benefactive phrases of the form for + person/organization (for everyone, for my school) were also difficult for the model to learn, most likely because, as adjuncts, they are free to appear in many different places in a sentence and the preposition is not constrained by its object, resulting in their frequency being divided among many different contexts.When a benefactive appeared in an argument position, the model’s most probable preposition was generally not the preposition for.In the sentence They described a part for a kid, the preposition of has a higher probability.The classifier was prevented from marking for + person/organization as a usage error in such contexts.To summarize, the classifier consisted of the ME model plus a program that blocked its application in cases of misspelling, likely punctuation errors, antonymous prepositions, and benefactives.Another difference between the training corpus and the testing corpus was that the latter contained grammatical errors.In the sentence, This was my first experience about choose friends, there is a verb error immediately following the preposition.Arguably, the preposition is also wrong since the sequence about choose is ill-formed.When the classifier marked the preposition as incorrect in an ungrammatical context, it was credited with correctly detecting a preposition error.Next, the classifier was tested on the set of 2,000 preposition contexts, with the confidence threshold set at 0.9.Each preposition in these essays was judged for correctness of usage by one or two human raters.The judged rate of occurrence of preposition errors was 0.109 for Rater 1 and 0.098 for Rater 2, i.e., about 1 out of every 10 prepositions was judged to be incorrect.The overall proportion of agreement between Rater1 and Rater 2 was 0.926, and kappa was 0.599.Table 3 (second column) shows the results for the Classifier vs. Rater 1, using Rater 1 as the gold standard.Note that this is not a blind test of the classifier inasmuch as the classifier’s confidence threshold was adjusted to maximize performance on this set.The overall proportion of agreement was 0.942, but kappa was only 0.365 due to the high level of agreement expected by chance, as the Classifier used the response category of “correct” more than 97% of the time.We found similar results when comparing the judgements of the Classifier to Rater 2: agreement was high and kappa was low.In addition, for both raters, precision was much higher than recall.As noted earlier, the table does not include the cases that the classifier skipped due to misspelling, antonymous prepositions, and benefactives.Both precision and recall are low in these comparisons to the human raters.We are particularly concerned about precision because the feedback that students receive from an automated writing analysis system should, above all, avoid false positives, i.e., marking correct usage as incorrect.We tried to improve precision by adding to the system a naive Bayesian classifier that uses the same features found in Table 1.As expected, its performance is not as good as the ME model (e.g., precision = 0.57 and recall = 0.29 compared to Rater 1 as the gold standard), but when the Bayesian classifier was given a veto over the decision of the ME classifier, overall precision did increase substantially (to 0.88), though with a reduction in recall (to 0.16).To address the problem of low recall, we have targeted another type of ESL preposition error: extraneous prepositions.Our strategy of training the ME classifier on grammatical, edited text precluded detection of extraneous prepositions as these did not appear in the training corpus.Of the 500-600 errors in the ESL test set, 142 were errors of this type.To identify extraneous preposition errors we devised two rule-based filters which were based on analysis of the development set.Both used POS tags and chunking information.Plural Quantifier Constructions This filter addresses the second most common extraneous preposition error where the writer has added a preposition in the middle of a plural quantifier construction, for example: some ofpeople.This filter works by checking if the target word is preceded by a quantifier (such as “some”, “few”, or “three”), and if the head noun of the quantifier phrase is plural.Then, if there is no determiner in the phrase, the target word is deemed an extraneous preposition error.Repeated Prepositions These are cases such as people can find friends with with the same interests where a preposition occurs twice in a row.Repeated prepositions were easily screened by checking if the same lexical item and POS tag were used for both words.These filters address two types of extraneous preposition errors, but there are many other types (for example, subcategorization errors, or errors with prepositions inserted incorrectly in the beginning of a sentence initial phrase).Even though these filters cover just one quarter of the 142 extraneous errors, they did improve precision from 0.778 to 0.796, and recall from 0.259 to 0.304 (comparing to Rater 1).We have presented a combined machine learning and rule-based approach that detects preposition errors in ESL essays with precision of 0.80 or higher (0.796 with the ME classifier and Extraneous Preposition filters; and 0.88 with the combined ME and Bayesian classifiers).Our work is novel in that we are the first to report specific performance results for a preposition error detector trained and evaluated on general corpora.While the training for the ME classifier was done on a separate corpus, and it was this classifier that contributed the most to the high precision, it should be noted that some of the filters were tuned on the evaluation corpus.Currently, we are in the course of annotating additional ESL essays for preposition errors in order to obtain a larger-sized test set.While most NLP systems are a balancing act between precision and recall, the domain of designing grammatical error detection systems is distinguished in its emphasis on high precision over high recall.Essentially, a false positive, i.e., an instance of an error detection system informing a student that a usage is incorrect when in fact it is indeed correct, must be reduced at the expense of a few genuine errors slipping through the system undetected.Given this, we chose to set the threshold for the system so that it ensures high precision which in turn resulted in a recall figure (0.3) that leaves us much room for improvement.Our plans for future system development include: 1.Using more training data.Even a cursory examination of the training corpus reveals that there are many gaps in the data.Seven million seems like a large number of examples, but the selection of prepositions is highly dependent on the presence of other specific words in the context.Many fairly common combinations of Verb+Preposition+Noun or Noun+Preposition+Noun are simply not attested, even in a sizable corpus.Consistent with this, there is a strong correlation between the relative frequency of a preposition and the classifier’s ability to predict its occurrence in edited text.That is, prediction is better for prepositions that have many examples in the training set and worse for those with fewer examples.This suggests the need for much more data. model in this study contains no semantic information.One way to extend and improve its coverage might be to include features of verbs and their noun arguments from sources such as FrameNet (http://framenet.icsi.berkeley.edu/), which detail the semantics of the frames in which many English words appear.
Text Chunking Using Transformation-Based LearningEric Brill introduced transformation-based learning and showed that it can do part-ofspeech tagging with fairly high accuracy.The same method can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive &quot;baseNP&quot; chunks.For this purpose, it is convenient to view chunking as a tagging problem by encoding the chunk structure in new tags attached to each word.In automatic tests using Treebank-derived data, this technique achieved recall and precision rates of roughly 92% for baseNP chunks and 88% for somewhat more complex chunks that partition the sentence.Some interesting adaptations to the transformation-based learning approach are also suggested by this application.Text chunking involves dividing sentences into nonoverlapping segments on the basis of fairly superficial analysis.Abney (1991) has proposed this as a useful and relatively tractable precursor to full parsing, since it provides a foundation for further levels of analysis including verb-argument identification, while still allowing more complex attachment decisions to be postponed to a later phase.Since chunking includes identifying the non-recursive portions of noun phrases, it can also be useful for other purposes including index term generation.Most efforts at superficially extracting segments from sentences have focused on identifying low-level noun groups, either using hand-built grammars and finite state techniques or using statistical models like HMMs trained from corpora.In this paper, we target a somewhat higher level of chunk structure using Brill's (1993b) transformation-based learning mechanism, in which a sequence of transformational rules is learned from a corpus; this sequence iteratively improves upon a baseline model for some interpretive feature of the text.This technique has previously been used not only for part-of-speech tagging (Brill, 1994), but also for prepositional phrase attachment disambiguation (Brill and Resnik, 1994), and assigning unlabeled binary-branching tree structure to sentences (Brill, 1993a).Because transformation-based learning uses patternaction rules based on selected features of the local context, it is helpful for the values being predicted to also be encoded locally.In the text-chunking application, encoding the predicted chunk structure in tags attached to the words, rather than as brackets between words, avoids many of the difficulties with unbalanced bracketings that would result if such local rules were allowed to insert or alter inter-word brackets directly.In this study, training and test sets marked with two different types of chunk structure were derived algorithmically from the parsed data in the Penn Treebank corpus of Wall Street Journal text (Marcus et al., 1994).The source texts were then run through Brill's part-of-speech tagger (Brill, 1993c), and, as a baseline heuristic, chunk structure tags were assigned to each word based on its part-of-speech tag.Rules were then automatically learned that updated these chunk structure tags based on neighboring words and their part-of-speech and chunk tags.Applying transformation-based learning to text chunking turns out to be different in interesting ways from its use for part-of-speech tagging.The much smaller tagset calls for a different organization of the computation, and the fact that part-of-speech assignments as well as word identities are fixed suggests different optimizations.Abney (1991) has proposed text chunking as a useful preliminary step to parsing.His chunks are inspired in part by psychological studies of Gee and Grosjean (1983) that link pause durations in reading and naive sentence diagraming to text groupings that they called 0-phrases, which very roughly correspond to breaking the string after each syntactic head that is a content word.Abney's other motivation for chunking is procedural, based on the hypothesis that the identification of chunks can be done fairly dependably by finite state methods, postponing the decisions that require higher-level analysis to a parsing phase that chooses how to combine the chunks.Existing efforts at identifying chunks in text have been focused primarily on low-level noun group identification, frequently as a step in deriving index terms, motivated in part by the limited coverage of present broad-scale parsers when dealing with unrestricted text.Some researchers have applied grammar-based methods, combining lexical data with finite state or other grammar constraints, while others have worked on inducing statistical models either directly from the words or from automatically assigned part-of-speech classes.On the grammar-based side, Bourigault (1992) describes a system for extracting &quot;terminological noun phrases&quot; from French text.This system first uses heuristics to find &quot;maximal length noun phrases&quot;, and then uses a grammar to extract &quot;terminological units.&quot; For example, from the maximal NP le disque dur de la station de travail it extracts the two terminological phrases disque dur, and station de travail.Bourigault claims that the grammar can parse &quot;around 95% of the maximal length noun phrases&quot; in a test corpus into possible terminological phrases, which then require manual validation.However, because its goal is terminological phrases, it appears that this system ignores NP chunk-initial determiners and other initial prenominal modifiers, somewhat simplifying the parsing task.Voutilainen (1993), in his impressive NPtool system, uses an approach that is in some ways similar to the one used here, in that he adds to his part-of-speech tags a new kind of tag that shows chunk structure; the chunk tag &quot;©>N&quot;, for example, is used for determiners and premodifiers, both of which group with the following noun head.He uses a lexicon that lists all the possible chunk tags for each word combined with hand-built constraint grammar patterns.These patterns eliminate impossible readings to identify a somewhat idiosyncratic kind of target noun group that does not include initial determiners but does include postmodifying prepositional phrases (including determiners).Voutilainen claims recall rates of 98.5% or better with precision of 95% or better.However, the sample NPtool analysis given in the appendix of (Voutilainen, 1993), appears to be less accurate than claimed in general, with 5 apparent mistakes (and one unresolved ambiguity) out of the 32 NP chunks in that sample, as listed in Table 1.These putative errors, combined with the claimed high performance, suggest that NPtool's definition of NP chunk is also tuned for extracting terminological phrases, and thus excludes many kinds of NP premodifiers, again simplifying the chunking task.NPtool parse Apparent correct parse less [time] [less time] the other hand • the [other hand] many [advantages] [many advantages] [binary addressing] [binary addressing and and [instruction formats] instruction formats] a purely [binary computer] a [purely binary computer] Kupiec (1993) also briefly mentions the use of finite state NP recognizers for both English and French to prepare the input for a program that identified the correspondences between NPs in bilingual corpora, but he does not directly discuss their performance.Using statistical methods, Church's Parts program (1988), in addition to identifying parts of speech, also inserted brackets identifying core NPs.These brackets were placed using a statistical model trained on Brown corpus material in which NP brackets had been inserted semi-automatically.In the small test sample shown, this system achieved 98% recall for correct brackets.At about the same time, Ejerhed (1988), working with Church, performed comparisons between finite state methods and Church's stochastic models for identifying both non-recursive clauses and non-recursive NPs in English text.In those comparisons, the stochastic methods outperformed the hand built finite-state models, with claimed accuracies of 93.5% (clauses) and 98.6% (NPs) for the statistical models compared to to 87% (clauses) and 97.8% (NPs) for the finite-state methods.Running Church's program on test material, however, reveals that the definition of NP embodied in Church's program is quite simplified in that it does not include, for example, structures or words conjoined within NP by either explicit conjunctions like &quot;and&quot; and &quot;or&quot;, or implicitly by commas.Church's chunker thus assigns the following NP chunk structures: [a Skokie] , [Ill.] , [subsidiary] [newer] , [big-selling prescriptions drugs] [the inefficiency] , [waste] and [lack] of [coordination] [Kidder] , [Peabody] Sz [Co] It is difficult to compare performance figures between studies; the definitions of the target chunks and the evaluation methodologies differ widely and are frequently incompletely specified.All of the cited performance figures above also appear to derive from manual checks by the investigators of the system's predicted output, and it is hard to estimate the impact of the system's suggested chunking on the judge's determination.We believe that the work reported here is the first study which has attempted to find NP chunks subject only to the limitation that the structures recognized do not include recursively embedded NPs, and which has measured performance by automatic comparison with a preparsed corpus.We performed experiments using two different chunk structure targets, one that tried to bracket non-recursive &quot;baseNPs&quot; and one that partitioned sentences into non-overlapping N-type and V-type chunks, loosely following Abney's model.Training and test materials with chunk tags encoding each of these kinds of structure were derived automatically from the parsed Wall Street Journal text in the Penn Treebank (Marcus et al., 1994).While this automatic derivation process introduced a small percentage of errors of its own, it was the only practical way both to provide the amount of training data required and to allow for fully-automatic testing.The goal of the &quot;baseNP&quot; chunks was to identify essentially the initial portions of nonrecursive noun phrases up to the head, including determiners but not including postmodifying prepositional phrases or clauses.These chunks were extracted from the Treebank parses, basically by selecting NPs that contained no nested NPs1.The handling of conjunction followed that of the Treebank annotators as to whether to show separate baseNPs or a single baseNP spanning the conjunction2.Possessives were treated as a special case, viewing the possessive marker as the first word of a new baseNP, thus flattening the recursive structure in a useful way.The following sentences give examples of this baseNP chunk structure: During [N the third quarter N] , [N Compaq N] purchased [N a former Wang Laboratories manufacturing facility N] in [N Sterling N], [N Scotland N] , which will be used for [N international service and repair operations N] .[N The government NJ has [N other agencies and instruments N] for pursuing [N these other objectives N] .Even [N Mao Tse-tung iv] [N 's China Ad began in [N 1949 N] with [N a partnership N] between [N the communists N] and [N a number N] of [N smaller , non-communist parties N] • The chunks in the partitioning chunk experiments were somewhat closer to Abney's model, where the prepositions in prepositional phrases are included with the object NP up to the head in a single N-type chunk.This created substantial additional ambiguity for the system, which had to distinguish prepositions from particles.The handling of conjunction again follows the Treebank parse with nominal conjuncts parsed in the Treebank as a single NP forming a single N chunk, while those parsed as conjoined NPs become separate chunks, with any coordinating conjunctions attached like prepositions to the following N chunk.The portions of the text not involved in N-type chunks were grouped as chunks termed Vtype, though these &quot;V&quot; chunks included many elements that were not verbal, including adjective phrases.The internal structure of these V-type chunks loosely followed the Treebank parse, though V chunks often group together elements that were sisters in the underlying parse tree.Again, the possessive marker was viewed as initiating a new N-type chunk.The following sentences are annotated with these partitioning N and V chunks: [N Some bankers NI [v are reporting v] [N more inquiries than usual N] [N about CDs N] [N since Friday NJ .'This heuristic fails in some cases.For example, Treebank uses the label NAC for some NPs functioning as premodifiers, like &quot;Bank of England&quot; in &quot;Robin Leigh-Pemberton, Bank of England governor, conceded..&quot;; in such cases, &quot;governor&quot; is not included in any baseNP chunk.'Non-constituent NP conjunction, which Treebank labels NX, is another example that still causes problems.[N Eastern Airlines N] [N 'creditors N] [v have begun exploring v] [N alternative approaches N] [N to a Chapter 11 reorganization N] [v because v] [N they N][v are unhappy v] [N with the carrier N] [N 's latest proposal N] .[N Indexing NI [N for the most part N] [v has involved simply buying v] [v and then holding v] [N stocks NI [N in the correct mix N] [v to mirror Id [N a stock market barometer N] .These two kinds of chunk structure derived from the Treebank data were encoded as chunk tags attached to each word and provided the targets for the transformation-based learning.As shown in Fig.1, transformation-based learning starts with a supervised training corpus that specifies the correct values for some linguistic feature of interest, a baseline heuristic for predicting initial values for that feature, and a set of rule templates that determine a space of possible transformational rules.The patterns of the learned rules match to particular combinations of features in the neighborhood surrounding a word, and their action is to change the system's current guess as to the feature for that word.To learn a model, one first applies the baseline heuristic to produce initial hypotheses for each site in the training corpus.At each site where this baseline prediction is not correct, the templates are then used to form instantiated candidate rules with patterns that test selected features in the neighborhood of the word and actions that correct the currently incorrect tag assignment.This process eventually identifies all the rule candidates generated by that template set that would have a positive effect on the current tag assignments anywhere in the corpus.Those candidate rules are then tested against the rest of corpus, to identify at how many locations they would cause negative changes.One of those rules whose net score (positive changes minus negative changes) is maximal is then selected, applied to the corpus, and also written out as the first rule in the learned sequence.This entire learning process is then repeated on the transformed corpus: deriving candidate rules, scoring them, and selecting one with the maximal positive effect.This process is iterated, leading to an ordered sequence of rules, with rules discovered first ordered before those discovered later.The predictions of the model on new text are determined by beginning with the baseline heuristic prediction and then applying each rule in the learned rule sequence in turn.This section discusses how text chunking can be encoded as a tagging problem that can be conveniently addressed using transformational learning.We also note some related adaptations in the procedure for learning rules that improve its performance, taking advantage of ways in which this task differs from the learning of part-of-speech tags.Applying transformational learning to text chunking requires that the system's current hypotheses about chunk structure be represented in a way that can be matched against the pattern parts of rules.One way to do this would be to have patterns match tree fragments and actions modify tree geometries, as in Brill's transformational parser (1993a).In this work, we have found it convenient to do so by encoding the chunking using an additional set of tags, so that each word carries both a part-of-speech tag and also a &quot;chunk tag&quot; from which the chunk structure can be derived.In the baseNP experiments aimed at non-recursive NP structures, we use the chunk tag set {I, 0, B}, where words marked I are inside some baseNP, those marked 0 are outside, and the B tag is used to mark the left most item of a baseNP which immediately follows another baseNP.In these tests, punctuation marks were tagged in the same way as words.In the experiments that partitioned text into N and V chunks, we use the chunk tag set {BN, N, By, V, P}, where BN marks the first word and N the succeeding words in an N-type group while BV and V play the same role for V-type groups.Punctuation marks, which are ignored in Abney's chunk grammar, but which the Treebank data treats as normal- lexical items with their own part-of-speech tags, are unambiguously assigned the chunk tag P. Items tagged P are allowed to appear within N or V chunks; they are irrelevant as far as chunk boundaries are concerned, but they are still available to be matched against as elements of the left hand sides of rules.Encoding chunk structure with tags attached to words rather than non-recursive bracket markers inserted between words has the advantage that it limits the dependence between different elements of the encoded representation.While brackets must be correctly paired in order to derive a chunk structure, it is easy to define a mapping that can produce a valid chunk structure from any sequence of chunk tags; the few hard cases that arise can be handled completely locally.For example, in the baseNP tag set, whenever a B tag immediately follows an 0, it must be treated as an I, and, in the partitioning chunk tag set, wherever a V tag immediately follows an N tag without any intervening By, it must be treated as a By.Transformational learning begins with some initial &quot;baseline&quot; prediction, which here means a baseline assignment of chunk tags to words.Reasonable suggestions for baseline heuristics after a text has been tagged for part-of-speech might include assigning to each word the chunk tag that it carried most frequently in the training set, or assigning each part-of-speech tag the chunk tag that was most frequently associated with that part-of-speech tag in the training.We tested both approaches, and the baseline heuristic using part-of-speech tags turned out to do better, so it was the one used in our experiments.The part-of-speech tags used by this baseline heuristic, and then later also matched against by transformational rule patterns, were derived by running the raw texts in a prepass through Brill's transformational part-of-speech tagger (Brill, 1993c).In transformational learning, the space of candidate rules to be searched is defined by a set of rule templates that each specify a small number of particular feature sets as the relevant factors that a rule's left-hand-side pattern should examine, for example, the part-of-speech tag of the word two to the left combined with the actual word one to the left.In the preliminary scan of the corpus for each learning pass, it is these templates that are applied to each location whose current tag is not correct, generating a candidate rule that would apply at least at that one location, matching those factors and correcting the chunk tag assignment.When this approach is applied to part-of-speech tagging, the possible sources of evidence for templates involve the identities of words within a neighborhood of some appropriate size and their current part-of-speech tag assignments.In the text chunking application, the tags being assigned are chunk structure tags, while the part-of-speech tags are a fixed part of the environment, like the lexical identities of the words themselves.This additional class of available information causes a significant increase in the number of reasonable templates if templates for a wide range of the possible combinations of evidence are desired.The distributed version of Brill's tagger (Brill, 1993c) makes use of 26 templates, involving various mixes of word and part-of-speech tests on neighboring words.Our tests were performed using 100 templates; these included almost all of Brill's combinations, and extended them to include references to chunk tags as well as to words and part-of-speech tags.The set of 100 rule templates used here was built from repetitions of 10 basic patterns, shown on the left side of Table 2 as they apply to words.The same 10 patterns can also be used to match against part-of-speech tags, encoded as Po, P_1, etc.(In other tests, we have explored mixed templates, that match against both word and part-of-speech values, but no mixed templates were used in these experiments.)These 20 word and part-of-speech patterns were then combined with each of the 5 different chunk tag patterns shown on the right side of the table.The cross product of the 20 word and part-of-speech patterns with the 5 chunk tag patterns determined the full set of 100 templates used.The large increase in the number of rule templates in the text chunking application when compared to part-of-speech tagging pushed the training process against the available limits in terms of both space and time, particularly when combined with the desire to work with the largest possible training sets.Various optimizations proved to be crucial to make the tests described feasible.One change in the algorithm is related to the smaller size of the tag set.In Brill's tagger (Brill, 1993c), an initial calculation in each pass computes the confusion matrix for the current tag assignments and sorts the entries of that [old-tag x new-tag] matrix, so that candidate rules can then be processed in decreasing order of the maximum possible benefit for any rule changing, say, old tag I to new tag J.The search for the best-scoring rule can then be halted when a cell of the confusion matrix is reached whose maximum possible benefit is less than the net benefit of some rule already encountered.The power of that approach is dependent on the fact that the confusion matrix for part-ofspeech tagging partitions the space of candidate rules into a relatively large number of classes, so that one is likely to be able to exclude a reasonably large portion of the search space.In a chunk tagging application, with only 3 or 4 tags in the effective tagset, this approach based on the confusion matrix offers much less benefit.However, even though the confusion matrix does not usefully subdivide the space of possible rules when the tag set is this small, it is still possible to apply a similar optimization by sorting the entire list of candidate rules on the basis of their positive scores, and then processing the candidate rules (which means determining their negative scores and thus their net scores) in order of decreasing positive scores.By keeping track of the rule with maximum benefit seen so far, one can be certain of having found one of the globally best rules when one reaches candidate rules in the sorted list whose positive score is not greater than the net score of the best rule so far.In earlier work on transformational part-of-speech tagging (Ramshaw and Marcus, 1994), we noted that it is possible to greatly speed up the learning process by constructing a full, bidirectional index linking each candidate rule to those locations in the corpus at which it applies and each location in the corpus to those candidate rules that apply there.Such an index allows the process of applying rules to be performed without having to search through the corpus.Unfortunately, such complete indexing proved to be too costly in terms of physical memory to be feasible in this application.However, it is possible to construct a limited index that lists for each candidate rule those locations in the corpus at which the static portions of its left-hand-side pattern match.Because this index involves only the stable word identity and part-of-speech tag values, it does not require updating; thus it can be stored more compactly, and it is also not necessary to maintain back pointers from corpus locations to the applicable rules.This kind of partial static index proved to be a significant advantage in the portion of the program where candidate rules with relatively high positive scores are being tested to determine their negative scores, since it avoids the necessity of testing such rules against every location in the corpus.We also investigated a new heuristic to speed up the computation: After each pass, we disable all rules whose positive score is significantly lower than the net score of the best rule for the current pass.A disabled rule is then reenabled whenever enough other changes have been made to the corpus that it seems possible that the score of that rule might have changed enough to bring it back into contention for the top place.This is done by adding some fraction of the changes made in each pass to the positive scores of the disabled rules, and reenabling rules whose adjusted positive scores came within a threshold of the net score of the successful rule on some pass.Note that this heuristic technique introduces some risk of missing the actual best rule in a pass, due to its being incorrectly disabled at the time.However, empirical comparisons between runs with and without rule disabling suggest that conservative use of this technique can produce an order of magnitude speedup while imposing only a very slight cost in terms of suboptimality of the resulting learned rule sequence.The automatic derivation of training and testing data from the Treebank analyses allowed for fully automatic scoring, though the scores are naturally subject to any remaining systematic errors in the data derivation process as well as to bona fide parsing errors in the Treebank source.Table 3 shows the results for the baseNP tests, and Table 4 shows the results for the partitioning chunks task.Since training set size has a significant effect on the results, values are shown for three different training set sizes.(The test set in all cases was 50K words.Training runs were halted after the first 500 rules; rules learned after that point affect relatively few locations in the training set and have only a very slight effect for good or ill on test set performance.)The first line in each table gives the performance of the baseline system, which assigned a baseNP or chunk tag to each word on the basis of the POS tag assigned in the prepass.Performance is stated in terms of recall (percentage of correct chunks found) and precision (percentage of chunks found that are correct), where both ends of a chunk had to match exactly for it to be counted.The raw percentage of correct chunk tags is also given for each run, and for each performance measure, the relative error reduction compared to the baseline is listed.The partitioning chunks do appear to be somewhat harder to predict than baseNP chunks.The higher error reduction for the former is partly due to the fact that the part-of-speech baseline for that task is much lower.To give a sense of the kinds of rules being learned-, the first 10 rules from the 200K baseNP run are shown in Table 5.It is worth glossing the rules, since one of the advantages of transformationbased learning is exactly that the resulting model is easily interpretable.In the first of the baseNP rules, adjectives (with part-of-speech tag JJ) that are currently tagged I but that are followed by words tagged 0 have their tags changed to 0.In Rule 2, determiners that are preceded by two words both tagged I have their own tag changed to B, marking the beginning of a baseNP that happens to directly follow another.(Since the tag B is only used when baseNPs abut, the baseline system tags determiners as I.)Rule 3 takes words which immediately follow determiners tagged I that in turn follow something tagged 0 and changes their tag to also be I.Rules 4-6 are similar to Rule 2, marking the initial words of baseNPs that directly follow another baseNP.Rule 7 marks conjunctions (with part-of-speech tag CC) as I if they follow an I and precede a noun, since such conjunctions are more likely to be embedded in a single baseNP than to separate two baseNPs, and Rules 8 and 9 do the same.(The word &quot;&&quot; in rule 8 comes mostly from company names in the Wall St. Journal source data.)Finally, Rule 10 picks up cases like &quot;including about four million shares&quot; where &quot;about&quot; is used as a quantifier rather than preposition.A similar list of the first ten rules for the chunk task can be seen in Table 6.To gloss a few of these, in the first rule here, determiners (with part-of-speech tag DT), which usually begin N chunks and thus are assigned the baseline tag BN, have their chunk tags changed to N if they follow a word whose tag is also BN.In Rule 2, sites currently tagged N but which fall at the beginning of a sentence have their tags switched to BN.(The dummy tag Z and word ZZZ indicate that the locations one to the left are beyond the sentence boundaries.)Rule 3 changes N to BN after a comma (which is tagged P), and in Rule 4, locations tagged BN are switched to BV if the following location is tagged V and has the part-of-speech tag VB.The fact that this system includes lexical rule templates that refer to actual words sets it apart from approaches that rely only on part-of-speech tags to predict chunk structure.To explore how much difference in performance those lexical rule templates make, we repeated the above test runs omitting templates that refer to specific words.The results for these runs, in Tables 7 and 8, suggest that the lexical rules improve performance on the baseNP chunk task by about 1% (roughly 5% of the overall error reduction) and on the partitioning chunk task by about 5% (roughly 10% of the error reduction).Thus lexical rules appear to be making a limited contribution in determining baseNP chunks, but a more significant one for the partitioning chunks.A rough hand categorization of a sample of the errors from a baseNP run indicates that many fall into classes that are understandably difficult for any process using only local word and partof-speech patterns to resolve.The most frequent single confusion involved words tagged VBG and VBN, whose baseline prediction given their part-of-speech tag was 0, but which also occur frequently inside baseNPs.The system did discover some rules that allowed it to fix certain classes of VBG and VBN mistaggings, for example, rules that retagged VBNs as I when they preceded an NN or NNS tagged I.However, many also remained unresolved, and many of those appear to be cases that would require more than local word and part-of-speech patterns to resolve.The second most common class of errors involved conjunctions, which, combined with the former class, make up half of all the errors in the sample.The Treebank tags the words &quot;and&quot; and frequently &quot;,&quot; with the part-of-speech tag CC, which the baseline system again predicted would fall most often outside of a baseNP3.However, the Treebank parses do also frequently classify conjunctions of Ns or NPs as a single baseNP, and again there appear to be insufficient clues in the word and tag contexts for the current system to make the distinction.Frequently, in fact, the actual choice of structure assigned by the Treebank annotators seemed largely dependent on semantic indications unavailable to the transformational learner.We are planning to explore several different paths that might increase the system's power to distinguish the linguistic contexts in which particular changes would be useful.One such direction is to expand the template set by adding templates that are sensitive to the chunk structure.For example, instead of referring to the word two to the left, a rule pattern could refer to the first word in the current chunk, or the last word of the previous chunk.Another direction would be to enrich the vocabulary of chunk tags, so that they could be used during the learning process to encode contextual features for use by later rules in the sequence.We would also like to explore applying these same kinds of techniques to building larger scale structures, in which larger units are assembled or predicate/argument structures derived by combining chunks.One interesting direction here would be to explore the use of chunk structure tags that encode a form of dependency grammar, where the tag &quot;N+2&quot; might mean that the current word is to be taken as part.of the unit headed by the N two words to the right.By representing text chunking as a kind of tagging problem, it becomes possible to easily apply transformation-based learning.We have shown that this approach is able to automatically induce a chunking model from supervised training that achieves recall and precision of 92% for baseNP chunks and 88% for partitioning N and V chunks.Such chunking models provide a useful and feasible next step in textual interpretation that goes beyond part-of-speech tagging, and that serve as a foundation both for larger-scale grouping and for direct extraction of subunits like index terms.In addition, some variations in the transformation-based learning algorithm are suggested by this application that may also be useful in other settings.We would like to thank Eric Brill for making his system widely available, and Ted Briscoe and David Yarowsky for helpful comments, including the suggestion to test the system's performance without lexical rule templates.'Note that this is one of the cases where Church's chunker allows separate NP fragments to count as chunks.
CogNIAC: High Precision Coreference With Limited Knowledge And Linguistic ResourcesThis paper presents a high precision pronoun resolution system that is capable of greater than 90% precision with 60% and better recall for some pronouns.It is suggested that the system is resolving a sub-set of anaphors that do not require general world knowledge or sophisticated linguistic processing for successful resolution.The system does this by being very sensitive to ambiguity, and only resolving pronouns when very high confidence rules have been satisfied.The system is capable of 'noticing' ambiguity because it requires that there be a unique antecedent within a salience ranking, and the salience rankings are not total orders, i.e. two or more antecedents can be equally salient.Given the nature of the systems rules, it is very likely that they are largely domain independent and that they reflect processing strategies used by humans for general language comprehension.The system has been evaluated in two distinct experiments which support the overall validity of the approach.Pronoun resolution is one of the 'classic' computational linguistics problems.It is also widely considered to be inherently an A.I. complete' task-meaning that resolution of pronouns requires full world knowledge and inference.CogNIAC is a pronoun resolution engine designed around the assumption that there is a sub-class of anaphora that does not require general purpose reasoning.The kinds of information CogNIAC does require includes: sentence detection, part-of-speech tagging, simple noun phrase recognition, basic semantic category information like, gender, number, and in one configuration, partial parse trees.What distinguishes CogNIAC from algorithms that use similar sorts of information is that it will not resolve a pronoun in circumstances of ambiguity.Crucially, ambiguity is a function of how much knowledge an understander has.Since CogNIAC does not have as rich a representation of world knowledge as humans, it finds much more ambiguity in texts than humans do.2 A path to high precision pronominal resolution-- avoid guesswork in ambiguous contexts: It is probably safe to say that few referring pronouns are conveyed without the speaker/writer having an antecedent in mind.Ambiguity occurs when the perceiver cannot recover from the context what conveyer has in mind.I have found myself uttering pronouns which the hearer has no chance of recovering the antecedent to because they are not attending to the same part of the external environment, &quot;He sure looks familiar&quot;, or in text I am so focused on the context of what I am writing that use a pronoun to refer to a highly salient concept for me, but the antecedent may completely evade a reader without my familiarity with the topic.Of course it is possible to explicitly leave the reader hanging as in, &quot;Earl and Dave were working together when suddenly he fell into the threshing machine.&quot; Humans, unlike most coreference algorithms, notice such cases of ambiguity and can then ask for clarification or at least grumble about how we cannot climb into the writers head to figure out what they meant.But in that grumble we have articulated the essence of the problem--we don't have sufficient knowledge to satisfy ourselves that an antecedent has been found.Pronoun resolution systems have extremely limited knowledge sources, they cannot access a fraction of human common sense knowledge.To appreciate this consider the following text with grammatical tags replacing words with pronouns and names left in place: The city council VERBGROUP the women NP CC they VB NN Mariana VBD PP Sarah TO VB herself PP DT MD NN Without lexical knowledge a human attempting to resolve the pronouns is in much the knowledge impoverished position of the typical coreference algorithm.It is no surprise that texts with so little information provided in them tend to be more ambiguous than the texts in fleshed out form.The conclusion to draw from this example is that the limiting factor in CogNIAC is knowledge sources, not an artificial restriction on domains or kinds of coreference.This point will be resumed in the discussion section when what the consequences of fuller knowledge sources would be on CogNIAC.For noun phrase anaphora, gathering semantically possible antecedents amounts to running all the noun phrases in a text through various databases for number and gender, and perhaps then a classifier that determines whether a noun phrase is a company, person or place'.This set of candidate antecedents rarely has more than 5 members when some reasonable locality constraints are adhered to, and this set almost always contains the actual antecedent.The remainder of the coreference resolution process amounts to picking the right entity from this set.For the kinds of data considered here (narratives and newspaper articles) there is a rarely a need for general world knowledge in assembling the initial set of possible antecedents for pronouns.This does not address the issue of inferred antecedents, event reference, discourse deixis and many other sorts of referring phenomenon which clearly require the use of world knowledge but are beyond the scope of this work.As it happens, recognizing the possible antecedents of these pronouns is within the capabilities of current knowledge sources.Better knowledge sources could be used to reduce the space of possible antecedents.For example the well known [Winograd 19721 alternation: The city council refused to give the women a permit because they {feared/advocated} violence.There are two semantically possible antecedents to they: The city council, and the women.The problem is picking the correct one.Dependent on verb choice, they strongly prefers one antecedent to the other.Capturing this generalization requires a sophisticated theory of verb meaning as relates to pronoun resolution.Speaking anecdotally, these kinds of resolutions happen quite often in text.CogNIAC recognizes knowledge intensive coreference and does not attempt to resolve such instances.I The named entity task at MUC-6 used a similar classification task and the best system performance was 96% precision/97% recall.Fortunately not all instances of pronominal anaphora require world knowledge for successful resolution.In lieu of full world knowledge, CogNIAC uses regularities of English usage in an attempt to mimic strategies used by humans when resolving pronouns.For example, the syntax of a sentence highly constrains a reflexive pronoun's antecedent.Also if there is just one possible antecedent in entire the prior discourse, then that entity is nearly always the correct antecedent.CogNIAC consists of a set of such observations implemented in Perl.CogNIAC has been used with a range of linguistic resources, ranging from scenarios where almost no linguistic processing of the text is done at all to partial parse trees being provided.At the very least, there must be sufficient linguistic resources to recognize pronouns in the text and the space of candidate antecedents must be identified.For the first experiment the text has been part of speech tagged and basal noun phrases have been identified with 11' (i.e. noun phrases that have no nested noun phrases) as shown below: [ Mariana/NNP ] motioned/VBD for/IN [ Sarah/NNP] to/TO seatNB [herself/PRP ] on/IN [ a/DT twoseater/NN lounge/NN ] In addition, finite clauses were identified (by hand for experiment 1) and various regular expressions are used to identify subjects, objects and what verbs take as arguments for the purposes of coreference restrictions.With this level of linguistic annotation, nearly all the parts of CogNIAC can be used to resolve pronouns.The core rules of CogNIAC are given below, with their performance on training data provided (200 pronouns of narrative text).In addition, examples where the rules successfully apply have been provided for most of the rules with the relevant anaphors and antecedents in boldface.The term 'possible antecedents' refers to the set of entities from the discourse that are compatible with an anaphor's gender, number and coreference restrictions (i.e. non-reflexive pronouns cannot corefer with the other arguments of its verb/preposition etc.).Mariana motioned for Sarah to seat herself on a two-seater lounge. sentence, then pick i as the antecedent: 114 correct, and 2 incorrect.Rupert Murdock's News Corp. confirmed his interest in buying back the ailing New York Post.But analysts said that if he winds up bidding for the paper,.... possessive pronoun and there is a single exact string match i of the possessive in the prior sentence, then pick i as the antecedent: 4 correct, and 1 incorrect.After he was dry, Joe carefully laid out the damp towel in front of his locker.Travis went over to his locker, took out a towel and started to dry off.5) Unique Current Sentence: If there is a single possible antecedent in the read-in portion of the current sentence, then pick i as the antecedent: 21 correct, and 1 incorrect.Like a large bear, he sat motionlessly in the lounge in one of the faded armchairs, watching Constantin.After a week Constantin tired of reading the old novels in the bottom shelf of the bookcase-somewhere among the gray well thumbed pages he had hoped to find a message from one of his predecessorsIf the subject of the prior sentence contains a single possible antecedent i, and the anaphor is the subject of the current sentence, then pick i as the antecedent: 11 correct, and 0 incorrect.Besides, if he provoked Malek, uncertainties were introduced, of which there were already far too many.He noticed the supervisor enter the lounge ...The method of resolving pronouns within CogNIAC works as follows: Pronouns are resolved left-to-right in the text.For each pronoun, the rules are applied in the presented order.For a given rule, if an antecedent is found, then the appropriate annotations are made to the text and no more rules are tried for that pronoun, otherwise the next rule is tried.If no rules resolve the pronoun, then it is left unresolved.These rules are individually are high precision rules, and collectively they add up to reasonable recall.The precision is 97% (121/125) and the recall is 60% (121/201) for 198 pronouns of training data.The Naive Algorithm [Hobbs 1976] works by specifying a total order on noun phrases in the prior discourse and comparing each noun phrase against the selectional restrictions (i.e. gender, number) of the anaphor, and taking the antecedent to be the first one to satisfy them.The specification of the ordering constitutes a traversal order of the syntax tree of the anaphors clause and from there to embedding clauses and prior clauses.The Winograd sentences, with either verb, would yield the following ordering of possible antecedents: The city council > the women The algorithm would resolve they to The city council.This is incorrect on one choice of verb, but the algorithm does not integrate the verb information into the salience ranking.In comparison, none of the six rules of CogNIAC would resolve the pronoun.Rules have been tried that resolved a subject pronoun of a nested clause with the subject of the dominating clause, but no configuration has been found that yielded sufficient precision2.Consequently, they is not resolved'.The naive algorithm has some interesting properties.First it models relative salience as relative depth in a search space.For two candidate antecedents a and b, if a is encountered before b in the search space, then a is more salient than b.Second, the relative saliency of all candidate antecedents is totally ordered, that is, for any two candidate antecedents a and b , a is more salient than b xor b is more salient than a.2 In experiment 2, discussed below, the rule 'subject same clause' would resolve they to the city council, but it was added to the MUC-6 system without testing, and has shown itself to not be a high precision rule.CogNIAC shares several features of the Naive Algorithm: circumstances of many possible antecedents, and will not resolve pronouns in such cases.The Naive Algorithm has no means of noting ambiguity and will resolve a pronoun as long as there is at least one possible antecedent.Perhaps the most convincing reason to endorse partially ordered salience rankings is that salience distinctions fade as the discourse moves on.Earl was working with Ted the other day.He fell into the threshing machine.Earl was working with Ted the other day.All of the sudden, the cows started making a ruckus.The noise was unbelievable.He fell into the threshing machine.In the first example 'He' takes `Earl' as antecedent, which is what rule 6, Unique Subject/Subject Pronoun, would resolve the pronoun to.However in the second example, the use of `He' is ambiguous--a distinction that existed before is now gone.The Naive Algorithm would still maintain a salience distinction between 'Earl' and `Ted', where CogNIAC has no rule that makes a salience distinction between subject and object of a sentence which has two intervening sentences.The closest rule would be Unique in Discourse, rule 1, which does not yield a unique antecedent.CogNIAC has been evaluated in two different contexts.The goal of the first experiment was to establish relative performance of CogNIAC to Hobbs' Naive Algorithm--a convenient benchmark that allows indirect comparison to other algorithms.The second experiment reports results on Wall Street Journal data.The chosen domain for comparison with Hobbs' Naive Algorithm was narrative texts about two persons of the same gender told from a third person perspective.The motivation for this data was that we wanted to maximize the ambiguity of resolving pronouns.Only singular third person pronouns were considered.The text was pre-processed with a part-of-speech tagger over which basal noun phrases were delimited and finite clauses and their relative nesting were identified by machine.This pre-processing was subjected to hand correction in order to make comparison with Hobbs as fair as possible since that was an entirely hand executed algorithm, but CogNIAC was otherwise machine run and scored.Errors were not chained, i.e. in left-to-right processing of the text, earlier mistakes were corrected before processing the next noun phrase.Since the Naive Algorithm resolves all pronouns, two lower precision rules were added to rules 1-6) for comparisons sake.The rules are: The last two rules are lower precision than the first six, but perform well enough to merit their inclusion in a 'resolve all pronouns' configuration.Rule 7 performed reasonably well with 77% precision in training (10/13 correct for 201 pronouns), and rule 8 performed with 65% precision in training (44/63 correct).The first six rules each had a precision of greater than 90% for the training data with the exception of rule 4 which had a precision of 80% for 5 resolutions.The summary performance of the Naive Algorithm and CogNIAC (including all 8 rules) for the first 100 or so pronouns in three narrative texts are: Results for 298 third person pronouns in text about two same gender people.Since both the Naive Algorithm and the resolve all pronouns configuration of CogNIAC are required to resolve all pronouns, precision and recall figures are not appropriate.Instead % correct figures are given.The high precision version of CogNIAC is reported with recall (number correct/number of instances of coreference) and precision (number correct/number of guesses) measures.The conclusion to draw from these results is: if forced to commit to all anaphors, CogNIAC performs comparably to the Naive Algorithm.Lappin and Leass 3 Rule 7 is based on the primitives of Centering Theory (Grosz, Joshi and Weinstein '86).The Cb of an utterance is the highest ranked NP (Ranking being: Subject > All other NPs) from the prior finite clause realized anaphorically in the current finite clause.Please see Baldwin '95 for a full discussion of the details of the rule.1994 correctly resolved 86% of 360 pronouns in computer manuals.Lapin and Leass run Hobbs' algorithm on the their data and the Naive Algorithm is correct 82% of the time--4% worse.This allows indirect comparison with CogNIAC, with the suggestive conclusion that the resolve all pronouns configuration of CogNIAC, like the Naive Algorithm, is at least in the ballpark of more modern approaches& The breakdown of the individual rules is as follows: Performance of individual rules in Experiment 1.Note the high precision of rules 1 - 6).Recall = #correct/#actual, Precision = #correct/#guessed Far more interesting to consider is the performance of the high precision rules 1 through 6.The first four rules perform quite well at 96% precision (148/154) and 50% recall (148/298).Adding in rules 5 and 6 resolves a total of 190 pronouns correctly, with only 16 mistakes, a precision of 92% and recall of 64%.This contrasts strongly with the resolve-all-pronouns results of 78%.The last two rules, 7 and 8 performed quite badly on the test data.Despite their poor performance, CogNIAC still remained comparable to the Naive Algorithm.3.2.2 Experiment 2-- All pronouns in MUC-6 evaluation: CogNIAC was used as the pronoun component in the University Pennsylvania's coreference entry5 in the MUC-6 evaluation.Pronominal anaphora constitutes 17% of coreference annotations in the evaluation data used.The remaining instances of anaphora included common noun anaphora and coreferent instances of proper nouns.As a result being part of a larger system, changes were made to CogNIAC to make it fit in better with the other components of the overall system in addition to adding rules that were specialized for the new kinds of pronominal anaphora.These changes include: 4 This is not to say that RAP was not an advancement of the state of the art.A significant aspect of that research is that both RAP and the Naive Algorithm were machine executed--the Naive Algorithm was not machine executed in either the Hobbs 76 paper or in the evaluation in this work.A total of thirty articles were used in the formal evaluation, of which I chose the first fifteen for closer analysis.The remaining fifteen were retained for future evaluations.The performance of CogNIAC was as follows: The precision (73%) is quite a bit worse than that encountered in the narrative.The performance of the individual rules was quite different from the narrative texts, as shown in the table below: The results for CogNIAC for all pronouns in the first 15 articles of the MUC-6 evaluation.Upon closer examination approximately 75% of the errors were due to factors outside the scope of the CogNIAC pronominal resolution component.Software problems accounted for 20% of the incorrect cases, another 30% were due to semantic errors like misclassification of a noun phrase into person or company, singular/plural etc.The remaining errors were due to incorrect noun phrase identification, failure to recognize pleonastic-it or other cases where there is no instance of an antecedent.However, 25% of the errors were due directly to the rules of CogNIAC being plain wrong.CogNIAC is both an engineering effort and a different approach to information processing in variable knowledge contexts.Each point is addressed in turn.A question raised by a reviewer asked whether there was any use for high precision coreference given that it is not resolving as much coreference as other methods.In the first experiment, the high precision version of CogNIAC correctly resolved 62% of the pronouns as compared to the resolve all pronouns version which resolved 79% of them--a 27% loss of overall recall.The answer to this question quite naturally depends on the application coreference is being used in.Some examples follow.Information retrieval is characterized as a process by which a query is used to retrieve relevant documents from a text database.Queries are typically natural language based or Boolean expressions.Documents are retrieved and ranked for relevance using various string matching techniques with query terms in a document and the highest scoring documents are presented to the user first.The role that coreference resolution might play in information retrieval is that retrieval algorithms that a) count the number of matches to a query term in a document, or b) count the proximity of matches to query terms, would benefit by noticing alternative realizations of the terms like 'he' in place 'George Bush'.In such an application, high precision coreference would be more useful than high recall coreference if the information retrieval engine was returning too many irrelevant documents but getting a reasonable number of relevant documents.The coreference would only help the scores of presumably relevant documents, but at the expense of missing some relevant documents.A higher recall, lower precision algorithm would potentially add more irrelevant documents.A direct application of the &quot;ambiguity noticing&quot; ability of CogNIAC is in checking the coherence of pronoun use in text for children and English as a second language learners.Ambiguous pronoun use is a substantial problem for beginning writers and language learners.CogNIAC could scan texts as they are being written and evaluate whether there was sufficient syntactic support from the context to resolve the pronoun--if not, then the user could be notified of a potentially ambiguous use.It is not clear that CogNIAC's current levels of performance could support such an application, but it is a promising application.Information extraction amounts to filling in template like data structures from free text.Typically the patterns which are used to fill the templates are hand built.The latest MUC-6 evaluation involved management changes at companies.A major problem in information extraction is the fact that the desired information can be spread over many sentences in the text and coreference resolution is essential to relate relevant sentences to the correct individuals, companies etc.The MUC-6 coreference task was developed with the idea that it would aid information extraction technologies.The consequences for an incorrectly resolved pronoun can be devastating to the final template filling task--one runs the risk of conflating information about one individual with another.High precision coreference appears to be a natural candidate for such applications.CogNIAC effectively circumscribes those cases where coreference can be done with high confidence and those cases that require greater world knowledge, but how might CogNIAC be a part of a more knowledge rich coreference application?CogNIAC as a set of seven or so high precision rules would act as an effective filter on what a more knowledge rich application would have to resolve.But the essential component behind CogNIAC is not the rules themselves, but the control structure of behind its coreference resolution algorithm.This control structure could control general inference techniques as well.An interesting way to look at CogNIAC is as a search procedure.The Naive Algorithm can be over simplified as depth first search over parse trees.Depth first search is also a perfectly reasonable control structure for an inference engine-- as it is with PROLOG.The search structure of CogNIAC could be characterized as parallel iterative deepening with solutions being accepted only if a unique solution is found to the depth of the parallel search.But there is not enough room in this paper to explore the general properties of CogNIAC's search and evaluation strategy.Another angle on CogNIAC's role with more robust knowledge sources is to note that the recall limitations of CogNIAC for the class of pronouns/data considered are due to insufficient filtering mechanisms on candidate antecedents.There is not a need to expand the space of candidate antecedents with additional knowledge, but rather eliminate semantically plausible antecedents with constraints from verb knowledge and other sources of constraints currently not available to the system.However, there are classes of coreference that require strong knowledge representation to assemble the initial set of candidate antecedents.This includes the realm of inferred definites &quot;I went to the house and opened the door&quot; and synonymy between definite common nouns as in &quot;the tax' and 'the levy.Hobbs 1976 ultimately rejects the Naive Algorithm as a stand-alone solution to the pronoun resolution problem.In that rejection he states: The naive algorithm does not work.Anyone can think of examples where it fails.In these cases it not only fails; it gives no indication that it has failed and offers no help in finding the real antecedent.Hobbs then articulates a vision of what the appropriate technology is, which entails inference over an encoding of world knowledge.But is world knowledge inherent in resolving all pronouns as Hobbs skepticism seems to convey?It has not been clear up to this point whether any anaphora can be resolved with high confidence given that there are clear examples which can only be resolved with sophisticated world knowledge, e.g. the Winograd city council sentences.But the results from the first and second experiments demonstrate that it is possible to have respectable recall with very high precision (greater than 90%) for some kinds of pronominal resolution.However, good performance does not necessarily falsify Hobbs' skepticism.The high precision component of CogNIAC still makes mistakes, 8-9% error for the first experiment--it is harder to evaluate the second experiment.If it were the case that integration of world knowledge would have prevented those errors, then Hobbs' skepticism still holds since CogNIAC has only minimized the role of world knowledge, not eliminated it.In looking at the mistakes made in the second experiment, there were no examples that appeared to be beyond the scope of further improving the syntactic rules or expanding the basic categorization of noun phrases into person, company or place.For the data considered so far, there does appear to be a class of anaphors that can be reliably recognized and resolved with non-knowledge intensive techniques.Whether this holds in general remains an open question, but it is a central design assumption behind the system.A more satisfying answer to Hobbs' skepticism is contained in the earlier suggestive conjecture that world knowledge facilitates anaphora by eliminating ambiguity.This claim can be advanced to say that world knowledge comes into play in those cases of anaphora that do not fall under the purview of rules 1 through 7 and their refinements.If this is correct, then the introduction of better world knowledge sources will help in the recall of the system rather than the precision.Ultimately, the utility of CogNIAC is a function of how it performs.The high precision rules of CogNIAC performed very well, greater than 90% precision with good recall for the first experiment.In the second experiment, components other than the rules of CogNIAC began to degrade the performance of the system unduly.But there is promise in the high precision core of CogNIAC across varied domains.CogNIAC is currently the common noun and pronoun resolution component of the University of Pennsylvania's coreference resolution software and general NLP software (Camp).This paper does not address the common noun coreference aspects of the system but there are some interesting parallels with pronominal coreference.Some changes planned include the following sorts of coreference: The processing of split antecedents, John called Mary.They went to a movie.This class of coreference is quite challenging because the plural anaphor 'they' must be able to collect a set of antecedents from the prior discourse--but how far should it look back, and once it has found two antecedents, should it continue to look for more?Event reference is a class of coreference that will also prove to be quite challenging.For example: The computer won the match.It was a great triumph.The antecedent to 'It' could be any of 'The computer', 'the match' or the event of winning.The space of ambiguity will certainly grow substantially when events are considered as candidate antecedents.Currently the system uses no verb semantics to try and constrain possible coreference.While the Winograd sentences are too difficult for current robust lexical semantic systems, simpler generalizations about what can fill an argument are possible, consider: The price of aluminum rose today due to large purchases by ALCOA Inc.It claimed that it was not trying to corner the market.Since 'It' is an argument to 'claimed' , a verb that requires that its subject be animate, we can eliminate 'The price of aluminum' and 'today' from consideration, leaving `ALCOA Inc.' as the sole singular antecedent from the prior sentence.Work has been done along these lines by Dagan '90.I would like to thank my advisors Ellen Prince and Aravind Joshi for their support.Also the comments of two anonymous reviewers proved quite helpful.
Improved Alignment Models For Statistical Machine TranslationPP — 31.5 In all experiments, we use the following three error criteria: • WER (word error rate): The WER is computed as the minimum number of substitution, insertion and deletion operations that have to be performed to convert the generated string into the target string.This performance criterion is widely used in speech recognition.• PER (position-independent word error rate): A shortcoming of the WER is the fact that it requires a perfect word order.This is 26 2: for Text and Speech Input: error rate (WER), positionindependent word error rate (PER) and subjective sentence error rate (SSER) with/without preprocessing (147 sentences = 1 968 words of the Verbmobil task).Input Preproc.WER[%] PER[Vo] SSER[%] Single-Word Based Approach Text No 53.4 38.3 35.7 Yes 56.0 41.2 35.3 Speech No 67.8 50.1 54.8 Yes 67.8 51.4 52.7 Alignnient Templates Text No 49.5 35.3 31.5 Yes 48.3 35.1 27.2 Speech No 63.5 45.6 52.4 Yes 62.8 45.6 50.3 particularly a problem for the Verbmobil task, where the word order of the German- English sentence pair can be quite different.As a result, the word order of the automatically generated target sentence can be different from that of the target sentence, but nevertheless acceptable so that the WER measure alone could be misleading.In order to overcome this problem, we introduce as additional measure the positionindependent word error rate (PER).This measure compares the words in the two senthe word order into account.Words that have no matching counterparts are counted as substitution errors.Depending on whether the translated sentence is longer or shorter than the target translation, the remaining words result in either insertion or deletion errors in addition to substitution errors.The PER is guaranteed to be less than or equal to the WER.• SSER (subjective sentence error rate): For a more detailed analysis, subjective judgments by test persons are necessary.Each translated sentence was judged by a human examiner according to an error scale from 0.0 to 1.0.A score of 0.0 means that the translation is semantically and syntactically correct, a score of 0.5 means that a sentence is semantically correct but syntactically wrong and a score of 1.0 means that the sentence is semantically wrong.The human examiner was offered the translated sentences of the two approaches at the same As a result we expect a better possibility of reproduction.The results of the translation experiments using the single-word based approach and the alignment template approach on text input and on speech input are summarized in Table 2.The results are shown with and without the use of domain-specific preprocessing.The alignment template approach produces better translation results than the single-word based approach.From this we draw the conclusion that it is important to model word groups in source and target language.Considering the recognition word error rate of 31% the degradation of about 20% by speech input can be expected.The average translation time on an Alpha workstation for a single sentence is about one second for the alignment template appreach and 30 seconds for the single-word based search procedure.Within the Verbmobil project other translation modules based on rule-based, examplebased and dialogue-act-based translation are used.We are not able to present results with these methods using our test corpus.But in the current Verbmobil prototype the preliminary evaluations show that the statistical methods produce comparable or better results than other systems.An advantage of the systhat it is robust and always produces a translation result even if the input of the speech recognizer is quite incorrect.5 Summary We have described two approaches to perform statistical machine translation which extend the baseline alignment models.The single-word 27 based approach allows for the the possibility of one-to-many alignments.The alignment template approach uses two different alignment levels: a phrase level alignment between phrases and a word level alignment between single words.As a result the context of words has a greater influence and the changes in word order from source to target language can be learned explicitly.An advantage of both methods is that they learn fully automatically by using a bilingual training corpus and are capable of achieving better translation results on a limited-domain task than other example-based or rule-based translation systems.Acknowledgment This work has been partially supported as part of the Verbmobil project (contract number 01 IV 701 T4) by the German Federal Ministry of Education, Science, Research and Technology and as part of the EuTrans project by the by the European Community (ESPRIT project number 30268).The goal of machine translation is the translation of a text given in some source language into a target language.We are given a source string f fi...fj...fj, which is to be translated into a target string ef = ei...e,...el.Among all possible target strings, we will choose the string with the highest probability: The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language.Pr(ef) is the language model of the target language, whereas Pr(fillef) is the translation model.Many statistical translation models (Vogel et al., 1996; Tillmann et al., 1997; Niessen et al., 1998; Brown et al., 1993) try to model word-toword correspondences between source and target words.The model is often further restricted that each source word is assigned exactly one target word.These alignment models are similar to the concept of Hidden Markov models (HMM) in speech recognition.The alignment mapping is j —> i = aj from source position j to target position i = a3.The use of this alignment model raises major problems as it fails to capture dependencies between groups of words.As experiments have shown it is difficult to handle different word order and the translation of compound nouns.In this paper, we will describe two methods for statistical machine translation extending the baseline alignment model in order to account for these problems.In section 2, we shortly review the single-word based approach described in (Tillmann et al., 1997) with some recently implemented extensions allowing for one-to-many alignments.In section 3 we describe the alignment template approach which explicitly models shallow phrases and in doing so tries to overcome the above mentioned restrictions of singleword alignments.The described method is an improvement of (Och and Weber, 1998), resulting in an improved training and a faster search organization.The basic idea is to model two different alignment levels: a phrase level alignment between phrases and a word level alignment between single words within these phrases.Similar aims are pursued by (Alshawi et al., 1998; Wang and Waibel, 1998) but differently approached.In section 4 we compare the two methods using the Verbmobil task.In this section, we shortly review a translation approach based on the so-called monotonicity requirement (Tillmann et al., 1997).Our aim is to provide a basis for comparing the two different translation approaches presented.In Eq.(1), Pr (el) is the language model, which is a trigram language model in this case.For the translation model Pr (ll lei ) we make the assumption that each source word is aligned to exactly one target word (a relaxation of this assumption is described in section 2.2).For our model, the probability of alignment al for position j depends on the previous alignment position a3_1 (Vogel et al., 1996).Using this assumption, there are two types of probabilities: the alignment probabilities denoted by p(a31a3 _1) and the lexicon probabilities denoted by P(f3 lea2).The string translation probability can be re-written: For the training of the above model parameters, we use the maximum likelihood criterion in the so-called maximum approximation.When aligning the words in parallel texts (for IndoEuropean language pairs like Spanish-English, French-English, Italian-German,...), we typically observe a strong localization effect.In many cases, although not always, there is an even stronger restriction: over large portions of the source string, the alignment is monotone.In this approach, we first assume that the alignments satisfy the monotonicity requirement.Within the translation search, we will introduce suitably restricted permutations of the source string, to satisfy this requirement.For the alignment model, the monotonicity property allows only transitions from a3_1 to ai with a jump width 6: 6 a3 — ai_i E ICI, 1, 21.Theses jumps correspond to the following three cases (6 = 0, 1, 2): new target word is generated.• 8 = 2 (skip transition = non-aligned word): This case corresponds to skipping a word, i.e. there is a word in the target string with no aligned word in the source string.The possible alignments using the monotonicity assumption are illustrated in Fig.1.Monotone alignments are paths through this uniform trellis structure.Using the concept of monotone alignments a search procedure can be formulated which is equivalent to finding the best path through a translation lattice, where the following auxiliary quantity is evaluated using dynamic programming: Here, e and e' are Qe, (j, e) probability of the best partial hypothesis (el, an with ei = e, = e' and a3 = i. the two final words of the hypothesized target string.The auxiliary quantity is evaluated in a position-synchronous way, where j is the processed position in the source string.The result of this search is a mapping: j (a3, ea,), where each source word is mapped to a target position a3 and a word ea, at this position.For a trigram language model the following DP recursion equation is evaluated: p(8) is the alignment probability for the three cases above, pe I., .) denoting the trigram language model. e, e', e&quot;, e&quot; are the four final words which are considered in the dynamic programming taking into account the monotonicity restriction and a trigram language model.The DP equation is evaluated recursively to find the best partial path to each grid point (j, , e).No explicit length model for the length of the generated target string ef given the source string fi/ is used during the generation process.The length model is implicitly given by the alignment probabilities.The optimal translation is obtained by carrying out the following optimization: where J is the length of the input sentence and $ is a symbol denoting the sentence end.The complexity of the algorithm for full search is J- E4, where E is the size of the target language vocabulary.However, this is drastically reduced by beam-search.The baseline alignment model does not permit that a source word is aligned with two or more target words.Therefore, lexical correspondences like 'Zahnarzttermin' for dentist's appointment cause problems because a single source word must be mapped on two or more target words.To solve this problem for the alignment in training, we first reverse the translation direction, i. e. English is now the source language, and German is the target language.For this reversed translation direction, we perform the usual training and then check the alignment paths obtained in the maximum approximation.Whenever a German word is aligned with a sequence of the adjacent English words, this sequence is added to the English vocabulary as an additional entry.As a result, we have an extended English vocabulary.Using this new vocabulary, we then perform the stan2.3 Extension to Handle Non-Monotonicity Our approach assumes that the alignment is monotone with respect to the word order for the lion's share of all word alignments.For the translation direction German-English the monotonicity constraint is violated mainly with respect to the verb group.In German, the verb group usually consists of a left and a right verbal brace, whereas in English the words of the verb group usually form a sequence of consecutive words.For our DP search, we use a leftto-right beam-search concept having been introduced in speech recognition, where we rely on beam-search as an efficient pruning technique in order to handle potentially huge search spaces.Our ultimate goal is speech translation aiming at a tight integration of speech recognition and translation (Ney, 1999).The results presented were obtained by using a quasi-monotone search procedure, which proceeds from left to right along the position of the source sentence but allows for a small number of source positions that are not processed monotonically.The word re-orderings of the source sentence positions were restricted to the words of the German verb group.Details of this approach will be presented elsewhere.A general deficiency of the baseline alignment models is that they are only able to model correspondences between single words.A first countermeasure was the refined alignment model described in section 2.2.A more systematic approach is to consider whole phrases rather than single words as the basis for the alignment models.In other words, a whole group of adjacent words in the source sentence may be aligned with a whole group of adjacent words in the target language.As a result the context of words has a greater influence and the changes in word order from source to target language can be learned explicitly.3.1 The word level alignment: alignment templates In this section we will describe how we model the translation of shallow phrases.The key element of our translation model are the alignment templates._ An alignment template z is a triple (F, E, A) which describes the alignment A between a source class sequence F and a target class sequence E. The alignment A is represented as a matrix with binary values.A matrix element with value 1 means that the words at the corresponding positions are aligned and the value 0 means that the words are not aligned.If a source word is not aligned to a target word then it is aligned to the empty word eo which shall be at the imaginary position i = 0.This alignment representation is a generalization of the baseline alignments described in (Brown et al., 1993) and allows for many-to-many alignments.The classes used in P and E are automatically trained bilingual classes using the method described in (Och, 1999) and constitute a partition of the vocabulary of source and target language.The class functions T and e map words to their classes.The use of classes instead of words themselves has the advantage of a better generalization.If there exist classes in source and target language which contain all towns it is possible that an alignment template learned using a special town can be generalized to all towns.In Fig.2 an example of an alignment template is shown.An alignment template z -= (F, E, A) is applicable to a sequence of source words I if the alignment template classes and the classes of the source words are equal: .F( f) = F. The application of the alignment template z constrains the target words-6 to correspond to the target class sequence: e(e) = E. The application of an alignment template does not determine the target words, but only constrains them.For the selection of words from classes we use a statistical model for p(e-lz, f) based on the lexicon probabilities of a statistical lexicon p(f le).We assume a mixture alignment between the source and target language words constrained by the alignment matrix A: In order to describe the phrase level alignment in a formal way, we first decompose both the source sentence fiJ and the target sentence ef into a sequence of phrases (k = 1,. .., K): In order to simplify the notation and the presentation, we ignore the fact that there can be a large number of possible segmentations and assume that there is only one segmentation.In the previous section, we have described the alignment within the phrases.For the alignment af‘ between the source phrases ë and the target phrases fr, we obtain the following equation: For the phrase level alignment we use a first-order alignment model p(aklaki-1 K) = P(ak lak-i, K) which is in addition constrained to be a permutation of the K phrases.For the translation of one phrase, we introduce the alignment template as an unknown variable: The probability p(z1e) to apply an alignment template gets estimated by relative frequencies (see next section).The probability p(f lz, e) is decomposed by Eq.(2).In this section we show how we obtain the parameters of our translation model by using a parallel training corpus: rections f e and e —+ f by applying the EM-algorithm.However we do not apply maximum approximation in training, thereby obtaining slightly improved alignments.2.For each translation direction we calculate the Viterbi-alignment of the translation models determined in the previous step.Thus we get two alignment vectors crj1. and bf for each sentence.We increase the quality of the alignments by combining the two alignment vectors into one alignment matrix using the following method.A1 = {(aj , j)lj = 1 ...J} and A2 = {(i,bi)li = 1 ... /} denote the set of links in the two Viterbi-alignments.In a first step the intersection A = A1 n A2 is determined.The elements within A are justified by both Viterbi-alignments and are therefore very reliable.We now extend the alignment A iteratively by adding links (i, j) occurring only in A1 or in A2 if they have a neighbouring link already in A or if neither the word fi nor the word ei are aligned in A.The alignment (i, j) has the neighbouring links (i — 1,j), (i, j — 1), (i + 1,j), and (i, j + 1).In the Verbmobil task (Table 1) the precision of the baseline Viterbi alignments is 83.3 percent with English as source language and 81.8 percent with German as source language.Using this heuristic we get an alignment matrix with a precision of 88.4 percent without loss in recall.3.We estimate a bilingual word lexicon p(f le) by the relative frequencies of the alignment determined in the previous step: Here nA(f,e) is the frequency that the word f is aligned to e and n(e) is the frequency of e in the training corpus.4.We determine word classes for source and target language.A naive approach for doing this would be the use of monolingually optimized word classes in source and target language.Unfortunately we can not expect that there is a direct correspondence between independently optimized classes.Therefore monolingually optimized word classes do not seem to be useful for machine translation.We determine correlated bilingual classes by using the method described in (Och, 1999).The basic idea of this method is to apply a maximum-likelihood approach to the joint probability of the parallel training corpus.The resulting optimization criterion for the bilingual word classes is similar to the one used in monolingual maximumlikelihood word clustering.5.We count all phrase-pairs of the training corpus which are consistent with the alignment matrix determined in step 2.A phrase-pair is consistent with the alignment if the words within the source phrase are only aligned to words within the target phrase.Thus we obtain a count n(z) of how often an alignment template occurred in the aligned training corpus.The probability of using an alignment template needed by Eq.(5) is estimated by relative frequency: Fig.3 shows some of the extracted alignment templates.The extraction algorithm does not perform a selection of good or bad alignment templates - it simply extracts all possible alignment templates.For decoding we use the following search criterion: This decision rule is an approximation to Eq.(1) which would use the translation probability p(lef).Using the simplification it is easy to integrate translation and language model in the search process as both models predict target words.As experiments have shown this simplification does not affect the quality of translation results.To allow the influence of long contexts we use a class-based five-gram language model with backing-off.The search space denoted by Eq.(8) is very large.Therefore we apply two preprocessing steps before the translation of a sentence: 1.We_determine the set of all source phrases in f for which an applicable alignment template exists.Every possible application of an alignment template to a sub-sequence of the source sentence is called alignment template instantiation.2.We now perform a segmentation of the input sentence.We search for a sequence of This is done efficiently by dynamic programming.Because of the simplified decision rule (Eq.(8)) it is used in Eq.(9) P(zlik) instead of p(zI4).Afterwards the actual translation process begins.It has a search organization along the positions of the target language string.In search we produce partial hypotheses, each of which contains the following information: A partial hypothesis is extended by appending one target word.The set of all partial hypotheses can be structured as a graph with a source node representing the sentence start, leaf nodes representing full translations and intermediate nodes representing partial hypotheses.We recombine partial hypotheses which cannot be distinguished by neither language model nor translation model.When the elements 1 - 5 of two partial hypotheses do not allow to distinguish between two hypotheses it is possible to drop the hypothesis with higher costs for the subsequent search process.We also use beam-search in order to handle the huge search space.We compare in beamsearch hypotheses which cover different parts of the input sentence.This makes the comparison of the costs somewhat problematic.Therefore we integrate an (optimistic) estimation of the remaining costs to arrive at a full translation.This can be done efficiently by determining in advance for each word in the source language sentence a lower bound for the costs of the translation of this word.Together with the bit-vector stored in a partial hypothesis it is possible to achieve an efficient estimation of the remaining costs.The &quot;Verbmobil Task&quot; (Wahlster, 1993) is a speech translation task in the domain of appointment scheduling, travel planning, and hotel reservation.The task is difficult because it consists of spontaneous speech and the syntactic structures of the sentences are less restricted and highly variable.The translation direction is from German to English which poses special problems due to the big difference in the word order of the two languages.We present results on both the text transcription and the speech recognizer output using the alignment template approach and the single-word based approach.The text input was obtained by manually transcribing the spontaneously spoken sentences.There was no constraint on the length of the sentences, and some of the sentences in the test corpus contain more than 50 words.Therefore, for text input, each sentence is split into shorter units using the punctuation marks.The segments thus obtained were translated separately, and the final translation was obtained by concatenation.In the case of speech input, the speech recognizer along with a prosodic module produced so-called prosodic markers which are equivalent to punctuation marks in written language.The experiments for speech input were performed on the single-best sentence of the recognizer.The recognizer had a word error rate of 31.0%.Considering only the real words without the punctuation marks, the word error rate was smaller, namely 20.3%.A summary of the corpus used in the experiments is given in Table 1.Here the term word refers to full-form word as there is no morphological processing involved.In some of our experiments we use a domain-specific preprocessing which consists of a list of 803 (for German) and 458 (for English) word-joinings and wordsplittings for word compounds, numbers, dates and proper names.To improve the lexicon probabilities and to account for unseen words we added a manually created German-English dictionary with 13 388 entries.The classes used were constrained so that all proper names were included in a single class.Apart from this, the classes were automatically trained using the described bilingual clustering method.For each of the two languages 400 classes were used.For the single-word based approach, we used the manual dictionary as well as the preprocessing steps described above.Neither the translation model nor the language model used classes in this case.In principal, when re-ordering words of the source string, words of the German verb group could be moved over punctuation marks, although it was penalized by a constant cost.The WER is computed as the minimum number of substitution, insertion and deletion operations that have to be performed to convert the generated string into the target string.This performance criterion is widely used in speech recognition.A shortcoming of the WER is the fact that it requires a perfect word order.This is Table 2: Experiments for Text and Speech Input: Word error rate (WER), positionindependent word error rate (PER) and subjective sentence error rate (SSER) with/without preprocessing (147 sentences = 1 968 words of the Verbmobil task). particularly a problem for the Verbmobil task, where the word order of the GermanEnglish sentence pair can be quite different.As a result, the word order of the automatically generated target sentence can be different from that of the target sentence, but nevertheless acceptable so that the WER measure alone could be misleading.In order to overcome this problem, we introduce as additional measure the positionindependent word error rate (PER).This measure compares the words in the two sentences without taking the word order into account.Words that have no matching counterparts are counted as substitution errors.Depending on whether the translated sentence is longer or shorter than the target translation, the remaining words result in either insertion or deletion errors in addition to substitution errors.The PER is guaranteed to be less than or equal to the WER.For a more detailed analysis, subjective judgments by test persons are necessary.Each translated sentence was judged by a human examiner according to an error scale from 0.0 to 1.0.A score of 0.0 means that the translation is semantically and syntactically correct, a score of 0.5 means that a sentence is semantically correct but syntactically wrong and a score of 1.0 means that the sentence is semantically wrong.The human examiner was offered the translated sentences of the two approaches at the same time.As a result we expect a better possibility of reproduction.The results of the translation experiments using the single-word based approach and the alignment template approach on text input and on speech input are summarized in Table 2.The results are shown with and without the use of domain-specific preprocessing.The alignment template approach produces better translation results than the single-word based approach.From this we draw the conclusion that it is important to model word groups in source and target language.Considering the recognition word error rate of 31% the degradation of about 20% by speech input can be expected.The average translation time on an Alpha workstation for a single sentence is about one second for the alignment template appreach and 30 seconds for the single-word based search procedure.Within the Verbmobil project other translation modules based on rule-based, examplebased and dialogue-act-based translation are used.We are not able to present results with these methods using our test corpus.But in the current Verbmobil prototype the preliminary evaluations show that the statistical methods produce comparable or better results than the other systems.An advantage of the system is that it is robust and always produces a translation result even if the input of the speech recognizer is quite incorrect.We have described two approaches to perform statistical machine translation which extend the baseline alignment models.The single-word based approach allows for the the possibility of one-to-many alignments.The alignment template approach uses two different alignment levels: a phrase level alignment between phrases and a word level alignment between single words.As a result the context of words has a greater influence and the changes in word order from source to target language can be learned explicitly.An advantage of both methods is that they learn fully automatically by using a bilingual training corpus and are capable of achieving better translation results on a limited-domain task than other example-based or rule-based translation systems.This work has been partially supported as part of the Verbmobil project (contract number 01 IV 701 T4) by the German Federal Ministry of Education, Science, Research and Technology and as part of the EuTrans project by the by the European Community (ESPRIT project number 30268).

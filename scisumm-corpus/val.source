A good indicator of whether a person knows the meaning of a word is the ability to use it appropriately in a sentence (Miller and Gildea, 1987).Much information about usage can be obtained from quite a limited context: Choueka and Lusignan (1985) found that people can typically recognize the intended sense of a polysemous word by looking at a narrow window of one or two words around it.Statistically-based computer programs have been able to do the same with a high level of accuracy (Kilgarriff and Palmer, 2000).The goal of our work is to automatically identify inappropriate usage of specific vocabulary words in essays by looking at the local contextual cues around a target word.We have developed a statistical system, ALEK (Assessing Lexical Knowledge), that uses statistical analysis for this purpose.A major objective of this research is to avoid the laborious and costly process of collecting errors (or negative evidence) for each word that we wish to evaluate.Instead, we train ALEK on a general corpus of English and on edited text containing example uses of the target word.The system identifies inappropriate usage based on differences between the word's local context cues in an essay and the models of context it has derived from the corpora of well-formed sentences.A requirement for ALEK has been that all steps in the process be automated, beyond choosing the words to be tested and assessing the results.Once a target word is chosen, preprocessing, building a model of the word's appropriate usage, and identifying usage errors in essays is performed without manual intervention.ALEK has been developed using the Test of English as a Foreign Language (TOEFL) administered by the Educational Testing Service.TOEFL is taken by foreign students who are applying to US undergraduate and graduate-level programs.Approaches to detecting errors by non-native writers typically produce grammars that look for specific expected error types (Schneider and McCoy, 1998; Park, Palmer and Washburn, 1997).Under this approach, essays written by ESL students are collected and examined for errors.Parsers are then adapted to identify those error types that were found in the essay collection.We take a different approach, initially viewing error detection as an extension of the word sense disambiguation (WSD) problem.Corpus-based WSD systems identify the intended sense of a polysemous word by (1) collecting a set of example sentences for each of its various senses and (2) extracting salient contextual cues from these sets to (3) build a statistical model for each sense.They identify the intended sense of a word in a novel sentence by extracting its contextual cues and selecting the most similar word sense model (e.g., Leacock, Chodorow and Miller (1998), Yarowsky (1993)).Golding (1995) showed how methods used for WSD (decision lists and Bayesian classifiers) could be adapted to detect errors resulting from common spelling confusions among sets such as there, their, and they're.He extracted contexts from correct usage of each confusable word in a training corpus and then identified a new occurrence as an error when it matched the wrong context.However, most grammatical errors are not the result of simple word confusions.This complicates the task of building a model of incorrect usage.One approach we considered was to proceed without such a model: represent appropriate word usage (across senses) in a single model and compare a novel example to that model.The most appealing part of this formulation was that we could bypass the knowledge acquisition bottleneck.All occurrences of the word in a collection of edited text could be automatically assigned to a single training set representing appropriate usage.Inappropriate usage would be signaled by contextual cues that do not occur in training.Unfortunately, this approach was not effective for error detection.An example of a word usage error is often very similar to the model of appropriate usage.An incorrect usage can contain two or three salient contextual elements as well as a single anomalous element.The problem of error detection does not entail finding similarities to appropriate usage, rather it requires identifying one element among the contextual cues that simply does not fit.What kinds of anomalous elements does ALEK identify?Writers sometimes produce errors that violate basic principles of English syntax (e.g., a desks), while other mistakes show a lack of information about a specific vocabulary item (e.g., a knowledge).In order to detect these two types of problems, ALEK uses a 30-million word general corpus of English from the San Jose Mercury News (hereafter referred to as the general corpus) and, for each target word, a set of 10,000 example sentences from North American newspaper text' (hereafter referred to as the word-specific corpus).The corpora are extracted from the ACL-DCI corpora.In selecting the sentences for the word ALEK infers negative evidence from the contextual cues that do not co-occur with the target word — either in the word specific corpus or in the general English one.It uses two kinds of contextual cues in a ±2 word window around the target word: function words (closed-class items) and part-of-speech tags (Brill, 1994).The Brill tagger output is post-processed to &quot;enrich&quot; some closed class categories of its tag set, such as subject versus object pronoun and definite versus indefinite determiner.The enriched tags were adapted from Francis and Kaera (1982).After the sentences have been preprocessed, ALEK counts sequences of adjacent part-ofspeech tags and function words (such as determiners, prepositions, and conjunctions).For example, the sequence a/AT full-time/I.1 jobINN contributes one occurrence each to the bigrams AT+JJ, JJ+NN, a+JJ, and to the part-of-speech tag trigram AT+JJ+NN.Each individual tag and function word also contributes to its own unigram count.These frequencies form the basis for the error detection measures.From the general corpus, ALEK computes a mutual information measure to determine which sequences of part-of-speech tags and function words are unusually rare and are, therefore, likely to be ungrammatical in English (e.g., singular determiner preceding plural noun, as in *a desks).Mutual information has often been used to detect combinations of words that occur more frequently than we would expect based on the assumption that the words are independent.Here we use this measure for the opposite purpose — to find combinations that occur less often than expected.ALEK also looks for sequences that are common in general but unusual in the word specific corpus (e.g., the singular determiner a preceding a singular noun is common in English but rare when the noun is specific corpora, we tried to minimize the mismatch between the domains of newspapers and TOEFL essays.For example, in the newspaper domain, concentrate is usually used as a noun, as in orange juice concentrate but in TOEFL essays it is a verb 91% of the time.Sentence selection for the word specific corpora was constrained to reflect the distribution of part-of-speech tags for the target word in a random sample of TOEFL essays. knowledge).These divergences between the two corpora reflect syntactic properties that are peculiar to the target word.The system computes mutual information comparing the proportion of observed occurrences of bigrams in the general corpus to the proportion expected based on the assumption of independence, as shown below: Here, P(AB) is the probability of the occurrence of the AB bigram, estimated from its frequency in the general corpus, and P(A) and P(B) are the probabilities of the first and second elements of the bigram, also estimated from the general corpus.Ungrammatical sequences should produce bigram probabilities that are much smaller than the product of the unigram probabilities (the value of MI will be negative).Trigram sequences are also used, but in this case the mutual information computation compares the co-occurrence of ABC to a model in which A and C are assumed to be conditionally independent given B (see Lin, 1998).Once again, a negative value is often indicative of a sequence that violates a rule of English.ALEK also uses mutual information to compare the distributions of tags and function words in the word-specific corpus to the distributions that are expected based on the general corpus.The measures for bigrams and trigrams are similar to those given above except that the probability in the numerator is estimated from the wordspecific corpus and the probabilities in the denominator come from the general corpus.To return to a previous example, the phrase a knowledge contains the tag bigram for singular determiner followed by singular noun (AT NN).This sequence is much less common in the word-specific corpus for knowledge than would be expected from the general corpus unigram probabilities of AT and NN.In addition to bigram and trigram measures, ALEK compares the target word's part-ofspeech tag in the word-specific corpus and in the general corpus.Specifically, it looks at the conditional probability of the part-of-speech tag given the major syntactic category (e.g., plural noun given noun) in both distributions, by computing the following value.For example, in the general corpus, about half of all noun tokens are plural, but in the training set for the noun knowledge, the plural knowledges occurs rarely, if at all.The mutual information measures provide candidate errors, but this approach overgenerates — it finds rare, but still quite grammatical, sequences.To reduce the number of false positives, no candidate found by the MI measures is considered an error if it appears in the word-specific corpus at least two times.This increases ALEK's precision at the price of reduced recall.For example, a knowledge will not be treated as an error because it appears in the training corpus as part of the longer a knowledge of sequence (as in a knowledge of mathematics).ALEK also uses another statistical technique for finding rare and possibly ungrammatical tag and function word bigrams by computing the x2 (chi square) statistic for the difference between the bigram proportions found in the word-specific and in the general corpus: = The x2 measure faces the same problem of overgenerating errors.Due to the large sample sizes, extreme values can be obtained even though effect size may be minuscule.To reduce false positives, ALEK requires that effect sizes be at least in the moderate-to-small range (Cohen and Cohen, 1983).Direct evidence from the word specific corpus can also be used to control the overgeneration of errors.For each candidate error, ALEK compares the larger context in which the bigram appears to the contexts that have been analyzed in the word-specific corpus.From the wordspecific corpus, ALEK forms templates, sequences of words and tags that represent the local context of the target.If a test sentence contains a low probability bigram (as measured by the x2 test), the local context of the target is compared to all the templates of which it is a part.Exceptions to the error, that is longer grammatical sequences that contain rare subsequences, are found by examining conditional probabilities.To illustrate this, consider the example of a knowledge and a knowledge of The conditional probability of of given a knowledge is high, as it accounts for almost all of the occurrences of a knowledge in the wordspecific corpus.Based on this high conditional probability, the system will use the template for a knowledge of to keep it from being marked as an error.Other function words and tags in the +1 position have much lower conditional probability, so for example, a knowledge is will not be treated as an exception to the error.TOEFL essays are graded on a 6 point scale, where 6 demonstrates &quot;clear competence&quot; in writing on rhetorical and syntactic levels and 1 demonstrates &quot;incompetence in writing&quot;.If low probability n-grams signal grammatical errors, then we would expect TOEFL essays that received lower scores to have more of these ngrams.To test this prediction, we randomly selected from the TOEFL pool 50 essays for each of the 6 score values from 1.0 to 6.0.For each score value, all 50 essays were concatenated to form a super-essay.In every super-essay, for each adjacent pair and triple of tags containing a noun, verb, or adjective, the bigram and trigram mutual information values were computed based on the general corpus.Table 1 shows the proportions of bigrams and trigrams with mutual information less than —3.60.As predicted, there is a significant negative correlation between the score and the proportion of low probability bigrams (r,= -.94, n=6, p<.01, two-tailed) and trigrams (r,= -.84, n=6, p<.05, two-tailed).ALEK was developed using three target words that were extracted from TOEFL essays: concentrate, interest, and knowledge.These words were chosen because they represent different parts of speech and varying degrees of polysemy.Each also occurred in at least 150 sentences in what was then a small pool of TOEFL essays.Before development began, each occurrence of these words was manually labeled as an appropriate or inappropriate usage — without taking into account grammatical errors that might have been present elsewhere in the sentence but which were not within the target word's scope.Critical values for the statistical measures were set during this development phase.The settings were based empirically on ALEK's performance so as to optimize precision and recall on the three development words.Candidate errors were those local context sequences that produced a mutual information value of less than —3.60 based on the general corpus; mutual information of less than —5.00 for the specific/general comparisons; or a x2 value greater than 12.82 with an effect size greater than 0.30.Precision and recall for the three words are shown below.ALEK was tested on 20 words.These words were randomly selected from those which met two criteria: (1) They appear in a university word list (Nation, 1990) as words that a student in a US university will be expected to encounter and (2) there were at least 1,000 sentences containing the word in the TOEFL essay pool.To build the usage model for each target word, 10,000 sentences containing it were extracted from the North American News Corpus.Preprocessing included detecting sentence boundaries and part-of-speech tagging.As in the development system, the model of general English was based on bigram and trigram frequencies of function words and part-ofspeech tags from 30-million words of the San Jose Mercury News.For each test word, all of the test sentences were marked by ALEK as either containing an error or not containing an error.The size of the test set for each word ranged from 1,400 to 20,000 with a mean of 8,000 sentences.To evaluate the system, for each test word we randomly extracted 125 sentences that ALEK classified as containing no error (C-set) and 125 sentences which it labeled as containing an error (E-set).These 250 sentences were presented to a linguist in a random order for blind evaluation.The linguist, who had no part in ALEK's development, marked each usage of the target word as incorrect or correct and in the case of incorrect usage indicated how far from the target one would have to look in order to recognise that there was an error.For example, in the case of &quot;an period&quot; the error occurs at a distance of one word from period.When the error is an omission, as in &quot;lived in Victorian period&quot;, the distance is where the missing word should have appeared.In this case, the missing determiner is 2 positions away from the target.When more than one error occurred, the distance of the one closest to the target was marked.Table 3 lists the precision and recall for the 20 test words.The column labelled &quot;Recall&quot; is the proportion of human-judged errors in the 250sentence sample that were detected by ALEK.&quot;Total Recall&quot; is an estimate that extrapolates from the human judgements of the sample to the entire test set.We illustrate this with the results for pollution.The human judge marked as incorrect usage 91.2% of the sample from ALEK's E-set and 18.4% of the sample from its C-set.To estimate overall incorrect usage, we computed a weighted mean of these two rates, where the weights reflected the proportion of sentences that were in the E-set and C-set.The E-set contained 8.3% of the pollution sentences and the C-set had the remaining 91.7%.With the human judgements as the gold standard, the estimated overall rate of incorrect usage is (.083 x .912 + .917 x .184) = .245.ALEK's estimated recall is the proportion of sentences in the E-set times its precision, divided by the overall estimated error rate (.083 x .912) / .245 = .310.The precision results vary from word to word.Conclusion and pollution have precision in the low to middle 90's while individual's precision is 57%.Overall, ALEK's predictions are about 78% accurate.The recall is limited in part by the fact that the system only looks at syntactic information, while many of the errors are semantic.Nicholls (1999) identifies four error types: an unnecessary word (*affect to their emotions), a missing word (*opportunity of job.), a word or phrase that needs replacing (*every jobs), a word used in the wrong form (*pollutions).ALEK recognizes all of these types of errors.For closed class words, ALEK identified whether a word was missing, the wrong word was used (choice), and when an extra word was used.Open class words have a fourth error category, form, including inappropriate compounding and verb agreement.During the development stage, we found it useful to add additional error categories.Since TEOFL graders are not supposed to take punctuation into account, punctuation errors were only marked when they caused the judge to &quot;garden path&quot; or initially misinterpret the sentence.Spelling was marked either when a function word was misspelled, causing part-ofspeech tagging errors, or when the writer's intent was unclear.The distributions of categories for hits and misses, shown in Table 4, are not strikingly different.However, the hits are primarily syntactic in nature while the misses are both semantic (as in open-class:choice) and syntactic (as in closed-class:missing).ALEK is sensitive to open-class word confusions (affect vs effect) where the part of speech differs or where the target word is confused with another word (*In this aspect, ... instead of In this respect,...).In both cases, the system recognizes that the target is in the wrong syntactic environment.Misses can also be syntactic — when the target word is confused with another word but the syntactic environment fails to trigger an error.In addition, ALEK does not recognize semantic errors when the error involves the misuse of an open-class word in combination with the target (for example, make in &quot;they make benefits&quot;).Closed class words typically are either selected by or agree with a head word.So why are there so many misses, especially with prepositions?The problem is caused in part by polysemy — when one sense of the word selects a preposition that another sense does not.When concentrate is used spatially, it selects the preposition in, as &quot;the stores were concentrated in the downtown area&quot;.When it denotes mental activity, it selects the preposition on, as in &quot;Susan concentrated on her studies&quot;.Since ALEK trains on all senses of concentrate, it does not detect the error in &quot;Susan concentrated in her studies&quot;.Another cause is that adjuncts, especially temporal and locative adverbials, distribute freely in the wordspecific corpora, as in &quot;Susan concentrated in her room.&quot; This second problem is more tractable than the polysemy problem — and would involve training the system to recognize certain types of adjuncts.False positives, when ALEK &quot;identifies&quot; an error where none exists, fall into six major categories.The percentage of each false positive type in a random sample of 200 false positives is shown in Table 5.Domain mismatch: Mismatch of the newspaper-domain word-specific corpora and essay-domain test corpus.One notable difference is that some TOEFL essay prompts call for the writer's opinion.Consequently, TOEFL essays often contain first person references, whereas newspaper articles are written in the third person.We need to supplement the word-specific corpora with material that more closely resembles the test corpus.Tagger: Incorrect analysis by the part-of-speech tagger.When the part-of-speech tag is wrong, ALEK often recognizes the resulting n-gram as anomalous.Many of these errors are caused by training on the Brown corpus instead of a corpus of essays.Syntactic analysis: Errors resulting from using part-of-speech tags instead of supertags or a full parse, which would give syntactic relations between constituents.For example, ALEK false alarms on arguments of ditransitive verbs such as offer and flags as an error &quot;you benefits&quot; in &quot;offers you benefits&quot;.Free distribution: Elements that distribute freely, such as adverbs and conjunctions, as well as temporal and locative adverbial phrases, tend to be identified as errors when they occur in some positions.Punctuation: Most notably omission of periods and commas.Since these errors are not indicative of one's ability to use the target word, they were not considered as errors unless they caused the judge to misanalyze the sentence.Infrequent tags.An undesirable result of our &quot;enriched&quot; tag set is that some tags, e.g., the post-determiner last, occur too infrequently in the corpora to provide reliable statistics.Solutions to some of these problems will clearly be more tractable than to others.Comparison of these results to those of other systems is difficult because there is no generally accepted test set or performance baseline.Given this limitation, we compared ALEK's performance to a widely used grammar checker, the one incorporated in Microsoft's Word97.We created files of sentences used for the three development words concentrate, interest, and knowledge, and manually corrected any errors outside the local context around the target before checking them with Word97.The performance for concentrate showed overall precision of 0.89 and recall of 0.07.For interest, precision was 0.85 with recall of 0.11.In sentences containing knowledge, precision was 0.99 and recall was 0.30.Word97 correctly detected the ungrammaticality of knowledges as well as a knowledge, while it avoided flagging a knowledge of.In summary, Word97's precision in error detection is impressive, but the lower recall values indicate that it is responding to fewer error types than does ALEK.In particular, Word97 is not sensitive to inappropriate selection of prepositions for these three words (e.g., *have knowledge on history, *to concentrate at science).Of course, Word97 detects many kinds of errors that ALEK does not.Research has been reported on grammar checkers specifically designed for an ESL population.These have been developed by hand, based on small training and test sets.Schneider and McCoy (1998) developed a system tailored to the error productions of American Sign Language signers.This system was tested on 79 sentences containing determiner and agreement errors, and 101 grammatical sentences.We calculate that their precision was 78% with 54% recall.Park, Palmer and Washburn (1997) adapted a categorial grammar to recognize &quot;classes of errors [that] dominate&quot; in the nine essays they inspected.This system was tested on eight essays, but precision and recall figures are not reported.The unsupervised techniques that we have presented for inferring negative evidence are effective in recognizing grammatical errors in written text.Preliminary results indicate that ALEK's error detection is predictive of TOEFL scores.If ALEK accurately detects usage errors, then it should report more errors in essays with lower scores than in those with higher scores.We have already seen in Table 1 that there is a negative correlation between essay score and two of ALEK's component measures, the general corpus n-grams.However, the data in Table 1 were not based on specific vocabulary items and do not reflect overall system performance, which includes the other measures as well.Table 6 shows the proportion of test word occurrences that were classified by ALEK as containing errors within two positions of the target at each of 6 TOEFL score points.As predicted, the correlation is negative (F., = -1.00, n = 6,p <.001, two-tailed).These data support the validity of the system as a detector of inappropriate usage, even when only a limited number of words are targeted and only the immediate context of each target is examined.ALEK and by a human judge For comparison, Table 6 also gives the estimated proportions of inappropriate usage by score point based on the human judge's classification.Here, too, there is a negative correlation: rs = –.90, n = 5, p < .05, two-tailed.Although the system recognizes a wide range of error types, as Table 6 shows, it detects only about one-fifth as many errors as a human judge does.To improve recall, research needs to focus on the areas identified in section 3.2 and, to improve precision, efforts should be directed at reducing the false positives described in 3.3.ALEK is being developed as a diagnostic tool for students who are learning English as a foreign language.However, its techniques could be incorporated into a grammar checker for native speakers.We thank Susanne Wolff for evaluating the test sentences, and Robert Kantor, Ken Sheppard and 3 anonymous reviewers for their helpful suggestions.
Chinese sentences arc cx)mposed with string of characters without blanks to mark words.However the basic unit for sentence parsing and understanding is word.Therefore the first step of processing Chinese sentences is to identify the words( i.e. segment the character strings of the sentences into word strings).Most of the current Chinese natural language processing systems include a processor for word iden- tification.Also there are many word segmentation techniques been developed.Usually they use a lexicon with a large set of entries to match input sentences \[2,10,12,13,14,21\].It is very often that there are many l~)ssible different successful matchings.Therefore the major focus for word identification were on thc resolu- tion of ambiguities.However many other important as- pects, such as what should be done, in what depth and what are considered to be the correct identifications were totally ignored.High identification rates are claimed to be achieved, but none of them were mea- sured under equal bases.There is no agreement in what extend words are considered to be correctly iden- tified.For instance, compounds occur very often in Chi- nese text, but none of the existing systems except ours pay much attention to identify them.Proper name is another type of words which cannot be listed exhaus- tively in the lexicon.Therefore simple matching algo- rithms can not successfully identify either compounds or proper names.In this paper, we like to raise the ptx~blems and the difficulties in identifying words and suggest the possible solutions.
It is well known that Expectation-Maximization (EM) performs poorly in unsupervised induction of linguistic structure (Carroll and Charniak, 1992;Merialdo, 1994; Klein, 2005; Smith, 2006).In ret rospect one can certainly find reasons to explain this failure: after all, likelihood does not appear in thewide variety of linguistic tests proposed for identi fying linguistic structure (Fromkin, 2001).This paper focuses on unsupervised part-ofspeech (POS) tagging, because it is perhaps the sim plest linguistic induction task.We suggest that onereason for the apparent failure of EM for POS tagging is that it tends to assign relatively equal numbers of tokens to each hidden state, while the em pirical distribution of POS tags is highly skewed, like many linguistic (and non-linguistic) phenomena(Mitzenmacher, 2003).We focus on first-order Hid den Markov Models (HMMs) in which the hidden state is interpreted as a POS tag, also known as bitag models.In this setting we show that EM performs poorlywhen evaluated using a ?1-to-1 accuracy?evalua tion, where each POS tag corresponds to at most one hidden state, but is more competitive when evaluatedusing a ?many-to-1 accuracy?evaluation, where sev eral hidden states may correspond to the same POStag.We explain this by observing that the distribution of hidden states to words proposed by the EM estimated HMMs is relatively uniform, while the empirical distribution of POS tags is heavily skewed towards a few high-frequency tags.Based on this,we propose a Bayesian prior that biases the sys tem toward more skewed distributions and show that this raises the 1-to-1 accuracy significantly.Finally, we show that a similar increase in accuracy can be achieved by reducing the number of hidden states in the models estimated by EM.There is certainly much useful information that bitag HMMs models cannot capture.Toutanova etal.(2003) describe a wide variety of morphological and distributional features useful for POS tagging, and Clark (2003) proposes ways of incorporat ing some of these in an unsupervised tagging model.However, bitag models are rich enough to capture at least some distributional information (i.e., the tag 296for a word depends on the tags assigned to its neighbours).Moreover, more complex models add addi tional complicating factors that interact in ways stillpoorly understood; for example, smoothing is gen erally regarded as essential for higher-order HMMs,yet it is not clear how to integrate smoothing into un supervised estimation procedures (Goodman, 2001; Wang and Schuurmans, 2005).Most previous work exploiting unsupervised training data for inferring POS tagging models has focused on semi-supervised methods in the in which the learner is provided with a lexicon specifying the possible tags for each word (Merialdo, 1994; Smith and Eisner, 2005; Goldwater and Griffiths, 2007) or a small number of ?prototypes?for each POS(Haghighi and Klein, 2006).In the context of semi supervised learning using a tag lexicon, Wang and Schuurmans (2005) observe discrepencies between the empirical and estimated tag frequencies similar to those observed here, and show that constraining the estimation procedure to preserve the empiricalfrequencies improves tagging accuracy.(This ap proach cannot be used in an unsupervised setting since the empirical tag distribution is not available).However, as Banko and Moore (2004) point out, the accuracy achieved by these unsupervised methodsdepends strongly on the precise nature of the su pervised training data (in their case, the ambiguity of the tag lexicon available to the system), which makes it more difficult to understand the behaviour of such systems.All of the experiments described below have the same basic structure: an estimator is used to infera bitag HMM from the unsupervised training cor pus (the words of Penn Treebank (PTB) Wall Street Journal corpus (Marcus et al, 1993)), and then the resulting model is used to label each word of that corpus with one of the HMM?s hidden states.This section describes how we evaluate how well thesesequences of hidden states correspond to the gold standard POS tags for the training corpus (here, the PTB POS tags).The chief difficulty is determining the correspondence between the hidden states and the gold-standard POS tags.Perhaps the most straightforward method of es tablishing this correspondence is to deterministically map each hidden state to the POS tag it co-occurs most frequently with, and return the proportion of the resulting POS tags that are the same as the POS tags of the gold-standard corpus.We call this themany-to-1 accuracy of the hidden state sequence be cause several hidden states may map to the same POS tag (and some POS tags may not be mapped to by any hidden states at all).As Clark (2003) points out, many-to-1 accuracy has several defects.If a system is permitted to posit an unbounded number of hidden states (which is notthe case here) then it can achieve a perfect many-to 1 accuracy by placing every word token into its own unique state.Cross-validation, i.e., identifying themany-to-1 mapping and evaluating on different subsets of the data, would answer many of these objections.Haghighi and Klein (2006) propose constrain ing the mapping from hidden states to POS tags so that at most one hidden state maps to any POS tag.This mapping is found by greedily assigning hidden states to POS tags until either the hidden states or POS tags are exhausted (note that if the number ofhidden states and POS tags differ, some will be unas signed).We call the accuracy of the POS sequence obtained using this map its 1-to-1 accuracy.Finally, several authors have proposed using information-theoretic measures of the divergence between the hidden state and POS tag sequences.Goldwater and Griffiths (2007) propose using the Variation of Information (VI) metric described byMeila?(2003).We regard the assignments of hidden states and POS tags to the words of the cor pus as two different ways of clustering those words,and evaluate the conditional entropy of each clus tering conditioned on the other.The VI is the sum of these conditional entropies.Specifically, given a corpus labeled with hidden states and POS tags, if p?(y), p?(t) and p?(y, t) are the empirical probabilities of a hidden state y, a POS tag t, and the cooccurance of y and t respectively, then the mutual information I , entropies H and variation of information VI are defined as follows: H(Y ) = ? ?y p?(y) log p?(y) H(T ) = ? ?t p?(t) log p?(t) I(Y, T ) = ? y,t p?(y, t) log p?(y, t) p?(y)p?(t) H(Y |T ) = H(Y )?I(Y, T ) 297 H(T |Y ) = H(T )?I(Y, T ) VI (Y, T ) = H(Y |T ) +H(T |Y ) As Meila?(2003) shows, VI is a metric on the space of probability distributions whose value reflects the divergence between the two distributions, and only takes the value zero when the two distributions are identical.Expectation-Maximization There are several excellent textbook presentations of Hidden Markov Models and the Forward-Backward algorithm for Expectation-Maximization (Jelinek, 1997; Manning and Schu?tze, 1999; Bishop, 2006),so we do not cover them in detail here.Conceptu ally, a Hidden Markov Model generates a sequence of observations x = (x0, . . ., xn) (here, the wordsof the corpus) by first using a Markov model to gen erate a sequence of hidden states y = (y0, . . ., yn)(which will be mapped to POS tags during evalua tion as described above) and then generating each word xi conditioned on its corresponding state yi.We insert endmarkers at the beginning and ending of the corpus and between sentence boundaries, and constrain the estimator to associate endmarkers with a state that never appears with any other observationtype (this means each sentence can be processed in dependently by first-order HMMs; these endmarkers are ignored during evaluation).In more detail, the HMM is specified by multi nomials ?y and ?y for each hidden state y, where ?y specifies the distribution over states following y and ?y specifies the distribution over observations x given state y. yi | yi?1 = y ? Multi(?y) xi | yi = y ? Multi(?y) (1)We used the Forward-Backward algorithm to perform Expectation-Maximization, which is a procedure that iteratively re-estimates the model param eters (?, ?), converging on a local maximum of the likelihood.Specifically, if the parameter estimate attime ` is (?(`), ?(`)), then the re-estimated parame ters at time `+ 1 are: ?(`+1)y?|y = E[ny?,y]/E[ny] (2) ?(`+1)x|y = E[nx,y]/E[ny] 6.95E+06 7.00E+06 7.05E+06 7.10E+06 7.15E+06 0 250 500 750 1000 ? lo g lik el ih oo d Iteration Figure 1: Variation in negative log likelihood with increasing iterations for 10 EM runs from different random starting points.where nx,y is the number of times observation x oc curs with state y, ny?,y is the number of times state y?follows y and ny is the number of occurences of state y; all expectations are taken with respect to the model (?(`), ?(`)).We took care to implement this and the other al gorithms used in this paper efficiently, since optimal performance was often only achieved after several hundred iterations.It is well-known that EM oftentakes a large number of iterations to converge in likelihood, and we found this here too, as shown in Fig ure 1.As that figure makes clear, likelihood is still increasing after several hundred iterations.Perhaps more surprisingly, we often found dramatic changes in accuracy in the order of 5% occur ing after several hundred iterations, so we ran 1,000 iterations of EM in all of the experiments describedhere; each run took approximately 2.5 days compu tation on a 3.6GHz Pentium 4.It?s well-known thataccuracy often decreases after the first few EM it erations (which we also observed); however in our experiments we found that performance improves again after 100 iterations and continues improving roughly monotonically.Figure 2 shows how 1-to-1 accuracy varies with iteration during 10 runs from different random starting points.Note that 1-to-1 accuracy at termination ranges from 0.38 to 0.45; a spread of 0.07.We obtained a dramatic speedup by working directly with probabilities and rescaling after each ob servation to avoid underflow, rather than workingwith log probabilities (thanks to Yoshimasa Tsu 298 0.35 0.37 0.39 0.41 0.43 0.45 0.47 0 250 500 750 10001 to -1 a ccura cy IterationFigure 2: Variation in 1-to-1 accuracy with increas ing iterations for 10 EM runs from different random starting points.ruoka for pointing this out).Since we evaluatedthe accuracy of the estimated tags after each iteration, it was important that decoding be done effi ciently as well.While most researchers use Viterbidecoding to find the most likely state sequence, maximum marginal decoding (which labels the observa tion xi with the state yi that maximizes the marginal probability P(yi|x, ?, ?)) is faster because it re-uses the forward and backward tables already constructed by the Forward-Backward algorithm.Moreover, in separate experiments we found that the maximum marginal state sequence almost always scored higherthan the Viterbi state sequence in all of our evalua tions, and at modest numbers of iterations (up to 50) often scored more than 5% better.We also noticed a wide variance in the perfor mance of models due to random initialization (both ? and ? are initially jittered to break symmetry); thiswide variance was observed with all of the estima tors investigated in this paper.This means we cannot compare estimators on the basis of single runs, so we ran each estimator 10 times from different random starting points and report both mean and standard deviation for all scores.Finally, we also experimented with annealing, in which the parameters ? and ? are raised to the power 1/T , where T is a ?temperature?parameter that isslowly lowered toward 1 at each iteration accord ing to some ?annealing schedule?.We experimented with a variety of starting temperatures and annealing schedules (e.g., linear, exponential, etc), but wereunable to find any that produced models whose like 0E+0 1E+5 2E+5 Fre quen cy Tag / hidden state (sorted by frequency) PT B V B EM EM 25 Figure 3: The average number of words labeled with each hidden state or tag for the EM, VB (with ?x = ?y = 0.1) and EM-25 estimators (EM-25 is the EM estimator with 25 hidden states).lihoods were significantly higher (i.e., the models fit better) than those found without annealing.The evaluation of the models produced by the EM and other estimators is presented in Table 1.It is difficult to compare these with previous work, but Haghighi and Klein (2006) report that in a completely unsupervised setting, their MRF model, which uses a large set of additional features and amore complex estimation procedure, achieves an average 1-to-1 accuracy of 41.3%.Because they provide no information about the variance in this accuracy it is difficult to tell whether there is a signifi cant difference between their estimator and the EM estimator, but it is clear that when EM is run longenough, the performance of even very simple models like the bitag HMM is better than generally rec ognized.As Table 1 makes clear, the EM estimator pro duces models that are extremely competitive in many-to-1 accuracy and Variation of Information, but are significantly worse in 1-to-1 accuracy.Wecan understand these results by comparing the dis tribution of words to hidden states to the distribution of words to POS tags in the gold-standard evaluation corpus.As Figure 3 shows, the distribution of words to POS tags is highly skewed, with just 6 POS tags, NN, IN, NNP, DT, JJ and NNS, accounting for over 55% of the tokens in the corpus.By contrast, the EM distribution is much flatter.This also explains why the many-to-1 accuracy is so much better than the one-to-one accuracy; presumably several hidden 299 Estimator 1-to-1 Many-to-1 VI H(T |Y ) H(Y |T ) EM (50) 0.40 (0.02) 0.62 (0.01) 4.46 (0.08) 1.75 (0.04) 2.71 (0.06) VB(0.1, 0.1) (50) 0.47 (0.02) 0.50 (0.02) 4.28 (0.09) 2.39 (0.07) 1.89 (0.06) VB(0.1, 10?4) (50) 0.46 (0.03) 0.50 (0.02) 4.28 (0.11) 2.39 (0.08) 1.90 (0.07) VB(10?4, 0.1) (50) 0.42 (0.02) 0.60 (0.01) 4.63 (0.07) 1.86 (0.03) 2.77 (0.05) VB(10?4, 10?4) (50) 0.42 (0.02) 0.60 (0.01) 4.62 (0.07) 1.85 (0.03) 2.76 (0.06) GS(0.1, 0.1) (50) 0.37 (0.02) 0.51 (0.01) 5.45 (0.07) 2.35 (0.09) 3.20 (0.03) GS(0.1, 10?4) (50) 0.38 (0.01) 0.51 (0.01) 5.47 (0.04) 2.26 (0.03) 3.22 (0.01) GS(10?4, 0.1) (50) 0.36 (0.02) 0.49 (0.01) 5.73 (0.05) 2.41 (0.04) 3.31 (0.03) GS(10?4, 10?4) (50) 0.37 (0.02) 0.49 (0.01) 5.74 (0.03) 2.42 (0.02) 3.32 (0.02) EM (40) 0.42 (0.03) 0.60 (0.02) 4.37 (0.14) 1.84 (0.07) 2.55 (0.08) EM (25) 0.46 (0.03) 0.56 (0.02) 4.23 (0.17) 2.05 (0.09) 2.19 (0.08) EM (10) 0.41 (0.01) 0.43 (0.01) 4.32 (0.04) 2.74 (0.03) 1.58 (0.05)Table 1: Evaluation of models produced by the various estimators.The values of the Dirichlet prior param eters for ?x and ?y appear in the estimator name for the VB and GS estimators, and the number of hidden states is given in parentheses.Reported values are means over all runs, followed by standard deviations.10 runs were performed for each of the EM and VB estimators, while 5 runs were performed for the GSestimators.Each EM and VB run consisted of 1,000 iterations, while each GS run consisted of 50,000 it erations.For the estimators with 10 runs, a 3-standard error 95% confidence interval is approximately the same as the standard deviation.states are being mapped onto a single POS tag.This is also consistent with the fact that the cross-entropy H(T |Y ) of tags given hidden states is relatively low(i.e., given a hidden state, the tag is relatively predictable), while the cross-entropy H(Y |T ) is rela tively high.and Variational Bayes A Bayesian estimator combines a likelihood termP(x|?, ?) and a prior P(?, ?) to estimate the poste rior probability of a model or hidden state sequence.We can use a Bayesian prior to bias our estimatortowards models that generate more skewed distributions.Because HMMs (and PCFGs) are prod ucts of multinomials, Dirichlet distributions are a particularly natural choice for the priors since they are conjugate to multinomials, which simplifies both the mathematical and computational aspects of theproblem.The precise form of the model we investi gated is: ?y | ?y ? Dir(?y) ?y | ?x ? Dir(?x) yi | yi?1 = y ? Multi(?y) xi | yi = y ? Multi(?y)Informally, ?y controls the sparsity of the state-to state transition probabilities while ?x controls thesparsity of the state-to-observation emission proba bilities.As ?x approaches zero the prior strongly prefers models in which each hidden state emitsas few words as possible.This captures the intu ition that most word types only belong to one POS,since the minimum number of non-zero state-toobservation transitions occurs when each observa tion type is emitted from only one state.Similarly, as ?y approaches zero the state-to-state transitions become sparser.There are two main techniques for Bayesian esti mation of such models: Markov Chain Monte Carlo(MCMC) and Variational Bayes (VB).MCMC en compasses a broad range of sampling techniques, including component-wise Gibbs sampling, which is the MCMC technique we used here (Robert and Casella, 2004; Bishop, 2006).In general, MCMCtechniques do not produce a single model that char acterizes the posterior, but instead produce a stream of samples from the posterior.The application of MCMC techniques, including Gibbs sampling, to HMM inference problems is relatively well-known: see Besag (2004) for a tutorial introduction and Goldwater and Griffiths (2007) for an applicationof Gibbs sampling to HMM inference for semi 300 supervised and unsupervised POS tagging.The Gibbs sampler produces state sequences y sampled from the posterior distribution: P(y|x, ?) ?P(x,y|?, ?)P(?|?y)P(?|?x) d?d?Because Dirichlet priors are conjugate to multinomials, it is possible to integrate out the model parameters ? and ? to yield the conditional distribu tion for yi shown in Figure 4.For each observation xi in turn, we resample its state yi conditioned on the states y?i of the other observations; eventually the distribution of state sequences converges to the desired posterior.Each iteration of the Gibbs sampler is much faster than the Forward-Backward algorithm (both take time linear in the length of the string, but for an HMM with s hidden states, each iteration of the Gibbs sampler takes O(s) time while each iteration of the Forward-Backward algorithm takes O(s2) time), so we ran 50,000 iterations of all samplers (which takes roughly the same elapsed time as 1,000 Forward-Backward iterations).As can be seen from Table 1, the posterior state sequences we obtained are not particularly good.Further, when we examined how the posterior like lihoods varied with increasing iterations of Gibbs sampling, it became apparent that the likelihood was still increasing after 50,000 iterations.Moreover,when comparing posterior likelihoods from different runs with the same prior parameters but differ ent random number seeds, none of the likelihoods crossed, which one would expect if the samplers had converged and were mixing well (Robert and Casella, 2004).Just as with EM, we experimented with a variety of annealing regimes, but were unable to find any which significantly improved accuracy or posterior likelihood.We also experimented with evaluating state se quences found using maximum posterior decoding(i.e., model parameters are estimated from the posterior sample, and used to perform maximum posterior decoding) rather than the samples from the pos terior produced by the Gibbs sampler.We found that the maximum posterior decoding sequences usually scored higher than the posterior samples, but the scores converged after the first thousand iterations.Since the posterior samples are produced as a byproduct of Gibbs sampling while maximum poste rior decoding requires an additional time consuming step that does not have much impact on scores, we used the posterior samples to produce the results in Table 1.In contrast to MCMC, Variational Bayesian in ference attempts to find the function Q(y, ?, ?) thatminimizes an upper bound of the negative log likeli hood (Jordan et al, 1999): ? log P(x) = ? log ? Q(y, ?, ?) P(x,y, ?, ?) Q(y, ?, ?) dy d?d? ? ?Q(y, ?, ?) log P(x,y, ?, ?) Q(y, ?, ?) dy d?d?(3) The upper bound in (3) is called the Variational Free Energy.We make a ?mean-field?assumption thatthe posterior can be well approximated by a factor ized modelQ in which the state sequence y does not covary with the model parameters ?, ?(this will be true if, for example, there is sufficient data that the posterior distribution has a peaked mode): P(x,y, ?, ?) ?Q(y, ?, ?) = Q1(y)Q2(?, ?) The calculus of variations is used to minimize theKL divergence between the desired posterior distri bution and the factorized approximation.It turnsout that if the likelihood and conjugate prior be long to exponential families then the optimalQ1 andQ2 do too, and there is an EM-like iterative pro cedure that finds locally-optimal model parameters (Bishop, 2006).This procedure is especially attractive for HMMinference, since it involves only a minor modification to the M-step of the Forward-Backward algo rithm.MacKay (1997) and Beal (2003) describe Variational Bayesian (VB) inference for HMMs in detail, and Kurihara and Sato (2006) describe VBfor PCFGs (which only involves a minor modifica tion to the M-step of the Inside-Outside algorithm).Specifically, the E-step for VB inference for HMMs is the same as in EM, while theM-step is as follows: ??(`+1)y?|y = f(E[ny?,y] + ?y)/f(E[ny] + s?y) (4) ??(`+1)x|y = f(E[nx,y] + ?x)/f(E[ny] +m?x) f(v) = exp(?(v)) ?(v) = (v > 7) ? g(v ? 12) : (?(v + 1)?1)/v g(x) ? log(x) + 0.04167x?2 + 0.00729x?4 +0.00384x?6 ? 0.00413x?8 . . .(5) 301 P(yi|x,y?i, ?) ?( nxi,yi + ?x nyi +m?x ) ( nyi,yi?1 + ?y nyi?1 + s?y ) ( nyi+1,yi + I(yi?1 = yi = yi+1) + ?y nyi + I(yi?1 = yi) ) Figure 4: The conditional distribution for state yi used in the Gibbs sampler, which conditions on the states y?i for all observations except xi.Here m is the number of possible observations (i.e., the size of the vocabulary), s is the number of hidden states and I(?)is the indicator function (i.e., equal to one if its argument is true and zero otherwise), nx,y is the number of times observation x occurs with state y, ny?,y is the number of times state y?follows y, and ny is the number of times state y occurs; these counts are from (x?i,y?i), i.e., excluding xi and yi.0 1 2 0 1 2 Figure 5: The scaling function y = f(x) = exp?(x) (curved line), which is bounded above by the line y = x and below by the line y = x?0.5.where ? is the digamma function (the derivative ofthe log gamma function; (5) gives an asymptotic ap proximation), and the remaining quantities are just as in the EM updates (2), i.e., nx,y is the number of times observation x occurs with state y, ny?,y is the number of times state y?follows y, ny is the number of occurences of state y, s is the number of hiddenstates and m is the number of observations; all ex pectations are taken with respect to the variational parameters (??(`), ??(`)).A comparison between (4) and (2) reveals two dif ferences between the EM and VB updates.First, the Dirichlet prior parameters ? are added to the expected counts.Second, these posterior counts(which are in fact parameters of the Dirichlet pos terior Q2) are passed through the function f(v) = exp?(v), which is plotted in Figure 5.When v  0, f(v) ? v ? 0.5, so roughly speaking, VB for multinomials involves adding ??0.5 to the expected counts when they are much larger than zero, where ? is the Dirichlet prior parameter.Thus VB canbe viewed as a more principled version of the well known ad hoc technique for approximating Bayesian estimation with EM that involves adding ??1 to the expected counts.However, in the ad hoc approach the expected count plus ??1 may be less than zero,resulting in a value of zero for the corresponding parameter (Johnson et al, 2007; Goldwater and Grif fiths, 2007).VB avoids this problem because f(v) is always positive when v  0, even when v is small.Note that because the counts are passed through f , the updated values for ??and ??in (4) are in general not normalized; this is because the variational free energy is only an upper bound on the negative log likelihood (Beal, 2003).We found that in general VB performed much bet ter than GS.Computationally it is very similar to EM, and each iteration takes essentially the same time as an EM iteration.Again, we experimented with annealing in the hope of speeding convergence,but could not find an annealing schedule that significantly lowered the variational free energy (the quan tity that VB optimizes).While we had hoped that theBayesian prior would bias VB toward a common solution, we found the same sensitivity to initial condi tions as we found with EM, so just as for EM, we ran the estimator for 1,000 iterations with 10 different random initializations for each combination of prior parameters.Table 1 presents the results of VB runs with several different values for the Dirichlet priorparameters.Interestingly, we obtained our best per formance on 1-to-1 accuracy when the Dirchlet prior?x = 0.1, a relatively large number, but best per formance on many-to-1 accuracy was achieved with a much lower value for the Dirichlet prior, namely ?x = 10?4.The Dirichlet prior ?y that controls 302sparsity of the state-to-state transitions had little ef fect on the results.We did not have computational resources to fully explore other values for the prior (a set of 10 runs for one set of parameter values takes 25 computer days).As Figure 3 shows, VB can produce distributions of hidden states that are peaked in the same way that POS tags are.In fact, with the priors used here, VB produces state sequences in which only a subset ofthe possible HMM states are in fact assigned to ob servations.This shows that rather than fixing the number of hidden states in advance, the Bayesian prior can determine the number of states; this idea is more fully developed in the infinite HMM of Beal et al.(2002) and Teh et al (2006).EM already performs well in terms of the many-to-1 accuracy, but we wondered if there might be some way to improve its 1-to-1 accuracy and VI score.In section 3 we suggested that one reason for its poorperformance in these evaluations is that the distri butions of hidden states it finds tend to be fairly flat, compared to the empirical distribution of POS tags.As section 4 showed, a suitable Bayesian priorcan bias the estimator towards more peaked distribu tions, but we wondered if there might be a simpler way of achieving the same result.We experimented with dramatic reductions in the number of hidden states in the HMMs estimated by EM.This should force the hidden states to bemore densely populated and improve 1-to-1 accu racy, even though this means that there will be nohidden states that can possibly map onto the less fre quent POS tags (i.e., we will get these words wrong).In effect, we abandon the low-frequency POS tags in the hope of improving the 1-to-1 accuracy of the high-frequency tags.As Table 1 shows, this markedly improves both the 1-to-1 accuracy and the VI score.A 25-state HMM estimated by EM performs effectively as wellas the best VB model in terms of both 1-to-1 accu racy and VI score, and runs 4 times faster because it has only half the number of hidden states.6 Conclusion and future work.This paper studied why EM seems to do so badly in HMM estimation for unsupervised POS tagging.In fact, we found that it doesn?t do so badly at all: thebitag HMM estimated by EM achieves a mean 1-to 1 tagging accuracy of 40%, which is approximately the same as the 41.3% reported by (Haghighi and Klein, 2006) for their sophisticated MRF model.Then we noted the distribution of words to hidden states found by EM is relatively uniform, comparedto the distribution of words to POS tags in the eval uation corpus.This provides an explanation of why the many-to-1 accuracy of EM is so high while the 1-to-1 accuracy and VI of EM is comparatively low.We showed that either by using a suitable Bayesian prior or by simply reducing the number of hidden states it is possible to significantly improve both the 1-to-1 accuracy and the VI score, achieving a 1-to-1 tagging accuracy of 46%.We also showed that EM and other estimators take much longer to converge than usually thought, and often require several hundred iterations to achieve optimal performance.We also found that there is considerable variance in the performance of all of these estimators, so in general multiple runs fromdifferent random starting points are necessary in or der to evaluate an estimator?s performance.Finally, there may be more sophisticated ways of improving the 1-to-1 accuracy and VI score thanthe relatively crude methods used here that primarily reduce the number of available states.For example, we might obtain better performance by us ing EM to infer an HMM with a large number of states, and then using some kind of distributionalclustering to group similar HMM states; these clusters, rather than the underlying states, would be in terpreted as the POS tag labels.Also, the Bayesian framework permits a wide variety of different priors besides Dirichlet priors explored here.For example, it should be possible to encode linguistic knowledge such markedness preferences in a prior, and there are other linguistically uninformative priors, such the ?entropic priors?of Brand (1999), that may be worth exploring.AcknowledgementsI would like to thank Microsoft Research for providing an excellent environment in which to con duct this work, and my friends and colleagues at Microsoft Research, especially Bob Moore, ChrisQuirk and Kristina Toutanova, for their helpful com ments on this paper.303
Clustering techniques have been used successfully for many natural language processing tasks, such as document clustering (Willett, 1988; Zamir and Etzioni, 1998; Cutting et al, 1992; Vempala and Wang, 2005), word sense disambiguation (Shin and Choi, 2004), semantic role labeling (Baldewein et al., 2004), pitch accent type disambiguation (Levow, 2006).They are particularly appealing for tasks in which there is an abundance of language data available, but manual annotation of this data is very resource-intensive.Unsupervised clustering can eliminate the need for (full) manual annotation of the data into desired classes, but often at the cost of making evaluation of success more difficult.External evaluation measures for clustering can be applied when class labels for each data point in some evaluation set can be determined a priori.The clustering task is then to assign these data points toany number of clusters such that each cluster con tains all and only those data points that are membersof the same class Given the ground truth class la bels, it is trivial to determine whether this perfect clustering has been achieved.However, evaluating how far from perfect an incorrect clustering solution is a more difficult task (Oakes, 1998) and proposed approaches often lack rigor (Meila, 2007).In this paper, we describe a new entropy-based external cluster evaluation measure, V-MEASURE1 , designed to address the problem of quantifying such imperfection.Like all external measures, V-measurecompares a target clustering ? e.g., a manually an notated representative subset of the available data ?against an automatically generated clustering to de termine now similar the two are.We introduce twocomplementary concepts, completeness and homo geneity, to capture desirable properties in clustering tasks.In Section 2, we describe V-measure and how itis calculated in terms of homogeneity and complete ness.We describe several popular external cluster evaluation measures and draw some comparisons to V-measure in Section 3.In Section 4, we discusshow some desirable properties for clustering are satisfied by V-measure vs. other measures.In Sec tion 5, we present two applications of V-measure, ondocument clustering and on pitch accent type clus tering.V-measure is an entropy-based measure which explicitly measures how successfully the criteria of homogeneity and completeness have been satisfied.Vmeasure is computed as the harmonic mean of dis tinct homogeneity and completeness scores, just as1The ?V? stands for ?validity?, a common term used to de scribe the goodness of a clustering solution.410 precision and recall are commonly combined into F-measure (Van Rijsbergen, 1979).As F-measure scores can be weighted, V-measure can be weightedto favor the contributions of homogeneity or com pleteness.For the purposes of the following discussion, as sume a data set comprising N data points, and two partitions of these: a set of classes, C = {ci|i = 1, . . ., n} and a set of clusters, K = {ki|1, . . .,m}.Let A be the contingency table produced by the clus tering algorithm representing the clustering solution, such that A = {aij} where aij is the number of data points that are members of class ci and elements of cluster kj .To discuss cluster evaluation measures we introduce two criteria for a clustering solution: homogeneity and completeness.A clustering result sat isfies homogeneity if all of its clusters contain only data points which are members of a single class.A clustering result satisfies completeness if all the data points that are members of a given class are elementsof the same cluster.The homogenity and completeness of a clustering solution run roughly in opposition: Increasing the homogeneity of a clustering so lution often results in decreasing its completeness.Consider, two degenerate clustering solutions.In one, assigning every datapoint into a single cluster, guarantees perfect completeness ? all of the datapoints that are members of the same class are triv ially elements of the same cluster.However, this cluster is as unhomogeneous as possible, since allclasses are included in this single cluster.In another solution, assigning each data point to a dis tinct cluster guarantees perfect homogeneity ? each cluster trivially contains only members of a singleclass.However, in terms of completeness, this so lution scores very poorly, unless indeed each classcontains only a single member.We define the dis tance from a perfect clustering is measured as theweighted harmonic mean of measures of homogene ity and completeness.Homogeneity: In order to satisfy our homogeneity criteria, a clustering must assign only those datapoints that are members of a single class to a single cluster.That is, the class distribution within each cluster should beskewed to a single class, that is, zero entropy.We de termine how close a given clustering is to this ideal by examining the conditional entropy of the class distribution given the proposed clustering.In the perfectly homogeneous case, this value, H(C|K), is 0.However, in an imperfect situation, the size of this value, in bits, is dependent on the size of thedataset and the distribution of class sizes.There fore, instead of taking the raw conditional entropy, we normalize this value by the maximum reduction in entropy the clustering information could provide, specifically, H(C).Note that H(C|K) is maximal (and equals H(C)) when the clustering provides no new information ? the class distribution within each cluster is equal to the overall class distribiution.H(C|K) is 0 when each cluster contains only members of a single class,a perfectly homogenous clustering.In the degen erate case where H(C) = 0, when there is only a single class, we define homogeneity to be 1.For a perfectly homogenous solution, this normalization, H(C|K) H(C) , equals 0.Thus, to adhere to the conventionof 1 being desirable and 0 undesirable, we define ho mogeneity as: h = { 1 if H(C,K) = 0 1?H(C|K)H(C) else (1) where H(C|K) = ? |K| ? k=1 |C| ? c=1 ack N log ack ?|C| c=1 ack H(C) = ? |C| ? c=1 ?|K| k=1 ack n log ?|K| k=1 ack n Completeness: Completeness is symmetrical to homogeneity.Inorder to satisfy the completeness criteria, a clustering must assign all of those datapoints that are members of a single class to a single cluster.To eval uate completeness, we examine the distribution of cluster assignments within each class.In a perfectlycomplete clustering solution, each of these distribu tions will be completely skewed to a single cluster.We can evaluate this degree of skew by calculat ing the conditional entropy of the proposed clusterdistribution given the class of the component dat apoints, H(K|C).In the perfectly complete case, H(K|C) = 0.However, in the worst case scenario, 411each class is represented by every cluster with a dis tribution equal to the distribution of cluster sizes, H(K|C) is maximal and equals H(K).Finally, in the degenerate case where H(K) = 0, when there is a single cluster, we define completeness to be 1.Therefore, symmetric to the calculation above, we define completeness as: c = { 1 if H(K,C) = 0 1 ? H(K|C)H(K) else (2) where H(K|C) = ? |C| ? c=1 |K| ? k=1 ack N log ack ?|K| k=1 ack H(K) = ? |K| ? k=1 ?|C| c=1 ack n log ?|C| c=1 ack n Based upon these calculations of homogeneity and completeness, we then calculate a clustering solution?s V-measure by computing the weighted harmonic mean of homogeneity and completeness,V? = (1+?)?h?c(??h)+c . Similarly to the familiar F measure, if ? is greater than 1 completeness is weighted more strongly in the calculation, if ? is less than 1, homogeneity is weighted more strongly.Notice that the computations of homogeneity,completeness and V-measure are completely inde pendent of the number of classes, the number ofclusters, the size of the data set and the clustering al gorithm used.Thus these measures can be applied toand compared across any clustering solution, regard less of the number of data points (n-invariance), thenumber of classes or the number of clusters.More over, by calculating homogeneity and completenessseparately, a more precise evaluation of the perfor mance of the clustering can be obtained.Clustering algorithms divide an input data set into a number of partitions, or clusters.For tasks wheresome target partition can be defined for testing purposes, we define a ?clustering solution?as a map ping from each data point to its cluster assignments in both the target and hypothesized clustering.In the context of this discussion, we will refer to the target partitions, or clusters, as CLASSES, referring only to hypothesized clusters as CLUSTERS.Two commonly used external measures for as sessing clustering success are Purity and Entropy (Zhao and Karypis, 2001), defined as, Purity = ?kr=1 1n maxi(nir) Entropy = ?kr=1 nrn (?1log q ?q i=1 nir nr log nir nr ) where q is the number of classes, k the number of clusters, nr is the size of cluster r, and nir is the number of data points in class i clustered in cluster r. Both these approaches represent plausable ways to evaluate the homogeneity of a clustering solution.However, our completeness criterion is not measured at all.That is, they do not address the question of whether all members of a given class are in cluded in a single cluster.Therefore the Purity and Entropy measures are likely to improve (increased Purity, decreased Entropy) monotonically withthe number of clusters in the result, up to a degen erate maximum where there are as many clusters as data points.However, clustering solutions rated high by either measure may still be far from ideal.Another frequently used external clustering evaluation measure is commonly refered to as ?cluster ing accuracy?.The calculation of this accuracy isinspired by the information retrieval metric of F Measure (Van Rijsbergen, 1979).The formula for this clustering F-measure as described in (Fung et al., 2003) is shown in Figure 3.Let N be the number of data points, C the set of classes, K the set of clusters and nij be the number of members of class ci ? C that are elements of cluster kj ? K. F (C, K) = X ci?C |ci| N maxkj?K {F (ci, kj)} (3) F (ci, kj) = 2 ? R(ci, kj) ? P (ci, kj) R(ci, kj) + P (ci, kj) R(ci, kj) = nij |ci| P (ci, kj) = nij |kj | Figure 1: Calculation of clustering F-measure This measure has a significant advantage over Purity and Entropy, in that it does measure boththe homogeneity and the completeness of a cluster ing solution.Recall is calculated as the portion of items from class i that are present in cluster j, thus measuring how complete cluster j is with respect toclass i. Similarly, Precision is calculated as the por 412 Solution A Solution B F-Measure=0.5 F-Measure=0.5 V-Measure=0.14 V-Measure=0.39 Solution C Solution D F-Measure=0.6 F-Measure=0.6 V-Measure=0.30 V-Measure=0.41 Figure 2: Examples of the Problem of Matchingtion of cluster j that is a member of class i, thus mea suring how homogenous cluster j is with respect to class i.Like some other external cluster evaluation tech niques (misclassification index (MI) (Zeng et al, 2002), H (Meila and Heckerman, 2001), L (Larsenand Aone, 1999), D (van Dongen, 2000), micro averaged precision and recall (Dhillon et al, 2003)), F-measure relies on a post-processing step in which each cluster is assigned to a class.These techniques share certain problems.First, they calculate the goodness not only of the given clustering solution, but also of the cluster-class matching.Therefore, in order for the goodness of two clustering solutions to be compared using one these measures, an identicalpost-processing algorithm must be used.This problem can be trivially addressed by fixing the classcluster matching function and including it in the def inition of the measure as in H . However, a secondand more critical problem is the ?problem of matching?(Meila, 2007).In calculating the similarity between a hypothesized clustering and a ?true?cluster ing, these measures only consider the contributions from those clusters that are matched to a target class.This is a major problem, as two significantly differ ent clusterings can result in identical scores.In figure 2, we present some illustrative examples of the problem of matching.For the purposes of thisdiscussion we will be using F-Measure as the mea sure to describe the problem of matching, however, these problems affect any measure which requires a mapping from clusters to classes for evaluation.In the figures, the shaded regions represent CLUS TERS, the shapes represent CLASSES.In a perfect clustering, each shaded region would contain all and only the same shapes.The problem of matchingcan manifest itself either by not evaluating the en tire membership of a cluster, or by not evaluating every cluster.The former situation is presented in the figures A and B in figure 2.The F-Measure ofboth of these clustering solutions in 0.6.(The preci sion and recall for each class is 35 .) That is, for each class, the best or ?matched?cluster contains 3 of 5 elements of the class (Recall) and 3 of 5 elements of the cluster are members of the class (Precision).The make up of the clusters beyond the majority class is not evaluated by F-Measure.Solution B is a better clustering solution than solution A, in terms of both homogeneity (crudely, ?each cluster contains fewer2 classes?)and completeness (?each class is containedin fewer clusters?).Indeed, the V-Measure of so lution B (0.387) is greater than that of solution A (0.135).Solutions C and D represent a case in which not every cluster is considered in the evaluation of F-Measure.In this example, the F-Measure of both solutions is 0.5 (the harmonic mean of 35 and 37 ).The small ?unmatched?clusters are not measured at allin the calculation of F-Measure.Solution D is a bet ter clustering than solution C ? there are no incorrect clusterings of different classes in the small clusters.V-Measure reflects this, solution C has a V-measure of 0.30 while the V-measure of solution D is 0.41.A second class of clustering evaluation techniquesis based on a combinatorial approach which examines the number of pairs of data points that are clustered similarly in the target and hypothesized clus tering.That is, each pair of points can either be 1)clustered together in both clusterings (N11), 2) clustered separately in both clusterings (N00), 3) clustered together in the hypothesized but not the tar get clustering (N01) or 4) clustered together in the target but not in the hypothesized clustering (N10).Based on these 4 values, a number of measures have been proposed, including Rand Index (Rand, 1971), 2Homogeneity is not measured by V-measure as a count of the number of classes contained by a cluster but ?fewer?is an acceptable way to conceptualize this criterion for the purposes of these examples.413 Adjusted Rand Index (Hubert and Arabie, 1985), ?statistic (Hubert and Schultz, 1976), Jaccard (Mil ligan et al, 1983), Fowlkes-Mallows (Fowlkes andMallows, 1983) and Mirkin (Mirkin, 1996).We il lustrate this class of measures with the calculation of Rand Index.Rand(C,K) = N11+N00n(n?1)/2 Rand Index can be interpreted as the probability that a pair of points is clustered similarly (together or separately) in C and K .Meila (2007) describes a number of poten tial problems of this class of measures posed by (Fowlkes and Mallows, 1983) and (Wallace, 1983).The most basic is that these measures tend not to vary over the interval of [0, 1].Transformations likethose applied by the adjusted Rand Index and a mi nor adjustment to the Mirkin measure (see Section4) can address this problem.However, pair matching measures also suffer from distributional problems.The baseline for Fowlkes-Mallows varies sig nificantly between 0.6 and 0 when the ratio of datapoints to clusters is greater than 3 ? thus including nearly all real-world clustering problems.Similarly, the Adjusted Rand Index, as demonstrated using Monte Carlo simulations in (Fowlkes and Mal lows, 1983), varies from 0.5 to 0.95.This variance in the measure?s baseline prompts Meila to ask if the assumption of linearity following normalization can be maintained.If the behavior of the measure is so unstable before normalization can users reasonably expect stable behavior following normalization?A final class of cluster evaluation measures arebased on information theory.These measures analyze the distribution of class and cluster member ship in order to determine how successful a givenclustering solution is or how different two parti tions of a data set are.We have already examined one member of this class of measures, Entropy.From a coding theory perspective, Entropy is theweighted average of the code lengths of each cluster.Our V-measure is a member of this class of clustering measures.One significant advantage that in formation theoretic evaluation measures have is that they provide an elegant solution to the ?problem of matching?.By examining the relative sizes of the classes and clusters being evaluated, these measures all evaluate the entire membership of each cluster ? not just a ?matched?portion.Dom?s Q0 measure (Dom, 2001) uses conditional entropy, H(C|K) to calculate the goodness of a clustering solution.That is, given the hypothesized partition, what is the number of bits necessary to represent the true clustering?However, this term ? like the Purity andEntropy measures ? only evaluates the homogene ity of a solution.To measure the completeness of the hypothesized clustering, Dom includes a model cost term calculated using a coding theory argument.The overall clustering quality measure presented is the sum of the costs of representing the data (H(C|K)) and the model.The motivation for this approachis an appeal to parsimony: Given identical condi tional entropies, H(C|K), the clustering solution with the fewest clusters should be preferred.Dom also presents a normalized version of this term, Q2, which has a range of (0, 1] with greater scores being representing more preferred clusterings.Q0(C,K) = H(C|K)+ 1 n |K| ? k=1 log (h(k) + |C| ? 1 |C| ? 1 )where C is the target partition, K is the hypothe sized partition and h(k) is the size of cluster k. Q2(C,K) = 1 n ?|C| c=1 log (h(c)+|C|?1 |C|?1 ) Q0(C,K) We believe that V-measure provides two significantadvantages over Q0 that make it a more useful diag nostic tool.First, Q0 does not explicitly calculate the degree of completeness of the clustering solution.The cost term captures some of this information, since a partition with fewer clusters is likely to be more complete than a clustering solution with more clusters.However, Q0 does not explicitly address the interaction between the conditional entropy and the cost of representing the model.While this is an application of the minimum description length (MDL) principle (Rissanen, 1978; Rissanen, 1989), it does not provide an intuitive manner for assessingour two competing criteria of homogeneity and com pleteness.That is, at what point does an increase inconditional entropy (homogeneity) justify a reduc tion in the number of clusters (completeness).Another information-based clustering measure is variation of information (V I) (Meila, 2007), V I(C,K) = H(C|K)+H(K|C).V I is presented 414 as a distance measure for comparing partitions (or clusterings) of the same data.It therefore does notdistinguish between hypothesized and target cluster ings.V I has a number of useful properties.First, it satisfies the metric axioms.This quality allowsusers to intuitively understand how V I values combine and relate to one another.Secondly, it is ?con vexly additive?.That is to say, if a cluster is split, the distance from the new cluster to the original is the distance induced by the split times the size of the cluster.This property guarantees that all changes to the metric are ?local?: the impact of splitting ormerging clusters is limited to only those clusters in volved, and its size is relative to the size of these clusters.Third, VI is n-invariant: the number of data points in the cluster do not affect the value of the measure.V I depends on the relative sizes of the partitions of C and K , not on the number of points in these partitions.However, V I is bounded by themaximum number of clusters in C or K , k?.With out manual modification however, k?= n, where each cluster contains only a single data point.Thus, while technically n-invariant, the possible values of V I are heavily dependent on the number of datapoints being clustered.Thus, it is difficult to compare V I values across data sets and clustering algorithms without fixing k?, as V I will vary over differ ent ranges.It is a trivial modification to modify V I such that it varies over [0,1].Normalizing, V I by log n or 1/2 log k?guarantee this range.However, Meila (2007) raises two potential problems with thismodification.The normalization should not be applied if data sets of different sizes are to be com pared ? it negates the n-invariance of the measure.Additionally, if two authors apply the latter normal ization and do not use the same value for k?, their results will not be comparable.While V I has a number of very useful distance properties when analyzing a single data set across a number of settings, it has limited utility as a general purpose clustering evaluation metric for use across disparate clusterings of disparate data sets.Our homogeneity (h) and completeness (c) terms both range over [0,1] and are completely n-invariant andk?-invariant.Furthermore, measuring each as a ra tio of bit lengths has greater intuitive appeal than a more opportunistic normalization.V-measure has another advantage as a clusteringevaluation measure over V I and Q0.By evaluating homogeneity and completeness in a symmetrical, complementary manner, the calculation of V measure makes their relationship clearly observable.Separate analyses of homogeneity and completeness are not possible with any other cluster evalu ation measure.Moreover, by using the harmonic mean to combine homogeneity and completeness, V-measure is unique in that it can also prioritize one criterion over another, depending on the clustering task and goals.Dom (2001) describes a parametric technique for generating example clustering solutions.He then proceeds to define five ?desirable properties?that clustering accuracy measures should display, basedon the parameters used to generate the clustering solution.To compare V-measure more directly to alter native clustering measures, we evaluate V-measure and other measures against these and two additional desirable properties.The parameters used in generating a clustering so lution are as follows.|C| The number of classes ? |K| The number of clusters ? |Knoise| Number of ?noise?clusters; |Knoise| < |K| ? |Cnoise| Number of ?noise?classes; |Cnoise| < |C| ? ?Error probability; ? = ?1 + ?2 + ?3.? ?1 The error mass within ?useful?class-cluster pairs ? ?2 The error mass within noise clusters ? ?3 The error mass within noise classes The construction of a clustering solution begins with a matching of ?useful?clusters to ?useful?classes3.There are |Ku| = |K| ? |Knoise| ?useful?clusters and |Cu| = |C| ? |Cnoise| ?useful?classes.The claim is useful classes and clusters are matched to each other and matched pairs contain more data points than unmatched pairs.Probability mass of1 ? ?is evenly distributed across each match.Er ror mass of ?1 is evenly distributed across each pair 3The operation of this matching is omitted in the interest of space.Interested readers should see (Dom, 2001).415 of non-matching useful class/cluster pairs.Noise clusters are those that contain data points equally from each cluster.Error mass of ?2 is distributed across every ?noise?-cluster/ ?useful?-class pair.We extend the parameterization technique described in (Dom, 2001) in with |Cnoise| and ?3.Noise classes are those that contain data points equally from each cluster.Error mass of ?3 is distributed across every?useful?-cluster/?noise?-class pair.An example so lution, along with its generating parameters is given in Figure 3.C1 C2 C3 Cnoise1 K1 12 12 2 3 K2 2 2 12 3 Knoise1 4 4 4 0 Figure 3: Sample parametric clustering solution with n = 60, |K| = 3, |Knoise| = 1, |C| = 3, |Cnoise| = 1, ?1 = .1, ?2 = .2, ?3 = .1 The desirable properties proposed by Dom aregiven as P1-P5 in Table 1.We include two additional properties (P6,P7) relating the examined mea sure value to the number of ?noise?classes and ?3.P1 For |Ku| < |C| and ?|Ku| ?(|C| ? |Ku|), ?M ?|Ku| > 0 P2 For |Ku| ? |C|, ?M?|Ku| < 0 P3 ?M?|Knoise| < 0, if ?2 > 0 P4 ?M??1 ? 0, with equality only if |Ku| = 1 P5 ?M??2 ? 0, with equality only if |Knoise| = 0 P6 ?M?|Cnoise| < 0, if ?3 > 0 P7 ?M??3 ? 0, with equality only if |Cnoise| = 0 Table 1: Desirable Properties of a cluster evaluation measure MTo evaluate how different clustering measures satisfy each of these properties, we systematically var ied each parameter, keeping |C| = 5 fixed.|Ku|: 10 values: 2, 3,., 11 ? |Knoise|: 7 values: 0, 1,., 6 ? |Cnoise|: 7 values: 0, 1,., 6 ? ?1: 4 values: 0, 0.033, 0.066, 0.1 ? ?2: 4 values: 0, 0.066, 0.133, 0.2 ? ?3: 4 values: 0, 0.066, 0.133, 0.2 We evaluated the behavior of V-Measure, Rand, Mirkin, Fowlkes-Mallows, Gamma, Jaccard, VI,Q0, F-Measure against the desirable properties P1 P74.Based on the described systematic modificationof each parameter, only V-measure, VI and Q0 empirically satisfy all of P1-P7 in all experimental con ditions.Full results reporting how frequently each evaluated measure satisfied the properties based on these experiments can be found in table 2.All evaluated measures satisfy P4 and P7.However, Rand, Mirkin, Fowlkes-Mallows, Gamma, Jac card and F-Measure all fail to satisfy P3 and P6 inat least one experimental configuration.This indi cates that the number of ?noise?classes or clusterscan be increased without reducing any of these mea sures.This implies a computational obliviousness topotentially significant aspects of an evaluated clus tering solution.In this section, we present two clustering experiments.We describe a document clustering experiment and evaluate its results using V-measure, high lighting the interaction between homogeneity and completeness.Second, we present a pitch accent type clustering experiment.We present results fromboth of these experiments in order to show how V measure can be used to drawn comparisons across data sets.5.1 Document Clustering.Clustering techniques have been used widely to sort documents into topic clusters.We reproduce such an experiment here to demonstrate the usefulnessof V-measure.Using a subset of the TDT-4 cor pus (Strassel and Glenn, 2003) (1884 English newswire and broadcast news documents manually la beled with one of 12 topics), we ran clustering experiments using k-means clustering (McQueen, 1967) and evaluated the results using V-Measure,VI and Q0 ? those measures that satisfied the desirable properties defined in section 4.The top ics and relative distributions are as follows: Acts 4The inequalities in the desirable properties are inverted inthe evaluation of VI, Q0 and Mirkin as they are defined as dis tance, as opposed to similarity, measures.416 Property Rand Mirkin Fowlkes ? Jaccard F-measure Q0 VI V-Measure P1 0.18 0.22 1.0 1.0 1.0 1.0 1.0 1.0 1.0 P2 1.0 1.0 0.76 1.0 0.89 0.98 1.0 1.0 1.0 P3 0.0 0.0 0.30 0.19 0.21 0.0 1.0 1.0 1.0 P4 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 P5 0.50 0.57 1.0 1.0 1.0 1.0 1.0 1.0 1.0 P6 0.20 0.20 0.41 0.26 0.52 0.87 1.0 1.0 1.0 P7 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 Table 2: Rates of satisfaction of desirable propertiesof Violence/War (22.3%), Elections (14.4%), Diplomatic Meetings (12.9%), Accidents (8.75%), Natural Disasters (7.4%), Human Interest (6.7%), Scan dals (6.5%), Legal Cases (6.4%), Miscellaneous (5.3%), Sports (4.7), New Laws (3.2%), Science and Discovery (1.4%).We employed stemmed (Porter, 1980), tf*idf weighted term vectors extracted for each document as the clustering space for these experiments, which yielded a very high dimension space.To reduce this dimensionality, we performed a simple feature selection procedure including in the feature vector only those terms that represented the highest tf*idf value for at least one data point.This resulted in a feature vector containing 484 tf*idf values for each document.Results from k-means clustering are are shown in Figure 4.0 0.1 0.2 0.3 0.4 0.5 1 10 100 1000 3 3.5 4 4.5 5 5.5V m ea su re a nd Q 2 va lue s VI v al ue s number of clusters V-Measure VI Q2 Figure 4: Results of document clustering measured by V-Measure, VI and Q2 The first observation that can be drawn from these results is the degree to which VI is dependent on the number of clusters (k).This dependency severelylimits the usefulness of VI: it is inappropriate in selecting an appropriate parameter for k or for evaluating the distance between clustering solutions gen erated using different values of k. V-measure and Q2 demonstrate similar behavior in evaluating these experimental results.They both reach a maximal value with 35 clusters, however, Q2shows a greater descent as the number of clusters in creases.We will discuss this quality in greater detail in section 5.2.5.2 Pitch Accent Clustering.Pitch accent is how speakers of many languages make a word intonational prominent.In mostpitch accent languages, words can also be accented in different ways to convey different meanings (Hirschberg, 2002).In the ToBI labeling con ventions for Standard American English (Silvermanet al, 1992), for example, there are five different ac cent types (H*, L*, H+!H*, L+H*, L*+H).We extracted a number of acoustic features from accented words within the read portion of the Boston Directions Corpus (BDC) (Nakatani et al, 1995) andexamined how well clustering in these acoustic dimensions correlates to manually annotated pitch ac cent types.We obtained a very skewed distribution,with a majority of H* pitch accents.5 We there fore included only a randomly selected 10% sample of H* accents, providing a more even distribution of pitch accent types for clustering: H* (54.4%), L*(32.1%), L+H* (26.5%), L*+H (2.8%), H+!H* (2.1%).We extracted ten acoustic features from each ac cented word to serve as the clustering space for this experiment.Using Praat?s (Boersma, 2001) Get Pitch (ac)... function, we calculated the mean F0and ?F0, as well as z-score speaker normalized ver sions of the same.We included in the feature vector the relative location of the maximum pitch value inthe word as well as the distance between this max5Pitch accents containing a high tone may also be downstepped, or spoken in a compressed pitch range.Here we col lapsed all DOWNSTEPPED instances of each pitch accent with the corresponding non-downstepped instances.417 imum and the point of maximum intensity.Finally, we calculated the raw and speaker normalized slope from the start of the word to the maximum pitch, and from the maximum pitch to the end of the word.Using this feature vector, we performed k-meansclustering and evaluate how successfully these di mensions represent differences between pitch accenttypes.The resulting V-measure, VI and Q0 calcula tions are shown in Figure 5.0 0.05 0.1 0.15 0.2 1 10 100 1000 2 3 4 5 6 7 8V m ea su re a nd Q 2 va lue s VI v al ue s number of clusters VI V-measure Q2Figure 5: Results of pitch accent clustering mea sured by V-Measure, VI and Q0 In evaluating the results from these experiments,Q2 and V-measure reveal considerably different behaviors.Q2 shows a maximum at k = 10, and de scends at k increases.This is an artifact of the MDLprinciple.Q2 makes the claim that a clustering so lution based on fewer clusters is preferable to one using more clusters, and that the balance between the number of clusters and the conditional entropy, H(C|K), should be measured in terms of codinglength.With V-measure, we present a different argu ment.We contend that the a high value of k does notinherently reduce the goodness of a clustering solu tion.Using these results as an example, we find that at approximately 30 clusters an increase of clusters translates to an increase in V-Measure.This is due to an increased homogeneity (H(C|K)H(C) ) and a relatively stable completeness (H(K|C)H(K) ).That is, inclusion of more clusters leads to clusters with a more skewedwithin-cluster distribution and a equivalent distribu tion of cluster memberships within classes.This is intuitively preferable ? one criterion is improved, the other is not reduced ? despite requiring additionalclusters.This is an instance in which the MDL principle limits the usefulness of Q2.We again (see sec tion 5.1) observe the close dependency of VI and k.Moreover, in considering figures 5 and 4, simulta neously, we see considerably higher values achieved by the document clustering experiments.Given the na??ve approaches taken in these experiments, this is expected ? and even desired ? given the previous work on these tasks: document clustering has been notably more successfully applied than pitch accent clustering.These examples allow us to observe how transparently V-measure can be used to compare the behavior across distinct data sets.We have presented a new external cluster evaluation measure, V-measure, and compared it with existing clustering evaluation measures.V-measure is basedupon two criteria for clustering usefulness, homogeneity and completeness, which capture a clustering solution?s success in including all and only data points from a given class in a given cluster.We havealso demonstrated V-measure?s usefulness in com paring clustering success across different domainsby evaluating document and pitch accent cluster ing solutions.We believe that V-measure addressessome of the problems that affect other cluster measures.1) It evaluates a clustering solution indepen dent of the clustering algorithm, size of the data set, number of classes and number of clusters.2) It does not require its user to map each cluster to a class.Therefore, it only evaluates the quality of the clustering, not a post-hoc class-cluster mapping.3) It eval uates the clustering of every data point, avoiding the ?problem of matching?.4) By evaluating the criteria of both homogeneity and completeness, V-measure is more comprehensive than those that evaluate onlyone.5) Moreover, by evaluating these criteria separately and explicitly, V-measure can serve as an el egant diagnositic tool providing greater insight into clustering behavior.Acknowledgments The authors thank Kapil Thadani, Martin Jansche and Sasha Blair-Goldensohn and for their feedback.This work was funded in part by the DARPA GALE program under a subcontract to SRI International.418
Since its introduction by Och (2003), minimum error rate training (MERT) has been widely adopted for training statistical machine translation (MT) systems.However, MERT is limited in the number of feature weights that it can optimize reliably, with folk estimates of the limit ranging from 15 to 30 features.One recent example of this limitation is a series of experiments by Marton and Resnik (2008), in which they added syntactic features to Hiero (Chiang, 2005; Chiang, 2007), which ordinarily uses no linguistically motivated syntactic information.Each of their new features rewards or punishes a derivation depending on how similar or dissimilar it is to a syntactic parse of the input sentence.They found that in order to obtain the greatest improvement, these features had to be specialized for particular syntactic categories and weighted independently.Not being able to optimize them all at once using MERT, they resorted to running MERT many times in order to test different combinations of features.But it would have been preferable to use a training method that can optimize the features all at once.There has been much work on improving MERT’s performance (Duh and Kirchoff, 2008; Smith and Eisner, 2006; Cer et al., 2008), or on replacing MERT wholesale (Turian et al., 2007; Blunsom et al., 2008).This paper continues a line of research on online discriminative training (Tillmann and Zhang, 2006; Liang et al., 2006; Arun and Koehn, 2007), extending that of Watanabe et al. (2007), who use the Margin Infused Relaxed Algorithm (MIRA) due to Crammer et al.(2003; 2006).Our guiding principle is practicality: like Watanabe et al., we train on a small tuning set comparable in size to that used by MERT, but by parallel processing and exploiting more of the parse forest, we obtain results using MIRA that match or surpass MERT in terms of both translation quality and computational cost on a large-scale translation task.Taking this further, we test MIRA on two classes of features that make use of syntactic information and hierarchical structure.First, we generalize Marton and Resnik’s (2008) soft syntactic constraints by training all of them simultaneously; and, second, we introduce a novel structural distortion model.We obtain significant improvements in both cases, and further large improvements when the two feature sets are combined.The paper proceeds as follows.We describe our training algorithm in section 2; our generalization of Marton and Resnik’s soft syntactic constraints in section 3; our novel structural distortion features in section 4; and experimental results in section 5.The translation model is a standard linear model (Och and Ney, 2002), which we train using MIRA (Crammer and Singer, 2003; Crammer et al., 2006), following Watanabe et al. (2007).We describe the basic algorithm first and then progressively refine it.Let e, by abuse of notation, stand for both output strings and their derivations.We represent the feature vector for derivation e as h(e).Initialize the feature weights w. Then, repeatedly: passes through the training data are made, we only average the weight vectors from the last pass.)The technique of averaging was introduced in the context of perceptrons as an approximation to taking a vote among all the models traversed during training, and has been shown to work well in practice (Freund and Schapire, 1999; Collins, 2002).We follow McDonald et al. (2005) in applying this technique to MIRA.Note that the objective (1) is not the same as that used by Watanabe et al. ; rather, it is the same as that used by Crammer and Singer (2003) and related to that of Taskar et al.(2005).We solve this optimization problem using a variant of sequential minimal optimization (Platt, 1998): for each i, initialize αij = C for a single value of j such that eij = e∗i , and initialize αij = 0 for all other values of j.Then, repeatedly choose a sentence i and a pair of hypotheses j, j0, and let where where we set C = 0.01.The first term means that we want w0 to be close to w, and second term (the generalized hinge loss) means that we want w0 to score e∗i higher than each eij by a margin at least as wide as the loss `ij.When training is finished, the weight vectors from all iterations are averaged together.(If multiple where the function clip[x,y](z) gives the closest number to z in the interval [x, y].Assuming BLEU as the evaluation criterion, the loss `ij of ei j relative to e∗i should be related somehow to the difference between their BLEU scores.However, BLEU was not designed to be used on individual sentences; in general, the highest-BLEU translation of a sentence depends on what the other sentences in the test set are.Sentence-level approximations to BLEU exist (Lin and Och, 2004; Liang et al., 2006), but we found it most effective to perform BLEU computations in the context of a set O of previously-translated sentences, following Watanabe et al. (2007).However, we don’t try to accumulate translations for the entire dataset, but simply maintain an exponentially-weighted moving average of previous translations.More precisely: For an input sentence f, let e be some hypothesis translation and let {rk} be the set of reference translations for f. Let c(e; {rk}), or simply c(e) for short, be the vector of the following counts: |e|, the effective reference length mink |rk|, and, for 1 ≤ n ≤ 4, the number of n-grams in e, and the number of n-gram matches between e and {rk}.These counts are sufficient to calculate a BLEU score, which we write as BLEU(c(e)).The pseudo-document O is an exponentially-weighted moving average of these vectors.That is, for each training sentence, let eˆ be the 1-best translation; after processing the sentence, we update O, and its input length Of: We can then calculate the BLEU score of hypotheses e in the context of O.But the larger O is, the smaller the impact the current sentence will have on the BLEU score.To correct for this, and to bring the loss function roughly into the same range as typical margins, we scale the BLEU score by the size of the input: which we also simply write as B(e).Finally, the loss function is defined to be: We now describe the selection of e∗.We know of three approaches in previous work.The first is to force the decoder to output the reference sentence exactly, and select the derivation with the highest model score, which Liang et al. (2006) call bold updating.The second uses the decoder to search for the highest-BLEU translation (Tillmann and Zhang, 2006), which Arun and Koehn (2007) call max-BLEU updating.Liang et al. and Arun and Koehn experiment with these methods and both opt for a third method, which Liang et al. call local updating: generate an n-best list of translations and select the highest-BLEU translation from it.The intuition is that due to noise in the training data or reference translations, a high-BLEU translation may actually use peculiar rules which it would be undesirable to encourage the model to use.Hence, in local updating, the search for the highest-BLEU translation is limited to the n translations with the highest model score, where n must be determined experimentally.Here, we introduce a new oracle-translation selection method, formulating the intuition behind local updating as an optimization problem: Instead of choosing the highest-BLEU translation from an n-best list, we choose the translation that maximizes a combination of (approximate) BLEU and the model.We can also interpret (10) in the following way: we want e∗ to be the max-BLEU translation, but we also want to minimize (1).So we balance these two criteria against each other: where (B(e) − h(e) · w) is that part of (1) that depends on e∗, and µ is a parameter that controls how much we are willing to allow some translations to have higher BLEU than e∗ if we can better minimize (1).Setting µ = 0 would reduce to max-BLEU updating; setting µ = ∞ would never update w at all.Setting µ = 0.5 reduces to equation (10).Figure 1 shows the 10-best unique translations for a single input sentence according to equation (11) under various settings of µ.The points at far right are the translations that are scored highest according to the model.The p = 0 points in the upper-left corner are typical of oracle translations that would be selected under the max-BLEU policy: they indeed have a very high BLEU score, but are far removed from the translations preferred by the model; thus they would cause violent updates to w. Local updating would select the topmost point labeled p = 1.Our scheme would select one of the p = 0.5 points, which have BLEU scores almost as high as the max-BLEU translations, yet are not very far from the translations preferred by the model.What is the set {eij} of translation hypotheses?Ideally we would let it be the set of all possible translations, and let the objective function (1) take all of them into account.This is the approach taken by Taskar et al. (2004), but their approach assumes that the loss function can be decomposed into local loss functions.Since our loss function cannot be so decomposed, we select: The last case is what Crammer et al. (2006) call max-loss updating (where “loss” refers to the generalized hinge loss) and Taskar et al.(2005) call lossaugmented inference.The rationale here is that since the objective (1) tries to minimize maxj(l'ij − Ahij · w'), we should include the translations that have the highest (l'ij − Ahij · w) in order to approximate the effect of using the whole forest.See Figure 1 again for an illustration of the hypotheses selected for a single sentence.The maxBLEU points in the upper left are not included (and would have no effect even if they were included).The p = oo points in the lower-right are the negative examples: they are poor translations that are scored too high by the model, and the learning algorithm attempts to shift them to the left.To perform the forest rescoring, we need to use several approximations, since an exact search for BLEU-optimal translations is NP-hard (Leusch et al., 2008).For every derivation e in the forest, we calculate a vector c(e) of counts as in Section 2.2 except using unclipped counts of n-gram matches (Dreyer et al., 2007), that is, the number of matches for an ngram can be greater than the number of occurrences of the n-gram in any reference translation.This can be done efficiently by calculating c for every hyperedge (rule application) in the forest: We keep track of n-grams using the same scheme used to incorporate an n-gram language model into the decoder (Wu, 1996; Chiang, 2007).To find the best derivation in the forest, we traverse it bottom-up as usual, and for every set of alternative subtranslations, we select the one with the highest score.But here a rough approximation lurks, because we need to calculate B on the nodes of the forest, but B does not have the optimal substructure property, i.e., the optimal score of a parent node cannot necessarily be calculated from the optimal scores of its children.Nevertheless, we find that this rescoring method is good enough for generating high-BLEU oracle translations and low-BLEU negative examples.One convenient property of MERT is that it is embarrassingly parallel: we decode the entire tuning set sending different sentences to different processors, and during optimization of feature weights, different random restarts can be sent to different processors.In order to make MIRA comparable in efficiency to MERT, we must parallelize it.But with an online learning algorithm, parallelization requires a little more coordination.We run MIRA on each processor simultaneously, with each maintaining its own weight vector.A master process distributes different sentences from the tuning set to each of the processors; when each processor finishes decoding a sentence, it transmits the resulting hypotheses, with their losses, to all the other processors and receives any hypotheses waiting from other processors.Those hypotheses were generated from different weight vectors, but can still provide useful information.The sets of hypotheses thus collected are then processed as one batch.When the whole training process is finished, we simply average all the weight vectors from all the processors.Having described our training algorithm, which includes several practical improvements to Watanabe et al.’s usage of MIRA, we proceed in the remainder of the paper to demonstrate the utility of the our training algorithm on models with large numbers of structurally sensitive features.The first features we explore are based on a line of research introduced by Chiang (2005) and improved on by Marton and Resnik (2008).A hierarchical phrase-based translation model is based on synchronous context-free grammar, but does not normally use any syntactic information derived from linguistic knowledge or treebank data: it uses translation rules that span any string of words in the input sentence, without regard for parser-defined syntactic constituency boundaries.Chiang (2005) experimented with a constituency feature that rewarded rules whose source language side exactly spans a syntactic constituent according to the output of an external source-language parser.This feature can be viewed as a soft syntactic constraint: it biases the model toward translations that respect syntactic structure, but does not force it to use them.However, this more syntactically aware model, when tested in Chinese-English translation, did not improve translation performance.Recently, Marton and Resnik (2008) revisited the idea of constituency features, and succeeded in showing that finer-grained soft syntactic constraints yield substantial improvements in BLEU score for both Chinese-English and Arabic-English translation.In addition to adding separate features for different syntactic nonterminals, they introduced a new type of constraint that penalizes rules when the source language side crosses the boundaries of a source syntactic constituent, as opposed to simply rewarding rules when they are consistent with the source-language parse tree.Marton and Resnik optimized their features’ weights using MERT.But since MERT does not scale well to large numbers of feature weights, they were forced to test individual features and manually selected feature combinations each in a separate model.Although they showed gains in translation performance for several such models, many larger, potentially better feature combinations remained unexplored.Moreover, the best-performing feature subset was different for the two language pairs, suggesting that this labor-intensive feature selection process would have to be repeated for each new language pair.Here, we use MIRA to optimize Marton and Resnik’s finer-grained single-category features all at once.We define below two sets of features, a coarsegrained class that combines several constituency categories, and a fine-grained class that puts different categories into different features.Both kinds of features were used by Marton and Resnik, but only a few at a time.Crucially, our training algorithm provides the ability to train all the fine-grained features, a total of 34 feature weights, simultaneously.Coarse-grained features As the basis for coarsegrained syntactic features, we selected the following nonterminal labels based on their frequency in the tuning data, whether they frequently cover a span of more than one word, and whether they represent linguistically relevant constituents: NP, PP, S, VP, SBAR, ADJP, ADVP, and QP.We define two new features, one which fires when a rule’s source side span in the input sentence matches any of the above-mentioned labels in the input parse, and another which fires when a rule’s source side span crosses a boundary of one of these labels (e.g., its source side span only partially covers the words in a VP subtree, and it also covers some or all or the words outside the VP subtree).These two features are equivalent to Marton and Resnik’s XP= and XP' feature combinations, respectively.Fine-grained features We selected the following nonterminal labels that appear more than 100 times in the tuning data: NP, PP, S, VP, SBAR, ADJP, WHNP, PRT, ADVP, PRN, and QP.The labels that were excluded were parts of speech, nonconstituent labels like FRAG, or labels that occurred only two or three times.For each of these labels X, we added a separate feature that fires when a rule’s source side span in the input sentence matches X, and a second feature that fires when a span crosses a boundary of X.These features are similar to Marton and Resnik’s X= and X+, except that our set includes features for WHNP, PRT, and PRN.In addition to parser-based syntactic constraints, which were introduced in prior work, we introduce a completely new set of features aimed at improving the modeling of reordering within Hiero.Again, the feature definition gives rise to a larger number of features than one would expect to train successfully using MERT.In a phrase-based model, reordering is performed both within phrase pairs and by the phrasereordering model.Both mechanisms are able to learn that longer-distance reorderings are more costly than shorter-distance reorderings: phrase pairs, because phrases that involve more extreme reorderings will (presumably) have a lower count in the data, and phrase reordering, because models are usually explicitly dependent on distance.By contrast, in a hierarchical model, all reordering is performed by a single mechanism, the rules of the grammar.In some cases, the model will be able to learn a preference for shorter-distance reorderings, as in a phrase-based system, but in the case of a word being reordered across a nonterminal, or two nonterminals being reordered, there is no dependence in the model on the size of the nonterminal or nonterminals involved in reordering.So, for example, if we have rules we might expect that rule (12) is more common in general, but that rule (13) becomes more and more rare as X1 gets larger.The default Hiero features have no way to learn this.To address this defect, we can classify every nonterminal pair occurring on the right-hand side of each grammar rule as “reordered” or “not reordered”, that is, whether it intersects any other word alignment link or nonterminal pair (see Figure 2).We then define coarse- and fine-grained versions of the structural distortion model.Coarse-grained features Let R be a binaryvalued random variable that indicates whether a nonterminal occurrence is reordered, and let S be an integer-valued random variable that indicates how many source words are spanned by the nonterminal occurrence.We can estimate P(R  |S) via relativefrequency estimation from the rules as they are extracted from the parallel text, and incorporate this probability as a new feature of the model.Fine-grained features A difficulty with the coarse-grained reordering features is that the grammar extraction process finds overlapping rules in the training data and might not give a sensible probability estimate; moreover, reordering statistics from the training data might not carry over perfectly into the translation task (in particular, the training data may have some very freely-reordering translations that one might want to avoid replicating in translation).As an alternative, we introduce a fine-grained version of our distortion model that can be trained directly in the translation task as follows: define a separate binary feature for each value of (R, S), where R is as above and S E J*, 1, ... , 9, >_101 and * means any size.For example, if a nonterminal with span 11 has its contents reordered, then the features (true, >_10) and (true, *) would both fire.Grouping all sizes of 10 or more into a single feature is designed to avoid overfitting.Again, using MIRA makes it practical to train with the full fine-grained feature set—coincidentally also a total of 34 features.We now describe our experiments to test MIRA and our features, the soft-syntactic constraints and the structural distortion features, on an Arabic-English translation task.It is worth noting that this experimentation is on a larger scale than Watanabe et al.’s (2007), and considerably larger than Marton and Resnik’s (2008).The baseline model was Hiero with the following baseline features (Chiang, 2005; Chiang, 2007): The probability features are base-100 logprobabilities.The rules were extracted from all the allowable parallel text from the NIST 2008 evaluation (152+175 million words of Arabic+English), aligned by IBM Model 4 using GIZA++ (union of both directions).Hierarchical rules were extracted from the most in-domain corpora (4.2+5.4 million words) and phrases were extracted from the remainder.We trained the coarse-grained distortion model on 10,000 sentences of the training data.Two language models were trained, one on data similar to the English side of the parallel text and one on 2 billion words of English.Both were 5gram models with modified Kneser-Ney smoothing, lossily compressed using a perfect-hashing scheme similar to that of Talbot and Brants (2008) but using minimal perfect hashing (Botelho et al., 2005).We partitioned the documents of the NIST 2004 (newswire) and 2005 Arabic-English evaluation data into a tuning set (1178 sentences) and a development set (1298 sentences).The test data was the NIST 2006 Arabic-English evaluation data (NIST part, newswire and newsgroups, 1529 sentences).To obtain syntactic parses for this data, we tokenized it according to the Arabic Treebank standard using AMIRA (Diab et al., 2004), parsed it with the Stanford parser (Klein and Manning, 2003), and then forced the trees back into the MT system’s tokenization.1 We ran both MERT and MIRA on the tuning set using 20 parallel processors.We stopped MERT when the score on the tuning set stopped increasing, as is common practice, and for MIRA, we used the development set to decide when to stop training.2 In our runs, MERT took an average of 9 passes through the tuning set and MIRA took an average of 8 passes.(For comparison, Watanabe et al. report decoding their tuning data of 663 sentences 80 times.)Table 1 shows the results of our experiments with the training methods and features described above.All significance testing was performed against the first line (MERT baseline) using paired bootstrap resampling (Koehn, 2004).First of all, we find that MIRA is competitive with MERT when both use the baseline feature set.Indeed, the MIRA system scores significantly higher on the test set; but if we break the test set down by genre, we see that the MIRA system does slightly worse on newswire and better on newsgroups.(This is largely attributable to the fact that the MIRA translations tend to be longer than the MERT translations, and the newsgroup references are also relatively longer than the newswire references.)When we add more features to the model, the two training methods diverge more sharply.When training with MERT, the coarse-grained pair of syntax features yields a small improvement, but the finegrained syntax features do not yield any further improvement.By contrast, when the fine-grained features are trained using MIRA, they yield substantial improvements.We observe similar behavior for the structural distortion features: MERT is not able to take advantage of the finer-grained features, but MIRA is.Finally, using MIRA to combine both classes of features, 56 in all, produces the largest improvement, 2.6 BLEU points over the MERT baseline on the full test set.We also tested some of the differences between our training method and Watanabe et al.’s (2007); the results are shown in Table 2.Compared with local updating (line 2), our method of selecting the oracle translation and negative examples does better by 0.5 BLEU points on the development data.Using lossaugmented inference to add negative examples to local updating (line 3) does not appear to help.Nevertheless, the negative examples are important: for if Setting Dev full 53.6 local updating, no LAI local updating, LAI p = 0.5 oracle, no LAI no sharing of updates 53.1−− we use our method for selecting the oracle translation without the additional negative examples (line 4), the algorithm fails, generating very long translations and unable to find a weight setting to shorten them.It appears, then, that the additional negative examples enable the algorithm to reliably learn from the enhanced oracle translations.Finally, we compared our parallelization method against a simpler method in which all processors learn independently and their weight vectors are all averaged together (line 5).We see that sharing information among the processors makes a significant difference.In this paper, we have brought together two existing lines of work: the training method of Watanabe et al. (2007), and the models of Chiang (2005) and Marton and Resnik (2008).Watanabe et al.’s work showed that large-margin training with MIRA can be made feasible for state-of-the-art MT systems by using a manageable tuning set; we have demonstrated that parallel processing and exploiting more of the parse forest improves MIRA’s performance and that, even using the same set of features, MIRA’s performance compares favorably to MERT in terms of both translation quality and computational cost.Marton and Resnik (2008) showed that it is possible to improve translation in a data-driven framework by incorporating source-side syntactic analysis in the form of soft syntactic constraints.This work joins a growing body of work demonstrating the utility of syntactic information in statistical MT.In the area of source-side syntax, recent research has continued to improve tree-to-string translation models, soften the constraints of the input tree in various ways (Mi et al., 2008; Zhang et al., 2008), and extend phrase-based translation with sourceside soft syntactic constraints (Cherry, 2008).All this work shows strong promise, but Marton and Resnik’s soft syntactic constraint approach is particularly appealing because it can be used unobtrusively with any hierarchically-structured translation model.Here, we have shown that using MIRA to weight all the constraints at once removes the crucial drawback of the approach, the problem of feature selection.Finally, we have introduced novel structural distortion features to fill a notable gap in the hierarchical phrase-based approach.By capturing how reordering depends on constituent length, these features improve translation quality significantly.In sum, we have shown that removing the bottleneck of MERT opens the door to many possibilities for better translation.Thanks to Michael Bloodgood for performing initial simulations of parallelized perceptron training.Thanks also to John DeNero, Kevin Knight, Daniel Marcu, and Fei Sha for valuable discussions and suggestions.This research was supported in part by DARPA contract HR0011-06-C-0022 under subcontract to BBN Technologies and HR0011-06-02-001 under subcontract to IBM.
Statistical phrase-based systems (Och and Ney,2004; Koehn et al, 2003) have consistently delivered state-of-the-art performance in recent machine translation evaluations, yet these systems remain weak at handling word order changes.The re ordering models used in the original phrase-basedsystems penalize phrase displacements proportionally to the amount of nonmonotonicity, with no con sideration of the fact that some words are far more M M D S D !" #$ %& '( )* +, -./ eue nviro nme nt m inist ers hold mee tings in l uxem burg . 01 23 45 67 8 / the d evel opm ent and prog ress of the regi on . D M D D (b)(a)Figure 1: Phase orientations (monotone, swap, discontin uous) for Chinese-to-English translation.While previouswork reasonably models phrase reordering in simple ex amples (a), it fails to capture more complex reorderings, such as the swapping of ?of the region?(b).likely to be displaced than others (e.g., in English-to Japanese translation, a verb should typically move to the end of the clause).Recent efforts (Tillman, 2004; Och et al, 2004; Koehn et al, 2007) have directly addressed this issue by introducing lexicalized reordering models into phrase-based systems, which condition reordering probabilities on the words of each phrase pair.These models distinguish three orientations with respect to the previous phrase?monotone (M), swap (S), anddiscontinuous (D)?and as such are primarily de signed to handle local re-orderings of neighboring phrases.Fig.1(a) is an example where such a modeleffectively swaps the prepositional phrase in Luxembourg with a verb phrase, and where the noun min isters remains in monotone order with respect to the previous phrase EU environment.While these lexicalized re-ordering models have shown substantial improvements over unlexicalized phrase-based systems, these models only have a 848limited ability to capture sensible long distance re orderings, as can be seen in Fig.1(b).The phrase of the region should swap with the rest of the noun phrase, yet these previous approaches are unable to model this movement, and assume the orientation of this phrase is discontinuous (D).Observe that, in a shortened version of the same sentence (withoutand progress), the phrase orientation would be different (S), even though the shortened version has es sentially the same sentence structure.Coming from the other direction, such observations about phrase reordering between different languages are precisely the kinds of facts that parsing approaches to machinetranslation are designed to handle and do success fully handle (Wu, 1997; Melamed, 2003; Chiang, 2005).In this paper, we introduce a novel orientationmodel for phrase-based systems that aims to bet ter capture long distance dependencies, and that presents a solution to the problem illustrated in Fig.1(b).In this example, our reordering modeleffectively treats the adjacent phrases the develop ment and and progress as one single phrase, and the displacement of of the region with respect to thisphrase can be treated as a swap.To be able iden tify that adjacent blocks (e.g., the development and and progress) can be merged into larger blocks, ourmodel infers binary (non-linguistic) trees reminis cent of (Wu, 1997; Chiang, 2005).Crucially, our work distinguishes itself from previous hierarchical models in that it does not rely on any cubic-timeparsing algorithms such as CKY (used in, e.g., (Chiang, 2005)) or the Earley algorithm (used in (Watan abe et al, 2006)).Since our reordering model doesnot attempt to resolve natural language ambiguities, we can effectively rely on (linear-time) shiftreduce parsing, which is done jointly with left-toright phrase-based beam decoding and thus intro duces no asymptotic change in running time.Assuch, the hierarchical model presented in this paper maintains all the effectiveness and speed advantages of statistical phrase-based systems, while be ing able to capture some key linguistic phenomena (presented later in this paper) which have motivated the development of parsing-based approaches.We also illustrate this with results that are significantly better than previous approaches, in particular the lexical reordering models of Moses, a widely used phrase-based SMT system (Koehn et al, 2007).This paper is organized as follows: the train ing of lexicalized re-ordering models is described in Section 3.In Section 4, we describe how to combine shift-reduce parsing with left-to-right beamsearch phrase-based decoding with the same asymptotic running time as the original phrase-based decoder.We finally show in Section 6 that our ap proach yields results that are significantly better thanprevious approaches for two language pairs and dif ferent test sets.We compare our re-ordering model with related work (Tillman, 2004; Koehn et al, 2007) using alog-linear approach common to many state-of-the art statistical machine translation systems (Och and Ney, 2004).Given an input sentence f, which is to be translated into a target sentence e, the decodersearches for the most probable translation e?accord ing to the following decision rule: e?= argmax e { p(e|f) } (1) = argmax e { J ? j=1 ? jh j(f,e) } (2) h j(f,e) are J arbitrary feature functions over sentence pairs.These features include lexicalized re-ordering models, which are parameterized as follows: given an input sentence f, a sequence of target-language phrases e = (e1, . . .,en) currently hypothesized by the decoder, and a phrase alignment a = (a1, . . .,an) that defines a source f ai for eachtranslated phrase ei, these models estimate the prob ability of a sequence of orientations o = (o1, . . .,on) p(o|e, f) = n ? i=1 p(oi|ei, f ai ,ai?1,ai), (3)where each oi takes values over the set of possi ble orientations O = {M,S,D}.1 The probability is conditioned on both ai?1 and ai to make sure that the label oi is consistent with the phrase alignment.Specifically, probabilities in these models can be 1We note here that the parameterization and terminology in (Tillman, 2004) is slightly different.We purposely ignore thesedifferences in order to enable a direct comparison between Till man?s, Moses?, and our approach.849 ..b i . . .b i ..(a) (b) (c) b i s u v u v uv s s Figure 2: Occurrence of a swap according to the threeorientation models: word-based, phrase-based, and hier archical.Black squares represent word alignments, and gray squares represent blocks identified by phrase-extract.In (a), block bi = (ei, fai) is recognized as a swap accord ing to all three models.In (b), bi is not recognized as a swap by the word-based model.In (c), bi is recognized as a swap only by the hierarchical model.greater than zero only if one of the following con ditions is true: ? oi = M and ai ?ai?1 = 1 ? oi = S and ai ?ai?1 = ?1 ? oi = D and |ai ?ai?1| 6= 1At decoding time, rather than using the log probability of Eq.3 as single feature function, we follow the approach of Moses, which is to assign three distinct parameters (?m,?s,?d) for the three feature functions: ? fm = ?ni=1 log p(oi = M| . . .)fs = ?ni=1 log p(oi = S| . . .)fd = ?ni=1 log p(oi = D| . . .).There are two key differences between this work and previous orientation models (Tillman, 2004; Koehn et al, 2007): (1) the estimation of factors in Eq.3 from data; (2) the segmentation of e and f into phrases, which is static in the case of (Tillman, 2004; Koehn et al, 2007), while it is dynamically updatedwith hierarchical phrases in our case.These differ ences are described in the two next sections.We present here three approaches for computingp(oi|ei, f ai ,ai?1,ai) on word-aligned data using rel ative frequency estimates.We assume here that phrase ei spans the word range s, . . ., t in the target sentence e and that the phrase f ai spans the range ORIENTATION MODEL oi = M oi = S oi = D word-based (Moses) 0.1750 0.0159 0.8092 phrase-based 0.3192 0.0704 0.6104 hierarchical 0.4878 0.1004 0.4116Table 1: Class distributions of the three orientation mod els, estimated from 12M words of Chinese-English data using the grow-diag alignment symmetrization heuristic implemented in Moses, which is similar to the ?refined?heuristic of (Och and Ney, 2004).u, . . .,v in the source sentence f. All phrase pairs inthis paper are extracted with the phrase-extract algo rithm (Och and Ney, 2004), with maximum length set to 7.Word-based orientation model: This model an alyzes word alignments at positions (s?1,u?1) and (s?1,v+1) in the alignment grid shown in Fig.2(a).Specifically, orientation is set to oi = M if (s? 1,u? 1) contains a word alignment and (s?1,v+1) contains no word alignment.It is set to oi = S if (s?1,u?1) contains no word alignment and (s?1,v+1) contains a word alignment.In all other cases, it is set to oi = D. This procedure is exactly the same as the one implemented in Moses.2 Phrase-based orientation model: The modelpresented in (Tillman, 2004) is similar to the word based orientation model presented above, except that it analyzes adjacent phrases rather than specificword alignments to determine orientations.Specif ically, orientation is set to oi = M if an adjacent phrase pair lies at (s?1,u?1) in the alignmentgrid.It is set to S if an adjacent phrase pair cov ers (s?1,v+1) (as shown in Fig.2(b)), and is set to D otherwise.Hierarchical orientation model: This model analyzes alignments beyond adjacent phrases.Specifically, orientation is set to oi = M if the phrase extract algorithm is able to extract a phrase pair at (s?1,u?1) given no constraint on maximum phrase length.Orientation is S if the same is true at (s?1,v+1), and orientation is D otherwise.Table 1 displays overall class distributions according to the three models.It appears clearly that occurrences of M and S are too sparsely seen in the word based model, which assigns more than 80% of its 2http://www.statmt.org/moses/?n=Moses.AdvancedFeatures 850 word phrase hier.Monotone with previous p(oi = M|ei, f ai ,ai?1,ai) 1 ,4 and is 0.223 0.672 0.942 2 , and also 0.201 0.560 0.948 Swap with previous p(oi = S|ei, f ai ,ai?1,ai) 3 ?){ of china 0.303 0.617 0.651 4 ??, he said 0.003 0.030 0.395 Monotone with next p(oi = M|ei, f ai ,ai+1,ai) 5 ???, he pointed out that 0.601 0.770 0.991 6 l , however , 0.517 0.728 0.968 Swap with next p(oi = S|ei, f ai ,ai+1,ai) 7 {0 the development of 0.145 0.831 0.900 8 {? at the invitation of 0.272 0.834 0.925 Table 2: Monotone and swap probabilities for specific phrases according to the three models (word, phrase, and hierarchical).To ensure probabilities are representative, we only selected phrase pairs that occur at least 100 times in the training data.probability mass to D. Conversely, the hierarchical model counts considerably less discontinuous cases, and is the only model that accounts for the fact that real data is predominantly monotone.Since D is a rather uninformative default cat egory that gives no clue how a particular phraseshould be displaced, we will also provide MT evalu ation scores (in Section 6) for a set of classes that distinguishes between left and right discontinuity{M,S,Dl,Dr}, a choice that is admittedly more lin guistically motivated.Table 2 displays orientation probabilities for con crete examples.Each example was put under one of the four categories that linguistically seems thebest match, and we provide probabilities for that cat egory according to each model.Note that, whilewe have so far only discussed left-to-right reorder ing models, it is also possible to build right-to-leftmodels by substituting ai?1 with ai+1 in Eq.3.Ex amples for right-to-left models appear in the second half of the table.The table strongly suggests that the hierarchical model more accurately determinesthe orientation of phrases with respect to large contextual blocks.In Examples 1 and 2, the hierarchi cal model captures the fact that coordinated clauses almost always remain in the same order, and that words should generally be forbidden to move from one side of ?and?to the other side, a constraint thatis difficult to enforce with the other two reorder ing models.In Example 4, the first two models completely ignore that ?he said?sometimes rotates around its neighbor clause.Computing reordering scores during decoding with word-based3 and phrase-based models (Tillman, 2004) is trivial, since they only make use of localinformation to determine the orientation of a new in coming block bi.For a left-to-right ordering model, bi is scored based on its orientation with respect to bi?1.For instance, if bi has a swap orientation withrespect to the previous phrase in the current translation hypothesis, feature p(oi = S| . . .)becomes ac tive.Computing lexicalized reordering scores with the hierarchical model is more complex, since the model must identify contiguous blocks?monotone or swapping?that can be merged into hierarchical blocks.The employed method is an instance of thewell-known shift-reduce parsing algorithm, and re lies on a stack (S) of foreign substrings that have already been translated.Each time the decoder adds a new block to the current translation hypothesis, it shifts the source-language indices of the block ontoS, then repeatedly tries reducing the top two ele ments of S if they are contiguous.4 This parsingalgorithm was first applied in computational geome try to identify convex hulls (Graham, 1972), and its running time was shown to be linear in the length of the sequence (a proof is presented in (Huang et al., 2008), which applies the same algorithm to the binarization of SCFG rules).Figure 3 provides an example of the execution of this algorithm for the translation output shownin Figure 4, which was produced by a decoder in corporating our hierarchical reordering model.The decoder successively pushes source-language spans [1], [2], [3], which are successively merged into [1-3], and all correspond to monotone orientations.3We would like to point out an inconsistency in Moses be tween training and testing.Despite the fact that Moses estimates a word-based orientation model during training (i.e., it analyzes the orientation of a given phrase with respect to adjacent wordalignments), this model is then treated as a phrase-based orien tation model during testing (i.e., as a model that orients phrases with respect to other phrases).4It is not needed to store target-language indices onto thestack, since the decoder proceeds left to right, and thus suc cessive blocks are always contiguous with respect to the target language.851 Target phrase Source Op.oi Stack the russian side [1] S M hopes [2] R M [1] to [3] R M [1-2] hold [11] S D [1-3] consultations [12] R M [11], [1-3] with iran [9-10] R S [11-12], [1-3] on this [6-7] S D [9-12], [1-3] issue [8] R,R M [6-7], [9-12], [1-3] in the near future [4-5] R,R S [6-12], [1-3] . [13] R,A M [1-12]Figure 3: The application of the shift-reduce parsing algorithm for identifying hierarchical blocks.This execu tion corresponds to the decoding example of Figure 4.Operations (Op.)include shift (S), reduce (R), and accept (A).The source and stack columns contain source language spans, which is the only information needed to determine whether two given blocks are contiguous.oi isthe label predicted by the hierarchical model by compar ing the current block to the hierarchical phrase that is at the top of the stack./0 12 34 56 the russi an side hope s to hold cons ultati ons with iran on this issue in the near future ................................... h 1 h 2 h 3 Figure 4: Output of our phrase-based decoder using the hierarchical model on a sentence of MT06.Hierarchical phrases h1 and h2 indicate that with Iran and in the near future have a swap orientation.h3 indicates that ?to?and ?.?are monotone.In this particular example, distortion limit was set to 10.It then encounters a discontinuity that prevents the next block [11] from being merged with [1-3].As the decoder reaches the last words of the sentence (in the near future), [4-5] is successively merged with [6-12], then [1-3], yielding a stack that contains only [1-12].A nice property of this parsing algorithm is that it does not worsen the asymptotic running time of beam-search decoders such as Moses (Koehn, 2004a).Such decoders run in time O(n2), where n is the length of the input sentence.Indeed, each time a partial translation hypothesis is expanded intoa longer one, the decoder must perform an O(n) op eration in order to copy the coverage set (indicating which foreign words have already been translated) into the new hypothesis.Since this copy operationmust be executed O(n) times, the overall time complexity is quadratic.The incorporation of the shift reduce parser into such a decoder does not worsenoverall time complexity: whenever the decoder expands a given partial translation into a longer hy pothesis, it simply copies its stack into the newlycreated hypothesis (similarly to copying the cover age vector, this is an O(n) operation).Hence, the incorporation of the hierarchical models described in the paper into a phrase-based decoder preserves the O(n2) running time.In practice, we observe based on a set of experiments for Chinese-English and Arabic-English translation that our phrase-based decoder is on average only 1.35 times slower when it is running using hierarchical reordering features and the shift-reduce parser.We finally note that the decoding algorithm presented in this section can only be applied left-to right if the decoder itself is operating left-to-right.In order to predict orientations relative to the rightto-left hierarchical reordering model, we must resort to approximations at decoding time.We experi mented with different approximations, and the one that worked best (in the experiments discussed in Section 6) is described as follows.First, we note that an analysis of the alignment grid often reveals that certain orientations are impossible.For instance, the block issue in Figure 4 can only have discontinuousorientation with respect to what comes next in En glish, since words surrounding the Chinese phrasehave already been translated.When several hier archical orientations are possible according to thealignment grid, we choose according to the follow ing order of preference: (1) monotone, (2) swap, (3) discontinuous.For instance, in the case of with iranin Figure 4, only swap and discontinuous orientations are possible (monotone orientation is impossi ble because of the block hold consultations), hence we give preference to swap.This prediction turns out to be the correct one according to the decoding 852 steps that complete the alignment grid.We now analyze the system output of Figure 4 to fur ther motivate the hierarchical model, this time from the perspective of the decoder.We first observe that the prepositional phrase in the future should rotatearound a relatively large noun phrase headed by consultations.Unfortunately, localized reordering models such as (Tillman, 2004) have no means of identifying that such a displacement is a swap (S).Accord ing to these models, the orientation of in the futurewith respect to what comes previously is discontinuous (D), which is an uninformative fall-back category.By identifying h2 (hold ... issue) as a hierarchical block, the hierarchical model can properly deter mine that the block in the near future should have a swap orientation.5 Similar observations can be made regarding blocks h1 and h3, which leads our model to predict either monotone orientation (between h3and ?to?and between h3 and ?.?)or swap orienta tion (between h1 and with Iran) while local models would predict discontinuous in all cases.Another benefit of the hierarchical model is thatits representation of phrases remains the same dur ing both training and decoding, which is not the casefor word-based and phrase-based reordering mod els.The deficiency of these local models lies in thefact that blocks handled by phrase-based SMT sys tems tend to be long at training time and short attest time, which has adverse consequences on nonhierarchical reordering models.For instance, in Fig ure 4, the phrase-based reordering model categorizes the block in the near future as discontinuous, though if the sentence pair had been a training example,this block would count as a swap because of the ex tracted phrase on this issue.In our experiments, we use a re-implementationof the Moses decoder (Koehn et al, 2007).Except for lexical reordering models, all other fea tures are standard features implemented almost5Note that the hierarchical phrase hold ... issue is not a well formed syntactic phrase ? i.e., it neither matches the bracketing of the verb phrase hold ... future nor matches the noun phrase consultations ... issue ? yet it enables sensible reordering.exactly as in Moses: four translation features(phrase-based translation probabilities and lexically weighted probabilities), word penalty, phrase penalty, linear distortion, and language model score.We experiment with two language pairs: Chinese to-English (C-E) and Arabic-to-English (A-E).For C-E, we trained translation models using a subset of the Chinese-English parallel data released by LDC (mostly news, in particular FBIS and Xinhua News).This subset comprises 12.2M English words, and 11M Chinese words.Chinese words are segmented with a conditional random field (CRF) classifier that conforms to the Chinese Treebank (CTB) standard.The training set for our A-E systems also includes mostly news parallel data released by LDC, and contains 19.5M English words, and 18.7M Arabic tokens that have been segmented using the Arabic Treebank (ATB) (Maamouri et al, 2004) standard.6 For our language model, we trained a 5-gram model using the Xinhua and AFP sections of the Gigaword corpus (LDC2007T40), in addition to the target side of the parallel data.For both C-E and A-E, we manually removed documents of Gigaword that were released during periods that overlap with those of our development and test sets.The language model was smoothed with the modified Kneser-Ney algorithm, and we kept only trigrams, 4-grams, and 5-grams that respectively occurred two, three, and three times in the training data.Parameters were tuned with minimum error-rate training (Och, 2003) on the NIST evaluation set of 2006 (MT06) for both C-E and A-E.Since MERTis prone to search errors, especially with large num bers of parameters, we ran each tuning experimentfour times with different initial conditions.This pre caution turned out to be particularly important in the case of the combined lexicalized reordering models (the combination of phrase-based and hierarchical discussed later), since MERT must optimize up to 26 parameters at once in these cases.7 For testing, 6Catalog numbers for C-E: LDC2002E18, LDC2003E07, LDC2003E14, LDC2005E83, LDC2005T06, LDC2006E26, and LDC2006E8.For A-E: LDC2007E103, LDC2005E83, LDC2006E24, LDC2006E34, LDC2006E85, LDC2006E92, LDC2007E06, LDC2007E101, LDC2007E46, LDC2007E86, and LDC2008E40.7We combine lexicalized reordering models by simply treat ing them as distinct features, which incidentally increases the number of model parameters that must be tuned with MERT.853 30.5 31 31.5 32 32.5 33 33.5 34 0 2 4 6 8 10 12 14 BL EU [%], Ch ines e-E ngli sh distortion limit hierarchicalphrase-based word-basedbaseline 43 43.5 44 44.5 45 45.5 0 2 4 6 8 10 BL EU [%], Arabic Eng lish distortion limit hierarchicalphrase-based word-basedbaseline Figure 5: Performance on the Chinese-English andArabic-English development sets (MT06) with increasing distortion limits for all lexicalized reordering mod els discussed in the paper.Our novel hierarchical model systematically outperforms all other models for distortion limit equal to or greater than 4.The baseline is Moses with no lexicalized reordering model.we used the NIST evaluation sets of 2005 and 2008 (MT05 and MT08) for Chinese-English, and the test set of 2005 (MT05) for Arabic-English.Statistical significance is computed using the approximate randomization test (Noreen, 1989), whose application to MT evaluation (Riezler and Maxwell, 2005) was shown to be less sensitive totype-I errors (i.e., incorrectly concluding that im provement is significant) than the perhaps more widely used bootstrap resampling method (Koehn, 2004b).Tuning set performance is shown in Figure 5.Since this paper studies various ordering models,it is interesting to first investigate how the distor LEXICALIZED REORDERING MT06 MT05 MT08 none 31.85 29.75 25.22 word-based 32.96 31.45 25.86 phrase-based 33.24 31.23 26.01 hierarchical 33.80** 32.20** 26.38 phrase-based + hierarchical 33.86** 32.85** 26.53* Table 3: BLEU[%] scores (uncased) for Chinese-Englishand the orientation categories {M,S,D}.Maximum dis tortion is set to 6 words, which is the default in Moses.The stars at the bottom of the tables indicate when a given hierarchical model is significantly better than all localmodels for a given development or test set (*: signifi cance at the .05 level; **: significance at the .01 level).LEXICALIZED REORDERING MT06 MT05 MT08 phrase-based 33.79 32.32 26.32 hierarchical 34.01 32.35 26.58 phrase-based + hierarchical 34.36** 32.33 27.03** Table 4: BLEU[%] scores (uncased) for Chinese-English and the orientation categories {M,S,Dl ,Dr}.Since the distinction between these four categories is not available in Moses, hence we have no baseline results for this case.Maximum distortion is set to 6 words.tion limit affects performance.8 As has been shownin previous work in Chinese-English and Arabic English translation, limiting phrase displacements to six source-language words is a reasonable choice.For both C-E and A-E, the hierarchical model is sig nificantly better (p ? .05) than either other modelsfor distortion limits equal to or greater than 6 (ex cept for distortion limit 12 in the case of C-E).Since a distortion limit of 6 works reasonably well for both language pairs and is the default in Moses, we used this distortion limit value for all test-set experiments presented in this paper.Our main results for Chinese-English are shownin Table 3.It appears that hierarchical models provide significant gains over all non-hierarchical models.Improvements on MT06 and MT05 are very sig nificant (p ? .01).In the case of MT08, significant improvement is reached through the combination ofboth phrase-based and hierarchical models.We of ten observe substantial gains when we combine such models, presumably because we get the benefit of identifying both local and long-distance swaps.Since most orientations in the phrase-based model are discontinuous, it is reasonable to ask whether8Note that we ran MERT separately for each distinct distor tion limit.854 LEXICALIZED REORDERING MT06 MT05 none 44.03 54.87 word-based 44.64 54.96 phrase-based 45.01 55.09 hierarchical 45.51* 55.50* phrase-based + hierarchical 45.64** 56.01** Table 5: BLEU[%] scores (uncased) for Arabic-English and the reordering categories {M,S,D}.LEXICALIZED REORDERING MT06 MT05 phrase-based 44.74 55.52 hierarchical 45.53** 56.02** phrase-based + hierarchical 45.63** 56.07** Table 6: BLEU[%] scores (uncased) for Arabic-English and the reordering categories {M,S,Dl ,Dr}.the relatively poor performance of the phrase-basedmodel is the consequence of an inadequate set of ori entation labels.To try to answer this question, weuse the set of orientation labels {M,S,Dl,Dr} de scribed in Section 3.Results for this different set oforientations are shown in Table 4.While the phrasebased model appears to benefit more from the distinction between left- and right-discontinuous, sys tems that incorporate hierarchical models remain the most competitive overall: their best performance on MT06, MT05, and MT08 are respectively 34.36, 32.85, and 27.03.The best non-hierarchical models achieve only 33.79, 32.32, and 26.32, respectively.All these differences (i.e., .57, .53, and .71) are sta tistically significant at the .05 level.Our results for Arabic-English are shown in Ta bles 5 and 6.Similarly to C-E, we provide results for two orientation sets: {M,S,D} and {M,S,Dl,Dr}.We note that the four-class orientation set is overall less effective for A-E than for C-E.This is probably due to the fact that there is less probability mass in A-E assigned to the D category, and thus it is less helpful to split the discontinuous category into two.For both orientation sets, we observe in A-E that the hierarchical model significantly outperforms thelocal ordering models.Gains provided by the hierarchical model are no less significant than for Chinese to-English.This positive finding is perhaps a bitsurprising, since Arabic-to-English translation gen erally does not require many word order changes compared to Chinese-to-English translation, and thistranslation task so far has seldom benefited from hierarchical approaches to MT. In our case, one possi ble explanation is that Arabic-English translation is benefiting from the fact that orientation predictionsof the hierarchical model are consistent across train ing and testing, which is not the case for the otherordering models discussed in this paper (see Sec tion 4).Overall, hierarchical models are the most effective on the two sets: their best performances on MT06 and MT05 are respectively 45.64 and 56.07.The best non-hierarchical models obtain only 45.01 and 55.52 respectively for the same sets.All thesedifferences (i.e., .63 and .55) are statistically signifi cant at the .05 level.In this paper, we presented a lexicalized orientation model that enables phrase movements that are more complex than swaps between adjacent phrases.This model relies on a hierarchical structure that is builtas a by-product of left-to-right phrase-based decod ing without increase of asymptotic running time.Weshow that this model provides statistically signifi cant improvements for five NIST evaluation sets and for two language pairs.In future work, we plan to extend the parameterization of our models to not only predict phrase orientation, but also the length of each displacement as in (Al-Onaizan and Papineni, 2006).We believe such an extension would improve translation quality in the case of larger distortionlimits.We also plan to experiment with discriminative approaches to estimating reordering probabil ities (Zens and Ney, 2006; Xiong et al, 2006), whichcould also be applied to our work.We think the abil ity to condition reorderings on any arbitrary featurefunctions is also very effective in the case of our hi erarchical model, since information encoded in thetrees would seem beneficial to the orientation pre diction task.The authors wish to thank the anonymous reviewers for their comments on an earlier draft of this paper.This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM.The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred.855
Cross-lingual Textual Entailment (CLTE) has been recently proposed by (Mehdad et al., 2010; Mehdad et al., 2011) as an extension of Textual Entailment (Dagan and Glickman, 2004).The task consists of deciding, given a text (T) and an hypothesis (H) in different languages, if the meaning of H can be inferred from the meaning of T. As in other NLP applications, both for monolingual and cross-lingual TE,the availability of large quantities of annotated data is an enabling factor for systems development and evaluation.Until now, however, the scarcity of such data on the one hand, and the costs of creating new datasets of reasonable size on the other, have represented a bottleneck for a steady advancement of the state of the art.In the last few years, monolingual TE corpora for English and other European languages have been created and distributed in the framework of several evaluation campaigns, including the RTE Challenge1, the Answer Validation Exercise at CLEF2, and the Textual Entailment task at EVALITA3.Despite the differences in the design of the tasks, all the released datasets were collected through similar procedures, always involving expensive manual work done by expert annotators.Moreover, in the data creation process, large amounts of hand-crafted T-H pairs often have to be discarded in order to retain only those featuring full agreement, in terms of the assigned entailment judgements, among multiple annotators.The amount of discarded pairs is usually high, contributing to increase the costs of creating textual entailment datasets4.The issues related to the shortage of datasets and the high costs for their creation are more evident in the CLTE scenario, where: i) the only dataset currently available is an English-Spanish corpus obtained by translating the RTE-3 corpus (Negri and Mehdad, 2010), and ii) the application of the standard methods adopted to build RTE pairs requires proficiency in multiple languages, thus significantly increasing the costs of the data creation process.To address these issues, in this paper we devise a cost-effective methodology to create cross-lingual textual entailment corpora.In particular, we focus on the following problems: (1) Is it possible to collect T-H pairs minimizing the intervention of expert annotators?To address this question, we explore the feasibility of crowdsourcing the corpus creation process.As a contribution beyond the few works on TE/CLTE data acquisition, we define an effective methodology that: i) does not involve experts in the most complex (and costly) stages of the process, ii) does not require preprocessing tools, and iii) does not rely on the availability of already annotated RTE corpora. to non-experts, difficult to accomplish, and not suitable for the application of the quality-check mechanisms provided by current crowdsourcing services.Our “divide and conquer” solution represents the first attempt to address a complex task involving content generation and labelling through the definition of a cheap and reliable pipeline of simple tasks which are easy to define, accomplish, and control. guages.Moreover, since the core monolingual tasks of the process are carried out by manipulating English texts, we are able to address the very large community of English speaking workers, with a considerable reduction of costs and execution time.Finally, as a by-product of our method, the acquired pairs are fully aligned for all language combinations, thus enabling meaningful comparisons between scenarios of different complexity (monolingual TE, and CLTE between close or distant languages).We believe that, in the same spirit of recent works promoting large-scale annotation efforts around entailment corpora (Sammons et al., 2010; Bentivogli et al., 2010), the proposed approach and the resulting dataset5 will contribute to meeting the strong need for resources to develop and evaluate novel solutions for textual entailment.Crowdsourcing services, such as Amazon Mechanical Turk6 (MTurk) and CrowdFlower7, have been recently used with success for a variety of NLP applications (Callison-Burch and Dredze, 2010).The idea is that the acquisition and annotation of large amounts of data needed to train and evaluate NLP tools can be carried out in a cost-effective manner by defining simple Human Intelligence Tasks (HITs) routed to a crowd of non-expert workers (aka “Turkers”) hired through on-line marketplaces.As regards textual entailment, the first work exploring the use of crowdsourcing services for data annotation is described in (Snow et al., 2008), which shows high agreement between non-expert annotations of the RTE-1 dataset and existing gold standard labels assigned by expert labellers.Focusing on the actual generation of monolingual entailment pairs, (Wang and Callison-Burch, 2010) experiments the use of MTurk to collect facts and counter facts related to texts extracted from an existing RTE corpus annotated with named entities.Taking a step beyond the task of annotating existing datasets, and showing the feasibility of involving non-experts also in the generation of TE pairs, this approach is more relevant to our objectives.However, at least two major differences with our work have to be remarked.First, they still use available RTE data to obtain a monolingual TE corpus, whereas we pursue the more ambitious goal of generating from scratch aligned CLTE corpora for different language combinations.To this aim, we do not resort to already annotated data, nor languagespecific preprocessing tools.Second, their approach involves qualitative analysis of the collected data only a posteriori, after manual removal of invalid and trivial generated hypotheses.In contrast, our approach integrates quality control mechanisms at all stages of the data collection/annotation process, thus minimizing the recourse to experts to check the quality of the collected material.Related research in the CLTE direction is reported in (Negri and Mehdad, 2010), which describes the creation of an English-Spanish corpus obtained from the RTE-3 dataset by translating the English hypotheses into Spanish.Translations have been crowdsourced adopting a methodology based on translation-validation cycles, defined as separate HITs.Although simplifying the CLTE corpus creation problem, which is recast as the task of translating already available annotated data, this solution is relevant to our work for the idea of combining gold standard units and “validation HITS” as a way to control the quality of the collected data at runtime.The design of data acquisition HITs has to take into account several factors, each having a considerable impact on the difficulty of instructing the workers, the quality and quantity of the collected data, the time and overall costs of the acquisition.A major distinction has to be made between jobs requiring data annotation, and those involving content generation.In the former case, Turkers are presented with the task of labelling input data referring to a fixed set of possible values (e.g. making a choice between multiple alternatives, assigning numerical scores to rank the given data).In the latter case, Turkers are faced with creative tasks consisting in the production of textual material (e.g. writing a correct translation, or a summary of a given text).The ease of controlling the quality of the acquired data depends on the nature of the job.For annotation jobs, quality control mechanisms can be easily set up by calculating Turkers’ agreement, by applying voting schemes, or by adding hidden gold units to the data to be annotated8.In contrast, the quality of the results of content generation jobs is harder to assess, due to the fact that multiple valid results are acceptable (e.g. the same content can be expressed, translated, or summarized in different ways).In such situations the standard quality control mechanisms are not directly applicable, and the detection of errors requires either costly manual verification at the end of the acquisition process, or more complex and creative solutions integrating HITs for quality check.Most of the approaches to content generation proposed so far rely on post hoc verification to filter out undesired low-quality data (Mrozinski et al., 2008; Mihalcea and Strapparava, 2009; Wang and Callison-Burch, 2010).The few solutions integrating validation HITs address the translation of single sentences, a task that is substantially different from ours (Negri and Mehdad, 2010; Bloodgood and Callison-Burch, 2010).Compared to sentence translation, the task of creating CLTE pairs is both harder to explain without recurring to notions that are difficult to understand to non-experts (e.g.“semantic equivalence”, “unidirectional entailment”), and harder to execute without mastering these notions.To tackle these issues the “divide and conquer” approach described in the next section consists in the decomposition of a difficult content generation job into easier subtasks that are: i) self-contained and easy to explain, ii) easy to execute without any NLP expertise, and iii) suitable for the integration of a variety of runtime control mechanisms (regional qualifications, gold units, “validation HITs”) able to ensure a good quality of the collected material.8Both MTurk and CrowdFlower provide means to check workers’ reliability, and weed out untrusted ones without money waste.These include different types of qualification mechanisms, the possibility of giving work only to known trusted Turkers (only with MTurk), and the possibility of adding hidden gold standard units in the data to be annotated (offered as a built-in mechanism only by CrowdFlower). tions (e.g.L2/L2, L2/L3).The execution of the two “multilingual” stages is not strictly necessary but depends on: i) the availability of parallel sentences to start the process, and ii) the actual objectives in terms of language combinations to be covered10.As regards the first stage, in this work we started from a set of 467 English/Italian/German aligned sentences extracted from parallel documents downloaded from the Cafebabel European Magazine11.Concerning the second multilingual stage, we performed only one round of translations from English to Italian to extend the 3 combinations obtained without translations (ENG/ENG, ENG/ITA, and ENG/GER) with the new language combinations ITA/ITA, ITA/ENG, and ITA/GER.Our approach builds on a pipeline of HITs routed to MTurk’s workforce through the CrowdFlower interface.The objective is to collect aligned T-H pairs for different language combinations, reproducing an RTE-like annotation style.However, our annotation is not limited to the standard RTE framework, where only unidirectional entailment from T to H is considered.As a useful extension, we annotate any possible entailment relation between the two text fragments, including: i) bidirectional entailment (i.e. semantic equivalence between T and H), ii) unidirectional entailment from T to H, and iii) unidirectional entailment from H to T. The resulting pairs can be easily used to generate not only standard RTE datasets9, but also general-purpose collections featuring multi-directional entailment relations.We collect large amounts of CLTE pairs carrying out the most difficult part of the process (the creation of entailment-annotated pairs) at a monolingual level.Starting from a set of parallel sentences in n languages, (e.g.L1, L2, L3), n entailment corpora are created: one monolingual (L1/L1), and n-1 crosslingual (L1/L2, and L1/L3).The monolingual corpus is obtained by modifying the sentences only in one language (L1).Original and modified sentences are then paired and annotated to form an entailment dataset for L1.The CLTE corpora are obtained by combining the modified sentences in L1 with the original sentences in L2 and L3, and projecting to the multilingual pairs the annotations assigned to the monolingual pairs.In principle, only two stages of the process require crowdsourcing multilingual tasks, but do not concern entailment annotations.The first one, at the beginning of the process, aims to obtain a set of parallel sentences to start with, and can be done in different ways (e.g. crowdsourcing the translation of a set of sentences).The second one, at the end of the process, consists of translating the modified L1 sentences into other languages (e.g.L2) in order to extend the corpus to cover new language combina9With the positive examples drawn from bidirectional and unidirectional entailments from T to H, and the negative ones drawn from unidirectional entailments from H to T. The main steps of our corpus creation process, depicted in Figure 1, can be summarized as follows: Step1: Sentence modification.The original English sentences (ENG) are modified through (monolingual) generation HITs asking Turkers to: i) preserve the meaning of the original sentences using different surface forms, or ii) slightly change their meaning by adding or removing content.Our assumption, in line with (Bos et al., 2009), is that 10Starting from parallel sentences in n languages, the n corpora obtained without recurring to translations can be augmented, by means of translation HITs, to create the full set of language combinations.Each round of translation adds 1 monolingual corpus, and n-1 CLTE corpora. another way to think about entailment is to consider whether one text T1 adds new information to the content of another text T: if so, then T is entailed by T1.The result of this phase is a set of texts (ENG1) that can be of three types: Step2: TE Annotation.Entailment pairs composed of the original sentences (ENG) and the modified ones (ENG1) are used as input of (monolingual) annotation HITs asking Turkers to decide which of the two texts contains more information.As a result, each ENG/ENG1 pair is annotated as an example of uni-/bidirectional entailment, and stored in the monolingual English corpus.Since the original ENG texts are aligned with the ITA and GER texts, the entailment annotations of ENG/ENG1 pairs can be projected to the other language pairs and the ITA/ENG1 and GER/ENG1 pairs are stored in the CLTE corpus.The possibility of projecting TE annotations is based on the assumption that the semantic information is mostly preserved during the translation process.This particularly holds at the denotative level (i.e. regarding the truth values of the sentence) which is crucial to semantic inference.At other levels (e.g. lexical) there might be slight semantic variations which, however, are very unlikely to play a crucial role in determining entailment relations.Step3: Translation.The modified sentences (ENG1) are translated into Italian (ITA1) through (multilingual) generation HITs reproducing the approach described in (Negri and Mehdad, 2010).As a result, three new datasets are produced by automatically projecting annotations: the monolingual ITA/ITA1, and the cross-lingual ENG/ITA1 and GER/ITA1.Since the solution adopted for sentence translation does not present novelty factors, the remainder of this paper will omit further details on it.Instead, the following sections will focus on the more challenging tasks of sentence modification and TE annotation.Sentence modification and TE annotation have been decomposed into a pipeline of simpler monolingual English sub-tasks.Such pipeline, depicted in Figure 2, involves several types of generation/annotation HITs designed to be easily understandable to nonexperts.Each HIT consists of: i) a set of instructions for a specific task (e.g. paraphrasing a text), ii) the data to be manipulated (e.g. an English sentence), and iii) a test to check workers’ reliability.To cope with the quality control issues discussed in Section 3, such tests are realized using gold standard units, either hidden in the data to be annotated (annotation HITs) or defined as test questions that workers must correctly answer (generation HITs).Moreover, regional qualifications are applied to all HITs.As a further quality check, all the annotation HITs consider Turkers’ agreement as a way to filter out low quality results (only annotations featuring agreement among 4 out of 5 workers are retained).The six HITs defined for each subtask can be described as follows: new sentence workers are asked to judge which of two given English sentences is more detailed.4b.Remove Information (generation).Modify an English text to create a more general one by removing part of its content.As a reliability test, before generating the new sentence workers are asked to judge which of two given English sentences is less detailed.cide which of two English sentences (the original ENG, and a modified ENG1) provides more information.These HITs are combined in an iterative process that alternates text generation, grammaticality check, and entailment annotation steps.As a result, for each original ENG text we obtain multiple ENG1 variants of the three types (paraphrases, more general texts, and more specific texts) and, in turn, a set of annotated monolingual (ENG/ENG1) TE pairs.As described in Section 4.1, the resulting monolingual English TE corpus (ENG/ENG1) is used to create the following mono/cross-lingual TE corpora:This section provides a quantitative and qualitative analysis of the results of our corpus creation methodology, focusing on the collected ENG-ENG1 monolingual dataset.It has to be remarked that, as an effect of the adopted methodology, all the observations and the conclusions drawn hold for the collected CLTE corpora as well.Table 1 provides some details about each step of the pipeline shown in Figure 2.For each HIT the table presents: i) the number of items (sentences, or pairs of sentences) given in input, ii) the number of items (sentences or annotations) produced as output, iii) the number of items discarded when the agreement threshold was not reached, iv) the number of entailment pairs added to the corpus, v) the time (days and hours) required by the MTurk workforce to complete the job, and vi) the cost of the job.In HIT-1 (Paraphrase) 1,414 paraphrases were collected asking three different meaning-preserving modifications of each of the 467 original sentences12.From a practical point of view, such redundancy aims to ensure a sufficient number of grammatically correct and semantically equivalent modified sentences.From a theoretical point of view, collecting many variants of a small pool of original sentences aims to create pairs featuring different entailment relations with similar superficial forms.This, in principle, should allow to obtain a dataset which requires TE systems to focus more on deeper semantic phenomena than on the surface realization of the pairs.The collected paraphrases were sent as input to HIT-2 (Grammaticality).After this validation HIT, the number of acceptable paraphrases was reduced to 1,326 (with 88 discarded sentences, corresponding to 6.22% of the total).The retained paraphrases were paired with their corresponding original sentences, and sent to HIT-3 (Bidirectional Entailment) to be judged for semantic equivalence.The pairs marked as bidirectional entailments (1,205) were divided in three groups: 25% of the pairs (301) were directly stored in the final corpus, while the ENG1 paraphrases of the remaining 75% (904) were equally distributed to the next modification steps.In both HIT-4a (Add Information) and HIT-4b (Remove information) two new modified sentences were asked for each of the 452 paraphrases received as input.The sentences collected in these generation tasks were respectively 916 and 923.The new modified sentences were sent back to HIT-2 (Grammaticality) and HIT-3 (Bidirectional Entailment).As a result 1,438 new pairs were created; out of these, 148 resulted to be bidirectional entailments and were stored in the corpus.Finally, the 1,298 entailment pairs judged as nonbidirectional in the two previously completed HIT3 (8+1,290) were given as input to HIT-5 (Unidirectional Entailment).The pairs which passed the agreement threshold were classified according to the judgement received, and stored in the corpus as unidirectional entailment pairs.The analysis of Table 1 allows to formulate some considerations.First, the percentage of discarded items confirms the effectiveness of decomposing complex generation tasks into simpler subtasks that integrate validation HITs and quality checks based on non-experts’ agreement.In fact, on average, around 9.5% of the generated items were discarded without experts’ intervention13.Second, the amount of discarded items gives evidence about the relative difficulty of each HIT.As expected, we observe lower rejection rates, corresponding to higher inter-annotator agreement, for grammaticality HITs (5.55% on average) than for more complex entailment-related tasks (12.02% on average).Looking at costs and execution time, it is hard to draw definite conclusions due to several factors that influence the progress of the crowdsourced jobs (e.g. the fluctuations of Turkers’ performances, the time of the day at which jobs are posted, the difficulty to set the optimal cost for a given HIT14).On the one hand, as expected, the more creative “Add Info” task proved to be more demanding than the “Remove Info”: even though it was paid more, 13Moreover, it is worthwhile noticing that around 20% of the collected items were automatically rejected (and not paid) due to failures on the gold standard controls created both for generation and annotation tasks.14The payment for each HIT was set on the basis of a previous feasibility study aimed at determining the best trade-off between cost and execution time.However, replicating our approach would not necessarily result in the same costs. it still took little more time to be completed.On the other hand, although the “Unidirectional Entailment” task was expected to be more difficult and thus rewarded more than the “Bidirectional Entailment” one, in the end it took notably less time to be completed.Nevertheless, the overall figures (435 USD, and about 22.5 days of MTurk work to complete the process)15 clearly demonstrate the effectiveness of the approach.Even considering the time needed for an expert to manage the pipeline (i.e. one week to prepare gold units, and to handle the I/O of each HIT), these figures show that our methodology provides a cheaper and faster way to collect entailment data in comparison with the RTE average costs reported in Section 1.As regards the amount of data collected, the resulting corpus contains 1,620 pairs with the following distribution of entailment relations: i) 449 bidirectional entailments, ii) 491 ENG→ENG1 unidirectional entailments, and iii) 680 ENG←ENG1 unidirectional entailments.It must be noted that our methodology does not lead to the creation of pairs where some information is provided in one text and not in the other, and viceversa, as Example 1 shows: ENG: New theories were emerging in the field of psychology.ENG1: New theories were rising, which announced a kind of veiled racism.These negative examples in both directions represent a natural extension of the dataset, relevant also for specific application-oriented scenarios, and their creation will be addressed in future work.Besides the achievement of our primary objectives, the adopted approach led to some interesting by-products.First, the generated corpora are perfectly suitable to produce entailment datasets similar to those used in the traditional RTE evaluation framework.In particular, considering any possible entailment relation between two text fragments, our annotation subsumes the one proposed in RTE campaigns.This allows for the cost-effective generation of RTE-like annotations from the acquired cor15Although by projecting annotations the ENG1/ITA and ENG1/GER CLTE corpora came for free, the ITA1/ITA, ITA1/ENG, and ITA1/GER combinations created by crowdsourcing translations added 45 USD and approximately 5 days to these figures. pora by combining ENG↔ENG1 and ENG→ENG1 pairs to form 940 positive examples (449+491), keeping the 680 ENG←ENG1 as negative examples.Moreover, by swapping ENG and ENG1 in the unidirectional entailment pairs, 491 additional negative examples and 680 positive examples can be easily obtained.Finally, the output of HITs 1-2-3 in Table 1 represents per se a valuable collection of 1,205 paraphrases.This suggests the great potential of crowdsourcing for paraphrase acquisition.Through manual verification of more than 50% of the corpus (900 pairs), a total number of 53 pairs (5.9%) were found incorrect.The different errors were classified as follows: Type 1: Sentence modification errors.Generation HITs are a minor source of errors, being responsible for 10 problematic pairs.These errors are either introduced by generating a false statement (Example 2), or by forming a not fully understandable, awkward, or non-natural sentence (Example 3).Type 2: TE annotation errors.The notion of containing more/less information, used in the “Unidirectional Entailment” HIT, can mostly be applied straightforwardly to the entailment definition.However, the concept of “more/less detailed”, which generally works for factual statements, in some cases is not applicable.In fact, the MTurk workers have regularly interpreted the instructions about the amount of information as concerning the quantity of concepts contained in a sentence.This is not always corresponding to the actual entailment relation between the sentences.As a consequence, 43 pairs featuring wrong entailment annotations were encountered.These errors can be classified as follows: a) 13 pairs, where the added/removed information changes the meaning of the sentence.In these cases, the modified sentence was judged more/less specific than the original one, leading to unidirectional entailment annotation.On the contrary, in terms of the standard entailment definition, the correct annotation is “no entailment” (as in Example 4, which was annotated as ENG→ENG1): These pairs were labelled as unidirectional entailments (in the example above ENG→ENG1), under the assumption that a proper name is more specific and informative than a pronoun.However, adhering to the TE definition, co-referring expressions are equivalent, and their realization does not play any role in the entailment decision.This implies that the correct entailment annotation is “bidirectional”. c) 9 pairs where the sentences are semantically equivalent, but contain a piece of information which is explicit in one sentence, and implicit in the other.In these cases, Turkers judged the sentence containing the explicit mention as more specific, and thus the pair was annotated as unidirectional entailment.In Example 6, the expression “the trigger” in ENG1 implicitly means “the click of the trigger”, making the two sentences equivalent, and the entailment bidirectional (instead of ENG→ENG1). d) 7 pairs where the information removed from or added to the sentence is not relevant to the entailment relation.In these cases, the modified sentence was judged less/more specific than the original one (and thus considered as unidirectional entailment), even though the correct judgement is “bidirectional”, as in: e) 4 pairs where the added/removed information concerns universally quantified general statements, about which the interpretation of “more/less specific” given by Turkers resulted in the wrong annotation.In Example 8, the additional information (“multicultural”) restricts the set to which it refers (“couples”) making ENG entailed by ENG1, and not vice versa as resulted from Turkers’ annotation.In light of this analysis, we conclude that the sentence modification methodology proved to be successful, as the low number of Type 1 errors shows.Considering that the most expensive phase in the creation of a TE dataset is the generation of the pairs, this is a significant achievement.Differently, the entailment assessment phase appears to be more problematic, accounting for the majority of errors.As shown by Type 2 errors, this is due to a partial misalignment between the instructions given in our HITs, and the formal definition of textual entailment.For this reason, further experimentation will explore different ways to instruct workers (e.g. asking to consider proper names and pronouns as equivalent) in order to reduce the amount of errors produced.As a final remark, considering that in the creation of a TE dataset the manual check of the annotated pairs represents a minor cost, even the involvement of experts to filter out wrong annotations would not decrease the cost-effectiveness of the proposed methodology.There is an increasing need of annotated data to develop new solutions to the Textual Entailment problem, explore new entailment-related tasks, and set up experimental frameworks targeting real-world applications.Following the recent trends promoting annotation efforts that go beyond the established RTE Challenge framework (unidirectional entailment between monolingual T-H pairs), in this paper we addressed the multilingual dimension of the problem.Our primary goal was the creation of large-scale collections of entailment pairs for different language combinations.Besides that, we considered cost effectiveness and replicability as additional requirements.To achieve our objectives, we developed a “divide and conquer” methodology based on crowdsourcing.Our approach presents several key innovations with respect to the related works on TE data acquisition.These include the decomposition of a complex content generation task in a pipeline of simpler subtasks accessible to a large crowd of non-experts, and the integration of quality control mechanisms at each stage of the process.The result of our work is the first large-scale dataset containing both monolingual and cross-lingual corpora for several combinations of texts-hypotheses in English, Italian, and German.Among the advantages of our method it is worth mentioning: i) the full alignment between the created corpora, ii) the possibility to easily extend the dataset to new languages, and iii) the feasibility of creating general-purpose corpora, featuring multi-directional entailment relations, that subsume the traditional RTE-like annotation.This work has been partially supported by the ECfunded project CoSyne (FP7-ICT-4-24853).The authors would like to thank Emanuele Pianta for the helpful discussions, and Giovanni Moretti for the valuable support in the creation of the CLTE dataset.
In this paper we describe how co-training (Blum and Mitchell, 1998) can be used to bootstrap a pair of statistical parsers from a small amount of annotated training data.Co-training is a wealdy supervised learning algorithm in which two (or more) learners are iteratively retrained on each other's output.It has been applied to problems such as word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named-entity recognition (Collins and Singer, 1999).However, these tasks typically involved a small set of labels (around 2-3) and a relatively small parameter space.It is therefore instructive to consider co-training for more complex models.Compared to these earlier models, a statistical parser has a larger parameter space, and instead of class labels, it produces recursively built parse trees as output.Previous work in co-training statistical parsers (Sarkar, 2001) used two components of a single parsing framework (that is, a parser and a supertagger for that parser).In contrast, this paper considers co-training two diverse statistical parsers: the Collins lexicalized PCFG parser and a Lexicalized Tree Adjoining Grammar (LTAG) parser.Section 2 reviews co-training theory.Section 3 considers how co-training applied to training statistical parsers can be made computationally viable.In Section 4 we show that co-training outperforms self-training, and that co-training is most beneficial when the seed set of manually created parses is small.Section 4.4 shows that co-training is possible even when the set of initially labelled data is drawn from a different distribution to either the unlabelled training material or the test set; that is, we show that co-training can help in porting a parser from one genre to another.Finally, section 5 reports summary results of our experiments.Co-training can be informally described in the following manner (Blum and Mitchell, 1998): Effectively, by picking confidently labelled data from each model to add to the training data, one model is labelling data for the other.This is in contrast to self-training, in which a model is retrained only on the labelled examples that it produces (Nigam and Ghani, 2000).Blum and Mitchell prove that, when the two views are conditionally independent given the label, and each view is sufficient for learning the task, co-training can improve an initial weak learner using unlabelled data.Dasgupta et al. (2002) extend the theory of cotraining by showing that, by maximising their agreement over the unlabelled data, the two learners make few generalisation errors (under the same independence assumption adopted by Blum and Mitchell).Abney (2002) argues that this assumption is extremely restrictive and typically violated in the data, and he proposes a weaker independence assumption.Abney also presents a greedy algorithm that maximises agreement on unlabelled data.Goldman and Zhou (2000) show that, through careful selection of newly labelled examples, cotraining can work even when the classifiers' views do not fully satisfy the independence assumption.To apply the theory of co-training to parsing, we need to ensure that each parser is capable of learning the parsing task alone and that the two parsers have different views.We could also attempt to maximise the agreement of the two parsers over unlabelled data, using a similar approach to that given by Abney.This would be computationally very expensive for parsers, however, and we therefore propose some practical heuristics for determining which labelled examples to add to the training set for each parser.Our approach is to decompose the problem into two steps.First, each parser assigns a score for every unlabelled sentence it parsed according to some scoring function, f, estimating the reliability of the label it assigned to the sentence (e.g. the probability of the parse).Note that the scoring functions used by the two parsers do not necessarily have to be the same.Next, a selection method decides which parser is retrained upon which newly parsed sentences.Both scoring and selection phases are controlled by a simple incremental algorithm, which is detailed in section 3.2.An ideal scoring function would tell us the true accuracy rates (e.g., combined labelled precision and recall scores) of the trees that the parser produced.In practice, we rely on computable scoring functions that approximate the true accuracy scores, such as measures of uncertainty.In this paper we use the probability of the most likely parse as the scoring function: where w is the sentence and V is the set of parses produced by the parser for the sentence.Scoring parses using parse probability is motivated by the idea that parse probability should increase with parse correctness.During the selection phase, we pick a subset of the newly labelled sentences to add to the training sets of both parsers.That is, a subset of those sentences labelled by the LTAG parser is added to the training set of the Collins PCFG parser, and vice versa.It is important to find examples that are reliably labelled by the teacher as training data for the student.The term teacher refers to the parser providing data, and student to the parser receiving A and B are two different parsers.MA and ivriB are models of A and B at step i. U is a large pool of unlabelled sentences.Ui is a small cache holding subset of U at step i. L is the manually labelled seed data.L'A and LiB are the labelled training examples for A and B at step i. and assign scores to them according to their scoring functions JA and fB.Select new parses {PA} and {PB} according to some selection method S, which uses the scores from fA and fB.LiA+1- is LiA augmented with {PB} L1- is LiB augmented with {PA} data.In the co-training process the two parsers alternate between teacher and student.We use a method which builds on this idea, Stop-n, which chooses those sentences (using the teacher's labels) that belong to the teacher's n-highest scored sentences.For this paper we have used a simple scoring function and selection method, but there are alternatives.Other possible scoring functions include a normalized version of fprob which does not penalize longer sentences, and a scoring function based on the entropy of the probability distribution over all parses returned by the parser.Other possible selection methods include selecting examples that one parser scored highly and another parser scored lowly, and methods based on disagreements on the label between the two parsers.These methods build on the idea that the newly labelled data should not only be reliably labelled by the teacher, but also be as useful as possible for the student.The pseudo-code for the co-training process is given in Figure 1, and consists of two different parsers and a central control that interfaces between the two parsers and the data.At each co-training iteration, a small set of sentences is drawn from a large pool of unlabelled sentences and stored in a cache.Both parsers then attempt to parse every sentence in the cache.Next, a subset of the sentences newly labelled by one parser is added to the training data of the other parser, and vice versa.The general control flow of our system is similar to the algorithm described by Blum and Mitchell; however, there are some differences in our treatment of the training data.First, the cache is flushed at each iteration: instead of only replacing just those sentences moved from the cache, the entire cache is refilled with new sentences.This aims to ensure that the distribution of sentences in the cache is representative of the entire pool and also reduces the possibility of forcing the central control to select training examples from an entire set of unreliably labelled sentences.Second, we do not require the two parsers to have the same training sets.This allows us to explore several selection schemes in addition to the one proposed by Blum and Mitchell.In order to conduct co-training experiments between statistical parsers, it was necessary to choose two parsers that generate comparable output but use different statistical models.We therefore chose the following parsers:parser (Collins, 1999), model 2.Some code for (re)training this parser was added to make the co-training experiments possible.We refer to this parser as Collins-CFG.In order to perform the co-training experiments reported in this paper, LTAG derivation events Collins-CFG LTAG Bi-lexical dependencies are between Bi-lexical dependencies are between lexicalized nonterminals elementary trees Can produce novel elementary Can produce novel hi-lexical trees for the LTAG parser dependencies for Collins-CFG When using small amounts of seed data, When using small amounts of seed data, abstains less often than LTAG abstains more often than Collins-CFG were extracted from the head-lexicalized parse tree output produced by the Collins-CFG parser.These events were used to retrain the statistical model used in the LTAG parser.The output of the LTAG parser was also modified in order to provide input for the re-training phase in the Collins-CFG parser.These steps ensured that the output of the Collins-CFG parser could be used as new labelled data to re-train the LTAG parser and vice versa.The domains over which the two models operate are quite distinct.The LTAG model uses tree fragments of the final parse tree and combines them together, while the Collins-CFG model operates on a much smaller domain of individual lexicalized non-terminals.This provides a mechanism to bootstrap information between these two models when they are applied to unlabelled data.LTAG can provide a larger domain over which hi-lexical information is defined due to the arbitrary depth of the elementary trees it uses, and hence can provide novel lexical relationships for the Collins-CFG model, while the Collins-CFG model can paste together novel elementary trees for the LTAG model.A summary of the differences between the two models is given in Figure 2, which provides an informal argument for why the two parsers provide contrastive views for the co-training experiments.Of course there is still the question of whether the two parsers really are independent enough for effective co-training to be possible; in the results section we show that the Collins-CFG parser is able to learn useful information from the output of the LTAG parser.Figure 3 shows how the performance of the Collins-CFG parser varies as the amount of manually annotated training data (from the Wall Street Journal (WSJ) Penn Treebank (Marcus et al., 1993)) is increased.The graph shows a rapid growth in accuracy which tails off as increasing amounts of training data are added.The learning curve shows that the maximum payoff from co-training is likely to occur between 500 and 1,000 sentences.Therefore we used two sizes of seed data: 500 and 1,000 sentences, to see if cotraining could improve parser performance using these small amounts of labelled seed data.For reference, Figure 4 shows a similar curve for the LTAG parser.Each parser was first initialized with some labelled seed data from the standard training split (sections 2 to 21) of the WSJ Penn Treebank.Evaluation was in terms of Parseval (Black et al., 1991), using a balanced F-score over labelled constituents from section 0 of the Treebank.I The Fscore values are reported for each iteration of cotraining on the development set (section 0 of the Treebank).Since we need to parse all sentences in section 0 at each iteration, in the experiments reported in this paper we only evaluated one of the parsers, the Collins-CFG parser, at each iteration.All results we mention (unless stated otherwise) are F-scores for the Collins-CFG parser.Self-training experiments were conducted in which each parser was retrained on its own output.Self-training provides a useful comparison with co-training because any difference in the results indicates how much the parsers are benefiting from being trained on the output of another parser.This experiment also gives us some insight into the differences between the two parsing models.Self-training was used by Charniak (1997), where a modest gain was reported after re-training his parser on 30 million words.The results are shown in Figure 5.Here, both parsers were initialised with the first 500 sentences from the standard training split (sections 2 to 21) of the WSJ Penn Treebank.Subsequent unlabelled sentences were also drawn from this split.During each round of self-training, 30 sentences were parsed by each parser, and each parser was retrained upon the 20 self-labelled sentences which it scored most highly (each parser using its own joint probability (equation 1) as the score).The results vary significantly between the Collins-CFG and the LTAG parser, which lends weight to the argument that the two parsers are largely independent of each other.It also shows that, at least for the Collins-CFG model, a minor improvement in performance can be had from selftraining.The LTAG parser, by contrast, is hurt by self-training The first co-training experiment used the first 500 sentences from sections 2-21 of the Treebank as seed data, and subsequent unlabelled sentences were drawn from the remainder of these sections.During each co-training round, the LTAG parser parsed 30 sentences, and the 20 labelled sentences with the highest scores (according to the LTAG joint probability) were added to the training data of the Collins-CFG parser.The training data of the LTAG parser was augmented in the same way, using the 20 highest scoring parses from the set of 30, but using the Collins-CFG parser to label the sentences and provide the joint probability for scoring.Figure 6 gives the results for the Collins-CFG parser, and also shows the self-training curve for The upper curve is for co-training between Collins-CFG and LTAG; the lower curve is selftraining for Collins-CFG. comparison.2 The graph shows that co-training results in higher performance than self-training.The graph also shows that co-training performance levels out after around 80 rounds, and then starts to degrade.The likely reason for this dip is noise in the parse trees added by cotraining.Pierce and Cardie (2001) noted a similar behaviour when they co-trained shallow parsers. upper curve is for 1,000 sentences labelled data from Brown plus 100 WSJ sentences; the lower curve only uses 1,000 sentences from Brown.The second co-training experiment was the same as the first, except that more seed data was used: the first 1,000 sentences from sections 2-21 of the Treebank.Figure 7 gives the results, and, for comparison, also shows the previous performance curve for the 500 seed set experiment.The key observation is that the benefit of co-training is greater when the amount of seed material is small.Our hypothesis is that, when there is a paucity of initial seed data, coverage is a major obstacle that co-training can address.As the amount of seed data increases, coverage becomes less of a problem, and the co-training advantage is diminished.This means that, when most sentences in the testing set can be parsed, subsequent changes in performance come from better parameter estimates.Although co-training boosts the performance of the parser using the 500 seed sentences from 75% to 77.8% (the performance level after 100 rounds of co-training), it does not achieve the level of performance of a parser trained on 1,000 seed sentences.Some possible explanations are: that the newly labelled sentences are not reliable (i.e., they contain too many errors); that the sentences deemed reliable are not informative training examples; or a combination of both factors.This experiment examines whether co-training can be used to boost performance when the unlabelled data are taken from a different source than the initial seed data.Previous experiments in Gildea (2001) have shown that porting a statistical parser from a source genre to a target genre is a non-trivial task.Our two different sources were the parsed section of the Brown corpus and the Penn Treebank WSJ.Unlike the WSJ, the Brown corpus does not contain newswire material, and so the two sources differ from each other in terms of vocabulary and syntactic constructs.1,000 annotated sentences from the Brown section of the Penn Treebank were used as the seed data.Co-training then proceeds using the WSJ.3 Note that no manually created parses in the WSJ domain are used by the parser, even though it is evaluated using WSJ material.In Figure 8, the lower curve shows performance for the CollinsCFG parser (again evaluated on section 0).The difference in corpus domain does not hinder cotraining.The parser performance is boosted from 75% to 77.3%.Note that most of the improvement is within the first 5 iterations.This suggests that the parsing model may be adapting to the vocabulary of the new domain.We also conducted an experiment in which the initial seed data was supplemented with a tiny amount of annotated data (100 manually annotated WSJ sentences) from the domain of the unlabelled data.This experiment simulates the situation where there is only a very limited amount of labelled material in the novel domain.The upper curve in Figure 8 shows the outcome of this experiment.Not surprisingly, the 100 additional labelled WSJ sentences improved the initial performance of the parser (to 76.7%).While the amount of improvement in performance is less than the previous case, co-training provides an additional boost to the parsing performance, to 78.7%.The various experiments are summarised in Table 1.As is customary in the statistical parsing literature, we view all our previous experiments using section 0 of the Penn Treebank WSJ as contributing towards development.Here we report on system performance on unseen material (namely section 23 of the WSJ).We give F-score results for the Collins-CFG parser before and after cotraining for section 23.The results show a modest improvement under each co-training scenario, indicating that, for the Collins-CFG parser, there is useful information to be had from the output of the LTAG parser.However, the results are not as dramatic as those reported in other co-training papers, such as Blum and Mitchell (1998) for web-page classification and Collins and Singer (1999) for namedentity recognition.A possible reason is that parsing is a much harder task than these problems.An open question is whether co-training can produce results that improve upon the state-of-theart in statistical parsing.Investigation of the convergence curves (Figures 3 and 4) as the parsers are trained upon more and more manually-created treebank material suggests that, with the Penn Treebank, the Collins-CFG parser has nearly converged already.Given 40,000 sentences of labelled data, we can obtain a projected value of how much performance can be improved with additional reliably labelled data.This projected value was obtained by fitting a curve to the observed convergence results using a least-squares method from MAT LAB.When training data is projected to a size of 400K manually created Treebank sentences, the performance of the Collins-CFG parser is projected to be 89.2% with an absolute upper bound of 89.3%.This suggests that there is very little room for performance improvement for the Collins-CFG parser by simply adding more labelled data (using co-training or other bootstrapping methods or even manually).However, models whose parameters have not already converged might benefit from co-training For instance, when training data is projected to a size of 400K manually created Treebank sentences, the performance of the LTAG statistical parser would be 90.4% with an absolute upper bound of 91.6%.Thus, a bootstrapping method might improve performance of the LTAG statistical parser beyond the current state-of-the-art performance on the Treebank.In this paper, we presented an experimental study in which a pair of statistical parsers were trained on labelled and unlabelled data using co-training Our results showed that simple heuristic methods for choosing which newly parsed sentences to add to the training data can be beneficial.We saw that co-training outperformed self-training, that it was most beneficial when the seed set was small, and that co-training was possible even when the seed material was from another distribution to both the unlabelled material or the testing set.This final result is significant as it bears upon the general problem of having to build models when little or no labelled training material is available for some new domain.Co-training performance may improve if we consider co-training using sub-parses.This is because a parse tree is really a large collection of individual decisions, and retraining upon an entire tree means committing to all such decisions.Our ongoing work is addressing this point, largely in terms of re-ranked parsers.Finally, future work will also track comparative performance between the LTAG and Collins-CFG models.This work has been supported, in part, by the NSF/DARPA funded 2002 Language Engineering Workshop at Johns Hopkins University.We would like to thank Michael Collins, Andrew McCallum, and Fernando Pereira for helpful discussions, and the reviewers for their comments on this paper.
Many of the tasks required for effective seman tic tagging of phrases and texts rely on a list ofwords annotated with some lexical semantic fea tures.Traditional approaches to the development of such lists are based on the implicit assumption of classical truth-conditional theories of meaningrepresentation, which regard all members of a category as equal: no element is more of a member than any other (Edmonds, 1999).In this paper, we challenge the applicability of this assump tion to the semantic category of sentiment, whichconsists of positive, negative and neutral subcate gories, and present a dictionary-based Sentiment Tag Extraction Program (STEP) that we use to generate a fuzzy set of English sentiment-bearing words for the use in sentiment tagging systems 1.The proposed approach based on the fuzzy logic(Zadeh, 1987) is used here to assign fuzzy sen timent tags to all words in WordNet (Fellbaum, 1998), that is it assigns sentiment tags and a degreeof centrality of the annotated words to the sentiment category.This assignment is based on Word Net glosses.The implications of this approach for NLP and linguistic research are discussed.Set Some semantic categories have clear membership (e.g., lexical fields (Lehrer, 1974) of color, body parts or professions), while others are much more difficult to define.This prompted the developmentof approaches that regard the transition frommem bership to non-membership in a semantic category as gradual rather than abrupt (Zadeh, 1987; Rosch, 1978).In this paper we approach the category of sentiment as one of such fuzzy categories wheresome words ? such as good, bad ? are very central, prototypical members, while other, less central words may be interpreted differently by differ ent people.Thus, as annotators proceed from thecore of the category to its periphery, word mem 1Sentiment tagging is defined here as assigning positive,negative and neutral labels to words according to the senti ment they express.209 bership in this category becomes more ambiguous, and hence, lower inter-annotator agreement can be expected for more peripheral words.Under theclassical truth-conditional approach the disagree ment between annotators is invariably viewed as a sign of poor reliability of coding and is eliminatedby ?training?annotators to code difficult and am biguous cases in some standard way.While this procedure leads to high levels of inter-annotator agreement on a list created by a coordinated team of researchers, the naturally occurring differencesin the interpretation of words located on the pe riphery of the category can clearly be seen whenannotations by two independent teams are compared.The Table 1 presents the comparison of GI H4 (General Inquirer Harvard IV-4 list, (Stone et al., 1966)) 2 and HM (from (Hatzivassiloglou and McKeown, 1997) study) lists of words manuallyannotated with sentiment tags by two different re search teams.GI-H4 HM List composition nouns, verbs, adj., adv.adj.only Total list size 8, 211 1, 336 Total adjectives 1, 904 1, 336Tags assigned Positiv, Nega tiv or no tag Positiveor Nega tive Adj.with 1, 268 1, 336 non-neutral tags Intersection 774 (55% 774 (58% (% intersection) of GI-H4 adj) of HM) Agreement on tags 78.7%Table 1: Agreement between GI-H4 and HM an notations on sentiment tags.The approach to sentiment as a category withfuzzy boundaries suggests that the 21.3% dis agreement between the two manually annotatedlists reflects a natural variability in human annotators?judgment and that this variability is related to the degree of centrality and/or relative importance of certain words to the category of sen timent.The attempts to address this difference 2The General Inquirer (GI) list used in this study was manually cleaned to remove duplicate entries for words with same part of speech and sentiment.Only the Harvard IV-4 list component of the whole GI was used in this study, sinceother lists included in GI lack the sentiment annotation.Un less otherwise specified, we used the full GI-H4 list including the Neutral words that were not assigned Positiv or Negativ annotations.in importance of various sentiment markers have crystallized in two main approaches: automatic assignment of weights based on some statistical criterion ((Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2002; Kim and Hovy, 2004), and others) or manual annotation (Subasic andHuettner, 2001).The statistical approaches usually employ some quantitative criterion (e.g., mag nitude of pointwise mutual information in (Turney and Littman, 2002), ?goodness-for-fit?measure in(Hatzivassiloglou and McKeown, 1997), probabil ity of word?s sentiment given the sentiment if itssynonyms in (Kim and Hovy, 2004), etc.) to de fine the strength of the sentiment expressed by aword or to establish a threshold for the member ship in the crisp sets 3 of positive, negative andneutral words.Both approaches have their limi tations: the first approach produces coarse results and requires large amounts of data to be reliable,while the second approach is prohibitively expen sive in terms of annotator time and runs the risk ofintroducing a substantial subjective bias in anno tations.In this paper we seek to develop an approachfor semantic annotation of a fuzzy lexical cate gory and apply it to sentiment annotation of allWordNet words.The sections that follow (1) describe the proposed approach used to extract sen timent information from WordNet entries usingSTEP (Semantic Tag Extraction Program) algo rithm, (2) discuss the overall performance of STEP on WordNet glosses, (3) outline the method fordefining centrality of a word to the sentiment cate gory, and (4) compare the results of both automatic (STEP) and manual (HM) sentiment annotations to the manually-annotated GI-H4 list, which was used as a gold standard in this experiment.The comparisons are performed separately for each of the subsets of GI-H4 that are characterized by adifferent distance from the core of the lexical cat egory of sentiment.WordNet Entries Word lists for sentiment tagging applications can be compiled using different methods.Automatic methods of sentiment annotation at the word level can be grouped into two major categories: (1) corpus-based approaches and (2) dictionary-based3We use the term crisp set to refer to traditional, non fuzzy sets 210 approaches.The first group includes methods that rely on syntactic or co-occurrence patternsof words in large texts to determine their sentiment (e.g., (Turney and Littman, 2002; Hatzivassiloglou and McKeown, 1997; Yu and Hatzivassiloglou, 2003; Grefenstette et al, 2004) and oth ers).The majority of dictionary-based approaches use WordNet information, especially, synsets and hierarchies, to acquire sentiment-marked words (Hu and Liu, 2004; Valitutti et al, 2004; Kim and Hovy, 2004) or to measure the similarity between candidate words and sentiment-bearing words such as good and bad (Kamps et al, 2004).In this paper, we propose an approach to sentiment annotation of WordNet entries that was implemented and tested in the Semantic Tag Extrac tion Program (STEP).This approach relies bothon lexical relations (synonymy, antonymy and hyponymy) provided in WordNet and on the WordNet glosses.It builds upon the properties of dic tionary entries as a special kind of structured text:such lexicographical texts are built to establish se mantic equivalence between the left-hand and theright-hand parts of the dictionary entry, and there fore are designed to match as close as possible the components of meaning of the word.They have relatively standard style, grammar and syntactic structures, which removes a substantial source of noise common to other types of text, and finally, they have extensive coverage spanning the entire lexicon of a natural language.The STEP algorithm starts with a small set of seed words of known sentiment value (positive or negative).This list is augmented during thefirst pass by adding synonyms, antonyms and hy ponyms of the seed words supplied in WordNet.This step brings on average a 5-fold increase in the size of the original list with the accuracy of the resulting list comparable to manual annotations (78%, similar to HM vs. GI-H4 accuracy).At the second pass, the system goes through all WordNet glosses and identifies the entries that contain in their definitions the sentiment-bearing words from the extended seed list and adds these head words (or rather, lexemes) to the corresponding category ? positive, negative or neutral (the remainder).A third, clean-up pass is then performed to partially disambiguate the identified WordNet glosses with Brill?s part-of-speech tagger (Brill, 1995), which performs with up to 95% accuracy, and eliminates errors introduced into the list by part-of-speech ambiguity of some words acquired in pass 1 and from the seed list.At this step, we also filter outall those words that have been assigned contradict ing, positive and negative, sentiment values within the same run.The performance of STEP was evaluated using GI-H4 as a gold standard, while the HM list wasused as a source of seed words fed into the system.We evaluated the performance of our sys tem against the complete list of 1904 adjectives in GI-H4 that included not only the words that were marked as Positiv, Negativ, but also those that werenot considered sentiment-laden by GI-H4 annota tors, and hence were by default considered neutralin our evaluation.For the purposes of the evalua tion we have partitioned the entire HM list into 58non-intersecting seed lists of adjectives.The re sults of the 58 runs on these non-intersecting seed lists are presented in Table 2.The Table 2 showsthat the performance of the system exhibits sub stantial variability depending on the composition of the seed list, with accuracy ranging from 47.6%to 87.5% percent (Mean = 71.2%, Standard Devi ation (St.Dev) = 11.0%).Average Average run size % correct # of adj StDev % StDev PASS 1 103 29 78.0% 10.5% (WN Relations) PASS 2 630 377 64.5% 10.8% (WN Glosses) PASS 3 435 291 71.2% 11.0% (POS clean-up) Table 2: Performance statistics on STEP runs.The significant variability in accuracy of the runs (Standard Deviation over 10%) is attributable to the variability in the properties of the seed list words in these runs.The HM list includes some sentiment-marked words where not all meanings are laden with sentiment, but also the words where some meanings are neutral and even the wordswhere such neutral meanings are much more fre quent than the sentiment-laden ones.The runswhere seed lists included such ambiguous adjectives were labeling a lot of neutral words as sen timent marked since such seed words were more likely to be found in the WordNet glosses in their more frequent neutral meaning.For example, run # 53 had in its seed list two ambiguous adjectives 1 dim and plush, which are neutral in most of the contexts.This resulted in only 52.6% accuracy (18.6% below the average).Run # 48, on theother hand, by a sheer chance, had only unam biguous sentiment-bearing words in its seed list, and, thus, performed with a fairly high accuracy (87.5%, 16.3% above the average).In order to generate a comprehensive list cov ering the entire set of WordNet adjectives, the 58 runs were then collapsed into a single set of unique words.Since many of the clearly sentiment-laden adjectives that form the core of the category of sentiment were identified by STEP in multiple runs and had, therefore, multiple duplicates in thelist that were counted as one entry in the com bined list, the collapsing procedure resulted in a lower-accuracy (66.5% - when GI-H4 neutralswere included) but much larger list of English adjectives marked as positive (n = 3, 908) or neg ative (n = 3, 905).The remainder of WordNet?s 22, 141 adjectives was not found in any STEP run and hence was deemed neutral (n = 14, 328).Overall, the system?s 66.5% accuracy on thecollapsed runs is comparable to the accuracy re ported in the literature for other systems run onlarge corpora (Turney and Littman, 2002; Hatzi vassiloglou and McKeown, 1997).In order to make a meaningful comparison with the results reported in (Turney and Littman, 2002), we also did an evaluation of STEP results on positives andnegatives only (i.e., the neutral adjectives from GI H4 list were excluded) and compared our labels tothe remaining 1266 GI-H4 adjectives.The accuracy on this subset was 73.4%, which is compara ble to the numbers reported by Turney and Littman(2002) for experimental runs on 3, 596 sentiment marked GI words from different parts of speechusing a 2x109 corpus to compute point-wise mu tual information between the GI words and 14 manually selected positive and negative paradigm words (76.06%).The analysis of STEP system performancevs.GI-H4 and of the disagreements between man ually annotated HM and GI-H4 showed that the greatest challenge with sentiment tagging ofwords lies at the boundary between sentimentmarked (positive or negative) and sentiment neutral words.The 7% performance gain (from 66.5% to 73.4%) associated with the removal of neutrals from the evaluation set emphasizes the importance of neutral words as a major source of sentiment extraction system errors 4.Moreover, the boundary between sentiment-bearing (positive or negative) and neutral words in GI-H4 accountsfor 93% of disagreements between the labels assigned to adjectives in GI-H4 and HM by two in dependent teams of human annotators.The viewtaken here is that the vast majority of such inter annotator disagreements are not really errors but a reflection of the natural ambiguity of the words that are located on the periphery of the sentiment category.centrality to the semantic category The approach to sentiment category as a fuzzyset ascribes the category of sentiment some spe cific structural properties.First, as opposed to thewords located on the periphery, more central ele ments of the set usually have stronger and more numerous semantic relations with other categorymembers 5.Second, the membership of these cen tral words in the category is less ambiguous than the membership of more peripheral words.Thus, we can estimate the centrality of a word in a given category in two ways: 1.Through the density of the word?s relation-.ships with other words ? by enumerating its semantic ties to other words within the field, and calculating membership scores based on the number of these ties; and 2.Through the degree of word membership am-.biguity ? by assessing the inter-annotator agreement on the word membership in this category.Lexicographical entries in the dictionaries, suchas WordNet, seek to establish semantic equivalence between the word and its definition and provide a rich source of human-annotated relationships between the words.By using a bootstrap ping system, such as STEP, that follows the links between the words in WordNet to find similarwords, we can identify the paths connecting mem bers of a given semantic category in the dictionary.With multiple bootstrapping runs on different seed 4It is consistent with the observation by Kim and Hovy (2004) who noticed that, when positives and neutrals were collapsed into the same category opposed to negatives, the agreement between human annotators rose by 12%.5The operationalizations of centrality derived from thenumber of connections between elements can be found in so cial network theory (Burt, 1980) 212lists, we can then produce a measure of the density of such ties.The ambiguity measure de rived from inter-annotator disagreement can then be used to validate the results obtained from the density-based method of determining centrality.In order to produce a centrality measure, we conducted multiple runs with non-intersecting seed lists drawn from HM.The lists of wordsfetched by STEP on different runs partially over lapped, suggesting that the words identified by the system many times as bearing positive or negativesentiment are more central to the respective cate gories.The number of times the word has been fetched by STEP runs is reflected in the Gross Overlap Measure produced by the system.Insome cases, there was a disagreement between dif ferent runs on the sentiment assigned to the word.Such disagreements were addressed by comput ing the Net Overlap Scores for each of the found words: the total number of runs assigning the worda negative sentiment was subtracted from the to tal of the runs that consider it positive.Thus, the greater the number of runs fetching the word (i.e.,Gross Overlap) and the greater the agreement be tween these runs on the assigned sentiment, the higher the Net Overlap Score of this word.The Net Overlap scores obtained for each iden tified word were then used to stratify these wordsinto groups that reflect positive or negative dis tance of these words from the zero score.The zero score was assigned to (a) the WordNet adjectivesthat were not identified by STEP as bearing posi tive or negative sentiment 6 and to (b) the words with equal number of positive and negative hits on several STEP runs.The performance measuresfor each of the groups were then computed to al low the comparison of STEP and human annotator performance on the words from the core and from the periphery of the sentiment category.Thus, foreach of the Net Overlap Score groups, both automatic (STEP) and manual (HM) sentiment annota tions were compared to human-annotated GI-H4,which was used as a gold standard in this experi ment.On 58 runs, the system has identified 3, 908English adjectives as positive, 3, 905 as nega tive, while the remainder (14, 428) of WordNet?s 22, 141 adjectives was deemed neutral.Of these 14, 328 adjectives that STEP runs deemed neutral,6The seed lists fed into STEP contained positive or neg ative, but no neutral words, since HM, which was used as a source for these seed lists, does not include any neutrals.Figure 1: Accuracy of word sentiment tagging.884 were also found in GI-H4 and/or HM lists, which allowed us to evaluate STEP performance and HM-GI agreement on the subset of neutrals as well.The graph in Figure 1 shows the distributionof adjectives by Net Overlap scores and the aver age accuracy/agreement rate for each group.Figure 1 shows that the greater the Net Over lap Score, and hence, the greater the distance of the word from the neutral subcategory (i.e., from zero), the more accurate are STEP results and thegreater is the agreement between two teams of hu man annotators (HM and GI-H4).On average, for all categories, including neutrals, the accuracy of STEP vs. GI-H4 was 66.5%, human-annotated HM had 78.7% accuracy vs. GI-H4.For the words with Net Overlap of ?7 and greater, both STEPand HM had accuracy around 90%.The accu racy declined dramatically as Net Overlap scores approached zero (= Neutrals).In this category,human-annotated HM showed only 20% agree ment with GI-H4, while STEP, which deemedthese words neutral, rather than positive or neg ative, performed with 57% accuracy.These results suggest that the two measures ofword centrality, Net Overlap Score based on mul tiple STEP runs and the inter-annotator agreement (HM vs. GI-H4), are directly related 7.Thus, the Net Overlap Score can serve as a useful tool in the identification of core and peripheral membersof a fuzzy lexical category, as well as in predic 7In our sample, the coefficient of correlation between thetwo was 0.68.The Absolute Net Overlap Score on the sub groups 0 to 10 was used in calculation of the coefficient of correlation.213tion of inter-annotator agreement and system per formance on a subgroup of words characterized by a given Net Overlap Score value.In order to make the Net Overlap Score measure usable in sentiment tagging of texts and phrases,the absolute values of this score should be normalized and mapped onto a standard [0, 1] inter val.Since the values of the Net Overlap Score may vary depending on the number of runs used inthe experiment, such mapping eliminates the vari ability in the score values introduced with changesin the number of runs performed.In order to ac complish this normalization, we used the value ofthe Net Overlap Score as a parameter in the stan dard fuzzy membership S-function (Zadeh, 1975; Zadeh, 1987).This function maps the absolute values of the Net Overlap Score onto the interval from 0 to 1, where 0 corresponds to the absence of membership in the category of sentiment (in our case, these will be the neutral words) and 1 reflects the highest degree of membership in this category.The function can be defined as follows: S(u;?, ?, ?) = ? ?0 for u ? ?2(u??)2 for?u ? ?1?2(u??)2 for ? ?u ? ?1 for u ? ?where u is the Net Overlap Score for the word and ?, ?, ? are the three adjustable parameters: ? is set to 1, ? is set to 15 and ?, which represents a crossover point, is defined as ? = (?+ ?)/2 = 8.Defined this way, the S-function assigns highest degree of membership (=1) to words that have the the Net Overlap Score u ? 15.The accuracy vs. GI-H4 on this subset is 100%.The accuracy goes down as the degree of membership decreases and reaches 59% for values with the lowest degrees of membership.This paper contributes to the development of NLP and semantic tagging systems in several respects.The structure of the semantic category of sentiment.The analysis of the category of sentiment of English adjectives presented here suggests that this category is structured as a fuzzy set: the distance from the coreof the category, as measured by Net Over lap scores derived from multiple STEP runs,is shown to affect both the level of interannotator agreement and the system perfor mance vs. human-annotated gold standard.The list of sentiment-bearing adjectives.The list produced and cross-validated by multipleSTEP runs contains 7, 814 positive and negative English adjectives, with an average ac curacy of 66.5%, while the human-annotated list HM performed at 78.7% accuracy vs. the gold standard (GI-H4) 8.The remaining14, 328 adjectives were not identified as sen timent marked and therefore were considered neutral.The stratification of adjectives by their Net Overlap Score can serve as an indicatorof their degree of membership in the cate gory of (positive/negative) sentiment.Since low degrees of membership are associated with greater ambiguity and inter-annotator disagreement, the Net Overlap Score valuecan provide researchers with a set of vol ume/accuracy trade-offs.For example, by including only the adjectives with the Net Overlap Score of 4 and more, the researchercan obtain a list of 1, 828 positive and negative adjectives with accuracy of 81% vs. GI H4, or 3, 124 adjectives with 75% accuracy if the threshold is set at 3.The normalization of the Net Overlap Score values for the use inphrase and text-level sentiment tagging systems was achieved using the fuzzy member ship function that we proposed here for the category of sentiment of English adjectives.Future work in the direction laid out by thisstudy will concentrate on two aspects of sys tem development.First further incremental improvements to the precision of the STEPalgorithm will be made to increase the ac curacy of sentiment annotation through the use of adjective-noun combinatorial patterns within glosses.Second, the resulting list of adjectives annotated with sentiment and withthe degree of word membership in the cate gory (as measured by the Net Overlap Score) will be used in sentiment tagging of phrases and texts.This will enable us to compute the degree of importance of sentiment markers found in phrases and texts.The availability 8GI-H4 contains 1268 and HM list has 1336 positive andnegative adjectives.The accuracy figures reported here in clude the errors produced at the boundary with neutrals.214of the information on the degree of central ity of words to the category of sentiment mayimprove the performance of sentiment determination systems built to identify the senti ment of entire phrases or texts.?System evaluation considerations.The con tribution of this paper to the developmentof methodology of system evaluation is twofold.First, this research emphasizes the im portance of multiple runs on different seedlists for a more accurate evaluation of senti ment tag extraction system performance.Wehave shown how significantly the system re sults vary, depending on the composition of the seed list.Second, due to the high cost of manual an notation and other practical considerations, most bootstrapping and other NLP systems are evaluated on relatively small manually annotated gold standards developed for agiven semantic category.The implied assumption is that such a gold standard represents a random sample drawn from the pop ulation of all category members and hence, system performance observed on this goldstandard can be projected to the whole se mantic category.Such extrapolation is notjustified if the category is structured as a lex ical field with fuzzy boundaries: in this casethe precision of both machine and human annotation is expected to fall when more peripheral members of the category are pro cessed.In this paper, the sentiment-bearing words identified by the system were stratifiedbased on their Net Overlap Score and evaluated in terms of accuracy of sentiment an notation within each stratum.These strata, derived from Net Overlap scores, reflect the degree of centrality of a given word to the semantic category, and, thus, provide greater assurance that system performance on other words with the same Net Overlap Score will be similar to the performance observed on the intersection of system results with the gold standard.?The role of the inter-annotator disagree ment.The results of the study presented in this paper call for reconsideration of the roleof inter-annotator disagreement in the devel opment of lists of words manually annotated with semantic tags.It has been shown here that the inter-annotator agreement tends to fall as we proceed from the core of a fuzzysemantic category to its periphery.Therefore, the disagreement between the annota tors does not necessarily reflect a quality problem in human annotation, but rather a structural property of the semantic category.This suggests that inter-annotator disagree ment rates can serve as an important source of empirical information about the structural properties of the semantic category and canhelp define and validate fuzzy sets of seman tic category members for a number of NLP tasks and applications.
In the last few years, so called finite-state morphology, in general, and two-level morphology in particular, have become widely accepted as paradigms for the computational treatment of morphology.Finite-state morphology appeals to the notion of a finite-state transducer, which is simply a classical finite-state automaton whose transitions are labeled with pairs, rather than with single symbols.The automaton operates on a pair of tapes and advances over a given transition if the current symbols on the tapes match the pair on the transition.One member of the pair of symbols on a transition can be the designated null symbol, which we will write c. When this appears, the corresponding tape is not examined, and it does not advance as the machine moves to the next state.Finite-state morphology originally arose out of a desire to provide ways of analyzing surface forms using grammars expressed in terms of systems of ordered rewriting rules.Kaplan and Kay (in preparation) observed, that finite-state transducers could be used to mimic a large class of rewriting rules, possibly including all those required for phonology.The importance of this came from two considerations.First, transducers are indifferent as to the direction in which they are applied.In other words, they can be used with equal facility to translate between tapes, in either direction, to accept or reject pairs of tapes, or to generate pairs of tapes.Second, a pair of transducers with one tape in common is equivalent to a single transducer operating on the remaining pair of tapes.A simple algorithm exists for constructing the transition diagram for this composite machine given those of the original pair.By repeated application of this algorithm, it is therefore possible to reduce a cascade of transducers, each linked to the next by a common tape, to a single transducer which accepts exactly the same pair of tapes as was accepted by the original cascade as a whole.From these two facts together, it follows that an arbitrary ordered set of rewriting rules can be modeled by a finite-state transducer which can be automatically constructed from them and which serves as well for analyzing surface forms as for generating them from underlying lexical strings.A transducer obtained from an ordered set of rules in the way just outlined is a two level device in the sense that it mediates directly between lexical and surface forms without ever constructing the intermediate forms that would arise in the course of applying the original rules one by one.The term two-level morphology, however, is used in a more restricted way, to apply to a system in which no intermediate forms are posited, even in the original grammatical formalism.The writer of a grammar using a two-level formalism never needs to think in terms of any representations other than the lexical and the surface ones.What he does is to specify, using one formalism or another, a set of transducers, each of which mediates directly between these forms and each of which restricts the allowable pairs of strings in some way.The pairs that the system as a whole accepts are those are those that are rejected by none of the component transducers, modulo certain assumptions about the precise way in which they interact, whose details need not concern us.Once again, there is a formal procedure that can be used to combine the set of transducers that make up such a system into a single automaton with the same overall behavior, so that the final result is indistinguishable form that obtained from a set of ordered rules.However it is an advantage of parallel machines that they can be used with very little loss of efficiency without combining them in this way.While it is not the purpose of this paper to explore the formal properties of finite-state transducers, a brief excursion may be in order at this point to forestall a possible objection to the claim that a parallel configuration of transducers can be combined into a single one.On the face of it, this cannot generally be so because there is generally no finite-state transducer that will accept the intersection of the sets of tape pairs accepted by an arbitrary set of transducers.It is, for example, easy to design a transducer that will map a string of x's onto the same number of y's followed by an arbitrary number of z's.It is equally easy to design one that maps a string of x's onto the same number of z's preceded by an arbitrary number of x's.The intersection of these two sets contains just those pairs with some number of x's on one tape, and that same number of y's followed by the same number of z's on the other tape.The set of second tapes therefore contains a context-free language which it is clearly not within the power of any finite-state device to generate.Koskenniemi overcame this objection in his original work by adopting the view that all the transducers in the parallel configuration should share the same pair or read-write heads.The effect of this is to insist that they not only accept the same pairs of tapes, but that they agree on the particular sequence of symbol pairs that must be rehearsed in the course of accepting each of them.Kaplan has been able to put a more formal construction on this in the following way Let the empty symbols appearing in the pairs labeling any transition in the transducers be replaced by some ordinary symbol not otherwise part of the alphabet.The new set of transducers derived in this way clearly do not accept the same pairs of tapes as the original ones did, but there is an algorithm for constructing a single finite-state transducer that will accept the intersection of the pairs they all accept.Suppose, now, that this configuration of parallel transducers is put in series with two other standard transducers, one which carries the real empty symbol onto its surrogate, and everything else onto itself, and another transducer that carries the surrogate onto the real empty symbol, then the resulting configuration accepts just the desired set of languages, all of which are also acceptable by single transducers that can be algorithmically derived form the originals.It may well appear that the systems we have been considering properly belong to finite-state phonology or graphology, and not to morphology, properly construed.Computational linguists have indeed often been guilty of some carelessness in their use of this terminology.But it is not hard to see how it could have arisen.The first step in any process that treats natural text is to recognize the words it contains, and this generally involves analyzing each of them in terms of a constituent set of formatives of some kind.Most important among the difficulties that this entails are those having to do with the different shapes that formatives assume in different environments.In other words, the principal difficulties of morphological analysis are in fact phonological or graphological.The inventor of two-level morphology, Kimmo Koskenniemi, is fact provided a finite-state account not just of morphophonemics (or morphographemics), but also of morphotactics.He took it that the allowable set of words simply constituted a regular set of morheme sequences.This is probably the more controversial part of his proposal, but it is also the less technically elaborate, and therefore the one that has attracted less attention.As a result, the term &quot;two-level morphology&quot; has come to be commonly accepted as applying to any system of word recognition that involves two-level, finite-state, phonology or graphology.The approach to nonconcatenative morphology to be outlined in this paper will provide a more unified treatment of morphophonemics and morphotactics than has been usual I shall attempt to show how a two-level account might be given of nonconcatenative morphological phenomena, particularly those exhibited in the Semitic languages.The approach I intend to take is inspired, not only by finite-state morphology, broadly construed, but equally by autosegmental phonology as proposed by Goldsmith (1979) and the autosegmental morphology of McCarthy (1979) All the data that I have used in this work is taken from McCarthy (1979) and my debt to him will be clear throughout. forms that can be constructed on the basis of each of the stems shown.However, there is every reason to suppose that, though longer and greatly more complex in detail, that enterprise would not require essentially different mechanisms from the ones I shall describe.The overall principles on which the material in Table I is organized are clear from a fairly cursory inspection Each form contains the letters &quot;ktb&quot; somewhere in it.This is the root of the verb meaning &quot;write&quot;.By replacing these three letters with other appropriately chosen I take it as my task to describe how the members of a paradigm like the one in Table I might be generated and recognized effectively and efficiently, and in such a way as to capture and profit from the principal linguistic generalizations inherent in it.Now this is a slightly artificial problem because the forms given in Table I are not in fact words, but only verb stems.To get the verb forms that would be found in Arabic text, we should have to expand the table very considerably to show the inflected sequences of three consonants, we would obtain corresponding paradigms for other roots.With some notable exceptions, the columns of the table contain stems with the same sequence of vowels.Each of these is known as a vocalism and, as the headings of the columns show, these can serve to distinguish perfect from imperfective, active from passive, and the like.Each row of the table is characterized by a particular pattern according to which the vowels and consonants alternate.In other words, it is characteristic of a given row that the vowel in a particular position is long or short, or that a consonant is simple or geminate, or that material in one syllable is repeated in the following one.McCarthy refers to each of these patterns as a prosodic template, a term which I shall take over.Each of them adds a particular semantic component to the basic verb, making it reflexive, causative, or whatever.Our problem, will therefore involve designing an abstract device capable of combining components of these three kinds into a single sequence.Our solution will take the form of a set of one or more finite-state transducers that will work in parallel like those of Koskenniemmi(1983), but on four tapes rather than just two.There will not be space, in this paper, to give a detailed account, even of all the material in Table I, not to mention problems that would arise if we were to consider the full range of Arabic roots.What I do hope to do, however, is to establish a theoretical framework within which solutions to all of these problems could be developed.We must presumably expect the transducers we construct to account for the Arabic data to have transition functions from states and quadruples of symbols to states.In other words, we will be able to describe them with transition diagrams whose edges are labeled with a vector of four symbols.When the automaton moves from one state to another, each of the four tapes will advance over the symbol corresponding to it on the transition that sanctions the move.I shall allow myself some extensions to this basic scheme which will enhance the perspicuity and economy of the formalism without changing its essential character.In particular, these extensions will leave us clearly within the domain of finite-state devices.The extensions have to do with separating the process of reading or writing a symbol on a tape, from advancing the tape to the next position.The quadruples that label the transitions in the transducers we shall be constructing will be elements each consisting of two parts, a symbol, and an instruction concerning the movement of the tape I shall use the following notation for this.A unadorned symbol will be read in the traditional way, namely, as requiring the tape on which that symbol appears to move to the next position as soon as it has been read or written.If the symbol is shown in brackets, on the other hand, the tape will not advance, and the quadruple specifying the next following transition will therefore clearly have to be one that specifies the same symbol for that tape, since the symbol will still be under the read-write head when that transition is taken.With this convention, it is natural to dispense with the e symbol in favor of the notation &quot;[1&quot;, that is, an unspecified symbol over which the corresponding tape does not advance.A symbol can also be written in braces, in which case the corresponding tape will move if the symbol under the read-write head is the last one on the tape.This is intended to capture the notion of spreading, from autosegmental morphology, that is, the principal according to which the last item in a string may be reused when required to fill several positions.A particular set of quadruples, or frames, made up of symbols, with or without brackets or braces, will constitute the alphabet of the automata, and the &quot;useful&quot; alphabet must be the same for all the automata because none of them can move from one state to another unless the others make an exactly parallel transition.Not surprisingly, a considerable amount of information about the language is contained just in the constitution of the alphabet.Indeed, a single machine with one state which all transitions both leave and enter will generate a nontrivial subset of the material in Table I.An example of the steps involved in generating a form that depends only minimally on information embodied in a transducer is given in table II.The eight step are labeled (a) - (h).For each one, a box is shown enclosing the symbols currently under the read-write heads.The tapes move under the heads from the right and then continue to the left No symbols are shown to the right on the bottom tape, because we are assuming that the operation chronicled in these diagrams is one in which a surface form is being generated.The bottom tape—the one containing the surface form—is therefore being written and it is for this reason that nothing appears to the right.The other three tapes, in the order shown, contain the root, the prosodic template, and the vocalism.To the right of the tapes, the frame is shown which sanctions the move that will be made to advance from that position to the next.No such frame is given for the last configuration for the obvious reason that this represents the end of the process.The move from (a) to (b) is sanctioned by a frame in which the root consonant is ignored.There must be a &quot;V&quot; on the template tape and an &quot;a&quot; in the current position of the vocalism.However, the vocalism tape will not move when the automata move to their next states.Finally, there will be an &quot;a&quot; on the tape containing the surface form.In summary, given that the prosodie template calls for a vowel, the next vowel in the vocalism has been copied to the surface.Nondeterministically, the device predicts that this same contribution from the vocalism will also be required to fill a later position.The move from (b) to (c) is sanctioned by a frame in which the vocalism is ignored.The template requires a consonant and the frame accordingly specifies the same consonant on both the root and the surface tapes, advancing both of them.A parallel move, differing only in the identity of the consonant, is made from (c) to (d).The move from (d) to (e) is similar to that from (a) to (b) except that, this time, the vocalism tape does advance.The nondeterministic prediction that is being made in this case is that there will be no further slots for the &quot;a&quot; to fill.Just what it is that makes this the &quot;right&quot; move is a matter to which we shall return.The move from (e) to (f) differs from the previous two moves over root consonants in that the &quot;b&quot; is being &quot;spread&quot;.In other words, the root tape does not move, and this possibility is allowed on the specific grounds that it is the last symbol on the tape.Once again, the automata are making a nondeterministic decision, this time that there will be another consonant called for later by the prosodic template and which it will be possible to fill only if this last entry on the root tape does not move away.The moves from (f) to (g) and from (g) to (h) are like those from (d) to (e) and (b) to (c) respectively.Just what is the force of the remark, made from time to time in this commentary, that a certain move is made nondeterministically?These are all situations in which some other move was, in fact, open to the transducers but where the one displayed was carefully chosen to be the one that would lead to the correct result.Suppose that, instead of leaving the root tape stationary in the move from (e) to (f), it had been allowed to advance using a frame parallel to the one used in the moves from (b) to (c) and (c) to (d), a frame which it is only reasonable to assume must exist for all consonants, including &quot;b&quot;.The move from (f) to (g) could still have been made in the same way, but this would have led to a configuration in which a consonant was required by the prosodic template, but none was available from the root.A derivation cannot be allowed to count as complete until all tapes are exhausted, so the automata would have reached an impasse.We must assume that, when this happens, the automata are able to return to a preceding situation in which an essentially arbitrarily choice was made, and try a different alternative.Indeed, we must assume that a general backtracking strategy is in effect, which ensures that all allowable sequences of choices are explored.Now consider the nondeterministic choice that was made in the move from (a) to (b), as contrasted with the one made under essentially indistinguishable circumstances from (d) to (e) If the vocalism tape had advanced in the first of these situations, but not in the second, we should presumably have been able to generate the putative form &quot;aktibib&quot;, which does not exist.This can be excluded only if we assume that there is a transducer that disallows this sequence of events, or if the frames available for &quot;i&quot; are not the same as those for &quot;a&quot;.We are, in fact, making the latter assumption, on the grounds that the vowel &quot;i&quot; occurs only in the final position of Arabic verb stems.Consider, now, the forms in rows II and V of table I.In each of these, the middle consonant of the root is geminate in the surface.This is not a result of spreading as we have described it, because spreading only occurs with the last consonant of a root.If the prosodic template for row II is &quot;CVCCVC&quot;, how is that we do not get forms like &quot;katbab&quot; and &quot;kutbib&quot; beside the ones shown?This is a problem that is overcome in McCarthy's autosegmental account only at considerable cost.Indeed, is is a deficiency of that formalism that the only mechanisms available in it to account for gemination are as complex as they are, given how common the phenomenon is.Within the framework proposed here, gemination is provided for in a very natural way.Consider the following pair of frame schemata, in which c is and arbitrary consonant: The First of these is the one that was used for the consonants in the above example except in the situation for the first occurrence of &quot;b&quot;, where is was being spread into the final two consonantal positions of the form.The second frame differs from this is two respects.First, the prosodic template contains the hitherto unused symbol &quot;G&quot;. for &quot;geminate&quot;, and second, the root tape is not advanced.Suppose, now, that the the prosodic template for forms like &quot;kattab&quot; is not &quot;CVCCVC&quot;, but &quot;CVGCVC&quot;.It will be possible to discharge the &quot;G&quot; only if the root template does not advance, so that the following &quot;C&quot; in the template can only cause the same consonant to be inserted into the word a second time.The sequence &quot;GC&quot; in a prosodic template is therefore an idiom for consonant gemination.Needless to say, McCarthy's work, on which this paper is based, is not interesting simply for the fact that he is able to achieve an adequate description of the data in table I, but also for the claims he makes about the way that account extends to a wider class of phenomena, thus achieving a measure of explanatory power.In particular, he claims that it extends to roots with two and four consonants.Consider, in particular, the following sets of forms: ktanbab dhanraj kattab dahraj takattab tadahraj Those in the second column are based on the root /dhrj/.In the first column are the corresponding forms of /ktb/.The similarity in the sets of corresponding forms is unmistakable.They exhibit the same patterns of consonants and vowels, differing only in that, whereas some consonant appears twice in the forms in column one, the consonantal slots are all occupied by different segments in the forms on the right.For these purposes, the &quot;n&quot; of the first pair of forms should be ignored since it is contributed by the prosodic template, and not by the root. consonantal slot in the prosodic template only in the case of the shorter form.The structure of the second and third forms is equally straighforward, but it is less easy to see how our machinery could account for them.Once again, the template calls for four root consonants and, where only three are provided, one must do double duty.But in this case, the effect is achieved through gemination rather than spreading so that the gemination mechanism just outlined is presumably in play.That mechanism makes no provision for gemination to be invoked only when needed to fill slots in the prosodic template that would otherwise remain empty.If the mechanism were as just described, and the triliteral forms were &quot;CVGCVC&quot; and &quot;tVCVGCVC&quot; respectively, then the quadriliteral forms would have to be generated on a different base.It is in cases like this, of which there in fact many, that the finite-state transducers play a substantive role.What is required in this case is a transducer that allows the root tape to remain stationary while the template tape moves over a &quot;G&quot;, provided no spreading will be allowed to occur later to fill consonantal slots that would Given a triliteral and a quadriliteral root, otherwise he unclaimed.If extra consonants are the first pair are exactly as one would expect—the final root consonant is spread to fill the final required, then the first priority must he to let them occupy the slots marked with a &quot;0&quot; in the template.Fig.1 shows a schema for the transition diagram of a transducer that has this effect.I call it a &quot;schema&quot; only because each of the edges shown does duty for a number of actual transitions.The machine begins in the &quot;start&quot; state and continues to return there so long as no frame is encountered involving a &quot;G&quot; on the template tape.A &quot;G&quot; transition causes a nondeterministic choice.If the root tape moves at the same time as the &quot;G&quot; is scanned, the transducer goes into its &quot;no-spread&quot; state, to which it continues to return so long as every move over a &quot;C&quot; on the prosodic tape is accompanied by a move over a consonant on the root tape.In other words, it must be possible to complete the process without spreading consonants.The other alternative is that the transducer should enter the &quot;geminate&quot; state over a transition over a in the template with the root tape remaining stationary.The transitions at the &quot;geminate&quot; state allow both spreading and nonspreading transitions.In summary, spreading can occur only if the transducer never leaves the &quot;start&quot; state and there is no &quot;G&quot; in the template, or there is a &quot;G&quot; on the template which does not trigger gemination.A &quot;G&quot; can fail to trigger gemination only when the root contains enough consonants to fill all the requirements that the template makes for them.One quadriliteral case remains to be accounted for, namely the following: ktaabab dharjaj According to the strategy just elaborated, we should have expected the quadriliteral form to have been &quot;dhaaraj&quot;.But, apparently this form contains a slot that is used for vowel lengthening with triliteral roots, and as consonantal position for quadriliterals.We must therefore presumably take it that the prosodic template for this form is something like &quot;CCVXCVC&quot; where &quot;X&quot; is a segment, but not specified as either vocalic or consonantal.This much is in line with the proposal that McCarthy himself makes The question is, when should be filled by a vowel, and when by a consonant?The data in Table I is, of course, insufficient to answer question. but a plausible answer that strongly suggests itself is that the &quot;X&quot; slot prefers a consonantal filler except where that would result in gemination.If this is true, then it is another case where the notion of gemination, though not actually exemplified in the form, plays a central role.Supposing that the analysis is correct, the next question is, how is it to be implemented.The most appealing answer would be to make &quot;X&quot; the exact obverse of &quot;G&quot;, when filled with a consonant.In other words, when a root consonant fills such a slot, the root tape must advance so that the same consonant will no longer be available to fill the next position.The possibility that the next root consonant would simply be a repetition of the current one would be excluded if we were to take over from autosegmental phonology and morphology, some version of th Obligatory Contour Principle (OCP) (Goldsmith, 1979) which disallows repeated segments except in the prosodic template and in the surface string.McCarthy points out the roots like /smnn/, which appear to violate the OCP can invariably be reanalyzed as biliteral roots like /sm/ and, if this is done, our analysis, like his, goes through.The OCP does seem likely to cause some trouble when we come to treat one of the principal remaining problems, namely that of the forms in row I of table I.It turns out that the vowel that appears in the second syllable of these forms is not provided by the vocalism, but by the root.The vowel that appears in the perfect is generally different from the one that appears in the imperfect, and four different pairs are possible.The pair that is used with a given root is an idiosyncratic property of that root.One possibility is, therefore, that we treat the traditional triliteral roots as consisting not simply of three consonants, but as three consonants with a vowel intervening between the second and third, for a total of four segments.This flies in the face of traditional wisdom.It also runs counter to one of the motivating intuitions of autosegmental phonology which would have it that particular phonological features can be represented on at most one lexical tier, or tape.The intuition is that these tiers or tapes each contain a record or a particular kind of articulatory gesture; from the hearer's point of view, it is as though they contained a record of the signal received from a receptor that was attuned only to certain features.If we wish to maintain this model, there are presumably two alternatives open to us.Both involve assuming that roots are represented on at least two tapes in parallel, with the consonants separate from the vowel.According to one alternative, the root vowel would be written on the same tape as the vocalism; according to the other, it would be on a tape of its own.Unfortunately, neither alternative makes for a particularly happy solution.No problem arises from the proposal that a given morpheme should, in general, be represented on more than one lexical tape.However, the idea that the vocalic material associated with a root should appear on a special tape, reserved for it alone, breaks the clean lines of the system as so far presented in two ways.First, it spearates material onto two tapes, specifically the new one and the vocalism, on purely lexical grounds, having nothing to do with their phonetic or phonological constitution, and this runs counter to the idea of tapes as records of activity on phonetically specialized receptors.It is also at least slightly troublesome in that that newly introduced tape fills no function except in the generation of the first row of the table.Neither of these arguments is conclusive, and they could diminish considerably in force as a wider range of data was considered.Representing the vocalic contribution of the root on the same tape as the vacalism would avoid both of these objections, but would require that vocalic contribution to be recorded either before or after the vocalism itself.Since the root vowel affects the latter part of the root, it seems reasonable that it should be positioned to the right.Notice, however, that this is the only instance in which we have had to make any assumptions about the relative ordering of the morphemes that contribute to a stem.Once again, it may be possible to assemble further evidence reflecting on some such ordering, but I do not see it in these data.It is only right that I should point out the difficulty of accounting satisfactorily for the vocalic contribution of verbal roots.It is only right that I should also point out that the autosegmental solution fares no better on this score, resorting, as it must, to rules that access essentially non-phonological properties of the morphemes involved.By insisting that what I have called the spelling of a morpheme should by, by definition, be its only contribution to phonological processes, I have cut myself off from any such deus ex machina.Linguists in general, and computational linguists in particular, do well to employ finite-state devices wherever possible.They are theoretically appealing because they are computational weak and best understood from a mathematical point of view.They are computationally appealing because they make for simple, elegant, and highly efficient implementaions.In this paper, I hope I have shown how they can be applied to a problem in nonconcatenative morphology which seems initially to require heavier machinary.
A working definition of coreference resolution is partitioning the noun phrases we are interested in into equiv alence classes, each of which refers to a physical entity.We adopt the terminologies used in the Automatic Con tent Extraction (ACE) task (NIST, 2003a) and call eachindividual phrase a mention and equivalence class an en tity.For example, in the following text segment, (1): ?The American Medical Association voted yesterday to install the heir apparent as its president-elect, rejecting a strong, upstart challenge by a district doctor who argued that the nation?s largest physicians?group needs stronger ethics and new leadership.?mentions are underlined, ?American Medical Associa tion?, ?its?and ?group?refer to the same organization(object) and they form an entity.Similarly, ?the heir ap parent?and ?president-elect?refer to the same person and they form another entity.It is worth pointing out that the entity definition here is different from what used in the Message Understanding Conference (MUC) task (MUC, 1995; MUC, 1998) ? ACE entity is called coreference chain or equivalence class in MUC, and ACE mention is called entity in MUC.An important problem in coreference resolution is how to evaluate a system?s performance.A good performance metric should have the following two properties:
The Web contains a wealth of opinions about products, politicians, and more, which are expressed in newsgroupposts, review sites, and elsewhere.As a result, the prob lem of ?opinion mining?has seen increasing attention over the last three years from (Turney, 2002; Hu and Liu, 2004) and many others.This paper focuses on product reviews, though our methods apply to a broader range of opinions.Product reviews on Web sites such as amazon.com and elsewhere often associate meta-data with each review indicating how positive (or negative) it is using a 5-starscale, and also rank products by how they fare in the re views at the site.However, the reader?s taste may differ from the reviewers?.For example, the reader may feel strongly about the quality of the gym in a hotel, whereasmany reviewers may focus on other aspects of the ho tel, such as the decor or the location.Thus, the reader is forced to wade through a large number of reviews looking for information about particular features of interest.We decompose the problem of review mining into the following main subtasks: I. Identify product features.II.Identify opinions regarding product features.III.Determine the polarity of opinions.IV.Rank opinions based on their strength.This paper introduces OPINE, an unsupervised infor mation extraction system that embodies a solution to eachof the above subtasks.OPINE is built on top of the Know ItAll Web information-extraction system (Etzioni et al, 2005) as detailed in Section 3.Given a particular product and a corresponding set of reviews, OPINE solves the opinion mining tasks outlinedabove and outputs a set of product features, each accom panied by a list of associated opinions which are ranked based on strength (e.g., ?abominable?is stronger than?bad).This output information can then be used to gen erate various types of opinion summaries.This paper focuses on the first 3 review mining sub tasks and our contributions are as follows: 1.We introduce OPINE, a review-mining system whose.novel components include the use of relaxation labeling to find the semantic orientation of words in the context of given product features and sentences.review-mining system (Hu and Liu, 2004) and find that OPINE?s precision on the feature extraction task is 22% better though its recall is 3% lower on Hu?s data sets.We show that 1/3 of this increase in precision comes from using OPINE?s feature assessment mechanism on review data while the rest is due to Web PMI statistics.3.While many other systems have used extracted opin-.ion phrases in order to determine the polarity of sentences or documents, OPINE is the first to report its precision andrecall on the tasks of opinion phrase extraction and opin ion phrase polarity determination in the context of known product features and sentences.On the first task, OPINEhas a precision of 79% and a recall of 76%.On the sec ond task, OPINE has a precision of 86% and a recall of 89%.339 Input: product class C, reviews R. Output: set of [feature, ranked opinion list] tuples R??parseReviews(R); E?findExplicitFeatures(R?, C); O?findOpinions(R?, E); CO? clusterOpinions(O); I?findImplicitFeatures(CO, E); RO?rankOpinions(CO); {(f , oi, ...oj)...}?outputTuples(RO, I ? E); Figure 1: OPINE Overview.The remainder of this paper is organized as follows: Section 2 introduces the basic terminology, Section 3 gives an overview of OPINE, describes and evaluates its main components, Section 4 describes related work and Section 5 presents our conclusion.A product class (e.g., Scanner) is a set of products (e.g.,Epson1200).OPINE extracts the following types of prod uct features: properties, parts, features of product parts, related concepts, parts and properties of related concepts(see Table 1 for examples of such features in the Scan ner domains).Related concepts are concepts relevant to the customers?experience with the main product (e.g.,the company that manufactures a scanner).The relation ships between the main product and related concepts are typically expressed as verbs (e.g., ?Epson manufacturesscanners?)or prepositions (?scanners from Epson?).Features can be explicit (?good scan quality?)or im plicit (?good scans?implies good ScanQuality).OPINE also extracts opinion phrases, which are adjec tive, noun, verb or adverb phrases representing customer opinions.Opinions can be positive or negative and vary in strength (e.g., ?fantastic?is stronger than ?good?).This section gives an overview of OPINE (see Figure 1)and describes its components and their experimental eval uation.Goal Given product class C with instances I and reviews R, OPINE?s goal is to find a set of (feature, opin ions) tuples {(f, oi, ...oj)} s.t. f ? F and oi, ...oj ? O, where: a) F is the set of product class features in R. b) O is the set of opinion phrases in R. c) f is a feature of a particular product instance.d) o is an opinion about f in a particular sentence.d) the opinions associated with each feature f are ranked based on their strength.Solution The steps of our solution are outlined in Figure 1 above.OPINE parses the reviews using MINI PAR (Lin, 1998) and applies a simple pronoun-resolution module to parsed review data.OPINE then uses the datato find explicit product features (E).OPINE?s Feature As sessor and its use of Web PMI statistics are vital for the extraction of high-quality features (see 3.2).OPINE then identifies opinion phrases associated with features in Eand finds their polarity.OPINE?s novel use of relaxationlabeling techniques for determining the semantic orien tation of potential opinion words in the context of given features and sentences leads to high precision and recall on the tasks of opinion phrase extraction and opinion phrase polarity extraction (see 3.3).In this paper, we only focus on the extraction of explicit features, identifying corresponding customer opin ions about these features and determining their polarity.We omit the descriptions of the opinion clustering, im plicit feature generation and opinion ranking algorithms.3.0.1 The KnowItAll System.OPINE is built on top of KnowItAll, a Web-based,domain-independent information extraction system (Et zioni et al, 2005).Given a set of relations of interest,KnowItAll instantiates relation-specific generic extrac tion patterns into extraction rules which find candidate facts.KnowItAll?s Assessor then assigns a probability to each candidate.The Assessor uses a form of Point-wiseMutual Information (PMI) between phrases that is esti mated from Web search engine hit counts (Turney, 2001).It computes the PMI between each fact and automatically generated discriminator phrases (e.g., ?is a scanner?for the isA() relationship in the context of the Scanner class).Given fact f and discriminator d, the computed PMI score is: PMI(f, d) = Hits(d+ f )Hits(d)?Hits(f ) The PMI scores are converted to binary features for aNaive Bayes Classifier, which outputs a probability asso ciated with each fact (Etzioni et al, 2005).3.1 Finding Explicit Features.OPINE extracts explicit features for the given productclass from parsed review data.First, the system recur sively identifies both the parts and the properties of the given product class and their parts and properties, in turn,continuing until no candidates are found.Then, the sys tem finds related concepts as described in (Popescu et al., 2004) and extracts their parts and properties.Table 1 shows that each feature type contributes to the set of final features (averaged over 7 product classes).Explicit Features Examples % Total Properties ScannerSize 7% Parts ScannerCover 52% Features of Parts BatteryLife 24% Related Concepts ScannerImage 9% Related Concepts?Features ScannerImageSize 8% Table 1: Explicit Feature Information 340In order to find parts and properties, OPINE first ex tracts the noun phrases from reviews and retains thosewith frequency greater than an experimentally set threshold.OPINE?s Feature Assessor, which is an instantia tion of KnowItAll?s Assessor, evaluates each noun phrase by computing the PMI scores between the phrase and meronymy discriminators associated with the product class (e.g., ?of scanner?, ?scanner has?, ?scanner comeswith?, etc. for the Scanner class).OPINE distinguishes parts from properties using WordNet?s IS-A hi erarchy (which enumerates different kinds of properties) and morphological cues (e.g., ?-iness?, ?-ity?suffixes).3.2 Experiments: Explicit Feature Extraction.In our experiments we use sets of reviews for 7 product classes (1621 total reviews) which include the pub licly available data sets for 5 product classes from (Huand Liu, 2004).Hu?s system is the review mining sys tem most relevant to our work.It uses association rulemining to extract frequent review noun phrases as features.Frequent features are used to find potential opinion words (only adjectives) and the system uses Word Net synonyms/antonyms in conjunction with a set of seedwords in order to find actual opinion words.Finally, opinion words are used to extract associated infrequent fea tures.The system only extracts explicit features.On the 5 datasets in (Hu and Liu, 2004), OPINE?s precision is 22% higher than Hu?s at the cost of a 3% re call drop.There are two important differences between OPINE and Hu?s system: a) OPINE?s Feature Assessor uses PMI assessment to evaluate each candidate feature and b) OPINE incorporates Web PMI statistics in addition to review data in its assessment.In the following, we quantify the performance gains from a) and b).a) In order to quantify the benefits of OPINE?s Feature Assessor, we use it to evaluate the features extracted by Hu?s algorithm on review data (Hu+A/R).The Feature Assessor improves Hu?s precision by 6%.b) In order to evaluate the impact of using Web PMI statistics, we assess OPINE?s features first on reviews (OP/R) and then on reviews in conjunction with the Web (the corresponding methods are Hu+A/R+W andOPINE).Web PMI statistics increase precision by an av erage of 14.5%.Overall, 1/3 of OPINE?s precision increase over Hu?s system comes from using PMI assessment on reviews and the other 2/3 from the use of the Web PMI statistics.In order to show that OPINE?s performance is robustacross multiple product classes, we used two sets of reviews downloaded from tripadvisor.com for Hotels and amazon.com for Scanners.Two annotators la beled a set of unique 450 OPINE extractions as correct or incorrect.The inter-annotator agreement was 86%.The extractions on which the annotators agreed were usedto compute OPINE?s precision, which was 89%.Fur Data Explicit Feature Extraction: Precision Hu Hu+A/R Hu+A/R+W OP/R OPINE D1 0.75 +0.05 +0.17 +0.07 +0.19 D2 0.71 +0.03 +0.19 +0.08 +0.22 D3 0.72 +0.03 +0.25 +0.09 +0.23 D4 0.69 +0.06 +0.22 +0.08 +0.25 D5 0.74 +0.08 +0.19 +0.04 +0.21 Avg 0.72 +0.06 + 0.20 +0.07 +0.22Table 2: Precision Comparison on the Explicit Feature Extraction Task.OPINE?s precision is 22% better than Hu?sprecision; Web PMI statistics are responsible for 2/3 of the pre cision increase.All results are reported with respect to Hu?s. Data Explicit Feature Extraction: Recall Hu Hu+A/R Hu+A/R+W OP/R OPINE D1 0.82 -0.16 -0.08 -0.14 -0.02 D2 0.79 -0.17 -0.09 -0.13 -0.06 D3 0.76 -0.12 -0.08 -0.15 -0.03 D4 0.82 -0.19 -0.04 -0.17 -0.03 D5 0.80 -0.16 -0.06 -0.12 -0.02 Avg 0.80 -0.16 -0.07 -0.14 -0.03Table 3: Recall Comparison on the Explicit Feature Extraction Task.OPINE?s recall is 3% lower than the recall of Hu?s original system (precision level = 0.8).All results are reported with respect to Hu?s. thermore, the annotators extracted explicit features from800 review sentences (400 for each domain).The inter annotator agreement was 82%.OPINE?s recall on the set of 179 features on which both annotators agreed was 73%.3.3 Finding Opinion Phrases and Their Polarity.This subsection describes how OPINE extracts potentialopinion phrases, distinguishes between opinions and non opinions, and finds the polarity of each opinion in thecontext of its associated feature in a particular review sen tence.3.3.1 Extracting Potential Opinion PhrasesOPINE uses explicit features to identify potential opinion phrases.Our intuition is that an opinion phrase as sociated with a product feature will occur in its vicinity.This idea is similar to that of (Kim and Hovy, 2004) and (Hu and Liu, 2004), but instead of using a window of size k or the output of a noun phrase chunker, OPINE takes advantage of the syntactic dependencies computed by theMINIPAR parser.Our intuition is embodied by 10 ex traction rules, some of which are shown in Table 4.If an explicit feature is found in a sentence, OPINE applies the extraction rules in order to find the heads of potentialopinion phrases.Each head word together with its modi 341 fiers is returned as a potential opinion phrase1.Extraction Rules Examples if ?(M,NP = f)?po = M (expensive) scanner if ?(S = f, P,O)?po = O lamp has (problems) if ?(S, P,O = f)?po = P I (hate) this scanner if ?(S = f, P,O)?po = P program (crashed) Table 4: Examples of Domain-independent Rules forthe Extraction of Potential Opinion Phrases.Nota tion: po=potential opinion, M=modifier, NP=noun phrase,S=subject, P=predicate, O=object.Extracted phrases are en closed in parentheses.Features are indicated by the typewriter font.The equality conditions on the left-hand side use po?s head.Rule Templates Rules dep(w,w?) m(w,w?) ?v s.t. dep(w, v), dep(v, w?)?v s.t. m(w, v), o(v, w?)?v s.t. dep(w, v), dep(w?, v) ?v s.t. m(w, v), o(w?, v) Table 5: Dependency Rule Templates For Finding Words w, w?with Related SO Labels . OPINE instantiates these templates in order to obtain extraction rules.Notation: dep=dependent, m=modifier, o=object, v,w,w?=words.OPINE examines the potential opinion phrases in order to identify the actual opinions.First, the system finds thesemantic orientation for the lexical head of each poten tial opinion phrase.Every phrase whose head word has a positive or negative semantic orientation is then retained as an opinion phrase.In the following, we describe how OPINE finds the semantic orientation of words.3.3.2 Word Semantic Orientation OPINE finds the semantic orientation of a word w in the context of an associated feature f and sentence s. We restate this task as follows: Task Given a set of semantic orientation (SO) labels ({positive, negative, neutral}), a set of reviews and a set of tuples (w, f , s), where w is a potential opinion word associated with feature f in sentence s, assign a SO label to each tuple (w, f , s).For example, the tuple (sluggish, driver, ?I am not happy with this sluggish driver?)would be assigned a negative SO label.Note: We use ?word?to refer to a potential opinion word w and ?feature?to refer to the word or phrase which represents the explicit feature f . Solution OPINE uses the 3-step approach below: 1.Given the set of reviews, OPINE finds a SO label for.each word w. 2.Given the set of reviews and the set of SO labels for.words w, OPINE finds a SO label for each (w, f ) pair.1The (S,P,O) tuples in Table 4 are automatically generated from MINIPAR?s output.3.Given the set of SO labels for (w, f ) pairs, OPINE.finds a SO label for each (w, f , s) input tuple.Each of these subtasks is cast as an unsupervised col lective classification problem and solved using the samemechanism.In each case, OPINE is given a set of objects (words, pairs or tuples) and a set of labels (SO labels); OPINE then searches for a global assignment of la bels to objects.In each case, OPINE makes use of local constraints on label assignments (e.g., conjunctions and disjunctions constraining the assignment of SO labels to words (Hatzivassiloglou and McKeown, 1997)).A key insight in OPINE is that the problem of searching for a global SO label assignment to words, pairs or tupleswhile trying to satisfy as many local constraints on as signments as possible is analogous to labeling problems in computer vision (e.g., model-based matching).OPINE uses a well-known computer vision technique, relaxation labeling (Hummel and Zucker, 1983), in order to solve the three subtasks described above.3.3.3 Relaxation Labeling Overview Relaxation labeling is an unsupervised classification technique which takes as input: a) a set of objects (e.g., words) b) a set of labels (e.g., SO labels) c) initial probabilities for each object?s possible labels d) the definition of an object o?s neighborhood (a set of other objects which influence the choice of o?s label) e) the definition of neighborhood features f) the definition of a support function for an object labelThe influence of an object o?s neighborhood on its label L is quantified using the support function.The support function computes the probability of the label L being assigned to o as a function of o?s neighborhood fea tures.Examples of features include the fact that a certainlocal constraint is satisfied (e.g., the word nice partic ipates in the conjunction and together with some other word whose SO label is estimated to be positive).Relaxation labeling is an iterative procedure whoseoutput is an assignment of labels to objects.At each itera tion, the algorithm uses an update equation to reestimate the probability of an object label based on its previous probability estimate and the features of its neighborhood.The algorithm stops when the global label assignment stays constant over multiple consecutive iterations.We employ relaxation labeling for the following rea sons: a) it has been extensively used in computer-vision with good results b) its formalism allows for many typesof constraints on label assignments to be used simulta neously.As mentioned before, constraints are integratedinto the algorithm as neighborhood features which influ ence the assignment of a particular label to a particular object.OPINE uses the following sources of constraints: 342 a) conjunctions and disjunctions in the review textb) manually-supplied syntactic dependency rule templates (see Table 5).The templates are automatically instantiated by our system with different dependency re lationships (premodifier, postmodifier, subject, etc.) in order to obtain syntactic dependency rules which find words with related SO labels.c) automatically derived morphological relationships (e.g., ?wonderful?and ?wonderfully?are likely to have similar SO labels).d) WordNet-supplied synonymy, antonymy, IS-A andmorphological relationships between words.For exam ple, clean and neat are synonyms and so they are likely to have similar SO labels.Each of the SO label assignment subtasks previously identified is solved using a relaxation labeling step.In the following, we describe in detail how relaxation labeling is used to find SO labels for words in the given review sets.3.3.4 Finding SO Labels for Words For many words, a word sense or set of senses is used throughout the review corpus with a consistently positive, negative or neutral connotation (e.g., ?great?, ?awful?, etc.).Thus, in many cases, a word w?s SO label in the context of a feature f and sentence s will be the same as its SO label in the context of other features and sentences.In the following, we describe how OPINE?s relaxation la beling mechanism is used to find a word?s dominant SO label in a set of reviews.For this task, a word?s neighborhood is defined as the set of words connected to it through conjunctions,disjunctions and all other relationships previously intro duced as sources of constraints.RL uses an update equation to re-estimate the probability of a word label based on its previous probabil ity estimate and the features of its neighborhood (see Neighborhood Features).At iteration m, let q(w,L)(m) denote the support function for label L of w and let P (l(w) = L)(m) denote the probability that L is the label of w. P (l(w) = L)(m+1) is computed as follows: RL Update Equation (Rangarajan, 2000) P (l(w) = L)(m+1) = P (l(w) = L)(m)(1 + ?q(w,L)(m)) P L?P (l(w) = L ?)(m)(1 + ?q(w,L?)(m)) where L?{pos, neg, neutral} and ? > 0 is an experimentally set constant keeping the numerator and probabilities positive.RL?s output is an assignment of dominant SO labels to words.In the following, we describe in detail the initialization step, the derivation of the support function formula and the use of neighborhood features.RL Initialization Step OPINE uses a version of Turney?s PMI-based approach (Turney, 2003) in order to de rive the initial probability estimates (P (l(w) = L)(0)) for a subset S of the words.OPINE computes a SO score so(w) for each w in S as the difference between the PMI of w with positive keywords (e.g., ?excellent?)and the PMI of w with negative keywords (e.g., ?awful?).When so(w) is small, or w rarely co-occurs with the key words, w is classified as neutral.If so(w) > 0, then w is positive, otherwise w is negative.OPINE then uses the labeled S set in order to compute prior probabilities P (l(w) = L), L ? {pos, neg, neutral} by computing the ratio between the number of words in S labeled Land |S|.Such probabilities are used as initial probabil ity estimates associated with the labels of the remaining words.Support Function The support function computes the probability of each label for word w based on the labels of objects in w?s neighborhood N .Let Ak = {(wj , Lj)|wj ? N} , 0 < k ? 3|N | rep resent one of the potential assignments of labels to the words in N . Let P (Ak)(m) denote the probability of thisparticular assignment at iteration m. The support for la bel L of word w at iteration m is : q(w,L)(m) = 3|N|X k=1 P (l(w) = L|Ak)(m) ? P (Ak)(m)We assume that the labels of w?s neighbors are inde pendent of each other and so the formula becomes: q(w,L)(m) = 3|N|X k=1 P (l(w) = L|Ak)(m)?|N|Y j=1 P (l(wj) = Lj)(m) Every P (l(wj) = Lj)(m) term is the estimate for theprobability that l(wj) = Lj (which was computed at it eration m using the RL update equation).The P (l(w) = L|Ak)(m) term quantifies the influence of a particular label assignment to w?s neighborhood over w?s label.In the following, we describe how we estimate this term.Neighborhood Features Each type of word relationship which constrains the assignment of SO labels to words (synonymy, antonymy, etc.) is mapped by OPINE to a neighborhood feature.Thismapping allows OPINE to use simultaneously use multi ple independent sources of constraints on the label of aparticular word.In the following, we formalize this map ping.Let T denote the type of a word relationship in R (syn onym, antonym, etc.) and let Ak,T represent the labelsassigned by Ak to neighbors of a word w which are con nected to w through a relationship of type T . We have Ak = ? T Ak,T and P (l(w) = L|Ak)(m) = P (l(w) = L| [ T Ak,T )(m) For each relationship type T , OPINE defines a neighborhood feature fT (w,L,Ak,T ) which computes P (l(w) = L|Ak,T ), the probability that w?s label is L given Ak,T (see below).P (l(w) = L| ? T Ak,T )(m) isestimated combining the information from various fea tures about w?s label using the sigmoid function ?(): 343 P (l(w) = L|Ak)(m) = ?( jX i=1 f i(w,L,Ak,i)(m) ? ci) where c0, ...cj are weights whose sum is 1 and which reflect OPINE ?s confidence in each type of feature.Given word w, label L, relationship type T and neigh borhood label assignment Ak, let NT represent the subsetof w?s neighbors connected to w through a type T rela tionship.The feature fT computes the probability that w?s label is L given the labels assigned by Ak to wordsin NT . Using Bayes?s Law and assuming that these la bels are independent given l(w), we have the following formula for fT at iteration m: fT (w,L,Ak,T )(m) = P (l(w) = L)(m)?|NT |Y j=1 P (Lj |l(w) = L) P (Lj |l(w) = L) is the probability that word wj has label Lj if wj and w are linked by a relationship of type T and w has label L. We make the simplifying assumption that this probability is constant and depends only of T , L and L?, not of the particular words wj and w. For each tuple (T , L, Lj), L,Lj ? {pos, neg, neutral}, OPINE buildsa probability table using a small set of bootstrapped pos itive, negative and neutral words.3.3.5 Finding (Word, Feature) SO Labels This subtask is motivated by the existence of frequent words which change their SO label based on associatedfeatures, but whose SO labels in the context of the respec tive features are consistent throughout the reviews (e.g.,in the Hotel domain, ?hot water?has a consistently posi tive connotation, whereas ?hot room?has a negative one).In order to solve this task, OPINE first assigns each (w, f) pair an initial SO label which is w?s SO label.The system then executes a relaxation labeling step duringwhich syntactic relationships between words and, respec tively, between features, are used to update the default SO labels whenever necessary.For example, (hot, room) appears in the proximity of (broken, fan).If ?room?and ?fan?are conjoined by and, this suggests that ?hot?and ?broken?have similar SO labels in the context of their respective features.If ?broken?has a strongly negativesemantic orientation, this fact contributes to OPINE?s be lief that ?hot?may also be negative in this context.Since (hot, room) occurs in the vicinity of other such phrases (e.g., stifling kitchen), ?hot?acquires a negative SO label in the context of ?room?.3.3.6 Finding (Word, Feature, Sentence) SO Labels This subtask is motivated by the existence of (w,f ) pairs (e.g., (big, room)) for which w?s orientation changes based on the sentence in which the pair appears (e.g., ? I hated the big, drafty room because I ended up freezing.?vs. ?We had a big, luxurious room?.)In order to solve this subtask, OPINE first assigns each(w, f, s) tuple an initial label which is simply the SO la bel for the (w, f) pair.The system then uses syntactic relationships between words and, respectively, features in order to update the SO labels when necessary.For example, in the sentence ?I hated the big, drafty room because I ended up freezing.?, ?big?and ?hate?satisfy condition 2 in Table 5 and therefore OPINE expects themto have similar SO labels.Since ?hate?has a strong neg ative connotation, ?big?acquires a negative SO label in this context.In order to correctly update SO labels in this last step, OPINE takes into consideration the presence of negation modifiers.For example, in the sentence ?I don?t like a large scanner either?, OPINE first replaces the positive (w, f) pair (like, scanner) with the negative labeled pair (not like, scanner) and then infers that ?large?is likely to have a negative SO label in this context.3.3.7 Identifying Opinion Phrases After OPINE has computed the most likely SO labels for the head words of each potential opinion phrase in thecontext of given features and sentences, OPINE can ex tract opinion phrases and establish their polarity.Phraseswhose head words have been assigned positive or nega tive labels are retained as opinion phrases.Furthermore,the polarity of an opinion phrase o in the context of a fea ture f and sentence s is given by the SO label assigned to the tuple (head(o), f, s) (3.3.6 shows how OPINE takes into account negation modifiers).3.4 Experiments.In this section we evaluate OPINE?s performance on thefollowing tasks: finding SO labels of words in the context of known features and sentences (SO label extrac tion); distinguishing between opinion and non-opinion phrases in the context of known features and sentences (opinion phrase extraction); finding the correct polarityof extracted opinion phrases in the context of known fea tures and sentences (opinion phrase polarity extraction).While other systems, such as (Hu and Liu, 2004; Tur ney, 2002), have addressed these tasks to some degree, OPINE is the first to report results.We first ran OPINE on 13841 sentences and 538 previously extracted features.OPINE searched for a SO label assignment for 1756 different words in the context of the given features and sentences.We compared OPINE against two baseline meth ods, PMI++ and Hu++.PMI++ is an extended version of (Turney, 2002)?smethod for finding the SO label of a phrase (as an at tempt to deal with context-sensitive words).For a given(word, feature, sentence) tuple, PMI++ ignores the sentence, generates a phrase based on the word and the fea ture (e.g., (clean, room): ?clean room?)and finds its SO label using PMI statistics.If unsure of the label, PMI++ tries to find the orientation of the potential opinion word instead.The search engine queries use domain-specific keywords (e.g., ?scanner?), which are dropped if they 344 lead to low counts.Hu++ is a WordNet-based method for finding a word?s context-independent semantic orientation.It extends Hu?s adjective labeling method in a number of ways in order to handle nouns, verbs and adverbs in addition to adjectives and in order to improve coverage.Hu?s method starts with two sets of positive and negative words and iteratively grows each one by including synonyms andantonyms from WordNet.The final sets are used to pre dict the orientation of an incoming word.Type PMI++ Hu++ OPINE P R P R P R adj 0.73 0.91 +0.02 -0.17 +0.07 -0.03 nn 0.63 0.92 +0.04 -0.24 +0.11 -0.08 vb 0.71 0.88 +0.03 -0.12 +0.01 -0.01 adv 0.82 0.92 +0.02 -0.01 +0.06 +0.01 Avg 0.72 0.91 +0.03 -0.14 +0.06 -0.03 Table 6: Finding SO Labels of Potential Opinion Words in the Context of Given Product Features and Sentences.OPINE?s precision is higher than that of PMI++ and Hu++.All results are reported with respect to PMI++ . Notation: adj=adjectives, nn=nouns, vb=verbs, adv=adverbs 3.4.1 Experiments: SO LabelsOn the task of finding SO labels for words in the con text of given features and review sentences, OPINE obtains higher precision than both baseline methods at a smallloss in recall with respect to PMI++.As described be low, this result is due in large part to OPINE?s ability to handle context-sensitive opinion words.We randomly selected 200 (word, feature, sentence) tuples for each word type (adjective, adverb, etc.) andobtained a test set containing 800 tuples.Two annota tors assigned positive, negative and neutral labels to eachtuple (the inter-annotator agreement was 78%).We re tained the tuples on which the annotators agreed as the gold standard.We ran PMI++ and Hu++ on the test data and compared the results against OPINE?s results on the same data.In order to quantify the benefits of each of the threesteps of our method for finding SO labels, we also compared OPINE with a version which only finds SO la bels for words and a version which finds SO labels for words in the context of given features, but doesn?t take into account given sentences.We have learned from this comparison that OPINE?s precision gain over PMI++ andHu++ is mostly due to to its ability to handle context sensitive words in a large number of cases.Although Hu++ does not handle context-sensitive SO label assignment, its average precision was reasonable (75%) and better than that of PMI++.Finding a word?s SO label is good enough in the case of strongly positiveor negative opinion words, which account for the major ity of opinion instances.The method?s loss in recall is due to not recognizing words absent from WordNet (e.g., ?depth-adjustable?)or not having enough information to classify some words in WordNet.PMI++ typically does well in the presence of strongly positive or strongly negative words.Its high recall iscorrelated with decreased precision, but overall this sim ple approach does well.PMI++?s main shortcoming is misclassifying terms such as ?basic?or ?visible?which change orientation based on context.3.4.2 Experiments: Opinion Phrases In order to evaluate OPINE on the tasks of opinion phrase extraction and opinion phrase polarity extraction in the context of known features and sentences, we used aset of 550 sentences containing previously extracted fea tures.The sentences were annotated with the opinion phrases corresponding to the known features and with the opinion polarity.We compared OPINE with PMI++ and Hu++ on the tasks of interest.We found that OPINE hadthe highest precision on both tasks at a small loss in re call with respect to PMI++.OPINE?s ability to identify a word?s SO label in the context of a given feature and sentence allows the system to correctly extract opinionsexpressed by words such as ?big?or ?small?, whose se mantic orientation varies based on context.Measure PMI++ Hu++ OPINE OP Extraction: Precision 0.71 +0.06 +0.08 OP Extraction: Recall 0.78 -0.08 -0.02 OP Polarity: Precision 0.80 -0.04 +0.06 OP Polarity: Recall 0.93 +0.07 -0.04 Table 7: Extracting Opinion Phrases and Opinion Phrase Polarity Corresponding to Known Features and Sentences.OPINE?s precision is higher than that of PMI++ and of Hu++.All results are reported with respect to PMI++.The key components of OPINE described in this paper are the PMI feature assessment which leads to high-precisionfeature extraction and the use of relaxation-labeling in or der to find the semantic orientation of potential opinionwords.The review-mining work most relevant to our re search is that of (Hu and Liu, 2004) and (Kobayashi et al., 2004).Both identify product features from reviews, but OPINE significantly improves on both.(Hu and Liu, 2004) doesn?t assess candidate features, so its precision is lower than OPINE?s.(Kobayashi et al, 2004) employsan iterative semi-automatic approach which requires human input at every iteration.Neither model explicitly ad dresses composite (feature of feature) or implicit features.Other systems (Morinaga et al, 2002; Kushal et al, 2003) also look at Web product reviews but they do not extract 345 opinions about particular product features.OPINE?s use of meronymy lexico-syntactic patterns is similar to that of many others, from (Berland and Charniak, 1999) to (Almuhareb and Poesio, 2004).Recognizing the subjective character and polarity of words, phrases or sentences has been addressed by many authors, including (Turney, 2003; Riloff et al, 2003; Wiebe, 2000; Hatzivassiloglou and McKeown, 1997).Most recently, (Takamura et al, 2005) reports on the use of spin models to infer the semantic orientation of words.The paper?s global optimization approach and use of multiple sources of constraints on a word?s semantic orientation is similar to ours, but the mechanism differs and they currently omit the use of syntactic information.Subjective phrases are used by (Turney, 2002; Pang and Vaithyanathan, 2002; Kushal et al, 2003; Kim and Hovy, 2004) and others in order to classify reviews or sentences as positive or negative.So far, OPINE?s focus has been on extracting and analyzing opinion phrases corresponding to specific features in specific sentences, rather than on determining sentence or review polarity.OPINE is an unsupervised information extraction systemwhich extracts fine-grained features, and associated opinions, from reviews.OPINE?s use of the Web as a corpus helps identify product features with improved preci sion compared with previous work.OPINE uses a novel relaxation-labeling technique to determine the semantic orientation of potential opinion words in the context ofthe extracted product features and specific review sentences; this technique allows the system to identify cus tomer opinions and their polarity with high precision and recall.We would like to thank the KnowItAll project and theanonymous reviewers for their comments.Michael Gamon, Costas Boulis and Adam Carlson have also pro vided valuable feedback.We thank Minquing Hu andBing Liu for providing their data sets and for their com ments.Finally, we are grateful to Bernadette Minton and Fetch Technologies for their help in collecting additional reviews.This research was supported in part by NSF grant IIS-0312988, DARPA contract NBCHD030010, ONR grant N00014-02-1-0324 as well as gifts from Google and the Turing Center.
This study reports the results of using minimum description length (MDL) analysis to model unsupervised learning of the morphological segmentation of European languages, using corpora ranging in size from 5,000 words to 500,000 words.We develop a set of heuristics that rapidly develop a probabilistic morphological grammar, and use MDL as our primary tool to determine whether the modifications proposed by the heuristics will be adopted or not.The resulting grammar matches well the analysis that would be developed by a human morphologist.In the final section, we discuss the relationship of this style of MDL grammatical analysis to the notion of evaluation metric in early generative grammar.This is a report on the present results of a study on unsupervised acquisition of morphology.'The central task of morphological analysis is the segmentation of words into the components that form the word by the operation of concatenation.While that view is not free of controversy, it remains the traditional conception of morphology, and the one that we shall employ here.'Issues of interface with phonology, traditionally known as morphophonology, and with syntax are not directly addressed.'While some of the discussion is relevant to the unrestricted set of languages, some of the assumptions made in the implementation restrict the useful application of the algorithms to languages in which the average number of affixes per word is less than what is found in such languages as Finnish, Hungarian, and Swahili, and we restrict our testing in the present report to more widely studied European languages.Our general goal, however, is the treatment of unrestricted natural languages.* Department of Linguistics, University of Chicago, 1010 E. 59th Street, Chicago, IL 60637.E-mail: ja-goldsmith@uchicago.edu.1 Some of the work reported here was done while I was a visitor at Microsoft Research in the winter of 1998, and I am grateful for the support I received there.A first version was written in September, 1998, and a much-revised version was completed in December, 1999.This work was also supported in part by a grant from the Argonne National Laboratory-University of Chicago consortium, which I thank for its support.I am also grateful for helpful discussion of this material with a number of people, including Carl de Marcken, Jason Eisner, Zhiyi Chi, Derrick Higgins, Jorma Rissanen, Janos Simon, Svetlana Soglasnova, Hisami Suzuki, and Jessie Pinkham.As noted below, I owe a great deal to the remarkable work reported in de Marcken's dissertation, without which I would not have undertaken the work described here.I am grateful as well to several anonymous reviewers for their considerable improvements to the content of this paper.The program in question takes a text file as its input (typically in the range of 5,000 to 1,000,000 words) and produces a partial morphological analysis of most of the words of the corpus; the goal is to produce an output that matches as closely as possible the analysis that would be given by a human morphologist.It performs unsupervised learning in the sense that the program's sole input is the corpus; we provide the program with the tools to analyze, but no dictionary and no morphological rules particular to any specific language.At present, the goal of the program is restricted to providing the correct analysis of words into component pieces (morphemes), though with only a rudimentary categorical labeling.The underlying model that is utilized invokes the principles of the minimum description length (MDL) framework (Rissanen 1989), which provides a helpful perspective for understanding the goals of traditional linguistic analysis.MDL focuses on the analysis of a corpus of data that is optimal by virtue of providing both the most compact representation of the data and the most compact means of extracting that compression from the original data.It thus requires both a quantitative account whose parameters match the original corpus reasonably well (in order to provide the basis for a satisfactory compression) and a spare, elegant account of the overall structure.The novelty of the present account lies in the use of simple statements of morphological patterns (called signatures below), which aid both in quantifying the MDL account and in constructively building a satisfactory morphological grammar (for MDL offers no guidance in the task of seeking the optimal analysis).In addition, the system whose development is described here sets reasonably high goals: the reformulation in algorithmic terms of the strategies of analysis used by traditional morphologists.Developing an unsupervised learner using raw text data as its sole input offers several attractive aspects, both theoretical and practical.At its most theoretical, unsupervised learning constitutes a (partial) linguistic theory, producing a completely explicit relationship between data and analysis of that data.A tradition of considerable age in linguistic theory sees the ultimate justification of an analysis A of any single language L as residing in the possibility of demonstrating that analysis A derives from a particular linguistic theory LT, and that that LT works properly across a range of languages (not just for language L).There can be no better way to make the case that a particular analysis derives from a particular theory than to automate that process, so that all the linguist has to do is to develop the theory-as-computer-algorithm; the application of the theory to a particular language is carried out with no surreptitious help.From a practical point of view, the development of a fully automated morphology generator would be of considerable interest, since we still need good morphologies of many European languages and to produce a morphology of a given language &quot;by hand&quot; can take weeks or months.With the advent of considerable historical text available on-line (such as the ARTFL database of historical French), it is of great interest to develop morphologies of particular stages of a language, and the process of automatic morphology writing can simplify this stage—where there are no native speakers available—considerably.A third motivation for this project is that it can serve as an excellent preparatory phase (in other words, a bootstrapping phase) for an unsupervised grammar acquisition system.As we will see, a significant proportion of the words in a large corpus can be assigned to categories, though the labels that are assigned by the morphological analysis are corpus internal; nonetheless, the assignment of words into distinct morphologically motivated categories can be of great service to a syntax acquisition device.The problem, then, involves both the determination of the correct morphological split for individual words, and the establishment of accurate categories of stems based on the range of suffixes that they accept: inflectional suffixes on a word which contains a stem that is followed by two or more inflectional suffixes, and we would like to identify derivational prefixes and suffixes.We want to be told that in this corpus, the most important suffixes are -s, -ing, -ed, and so forth, while in the next corpus, the most important suffixes are -e, -en, -heit, -ig, and so on.Of course, the program is not a language identification program, so it will not name the first as &quot;English&quot; and the second as &quot;German&quot; (that is a far easier task), but it will perform the task of deciding for each word what is stem and what is affix.2.Range of suffixes: The most salient characteristic of a stem in the languages that we will consider here is the range of suffixes with which it can appear.Adjectives in English, for example, will appear with some subset of the suffixes -er, -est, -ity, -ness, etc.We would like to determine automatically what the range of the most regular suffix groups is for the language in question, and rank suffix groupings by order of frequency in the corpus.'To give a sense of the results of the program, consider one aspect of its analysis of the novel The Adventures of Tom Sawyer—and this result is consistent, by and large, regardless of the corpus one chooses.Consider the top-ranked signatures, illustrated in Table 1: a signature is an alphabetized list of affixes that appear with a particular stem in a corpus.(A larger list of these patterns of suffixation in English are given in Table 2, in Section 5.)The present morphology learning algorithm is contained in a C++ program called Linguistica that runs on a desktop PC and takes a text file as its input.'Analyzing a corpus of 500,000 words in English requires about five minutes on a Pentium II 333.Perfectly respectable results can be obtained from corpora as small as 5,000 words.The system has been tested on corpora in English, French, German, Spanish, Italian, Dutch, Latin, and Russian; some quantitative results are reported below.The corpora that serve as its input are largely materials that have been obtained over the Internet, and I have endeavored to make no editorial changes to the files that are the input.In this paper, I will discuss prior work in this area (Section 2), the nature of the MDL model we propose (Section 3), heuristics for the task of the initial splitting of words into stem and affix (Section 4), the resulting signatures (Section 5), use of MDL to search the space of morphologies (Section 6), results (Section 7), the identification of entirely spurious generalizations (section 8), the grouping of signatures into larger units (Section 9), and directions for further improvements (Section 10).Finally, I will offer some speculative observations about the larger perspective that this work suggests and work in progress (Section 11).The task of automatic word analysis has intrigued workers in a range of disciplines, and the practical and theoretical goals that have driven them have varied considerably.Some, like Zellig Harris (and the present writer), view the task as an essential one in defining the nature of the linguistic analysis.But workers in the area of data compression, dictionary construction, and information retrieval have all contributed to the literature on automatic morphological analysis.(As noted earlier, our primary concern here is with morphology and not with regular allomorphy or morphophonology, which is the study of the changes in the realization of a given morpheme that are dependent on the grammatical context in which it appears, an area occasionally confused for morphology.Several researchers have explored the morphophonologies of natural language in the context of two-level systems in the style of the model developed by Kimmo Koskenniemi [1983], Lauri Karttunen [1993], and others.)The only general review of work in this area that I am aware of is found in Langer (1991), which is ten years old and unpublished.Work in automatic morphological analysis can be usefully divided into four major approaches.The first approach proposes to identify morpheme boundaries first, and thus indirectly to identify morphemes, on the basis of the degree of predictability of the n + 1st letter given the first n letters (or the mirror-image measure).This was first proposed by Zellig Harris (1955, 1967), and further developed by others, notably by Hafer and Weiss (1974).The second approach seeks to identify bigrams (and trigrams) that have a high likelihood of being morpheme internal, a view pursued in work discussed below by Klenk, Langer, and others.The third approach focuses on the discovery of patterns (we might say, of rules) of phonological relationships between pairs of related words.The fourth approach, which includes that used in this paper, is top-down, and seeks an analysis that is globally most concise.In this section, we shall review some of the work that has pursued these approaches—briefly, necessarily.'While not all of the approaches discussed here use no prior language-particular knowledge (which is the goal of the present system), I exclude from discussions those systems that are based essentially on a prior human-designed analysis of the grammatical morphemes of a language, aiming at identifying the stem(s) and the correct parsing; such is the case, for example, in Pacak and Pratt (1976), Koch, Küstner, and Riidiger (1989), and Wothke and Schmidt (1992).With the exception of Harris's algorithm, the complexity of the algorithms is such as to make implementation for purposes of comparison prohibitively time-consuming.At the heart of the first approach, due to Harris, is the desire to place boundaries between letters (respectively, phonemes) in a word based on conditional entropy, in the following sense.We construct a device that generates a finite list of words, our corpus, letter by letter and with uniform probability, in such a way that at any point in its generation (having generated the first n letters 11 1213 ..In) we can inquire of it what the entropy is of the set consisting of the next letter of all the continuations it might make.(In current parlance, we would most naturally think of this as a path from the root of a trie to one of its terminals, inquiring of each node its associated one-letter entropy, based on the continuations from that node.)Let us refer to this as the prefix conditional entropy; clearly we may be equally interested in constructing a trie from the right edge of words, which then provides us with a suffix conditional entropy, in mirror-image fashion.Harris himself employed no probabilistic notions, and the inclusion of entropy in the formulation had to await Hafer and Weiss (1974); but allowing ourselves the anachronism, we may say that Harris proposed that local peaks of prefix (and suffix) conditional entropy should identify morpheme breaks.The method proposed in Harris (1955) appealed to what today we would call an oracle for information about the language under scrutiny, but in his 1967 article, Harris implemented a similar procedure on a computer and a fixed corpus, restricting his problem to that of finding morpheme boundaries within words.Harris's method is quite good as a heuristic for finding a good set of candidate morphemes, comparable in quality to the mutual informationbased heuristic that I have used, and which I describe below.It has the same problem that good heuristics frequently have: it has many inaccuracies, and it does not lend itself to a next step, a qualitatively more reliable approximation of the correct solution.'Hafer and Weiss (1974) explore in detail various ways of clarifying and improving on Harris's algorithm while remaining faithful to the original intent.A brief summary does not do justice to their fascinating discussion, but for our purposes, their results confirm the character of the Harrisian test as heuristic: with Harris's proposal, a quantitative measure is proposed (and Hafer and Weiss develop a range of 15 different measures, all of them rooted in Harris's proposal), and best results for morphological analysis are obtained in some cases by seeking a local maximum of prefix conditional entropy, in others by seeking a value above a threshold, and in yet others, good results are obtained only when this measure is paired with a similar measure constructed in mirror-image fashion from the end of the word—and then some arbitrary thresholds are selected which yield the best results.While no single method emerges as the best, one of the best yields precision of 0.91 and recall of 0.61 on a corpus of approximately 6,200 word types.(Precision here indicates proportion of predicted morpheme breaks that are correct, and recall denotes the proportion of correct breaks that are predicted.)The second approach that can be found in the literature is based on the hypothesis that local information in the string of letters (respectively, phonemes) is sufficient to identify morpheme boundaries.This hypothesis would be clearly correct if all morpheme boundaries were between pairs of letters 11-12 that never occur in that sequence morpheme internally, and the hypothesis would be invalidated if conditional probabilities of a letter given the previous letter were independent of the presence of an intervening boundary.The question is where real languages distribute themselves along the continuum that stretches between these two extremes.A series of publications has explored this question, including Janssen (1992), Klenk (1992), and Flenner (1994, 1995).Any brief description that overlooks the differences among these publications is certain to do less than full justice to all of them.The procedure described in Janssen (1992) and Flenner (1994, 1995) begins with a training corpus with morpheme boundaries inserted by a human, and hence the algorithm is not in the domain of unsupervised learning.Each bigram (and the algorithm has been extended in the natural way to treating trigrams as well) is associated with a triple (whose sum must be less than or equal to 1.0) indicating the frequency in the training corpus of a morpheme boundary occurring to the left of, between, or to the right of that bigram.In a test word, each space between letters (respectively, phonemes) is assigned a score that is the sum of the relevant values derived from the training session: in the word string, for example, the score for the potential cut between str and ing is the sum of three values: the probability of a morpheme boundary after tr (given tr), the probability of a morpheme boundary between r and i (given ri), and the probability of a morpheme boundary before in (given in).That these numbers should give some indication of the presence of a morpheme boundary is certain, for they are the sums of numbers that were explicitly assigned on the basis of overtly marked morpheme boundaries.But it remains unclear how one should proceed further with the sum.As Hafer and Weiss discover with Harris's measure, it is unclear whether local peaks of this measure should predict morpheme boundaries, or whether a threshold should be set, above which a morpheme boundary is predicted.Flenner (1995, 64-65) and proponents of this approach have felt some freedom on making this choice in an ad hoc fashion.Janssen (1992, 81-82) observes that the French word linguistique displays three peaks, predicting the analysis lthguist-ique, employing a trigram model.The reason for the strong, but spurious, peak after lin is that lin occurs with high frequency word finally, just as gui appears with high frequency word initially.One could respond to this observation in several ways: word-final frequency should not contribute to word-internal, morpheme-final status; or perhaps frequencies of this sort should not be added.Indeed, it is not clear at all why these numbers should be added; they do not, for example, represent probabilities that can be added.Janssen notes that the other two trigrams that enter into the picture (ing and ngu) had a zero frequency of morpheme break in the desired spot, and proposes that the presence of any zeros in the sum forces the sum to be 0, raising again the question of what kind of quantity is being modeled; there is no scholarly tradition according to which the presence of zero in a sum should lead to a total of 0.I do not have room to discuss the range of greedy affix-parsing algorithms these authors explore, but that aspect of their work has less bearing on the comparison with the present paper, whose focus is on data-driven learning.The major question to carry away from this approach is this: can the information that is expressed in the division of a set of words into morphemes be compressed into local information (bigrams, trigrams)?The answer, I believe, is in general negative.Morphology operates at a higher level, so to speak, and has only weak statistical links to local sequencing of phonemes or letters.'The third approach focuses on the discovery of patterns explicating the overt shapes of related forms in a paradigm.Dzeroski and Erjavec (1997) report on work that they have done on Slovene, a South Slavic language with a complex morphology, in the context of a similar project.Their goal essentially was to see if an inductive logic program could infer the principles of Slovene morphology to the point where it could correctly predict the nominative singular form of a word if it were given an oblique (nonnominative) form.Their project apparently shares with the present one the requirement that the automatic learning algorithm be responsible for the decision as to which letters constitute the stem and which are part of the suffix(es), though the details offered by Dzeroski and Etjavec are sketchy as to how this is accomplished.In any event, they present their learning algorithm with a labeled pair of words—a base form and an inflected form.It is not clear from their description whether the base form that they supply is a surface form from a particular point in the inflectional paradigm (the nominative singular), or a more articulated underlying representation in a generative linguistic sense; the former appears to be their policy.Dzeroski and Erjavec's goal is the development of rules couched in traditional linguistic terms; the categories of analysis are decided upon ahead of time by the programmer (or, more specifically, by the tagger of the corpus), and each individual word is identified with regard to what morphosyntactic features it bears.The form bolecina is marked, for example, as a feminine noun singular genitive.In sum, their project thus gives the system a good deal more information than the present project does.'Two recent papers, Jacquemin (1997) and Gaussier (1999), deserve consideration here.'Gaussier (1999) approaches a very similar task to that which we consider, and takes some similar steps.His goal is to acquire derivational rules from an inflectional lexicon, thus insuring that his algorithm has access to the lexical category of the words it deals with (unlike the present study, which is allowed no such access).Using the terminology of the present paper, Gaussier considers candidate suffixes if they appear with at least two stems of length 5.His first task is (in our terms) to infer paradigms from signatures (see Section 9), which is to say, to find appropriate clusters of signatures.One example cited is depart, departure, departer.He used a hierarchical agglomerative clustering method, which begins with all signatures forming distinct clusters, and successively collapses the two most similar clusters, where similarity between stems is defined as the number of suffixes that two stems share, and similarity between clusters is defined as the similarity between the two least similar stems in the respective clusters.He reports a success rate of 77%, but it is not clear how to evaluate this figure.'The task that Gaussier addresses is defined from the start to be that of derivational morphology, and because of that, his analysis does not need to address the problem of inflectional morphology, but it is there (front and center, so to speak) that the difficult clustering problem arises, which is how to ensure that the signatures NULL.s.'s (for nouns in English) and the signature NULL.ed.s (or NULL.ed.ing.$) are not assigned to single clusters.12 That is, in English both nouns and verbs freely occur with the suffixes NULL and -s, and while -ed and -'s disambiguate the two cases, it is very difficult to find a statistical and morphological basis for this knowledge.'Jacquemin (1997) explores an additional source of evidence regarding clustering of hypothesized segmentation of words into stems and suffixes; he notes that the hypothesis that there is a common stem gen in gene and genetic, and a common stem express in expression and expressed, is supported by the existence of small windows in corpora containing the word pair genetic... expression and the word pair gene... expressed (as indicated, the words need not be adjacent in order to provide evidence for the relationship).As this example suggests, Jacquemin's work is situated within the context of a desire for superior information retrieval.In terms of the present study, Jacquemin's algorithm consists of (1) finding signatures with the longest possible stems and (2) establishing pairs of stems that occur together in two or more windows of length 5 or less.He tests his results on 100 random pairs discovered in this fashion, placing upper bounds on the length of the suffix permitted between one and five letters, and independently varying the length of the window in question.He does not vary the minimum size of the stem, a consideration that turns out to be quite important in Germanic languages, though less so in Romance languages.He finds that precision varies from 97% when suffixes are limited to a length of one letter, to 64% when suffixes may be five letters long, with both figures assuming an adjacency window of two words; precision falls to 15% when a window of four words is permitted.Jacquemin also employs the term signature in a sense not entirely dissimilar to that employed in the present paper, referring to the structured set of four suffixes that appear in the two windows (in the case above, the suffixes are -ion, -ed; NULL, -tic).He notes that incorrect signatures arise in a large number of cases (e.g., good: optical control — optimal control; adoptive transfer — adoptively tranfer, paralleled by bad: ear disease early disease), and suggests a quality function along the following lines: Stems are linked in pairs (adopt-transfer, ear-disease); compute then the average length of the shorter stem in each pair (that is, create a set of the shorter member of each pair, and find the average length of that set).The quality function is defined as that average divided by the length of the largest suffix in the signature; reject any signature class for which that ratio is less than 1.0.This formula, and the threshold, is purely empirical, in the sense that there is no larger perspective that bears on determining the appropriateness of the formula, or the values of the parameters.The strength of this approach, clearly, is its use of information that co-occurrence in a small window provides regarding semantic relatedness.This allows a more aggressive stance toward suffix identification (e.g., alpha interferon — alpha2 interferon).There can be little question that the type of corpus studied (a large technical medical corpus, and a list of terms—partially multiword terms) lends itself particularly to this style of inference, and that similar patterns would be far rarer in unrestricted text such as Tom Sawyer or the Brown corpus.'13 Gaussier also offers a discussion of inference of regular morphophonemics, which we do not treat in the present paper, and a discussion in a final section of additional analysis, though without test results.Gaussier aptly calls our attention to the relevance of minimum edit distance relating two potential allomorphs, and he proposes a probabilistic model based on patterns established between allomorphs.In work not discussed in this paper, I have explored the integration of minimum edit distance to an MDL account of allomorphy as well, and will discuss this material in future work.The fourth approach to morphology analysis is top-down, and seeks a globally optimal analysis of the corpus.This general approach is based on the insight that the number of letters in a list of words is greater than the number of letters in a list of the stems and affixes that are present in the original list.This is illustrated in Figure 1.This simple observation lends hope to the notion that we might be able to specify a relatively simple figure of merit independently of how we attempt to find analyses of particular data.This view, appropriately elaborated, is part of the minimum description length approach that we will discuss in detail in this paper.Kazakov (1997) presents an analysis in this fourth approach, using a straightforward measurement of the success of a morphological analysis that we have mentioned, counting the number of letters in the inventory of stems and suffixes that have been hypothesized; the improvement in this count over the number of letters in the original word list is a measure of the fitness of the analysis.'He used a list of 120 French words in one experiment, and 39 forms of the same verb in another experiment, and employed what he terms a genetic algorithm to find the best cut in each word.He associated each of the 120 words (respectively, 39) with an integer (between 1 and the length of the word minus 1) indicating where the morphological split was to be, and measured the fitness of that grammar in terms of its decrease in number of total letters.He does not describe the fitness function used, but seems to suggest that the single top-performing grammar of each generation is preserved, all others are eliminated, and the top-performing grammar is then subjected to mutation.That is, in a case-by-case fashion, the split between stems and suffixes is modified (in some cases by a shift of a single letter, in others by an unconstrained shift to another location within the word) to form a new grammar.In one experiment described by Kazakov, the population was set to 800, and 2,000 generations were modeled.On a Pentium 90 and a vocabulary of 120 items, the computation took over eight hours.Work by Michael Brent (1993) and Carl de Marcken (1995) has explored analyses of the fourth type as well.Researchers have been aware of the utility of the informationtheoretic notion of compression from the earliest days of information theory, and there have been efforts to discover useful, frequent chunks of letters in text, such as Radhakrishnan (1978), but to my knowledge, Brent's and de Marcken's works were the first to explicitly propose the guiding of linguistic hypotheses by such notions.Brent's work addresses the question of determining the correct morphological analysis of a corpus of English words, given their syntactic category, utilizing the notion of minimal encoding, while de Marcken's addresses the problem of determining the &quot;breaking&quot; of an unbroken stream of letters or phonemes into chunks that correspond as well as possible to our conception of words, implementing a well-articulated algorithm couched in a minimum description length framework, and exploring its effects on several large corpora.Brent (1993) aims at finding the appropriate set of suffixes from a corpus, rather than the more comprehensive goal of finding the correct analysis for each word, both stem and suffix, and I think it would not be unfair to describe it as a test-of-concept trial on a corpus ranging in size from 500 to 8,000 words; while this is not a small number of words, our studies below focus on corpora with on the order of 30,000 distinct words.Brent indicates that he places other limitations as well on the hypothesis space, such as permitting no suffix which ends in a sequence that is also a suffix (i.e., if s is a suffix, then less and ness are not suffixes, and if y is a suffix, ity is not).Brent's observation is very much in line with the spirit of the present analysis: &quot;The input lexicons contained thousands of non-morphemic endings and mere dozens of morphemic suffixes, but the output contained primarily morphemic suffixes in all cases but one.Thus, the effects of non-morphemic regularities are minimal&quot; (p. 35).Brent's corpora were quite different from those used in the experiments reported below; his were based on choosing the n most common words in a Wall Street Journal corpus, while the present study has used large and heterogeneous sources for corpora, which makes for a considerably more difficult task.In addition, Brent scored his algorithm solely on how well it succeeded in identifying suffixes (or combinations of suffixes), rather than on how well it simultaneously analysed stem and suffix for each word, the goal of the present study.&quot; Brent makes clear the relevance and importance of information-theoretic notions, but does not provide a synthetic and overall measure of the length of the morphological grammar.16 Brent's description of his algorithm is not detailed enough to satisfy the curiosity of someone like the present writer, who has encountered problems that Brent's approach would seem certain to encounter equally.As we shall see below, the central practical problem to grapple with is the fact that when considering suffixes (or candidate suffixes) consisting of only a single letter (let us say, s, for example), it is extremely difficult to get a good estimate of how many of the potential occurrences (of word-final s) are suffixal s and how many are not.As we shall suggest towards the end of this paper, the only accurate way to make an estimate is on the basis of a multinornial estimate once larger suffix signatures have been established.Without this, it is difficult not to overestimate the frequency of single-letter suffixes, a result that may often, in my experience, deflect the learning algorithm from discovering a correct two-letter suffix (e.g., the suffix -al in French).Goldsmith Unsupervised Learning of the Morphology of a Natural Language De Marcken (1995) addresses a similar but distinct task, that of determining the correct breaking of a continuous stream of segments into distinct words.This problem has been addressed in the context of Asian (Chinese-Japanese-Korean) languages, where standard orthography does not include white space between words, and it has been discussed in the context of language acquisition as well.De Marcken describes an unsupervised learning algorithm for the development of a lexicon using a minimum description length framework.He applies the algorithm to a written corpus of Chinese, as well as to written and spoken corpora of English (the English text has had the spaces between words removed), and his effort inspired the work reported here.De Marcken's algorithm begins by taking all individual characters to be the baseline lexicon, and it successively adds items to the lexicon if the items will be useful in creating a better compression of the corpus in question, or rather, when the improvement in compression yielded by the addition of a new item to the codebook is greater than the length (or &quot;cost&quot;) associated with the new item in the codebook.In general, a lexical item of frequency F can be associated with a compressed length of — log F, and de Marcken's algorithm computes the compressed length of the Viterbi-best parse of the corpus, where the compressed length of the whole is the sum of the compressed lengths of the individual words (or hypothesized chunks, we might say) plus that of the lexicon.In general, the addition of chunks to the lexicon (beginning with such high-frequency items as th) will improve the compression of the corpus as a whole, and de Marcken shows that successive iterations add successively larger pieces to the lexicon.De Marcken's procedure builds in a bottom-up fashion, looking for larger and larger chunks that are worth (in an MDL sense) assigning the status of dictionary entries.Thus, if we look at unbroken orthographic texts in English, the two-letter combination th will become the first candidate chosen for lexical status; later, is will achieve that status too, and soon this will as well.The entry this will not, in effect, point to its four letters directly, but will rather point to the chunks th and is, which still retain their status in the lexicon (for their robust integrity is supported by their appearance throughout the lexicon).The creation of larger constituents will occasionally lead to the elimination of smaller chunks, but only when the smaller chunk appears almost always in a single larger unit.An example of an analysis provided by de Marcken's algorithm is given in (1), taken from de Marcken (1995), in which I have indicated the smallest-level constituent by placing letters immediately next to one another, and then higher structure with various pair brackets (parentheses, etc.) for orthographic convenience; there is no theoretical significance to the difference between &quot;( )&quot; and &quot;0&quot;, etc.De Marcken's analysis succeeds quite well at identifying words, but does not make any significant effort at identifying morphemes as such.Applying de Marcken's algorithm to a &quot;broken&quot; corpus of a language in which word boundaries are indicated (for example, English) provides interesting results, but none that provide anything even approaching a linguistic analysis, such as identification of stems and affixes.The broken character of the corpus serves essentially as an upper bound for the chunks that are postulated, while the letters represent the lower bound.De Marcken's MDL-based figure of merit for the analysis of a substring of the corpus is the sum of the inverse log frequencies of the components of the string in question; the best analysis is that which minimizes that number (which is, again, the optimal compressed length of that substring), plus the compressed length of each of the lexical items that have been hypothesized to form the lexicon of the corpus.It would certainly be natural to try using this figure of merit on words in English, along with the constraint that all words should be divided into exactly two pieces.Applied straightforwardly, however, this gives uninteresting results: words will always be divided into two pieces, where one of the pieces is the first or the last letter of the word, since individual letters are so much more common than morphemes.'(I will refer to this effect as peripheral cutting below.)In addition—and this is less obvious—the hierarchical character of de Marcken's model of chunking leaves no place for a qualitative difference between high-frequency &quot;chunks,&quot; on the one hand, and true morphemes, on the other: str is a high-frequency chunk in English (as schl is in German), but it is not at all a morpheme.The possessive marker 's, on the other hand, is of relatively low frequency in English, but is clearly a morpheme.MDL is nonetheless the key to understanding this problem.In the next section, I will present a brief description of the algorithm used to bootstrap the problem, one which avoids the trap mentioned briefly in note 21.This provides us with a set of candidate splittings, and the notion of the signature of the stem becomes the working tool for determining which of these splits is linguistically significant.MDL is a framework for evaluating proposed analyses, but it does not provide a set of heuristics that are nonetheless essential for obtaining candidate analyses, which will be the subject of the next two sections.The central idea of minimum description length analysis (Rissanen 1989) is composed of four parts: first, a model of a set of data assigns a probability distribution to the sample space from which the data is assumed to be drawn; second, the model can then be used to assign a compressed length to the data, using familiar information-theoretic notions; third, the model can itself be assigned a length; and fourth, the optimal analysis of the data is the one for which the sum of the length of the compressed data and the length of the model is the smallest.That is, we seek a minimally compact specification of both the model and the data, simultaneously.Accordingly, we use the conceptual vocabulary of information theory as it becomes relevant to computing the length, in bits, of various aspects of the morphology and the data representation.Let us suppose that we know (part of) the correct analysis of a set of words, and we wish to create a model using that knowledge.In particular, we know which words have no morphological analysis, and for all the words that do have a morphological analysis, we know the final suffix of the word.(We return in the next section to how we might arrive at that knowledge.)An MDL model can most easily be conceptualized if we encode all such knowledge by means of lists; see Figure 2.In the present case, we have three lists: a list of stems, of suffixes, and of signatures.We construct a list of the stems of the corpus defined as the set of the unanalyzed words, plus the material that precedes the final suffix of each morphologically analyzed word.We also construct a list of suffixes that occur with at least one stem.Finally, each stem is empirically associated with a set of suffixes (those with which it appears in the corpus); we call this set the stem's signature, and we construct a third list, consisting of the signatures that appear in this corpus.This third list, however, contains no letters (as the other A sample morphology.This morphology covers the words: cat, cats, dog, dogs, hat, hats, save, saves, saving, savings, jump, jumped, jumping, jumps, laugh, laughed, laughing, laughs, walk, walked, walking, walks, the, John. lists do), but rather pointers to stems and suffixes.We do this, in one sense, because our goal is to construct the smallest morphology, and in general a pointer requires less information than an explicit set of letters.But in a deeper sense, it is the signatures whose compactness provides the explicit measurement of the conciseness of the entire analysis.Note that by construction, each stem is associated with exactly one signature.Since stem, suffix, and signature all begin with s, we opt for using t to represent a stem, f to represent a suffix, and a to represent a signature, while the uppercase T, F, E represent the sets of stems, suffixes, and signatures, respectively.The number of members of such a set will be represented (T) , (F), etc., while the number of occurrences of a stem, suffix, etc., will be represented as [t], [f], etc.The set of all words in the corpus will be represented as W; hence the length of the corpus is [W], and the size of the vocabulary is (W).Note the structure of the signatures in Figure 2.Logically a signature consists of two lists of pointers, one a list of pointers to stems, the other a list of pointers to suffixes.To specify a list of length N, we must specify at the beginning of the signature that N items will follow, and this requires just slightly more than log2 N bits to do (see Rissanen [1989,33-34] for detailed discussion); I will use the notation A(N) to indicate this function.A pointer to a stem t, in turn, is of length — log prob (t), a basic principle of information theory (Li and Vitanyi 1997).Hence the length of a signature is the sum of the (inverse) log probabilities of its stems, plus that of its suffixes, plus the number of bits it takes to specify the number of its stems and suffixes, using the A function.We will return in a moment to how we determine the probabilities of the stems and suffixes; looking ahead, it will be the empirical frequency.Let us consider the length of stem list T. As we have already observed, its length is A((T))—this is the length of the information specifying how long the list is—plus the length of each stem specification.In most of our work, we make the assumption that the length of a stem is the number of letters in it, weighted by the factor log 26 converting to binary bits, in a language with 26 letters!'The same reasoning holds for the suffix list F: its length is X((F)) plus the length of each suffix, which we may take to be the total number of letters in the suffix times log 26.We return to the question of how long the pointer (found inside a signature) to a stem or suffix is.The probability of a stem is its (empirical) frequency, i.e., the total number of words in the corpus corresponding to the words whose analysis includes the stem in question; the probability of a suffix is defined in parallel fashion.Using W to indicate all the words of the corpus, we may say that the length of a pointer to a stem t is of length a pointer to suffix f is of length 18 This is a reasonable, and convenient, assumption, but it may not be precise enough for all work.A more refined measure would take the length of a letter to be —1 times the binary log of its frequency.A still more refined measure would base the probability of a letter on bigram context; this matters for English, where stem final t is very common.In addition, there is information in the linear order in which the letters are stored, roughly equal to for a string of length n (compare the information that distinguishes the lexical representation of anagrams).This is an additional consideration in an MDL analysis of morphology pressing in favor of breaking words into morphemes when possible.Goldsmith Unsupervised Learning of the Morphology of a Natural Language and a pointer to a signature a is of length We have now settled the question of how to determine the length of our initial model; we next must determine the probability that the model assigns to each word in the corpus, and armed with that knowledge, we will be able to compute the compressed length of the corpus.The morphology assigns a probability to each word w as the product of the probability of w's signature times w's stem, given its signature, and w's suffix, given its signature: prob (w = t +f) = prob (a) prob (t I a) prob (f I a), where a is the signature associated with t: a = sig(t).Thus while stems and suffixes, which are defined relative to a particular morphological model, are assigned their empirical frequency as their probability, words are assigned a probability based on the model, one which will always depart from the empirical frequency.The compression to the corpus is thus worse than would be a compression based on word frequency alone,' or to put it another way, the morphological analysis in which all words are unanalyzed is the analysis in which each word is trivially assigned its own empirical frequency (since the word equals the stem).But this decrease in compression that comes with morphological analysis is the price willingly paid for not having to enter every distinct word in the stem list of the morphology.Summarizing, the compressed length of the corpus is where we have summed over the words in the corpus, and a(w) is the signature to which word w is assigned.The compressed length of the model is the length of the stem list, the suffix list, and the signature list.The length in bits of the stem list is and the length of the suffix list is where Lt5p00 is the measurement of the length of a string of letters in bits, which we take to be log2 26 times the number of letters (but recall note 18).The length of the signature list is where L(a) is the length of signature a.If the set of stems linked to signature a is T(a) and the set of suffixes linked to signature a is F(a), then (The denominator in the last term consists of the token count of words in a particular signature with the given suffix f, and we will refer to this below more simply as [f in ol.)It is no doubt easy to get lost in the formalism, so it may be helpful to point out what the contribution of the additional structure accomplishes.We observed above that the MDL analysis is an elaboration of the insight that the best morphological analysis of a corpus is obtained by counting the total number of letters in the list of stems and suffixes according to various analyses, and choosing the analysis for which this sum is the least (cf.Figure 2).This simple insight fails rapidly when we observe in a language such as English that there are a large number of verb stems that end in t. Verbs appear with a null suffix (that is, in bare stem form), with the suffixes -s, -ed, and -ing.But once we have 11 stems ending in t, the naive letter-counting approach will judge it a good idea to create a new set of suffixes: -t, -ted, -ts, and -ting, because those 10 letters will allow us to remove 11 or more letters from the list of stems.It is the creation of the lists, notably the signature list, and an information cost which increases as probability decreases, that overcomes that problem.Creating a new signature may save some information associated with the stem list in the morphology, but since the length of pointers to a signature a is — log freq (a), the length of the pointers to the signatures for all of the words in the corpus associated with the old signature (-0, -ed, -s, -ing) or the new signature (-ts, -ted, -ting, -ts) will be longer than the length of the pointers to a signature whose token count is the sum of the token count of the two combined, i.e., The model presented above is too simple in that it underestimates the gain achieved by morphological analysis in case the word that is analyzed is also a stem of a larger word.For example, if a corpus contains the words work and working, then morphological analysis will allow us to dispense with the form working; it is modeled by the stem work and the suffixes -0 and -ing.If the corpus also includes workings, the analysis working-s additionally lowers the cost of the stem working.Clearly we would like stems to be in turn analyzable as stems + suffixes.Implementing this suggestion involves the following modifications: (i) Each pointer to a stem (and these are found both in the compressed representation of each individual word in the corpus, and inside the individual signatures of the morphological model) must contain a flag indicating whether what follows is a pointer to a simple member of the stem list (as in the original model), or a triple pointer to a signature, stem, and suffix.In the latter case, which would be the case for the word iwork-ingl-s, the pointer to the stem consists of a triple identical to the signature for the word work-ing.(ii) The number of words in the corpus has now changed, in that the word [work-ingl-s now contains two words, not one.We will need to distinguish between counts of a word w where w is a freestanding word, and counts where it is part of a larger word; we shall refer to the latter class as secondary counts.In order to simplify computation and exposition, we have adopted the convention that the total number of words remains fixed, even when nested structure is posited by the morphology, thus forcing the convention that counts are distributed in a nonintegral fashion over the two or more nested word structures found in complex words.We consider the more complex case in the appendix.'Goldsmith Unsupervised Learning of the Morphology of a Natural Language We may distinguish between those words, like work or working, whose immediate analysis involves a stem appearing in the stem list (we may call these WsiMPLE) and those whose analysis, like workings, involves recursive structure (we may call these Wcompux).As we have noted, every stem entry in a signature begins with a flag indicating which kind of stem it is, and this flag will be of length for simple stems, and of length for complex stems.We also keep track separately of the total number of words in the corpus (token count) that are morphologically analyzed, and refer to this set as WA; this consists of all words except those that are analyzed as having no suffix (see item (ii) in (2), below).(perhaps work-ing) did not appear independently as a freestanding word in the corpus; we will refer to these inferred words as being &quot;virtual&quot; words with virtual counts.MDL thus provides a figure of merit that we wish to minimize, and we will seek heuristics that modify the morphological analysis in such a fashion as to decrease this figure of merit in a large proportion of cases.In any given case, we will accept a modification to our analysis just in case the description length decreases, and we will suggest that this strategy coincides with traditional linguistic judgment in all clear cases.The MDL model designed in the preceding section will be of use only if we can provide a practical means of creating one or more plausible morphologies for a given corpus.That is, we need bootstrapping heuristics that enable us to go from a corpus to such a morphology.As we shall see, it is not in fact difficult to come up with a plausible initial morphology, but I would like to consider first an approach which, though it might seem like the most natural one to try, fails, and for an interesting reason.The problem we wish to solve can be thought of as one suited to an expectationmaximization (EM) approach (Dempster, Laird, and Rubin 1977).Along such a line, each word w of length N would be initially conceived of as being analyzed in N different ways, cutting the word into stem + suffix after i letters, 1 < i < N, with each of these N analyses being assigned probability mass of [w] N[W] Goldsmith Unsupervised Learning of the Morphology of a Natural Language That probability mass is then summed over the resulting set of stems and suffixes, and on successive iterations, each of the N cuts into stem + suffix is weighted by its probability; that is, if the ith cut of word w, of length 1, cuts it into a stem t of length i and suffix of length 1 — i, then the probability of that cut is defined as where wj,k refers to the substring of w from the jth to the kth letter.Probability mass for the stem and the suffix in each such cut is then augmented by an amount equal to the frequency of word w times the probability of the cut.After several iterations (approximately four), estimated probabilities stabilize, and each word is analyzed on the basis of the cut with the largest probability.This initially plausible approach fails because it always prefers an analysis in which either the stem or (more often) the suffix consists of a single letter.More importantly, the probability that a sequence of one or more word-final letters is a suffix is very poorly modeled by the sequence's frequency.'To put the point another way, even the initial heuristic analyzing one particular word must take into account all of the other analyses in a more articulated way than this particular approach does.I will turn now to two alternative heuristics that succeed in producing an initial morphological analysis (and refer to a third in a note).It seems likely that one could construct a number of additional heuristics of this sort.The point to emphasize is that the primary responsibility of the overall morphology is not that of the initial heuristic, but rather of the MDL model described in the previous section.The heuristics described in this section create an initial morphology that can serve as a starting point in a search for the shortest overall description of the morphology.We deal with that process in Section 5.A heuristic that I will call the take-all-splits heuristic, and which considers all cuts of a word of length 1 into stem+suffix wi, + w,±1,/, where 1 < i < 1, much like the EM approach mentioned immediately above, works much more effectively if the probability is assigned on the basis of a Boltzmann distribution; see (4) below.The function H(.) in (4) assigns a value to a split of word w of length 1: w1,, + H does not assign a proper distribution; we use it to assign a probability to the cut of w into w1,, + w,+1,/ as in (5).Clearly the effect of this model is to encourage splits containing relatively long suffixes and stems.21 It is instructive to think about why this should be so.Consider a word such as diplomacy.If we cut the word into the pieces diplomac + y, its probability is freq (diplomac)* freq (y), and constrast that value with the corresponding values of two other analyses: freq (diploma)* freq (cy), and freq (diplom)* freq (acy).Now, the ratio of the frequency of words that begin with diploma and those that begin with diplomac is less than 3, while the ratio of the frequency of words that end in y and those that end in cy is much greater.In graphical terms, we might note that tries (the data structure) based on forward spelling have by far the greatest branching structure early in the word, while tries based on backward spelling have the greatest branching structure close to the root node, which is to say at the end of the word. where = + wi+v) For each word, we note what the best parse is, that is, which parse has the highest rating by virtue of the H-function.We iterate until no word changes its optimal parse, which empirically is typically less than five iterations on the entire lexicon.22 We now have an initial split of all words into stem plus suffix.Even for words like this and stomach we have such an initial split.The second approach that we have employed provides a much more rapid convergence on the suffixes of a language.Since our goal presently is to identify word-final suffixes, we assume by convention that all words end with an end-of-word symbol (traditionally &quot;#&quot;), and we then tally the counts of all n-grams of length between two and six letters that appear word finally.Thus, for example, the word elephant# contains one occurrence of the word-final bigram t#, one occurrence of the word-final trigram nt# , and so forth; we stop at 6-grams, on the grounds that no grammatical morphemes require more than five letters in the languages we are dealing with.We also require that the n-gram in question be a proper substring of its word.We employ as a rough indicator of likelihood that such an n-gram n1 n2.. nk is a grammatical morpheme the measure: Total count of k-grams log [ni][n21 ...[nk]' which we may refer to as the weighted mutual information.We choose the top 100 n-grams on the basis of this measure as our set of candidate suffixes.We should bear in mind that this ranking will be guaranteed to give incorrect results as well as correct ones; for example, while ing is very highly ranked in an English corpus, ting and ng will also be highly ranked, the former because so many stems end in t, the latter because all ings end in ng, but of the three, only ing is a morpheme in English.We then parse all words into stem plus suffix if such a parse is possible using a suffix from this candidate set.A considerable number of words will have more than one such parse under those conditions, and we utilize the figure of merit described in the preceding section to choose among those potential parses.Regardless of which of the two approaches we have taken, our task now is to decide which splits are worth keeping, which ones need to be dropped, and which ones need to be modified.23 In addition, if we follow the take-all-splits approach, we have many 22 Experimenting with other functions suggests empirically that the details of our choices for a figure of merit, and the distribution reported in the text, are relatively unimportant.As long as the measurement is capable of ensuring that the cuts are not strongly pushed towards the periphery, the results we get are robust.23 Various versions of Harris's method of morpheme identification can be used as well.Harris's approach has the interesting characteristic (unlike the heuristics discussed in the text) that it is possible to impose restrictions that improve its precision while at the same time worsening its recall to unacceptably low levels.In work in progress, we are exploring the consequences of using such an initial heuristic with significantly higher precision, while depending on MDL considerations to extend the recall of the entire morphology.Goldsmith Unsupervised Learning of the Morphology of a Natural Language splits which (from our external vantage point) are splits between prefix and stem: words beginning with de (defense, demand, delete, etc.) will at this point all be split after the initial de.So there is work to be done, and for this we return to the central notion of the signature.Each word now has been assigned an optimal split into stem and suffix by the initial heuristic chosen, and we consider henceforth only the best parse for that word, and we retain only those stems and suffixes that were optimal for at least one word.For each stem, we make a list of those suffixes that appear with it, and we call an alphabetized list of such suffixes (separated by an arbitrary symbol, such as period) the stem's signature; we may think of it as a miniparadigm.For example, in one English corpus, the stems despair, pity, appeal, and insult appear with the suffixes ing and ingly.However, they also appear as freestanding words, and so we use the word NULL, to indicate a zero suffix.Thus their signature is NULL.ing.ingly.Similarly, the stems assist and ignor are assigned the signature ance.ant.ed.ing in a certain corpus.Because each stem is associated with exactly one signature, we will also use the term signature to refer to the set of affixes along with the associated set of stems when no ambiguity arises.We establish a data structure of all signatures, keeping track for each signature of which stems are associated with that signature.As an initial heuristic, subject to correction below, we discard all signatures that are associated with only one stem (these latter form the overwhelming majority, well over 90%) and all signatures with only one suffix.The remaining signatures we shall call regular signatures, and we will call all of the suffixes that we find in them the regular suffixes.As we shall see, the regular suffixes are not quite the suffixes we would like to establish for the language, but they are a very good approximation, and constitute a good initial analysis.The nonregular signatures produced by the take-all-splits approach are typically of no interest, as examples such as ch.e.eriaLerials.rimony.rons.uring and el.ezed.nce.reupon.ther illustrate.The reader may identify the single English pseudostem that occurs with each of these signatures.The regular signatures are thus those that specify exactly the entire set of suffixes used by at least two stems in the corpus.The presence of a signature rests upon the existence of a structure as in (6), where there are at least two members present in each column, and all combinations indicated in this structure are present in the corpus, and, in addition, each stem is found with no other suffix.(This last condition does not hold for the suffixes; a suffix may well appear in other signatures, and this is the difference between stems and affixes.)&quot; If we have a morphological pattern of five suffixes, let us say, and there is a large set of stems that appear with all five suffixes, then that set will give rise to a regular signature with five suffixal members.This simple pattern would be perturbed by the (for our purpose) extraneous fact that a stem appearing with these suffixes should also appear with some other suffix; and if all stems that associate with these five suffixes appear with idiosyncratic suffixes (i.e., each different from the others), then the signature of those five suffixes would never emerge.In general, however, in a given corpus, a good proportion of stems appears with a complete set of what a grammarian would take to be the paradigmatic set of suffixes for its class: this will be neither the stems with the highest nor the stems with the lowest frequency, but those in between.In addition, there will be a large range of words with no acceptable morphological analysis, which is just as it should be: John, stomach, the, and so forth.To get a sense of what are identified as regular signatures in a language such as English, let us look at the results of a preliminary analysis in Table 2 of the 86,976 words of The Adventures of Tom Sawyer, by Mark Twain.The signatures in Table 2 are ordered by the breadth of a signature, defined as follows.A signature a has both a stem count (the number of stems associated with it) and an affix count (the number of affixes it contains), and we use log (stem count) * log (affix count) as a rough guide to the centrality of a signature in the corpus.The suffixes identified are given in Table 3 for the final analysis of this text.In this corpus of some 87,000 words, there are 202 regular signatures identified through the procedure we have outlined so far (that is, preceding the refining operations described in the next section), and 803 signatures composed entirely of regular suffixes (the 601 additional signatures either have only one suffix, or pertain to only a single stem).The top five signatures are: NULL.ed.ing, e.ed.ing, NULL.s, NULL.ed.s, and NULLed.ing.s; the third is primarily composed of noun stems (though it includes a few words from other categories—hundred, bleed, new), while the others are verb stems.Number 7, NULL.ly, identifies 105 words, of which all are adjectives (apprehensive, sumptuous, gay, ...) except for Sal, name, love, shape, and perhaps earth.The results in English are typical of the results in the other European languages that I have studied.These results, then, are derived by the application of the heuristics described above.The overall sketch of the morphology of the language is quite reasonable already in its outlines.Nevertheless, the results, when studied up close, show that there remain a good number of errors that must be uncovered using additional heuristics and evaluated using the MDL measure.These errors may be organized in the following ways: In the next section, we discuss some of the approaches we have taken to resolving these problems.Signature NULL.ly.st, for stems ence such as safebehold, deaf, weak, sunk, etc. ily Error: analyzed le.ly for e.y (stems ward such as feeb-, audib-, simp-).We can use the description length of the grammar formulated in (2) and (3) to evaluate any proposed revision, as we have already observed: note the description length of the grammar and the compressed corpus, perform a modification of the grammar, recompute the two lengths, and see if the modification improved the resulting description length.'Goldsmith Unsupervised Learning of the Morphology of a Natural Language Following the morphological analysis of words described in the previous section, suffixes are checked to determine if they are spurious amalgams of independently motivated suffixes: ments is typically, but wrongly, analyzed as a suffix.Upon identification of such suffixes as spurious, the vocabulary containing these words is reanalyzed.For example, in Tom Sawyer, the suffix ings is split into ing and s, and thus the word beings is split into being plus s; the word being is, of course, already in the lexicon.The word breathings is similarly reanalyzed as breathing plus s, but the word breathing is not found in the lexicon; it is entered, with the morphological analysis breath+ing.Words that already existed include chafing, dripping, evening, feeling, and flogging, while new &quot;virtual&quot; words include belonging, bustling, chafing, and fastening.The only new word that arises that is worthy of notice is jing, derived from the word jings found in Twain's expression by jings!In a larger corpus of 500,000 words, 64 suffixes are tested for splitting, and 31 are split, including tions, ists, ians, ened, lines, ents, and ively.Note that what it means to say that &quot;suffixes are checked to see if they are spurious amalgams&quot; is that each suffix is checked to see if it is the concatenation of two independently existing suffixes, and then if that is the case, the entire description length of the corpus is recomputed under the alternative analysis; the reanalysis is adopted if and only if the description length decreases.The same holds for the other heuristics discussed immediately below.'Following this stage, the signatures are studied to determine if there is a consistent pattern in which all suffixes from the signature begin with the same letter or sequence of letters, as in te.ting.ts.'Such signatures are evaluated to determine if the description length improves when such a signature is modified to become e.ing.s, etc.It is necessary to precede this analysis by one in which all signatures are removed which consist of a single suffix composed of a single letter.This set of signatures includes, for example, the singleton signature e, which is a perfectly valid suffix in English; however, if we permit all words ending in e, but having no other related forms, to be analyzed as containing the suffix e, then the e will be inappropriately highly valued in the analysis.(We return to this question in Section 11, where we address the question of how many occurrences of a stem with a single suffix we would expect to find in a corpus.)In the next stage of analysis, triage, signatures containing a small number of stems or a single suffix are explored in greater detail.The challenge of triage is to determine when the data is rich and strong enough to support the existence of a linguistically real signature.A special case of this is the question of how many stems must exist to motivate the existence of a signature (and hence, a morphological analysis for the words in question) when the stems only appear with a single suffix.For example, if a set of words appear in English ending with hood, should the morphological analysis split the words in that fashion, even if the stems thereby created appear with no other suffixes?And, at the other extreme, what about a corpus which contains the words look, book, loot, and boot?Does that data motivate the signature 1.k, for the stems boo and loo?The matter is rendered more complex by a number of factors.The length of the stems and suffixes in question clearly plays a role: suffixes of one letter are, all other things being equal, suspicious; the pair of stems loo and boo, appearing with the signature k. t, does not provide an example of a convincing linguistic pattern.On the other hand, if the suffix is long enough, even one stem may be enough to motivate a signature, especially if the suffix in question is otherwise quite frequent in the language.A single stem occurring with a single pair of suffixes may be a very convincing signature for other reasons as well.In Italian, for example, even in a relatively small corpus we are likely to find a signature such as a.ando.ano.are.ata.ate.ati.ato.azione.6 with several stems in it; once we are sure that the 10-suffix signature is correct, then the discovery of a subsignature along with a stem is perfectly natural, and we would not expect to find multiple stems associated with each of the occurring combinations.(A similar example in English from Tom Sawyer is NULLedfuLing.ive.less for the single stem rest.)And a signature may be &quot;contaminated,&quot; so to speak, by a spurious intruder.A corpus containing rag, rage, raged, raging, and rags gave rise to a signature: NULL.e.ed.ing.s for the stem rag.It seems clear that we need to use information that we have obtained regarding the larger, robust patterns of suffix combinations in the language to influence our decisions regarding smaller combinations.We return to the matter of triage below.We are currently experimenting with methods to improve the identification of related stems.Current efforts yield interesting but inconclusive results.We compare all pairs of stems to determine whether they can be related by a simple substitution process (one letter for none, one letter for one letter, one letter for two letters), ignoring those pairs that are related by virtue of one being the stem of the other already within the analysis.We collect all such rules, and compare by frequency.In a 500,000-word English corpus, the top two such pairs of 1:1 relationships are (1) 46 stems related by a final d/ s alternation, including intrud/intrus, apprendend/apprenhens, provid/provis, suspend/sus pens, and elud/elus, and (2) 43 stems related by a final i/ y alternation, including reli/rely, ordinari/ordinary, decri/decry, suppli/supply, and accompani/accompany.This approach can quickly locate patterns of allomorphy that are well known in the European languages (e.g., alternation between a and a in German, between o and ue in Spanish, between c and ç in French).However, we do not currently have a satisfactory means of segregating meaningful cases, such as these, from the (typically less frequent and) spurious cases of stems whose forms are parallel but ultimately not related.On the whole, the inclusion of the strategies described in the preceding sections leads to very good, but by no means perfect, results.In this section we shall review some of these results qualitatively, some quantitatively, and discuss briefly the origin of the incorrect parses.We obtain the most striking result by looking at the top list of signatures in a language, if we have some familiarity with the language: it is almost as if the textbook patterns have been ripped out and placed in a chart.As these examples suggest, the large morphological patterns identified tend to be quite accurately depicted.To illustrate the results on European languages, we include signatures found from a 500,000-word corpus of English (Table 4), a 350,000-word corpus of French (Table 5), Don Quijote, which contains 124,716 words of Spanish (Table 6), a 125,000-word corpus of Latin (Table 7), and 100,000 words and 1,000,000 words of Italian (Tables 8 and 9).The 500,000-word (token-count) corpus of English (the first part of the Brown Corpus) contains slightly more than 30,000 distinct words.To illustrate the difference of scale that is observed depending on the size of the corpus, compare the signatures obtained in Italian on a corpus of 100,000 words (Table 8) and a corpus of 1,000,000 words (Table 9).When one sees the rich inflectional pattern emerging, as with the example of the 10 suffixes on first-conjugation stems (a.ando.ano.are.ata.ate.ati.ato.azione.6), one cannot but be struck by the grammatical detail that is emerging from the study of a larger corpus.'Turning to French, we may briefly inspect the top 10 signatures that we find in a 350,000-word corpus in Table 5.It is instructive to consider the signature a.aient.ait.ant.e. ent.er.es.erent.e.ee.es, which is ranked ninth among signatures.It contains a large part of the suffixal pattern from the most common regular conjugation, the first conjugation.Within the scope of the effort covered by this project, the large-scale generalizations extracted about these languages appear to be quite accurate (leaving for further discussion below the questions of how to link related signatures and related stems).It is equally important to take a finer-grained look at the results and quantify them.To brachi carmel cenacul damn evangeli hysop lectul liban offici ole 5. i.is.o.orum.os.um.us 8. a.ae.am.as.i.is.o.orum.os.um.us do this, we have selected from the English and the French analyses a set of 1,000 consecutive words in the alphabetical list of words from the corpus and divided them into distinct sets regarding the analysis provided by the present algorithm.See Tables 10 and 11.The first category of analyses, labeled Good, is self-explanatory in the case of most words (e.g., proceed, proceeded, proceeding, proceeds), and many of the errors are equally easy to identify by eye (abide with no analysis, next to abid-e and abid-ing, or Abn-er).Quite honestly, I was surprised how many words there were in which it was difficult to say what the correct analysis was.For example, consider the pair aboli-tion and abolish.The words are clearly related, and abolition clearly has a suffix; but does it have the suffix -ion, -tion, or -ition, and does abolish have the suffix -ish, or -sh?It is hard to say.Good 833 83.3% Wrong analysis 61 6.1% Failed to analyze 42 4.2% Spurious analysis 64 6.4% In a case of this sort, my policy for assigning success or failure has been influenced by two criteria.The first is that analyses are better insofar as they explicitly relate words that are appropriately parallel in semantics, as in the abolish/abolition case; thus I would give credit to either the analysis aboli-tion/aboli-sh or the analysis abol-ition/abol-ish.The second criterion is a bit more subtle.Consider the pair of words alumnus and alumni.Should these be morphologically analyzed in a corpus of English, or rather, should failure to analyze them be penalized for this morphology algorithm?(Compare in like manner alibi or allegretti; do these English words contain suffixes?).My principle has been that if I would have given the system additional credit by virtue of discovering that relationship, I have penalized it if it did not discover it; that is a relatively harsh criterion to apply, to be sure.Should proper names be morphologically analyzed?The answer is often unclear.In the 500,000 word English corpus, we encounter Alex and Alexis, and the latter is analyzed as alex-is.I have scored this as correct, much as I have scored as correct the analyses of Alexand-er and Alexand-re.On the other hand, the failure to analyze Alexeyeva despite the presence of Alex and Alexei does not seem to me to be an error, while the analysis Anab-el has been scored as an error, but John-son (and a bit less obviously Wat-son) have not been treated as errors.'Difficult to classify, too, is the treatment of words such as abet/abetted/abetting.The present algorithm selects the uniform stem abet in that case, assigning the signature NULL.ted.ting.Ultimately what we would like to have is a means of indicating that the doubled t is predictable, and that the correct signature is NULL.ed.ing.At present this is not implemented, and I have chosen to mark this as correct, on the grounds that it is more important to identify words with the same stem than to identify the (in some sense) correct signature.Still, unclear cases remain: for example, consider the words accompani-ed/accompani-ment/accompani-st.The word accompany does not appear as such, but the stem accompany is identified in the word accompany-ing.The analysis accompani-st fails to identify the suffix -ist, but it will successfully identify the stem as being the same as the one found in accompanied and accompaniment, which it would not have done if it had associated the i with the suffix.I have, in any event, marked this analysis as wrong, but without much conviction behind the decision.Similarly, the analysis of French putative stem embelli with suffixes e/rent/t passes the low test of treating related words with the same stem, but I have counted it as in error, on the grounds that the analysis is unquestionably one letter off from the correct, traditional analysis of second-conjugation verbs.This points to a more general issue regarding French morphology, which is more complex than that of English.The infinitive ecrire 'to write' would ideally be analyzed as a stem ecr plus a derivational suffix i followed by an infinitival suffix re.Since the derivational suffix i occurs in all its inflected forms, it is not unreasonable to find an analysis in which the i is integrated into the stem itself.This is what the algorithm does, employing the stem ecri for the words ecri-re and Ecri-t. Ecrit in turn is the stem for ecrite, ecrite, ecrites, ecrits, and ecriture.An alternate stem form ecriv is used for past tense forms (and the nominalization ecrivain) with the suffixes aient, ait, ant, irent, it.The algorithm does not make explicit the connection between these two stems, as it ideally would.Thus in the tables, Good indicates the categories of words where the analysis was clearly right, while the incorrect analyses have been broken into several categories.Wrong Analysis is for bimorphemic words that are analyzed, but incorrectly analyzed, by the algorithm.Failed to Analyze are the cases of words that are bimorphemic but 29 My inability to determine the correct morphological analysis in a wide range of words that I know perfectly well seems to me to be essentially the same response as has often been observed in the case of speakers of Japanese, Chinese, and Korean when forced to place word boundaries in e-mail romanizations of their language.Ultimately the quality of a morphological analysis must be measured by how well the algorithm handles the clear cases, how well it displays the relationships between words perceived to be related, and how well it serves as the language model for a stochastic morphology of the language in question.Goldsmith Unsupervised Learning of the Morphology of a Natural Language for which no analysis was provided by the algorithm, and Spurious Analysis are the cases of words that are not morphologically complex but were analyzed as containing a suffix.For both English and French, correct performance is found in 83% of the words; details are presented in Tables 10 and 11.For English, these figures correspond to precision of 829/(829 + 52 + 83) = 85.9%, and recall of 829/(829 + 52 + 36) = 90.4%.8.Triage As noted above, the goal of triage is to determine how many stems must occur in order for the data to be strong enough to support the existence of a linguistically real signature.MDL provides a simple but not altogether satisfactory method of achieving this end.Using MDL for this task amounts to determining whether the total description length decreases when a signature is eliminated by taking all of its words and eliminating their morphological structure, and reanalyzing the words as morphologically simple (i.e., as having no morphological structure).This is how we have implemented it, in any event; one could well imagine a variant under which some or all subparts of the signature that comprised other signatures were made part of those other signatures.For example, the signature NULLinely is motivated just for the stem just.Under the former triage criterion, Justine and justly would be treated as unanalyzed words, whereas under the latter, just and justly would be made members of the (large) NULL.ly signature, and just and justine might additionally be treated as comprising parts of the signature NULL.ine along with bernard, gerald, eng, capitol, elephant, def, and sup (although that would involve permitting a single stem to participate in two distinct signatures).Our MDL-based measure tests the goodness of a signature by testing each signature o- to see if the analysis is better when that signature is deleted.This deletion entails treating the signature's words as members of the signature of unanalyzed words (which is the largest signature, and hence such signature pointers are relatively short).Each word member of the signature, however, now becomes a separate stem, with all of the increase in pointer length that that entails, as well as increase in letter content for the stem component.One may draw the following conclusions, I believe, from the straightforward application of such a measure.On the whole, the effects are quite good, but by no means as close as one would like to a human's decisions in a certain number of cases.In addition, the effects are significantly influenced by two decisions that we have already discussed: (i) the information associated with each letter, and (ii) the decision as to whether to model suffix frequency based solely on signature-internal frequences, or based on frequency across the entire morphology.The greater the information associated with each letter, the more worthwhile morphology is (because maintaining multiple copies of nearly similar stems becomes increasingly costly and burdensome).When suffix frequencies (which are used to compute the compressed length of any analyzed word) are based on the frequency of the suffixes in the entire lexicon, rather than conditionally within the signature in question, the loss of a signature entails a hit on the compression of all other words in the lexicon that employed that suffix; hence triage is less dramatic under that modeling assumption.Consider the effect of this computation on the signatures produced from a 500,000word corpus of English.After the modifications discussed to this point, but before triage, there were 603 signatures with two or more stems and two or more suffixes, and there were 1,490 signatures altogether.Application of triage leads to the loss of only 240 signatures.The single-suffix signatures that were eliminated were: ide, it, rs, he, ton, o, and ie, all of which are spurious.However, a number of signatures that should not have been lost were eliminated, most strikingly: NULL.ness, with 51 good analyses, NULL.ful, with 18 good analyses, and NULL.ish with only 8 analyses.Most of the cases eliminated, however, were indeed spurious.Counting only those signatures that involves suffixes (rather than compounds) and that were in fact correct, the percentage of the words whose analysis was incorrectly eliminated by triage was 21.9% (236 out of 1,077 changes).Interestingly, in light of the discussion on results above, one of the signatures that was lost was i.us for the Latin plural (based in this particular case on genii/genius).Also eliminated (and this is most regrettable) was NULL.n't (could/had/does/were/would /did).Because maximizing correct results is as important as testing the MDL model proposed here, I have also utilized a triage algorithm that departs from the MDLbased optimization in certain cases, which I shall identify in a moment.I believe that when the improvements identified in Section 10 below are made, the purely MDLbased algorithm will be more accurate; that prediction remains to be tested, to be sure.On this account, we discard any signature for which the total number of stem letters is less than five, and any signature consisting of a single, one-letter suffix; we keep, then, only signatures for which the savings in letter counts is greater than 15 (where savings in letter counts is simply the difference between the sum of the length of words spelled out as a monomorphemic word and the sum of the lengths of the stems and the suffixes); 15 is chosen empirically.As we noted briefly above, the existence of a regular pattern of suffixation with n distinct suffixes will generally give rise to a large set of stems displaying all n suffixes, but it will also give rise in general to stems displaying most possible combinations of subsets of these suffixes.Thus, if there is a regular paradigm in English consisting of the suffixes NULL, -s, -ing, and -ed, we expect to find stems appearing with most possible combinations of these suffixes as well.As this case clearly shows, not all such predicted subpatterns are merely partially filled paradigms.Of stems appearing with the signature NULL.s, some are verbs (such as occur/occurs), but the overwhelming majority, of course, are nouns.In the present version of the algorithm, no effort is made to directly relate signatures to one another, and this has a significant and negative impact on performance, because analyses in which stems are affiliated with high-frequency signatures are more highly valued than those in which they are affiliated with low-frequency signatures; it is thus of capital importance not to underestimate the total frequency of a signature.'When two signatures as we have defined them here are collapsed, there are two major effects on the description length: pointers to the merged signature are shorter—leading to a shorter total description length—but, in general, predicted frequencies of the corn30 As long as we keep the total number of words fixed, the global task of minimizing description length can generally be obtained by the local strategy of finding the largest cohort for a group of forms to associate with: if the same data can be analyzed in two ways, with the data forming groups of sizes {q} in one case, and {a2, }in the other, maximal compression is obtained by choosing the case (k = 1,2) for which is the greatest.Goldsmith Unsupervised Learning of the Morphology of a Natural Language posite words are worse than they were, leading to a poorer description (via increased cross-entropy, we might say).In practice, the collapsing of signatures is rejected by the MDL measure that we have implemented here.In work in progress, we treat groups of signatures (as defined here) as parts of larger groups, called paradigms.A paradigm consisting of the suffixes NULL.ed.ing.s, for example, includes all 15 possible combinations of these suffixes.We can in general estimate the number of stems we would expect to appear with zero counts for one or more of the suffixes, given a frequency distribution, such as a multinomial distribution, for the suffixes.'In this way, we can establish some reasonable frequencies for the case of stems appearing in a corpus with only a single suffix.It appears at this time that the unavailability of this information is the single most significant cause of inaccuracies in the present algorithm.It is thus of considerable importance to get a handle on such estimates.'A number of practical questions remain at this point.The most important are the following: principles at work relating pairs of stems, as in English many stems (like win) are related to another stem with a doubled consonant (winn, as in winn-ing).We have been reasonably successful in identifying such semiregular morphology, and will report this in a future publication.There is a soft line between the discovery of related stems, on the one hand, and the parsing of a word into several suffixes.For example, in the case mentioned briefly above for French, it is not unreasonable to propose two stems for 'to write' ecri and ecriv, each used in distinct forms.It would also be reasonable, in this case, to analyze the latter stem ecriv as composed of ecri plus a suffix -v, although in this case, there are no additional benefits to be gained from the more fine-grained analysis.31 In particular, consider a paradigm with a set {0 of suffixes.We may represent a subsignature of that signature as a string of Os and is (a Boolean string b, of the form f 0,1f*, abbreviated bk) indicating whether (or not) the ith suffix is contained in the subsignature.If a stem t occurs [t] times, then the probability that it occurs without a particular suffix]; is (1 — prob(fri))[t]; the probability that it occurs without all of the suffixes missing from the particular subsignature b = {bk} is and the probability that the particular subsignature b will arise at all is the sum of those values over all of the stems in the signature: Thus all that is necessary is to estimate the hidden parameters of the frequencies of the individual suffixes in the entire paradigm.See the following note as well.32 There may appear to be a contradiction between this observation about paradigms and the statement in the preceding paragraph that MDL rejects signature mergers—but there is no contradiction.The rejection of signature mergers is performed (so to speak) by the model which posits that frequencies of suffixes inside a signature are based only on suffix frequencies of the stems that appear with exactly the same set of suffixes in the corpus.It is that modeling assumption that needs to be dropped, and replaced by a multinomial-based frequency prediction based on counts over the 2n — 1 signatures belonging to each paradigm of length n. 2.Identifying paradigms from signatures.We would like to automatically identify NULLed.ing as a subcase of the more general NULL.ed.ing.s.This is a difficult task to accomplish well, as English illustrates, for we would like to be able to determine that NULL.s is primarily a subcase of 's.NULL.s, and not of (e.g.)NULLed.s.'3.Determining the relationship between prefixation and suffixation.The system currently assumes that prefixes are to be stripped off the stem that has already been identified by suffix stripping.In future work, we would like to see alternative hypotheses regarding the relationship of prefixation and suffixation tested by the MDL criterion.4.Identifying compounds.In work reported in Goldsmith and Reutter (1998), we have explored the usefulness of the present system for determining the linking elements used in German compounds, but more work remains to be done to identify compounds in general.Here we run straight into the problem of assigning very short strings a lower likelihood of being words than longer strings.That is, it is difficult to avoid positing a certain number of very short stems, as in English m and an, the first because of pairs such as me and my, the second because of pairs such as an and any, but these facts should not be taken as strong evidence that man is a compound.5.As noted at the outset, the present algorithm is limited in its ability to discover the morphology of a language in which there are not a sufficient number of words with only one suffix in the corpus.In work in progress, we are developing a related algorithm that deals with the 33 We noted in the preceding section that we can estimate the likelihood of a subsignature assuming a multirtomial distribution.We can in fact do better than was indicated there, in the sense that for a given observed signature a*, whose suffixes constitute a subset of a larger signature a, we can compute the likelihood that a is responsible for the generation of a&quot;, where {cb,} are the frequencies (summing to 1.0) associating with each of the suffixes in a, and are the counts of the corresponding suffixes in the observed signature a*: <PI from Stirling's approximation.If we normalize the c,s to form a distribution (by dividing by [t]) and denote these by di, then this can be simply expressed in terms of the Kullback-Leibler distance Goldsmith Unsupervised Learning of the Morphology of a Natural Language more general case.In the more general case, it is even more important to develop a model that deals with the layered relationship among suffixes in a language.The present system does not explicitly deal with these relationships: for example, while it does break up ments into ment and s, it does not explicitly determine which suffixes s may attach to, etc.This must be done in a more adequate version.6.In work in progress, we have added to the capability of the algorithm the ability to posit suffixes that are in part subtractive morphemes.That is, in English, we would like to establish a single signature that combines NULL.ed.ing.s and e.ed.es.ing (for jump and love, respectively).We posit an operator (x) which deletes a preceding character x, and with the mechanism, we can establish a single signature NULL.(e)ed.(e)ing.s, composed of familiar suffixes NULL and s, plus two suffixes (e)ed and (e)ing, which delete a preceding (stem-final) e if one is present.11.Conclusion Linguists face at the present time the question of whether, and to what extent, information-theoretic notions will play a significant role in our understanding of linguistic theory over the years to come, and the present system perhaps casts a small ray of light in this area.As we have already noted, MDL analysis makes clear what the two areas are in which an analysis can be judged: it can be judged in its ability to deal with the data, as measured by its ability to compress the data, and it can be judged on its complexity as a theory.While the former view is undoubtedly controversial when viewed from the light of mainstream linguistics, it is the prospect of being able to say something about the complexity of a theory that is potentially the most exciting.Even more importantly, to the extent that we can make these notions explicit, we stand a chance of being able to develop an explicit model of language acquisition employing these ideas.A natural question to ask is whether the algorithm presented here is intended to be understood as a hypothesis regarding the way in which human beings acquire morphology.I have not employed, in the design of this algorithm, a great deal of innate knowledge regarding morphology, but that is for the simple reason that knowledge of how words divide into subpieces is an area of knowledge which no one would take to be innate in any direct fashion: if sanity is parsed as san + ity in one language, it may perfectly well be parsed as sa + nity in another language.That is, while passion may flame disagreements between partisans of Universal Grammar and partisans of statistically grounded empiricism regarding the task of syntax acquisition, the task which we have studied here is a considerably more humble one, which must in some fashion or other be figured out by grunt work by the language learner.It thus allows us a much sharper image of how powerful the tools are likely to be that the language acquirer brings to the task.And does the human child perform computations at all like the ones proposed here?From most practical points of view, nothing hinges on our answer to this question, but it is a question that ultimately we cannot avoid facing.Reformulated a bit, one might pose the question, does the young language learner—who has access not only to the spoken language, but perhaps also to the rudiments of the syntax and to the intended meaning of the words and sentences—does the young learner have access to additional information that simplifies the task of morpheme identification?It is the belief that the answer to this question is yes that drives the intuition (if one has this intuition) that an MDL-based analysis of the present sort is an unlikely model of human language acquisition.But I think that such a belief is very likely mistaken.Knowledge of semantics and even grammar is unlikely to make the problem of morphology discovery significantly easier.In surveying the various approaches to the problem that I have explored (only the best of which have been described here), I do not know of any problem (of those which the present algorithm deals with successfully) that would have been solved by having direct access to either syntax or semantics.To the contrary: I have tried to find the simplest algorithm capable of dealing with the facts as we know them.The problem of determining whether two distinct signatures derive from a single larger paradigm would be simplified with such knowledge, but that is the exception and not the rule.So in the end, I think that the hypothesis that the child uses an MDL-like analysis has a good deal going for it.In any event, it is far from clear to me how one could use information, either grammatical or contextual, to elucidate the problem of the discovery of morphemes without recourse to notions along the lines of those used in the present algorithm.Of course, in all likelihood, the task of the present algorithm is not the same as the language learner's task; it seems unlikely that the child first determines what the words are in the language (at least, the words as they are defined in traditional orthographic terms) and then infers the morphemes.The more general problem of language acquisition is one that includes the problems of identifying morphemes, of identifying words both morphologically analyzed and nonanalyzed, of identifying syntactic categories of the words in question, and of inferring the rules guiding the distribution of such syntactic categories.It seems to me that the only manageable kind of approach to dealing with such a complex task is to view it as an optimization problem, of which MDL is one particular style.Chomsky's early conception of generative grammar (Chomsky 1975 [1955]; henceforth LSLT) was developed along these lines as well; his notion of an evaluation metric for grammars was equivalent in its essential purpose to the description length of the morphology utilized in the present paper.The primary difference between the LSLT approach and the MDL approach is this: the LSLT approach conjectured that the grammar of a language could be factored into two parts, one universal and one languageparticular; and when we look for the simplest grammatical description of a given corpus (the child's input) it is only the language-particular part of the description that contributes to complexity—that is what the theory stipulates.By contrast, the MDL approach makes minimal universal assumptions, and so the complexity of everything comprising the description of the corpus must be counted in determining the complexity of the description.The difference between these hypotheses vanishes asymptotically (as Janos Simon has pointed out to me) as the size of the language increases, or to put it another way, strong Chomskian rationalism is indistinguishable from pure empiricism as the information content of the (empiricist) MDL-induced grammar increases in size relative to the information content of UG.Rephrasing that slightly, the significance of Chomskian-style rationalism is greater, the simpler language-particular grammars are, and it is less significant as language-particular grammars grow larger, and in the limit, as the size of grammars grows asymptotically, traditional generative grammar is indistinguishable from MDL-style rationalism.We return to this point below.There is a striking point that has so far remained tacit regarding the treatment of this problem in contemporary linguistic theory.That point is this: the problem addressed in this paper is not mentioned, not defined, and not addressed.The problem of dividing up words into morphemes is generally taken as one that is so trivial and Goldsmith Unsupervised Learning of the Morphology of a Natural Language devoid of interest that morphologists, or linguists more generally, simply do not feel obliged to think about the problem.34 In a very uninteresting sense, the challenge presented by the present paper to current morphological theory is no challenge at all, because morphological theory makes no claims to knowing how to discover morphological analysis; it claims only to know what to do once the morphemes have been identified.The early generative grammar view, as explored in LSLT, posits a grammar of possible grammars, that is, a format in which the rules of the morphology and syntax must be written, and it establishes the semantics of these rules, which is to say, how they function.This grammar of grammars is called variously Universal Grammar, or Linguistic Theory, and it is generally assumed to be accessible to humans on the basis of an innate endowment, though one need not buy into that assumption to accept the rest of the theory.In Syntactic Structures (Chomsky 1957, 51ff.), Chomsky famously argued that the goal of a linguistic theory that produces a grammar automatically, given a corpus as input, is far too demanding a goal.His own theory cannot do that, and he suggests that no one else has any idea how to accomplish the task.He suggests furthermore that the next weaker position—that of developing a linguistic theory that could determine, given the data and the account (grammar), whether this was the best grammar—was still significantly past our theoretical reach, and he suggests finally that the next weaker position is a not unreasonable one to expect of linguistic theory: that it be able to pass judgment on which of two grammars is superior with respect to a given corpus.That position is, of course, exactly the position taken by the MDL framework, which offers no help in coming up with analyses, but which is excellent at judging the relative merits of two analyses of a single corpus of data.In this paper, we have seen this point throughout, for we have carefully distinguished between heuristics, which propose possible analyses and modifications of analyses, on the one hand, and the MDL measurement, which makes the final judgment call, deciding whether to accept a modification proposed by the heuristics, on the other.On so much, the early generative grammar of LSLT and MDL agree.But they disagree with regard to two points, and on these points, MDL makes clearer, more explicit claims, and both claims appear to be strongly supported by the present study.The two points are these: the generative view is that there is inevitably an idiosyncratic character to Universal Grammar that amounts to a substantive innate capacity, on the grounds (in part) that the task of discovering the correct grammar of a human language, given only the corpus available to the child, is insurmountable, because this corpus is not sufficient to home in on the correct grammar.The research strategy associated with this position is to hypothesize certain compression techniques (generally called &quot;rule formalisms&quot; in generative grammar) that lead to significant reduction in the size of the grammars of a number of natural languages, compared to what would have been possible without them.Sequential rule ordering is one such suggestion discussed at length in LSLT.To reformulate this in a fashion that allows us to make a clearer comparison with MDL, we may formulate early generative grammar in the following way: To select the correct Universal Grammar out of a set of proposed Universal Grammars {UG}, given corpora for a range of human languages, select that UG for which the sum of the sizes of the grammars for all of the corpora is the smallest.It does not follow—it need not be the case—that the grammar of English (or German, etc.) selected by the winning UG is the shortest one of all the candidate English grammars, but the winning UG is all-round the supplier of the shortest grammars around the world.m MDL could be formulated in those terms, undoubtedly, but it also can be formulated in a language-particular fashion, which is how it has been used in this paper.Generative grammar is inherently universalist; it has no language-particular format, other than to say that the best grammar for a given language is the shortest grammar.But we know that such a position is untenable, and it is precisely out of that knowledge that MDL was born.The position is untenable because we can always make an arbitrarily small compression of a given set of data, if we are allowed to make the grammar arbitrarily complex, to match and, potentially, to overfit the data, and it is untenable because generative grammar offers no explicit notion of how well a grammar must match the training data.MDL's insight is that it is possible to make explicit the trade-off between complexity of the analysis and snugness of fit to the data-corpus in question.The first tool in that computational trade-off is the use of a probabilistic model to compress the data, using stock tools of classical information theory.These notions were rejected as irrelevant by early workers in early generative grammar (Goldsmith 2001).Notions of probabilistic grammar due to Solomonoff (1995) were not integrated into that framework, and the possibility of using them to quantify the goodness of fit of a grammar to a corpus was not exploited.It seems to me that it is in this context that we can best understand the way in which traditional generative grammar and contemporary probabilistic grammar formalism can be understood as complementing each other.I, at least, take it in that way, and this paper is offered in that spirit.Since what we are really interested in computing is not the minimum description length as such, but rather the difference between the description length of one model and that of a variant, it is convenient to consider the general form of the difference between two MDL computations.In general, let us say we will compare two analyses Si and S2 for the same corpus, where S2 typically contains some item(s) that Si does not (or they may differ by where they break a string into factors).Let us write out the difference in length between these two analyses, as in (7)—(11), calculating the length of Si minus the length of S2.The general formulas derived in (7)—(11) are not of direct computational interest; they serve rather as a template that can be filled in to compute the change in description length occasioned by a particular structural change in the morphology proposed by a particular heuristic.This template is rather complex in its most general form, but it simplifies considerably in any specific application.The heuristic determines which of the terms in these formulas take on nonzero values, and what their values are; the overall formula determines whether the change in question improves the description length.In addition, we may regard the formulas in 35 As the discussion in the text may suggest, I am skeptical of the generative position, and I would like to identify what empirical result would confirm the generative position and dissolve my skepticism.The result would be the discovery of two grammars of English, G1 and G2, with the following properties: G1 is inherently simpler than G2, using some appropriate notion of Turing machine program complexity, and yet G2 is the correct grammar of English, based on some of the complexity of G2 being the responsibility of linguistic theory, hence &quot;free&quot; in the complexity competition between G1 and G2.That is, the proponent of the generative view must be willing to acknowledge that overall complexity of the grammar of a language may be greater than logically necessary due to evolution's investment in one particular style of programming language.Goldsmith Unsupervised Learning of the Morphology of a Natural Language (7)-(11) as offering us an exact and explicit statement of how a morphology can be improved.The notation can be considerably simplified if we take some care in advance.Note first that in (7) and below, several items are subscripted to indicate whether they should be counted as in Si or S2.Much of the simplification comes from observing, first, that second, that this difference is generally computed inside a summation over a set of morphemes, and hence the first term simplifies to a constant times the type count of the morphemes in the set in question.Indeed, so prevalent in these calculations is the formula where the numerator is a count in Si, and the denominator a count of the same variable in S2; if no confusion would result, we write Ax.36 Let us review the terms listed in (7)-(11).W is a measure of the change in the number of total words due to the proposed modification (the difference between the Si and S2 analyses); an increase in the total number of words results in a slightly negative value.In the text above, I indicated that we could, by judicious choice of word count distribution, keep Wi = W2; I have included the more general case in (7)-(11) where the two may be different.INs and WC are similar measures in the change of words that have morphologically simple, and morphologically complex, stems, respectively.They measure the global effects of the typically small changes brought about by a hypothetical change in morphological model.In the derivation of each formula, we consider first the case of those morphemes that are found in both Si and S2 (indicated (S1, S2)), followed by those found only in Si (S1, - S2), and then those only found in S2 Si, 52).Recall that angle brackets are used to indicate the type count of a set, the number of typographically distinct members of a set.In (8), we derive a formula for the change in length of the suffix component of the morphology.Observe the final formulation, in which the first two terms involve suffixes present in both Si and S2, while the third term involves suffixes present only in Si and the fourth term involves suffixes present only in S2.This format will appear in all of the components of this computation.Recall that the function Ltypo specifies the length of a string in bits, which we may take here to be simply log(26) times the number of characters in the string.In (9), we derive the corresponding formula for the stem component.The general form of the computation of the change to the signature component (10) is more complicated, and this complexity motivates a little bit more notation to simplify it.First, we can compute the change in the pointers to the signatures, and the information that each signature contains regarding the count of its stems and suffixes 36 We beg the reader's indulgence in recognizing that we prepend the operator A immediately to the left of the name of a set to indicate the change in the size of the counts of the set, which is to say, &quot;AW&quot; is shorthand for &quot;A([W])&quot;, and &quot;A(W)&quot; for &quot;A((W))&quot;. as in (10a).But the heart of the matter is the treatment of the stems and suffixes within the signatures, given in (10b)—(10d).Bear in mind, first of all, that each signature consists of a list of pointers to stems, and a list of pointers to suffixes.The treatment of suffixes is given in (10d), and is relatively straightforward, but the treatment of stems (10c) is a bit more complex.Recall that all items on the stem list will be pointed to by exactly one stem pointer, located in some particular signature.All stem pointers in a signature that point to stems on the suffix list are directly described a &quot;simple&quot; word, a notion we have already encountered: a word whose stem is not further analyzable.But other words may be complex, that is, may contain a stem whose pointer is to an analyzable word, and hence the stem's representation consists of a pointer triple: a pointer to a signature, a stem within the signature, and a suffix within the signature.And each stem pointer is preceded by a flag indicating which type of stem it is.We thus have three things whose difference in the two states, S1 and S2, we wish to compute.The difference of the lengths of the flag is given in (10c.i).In (10c.ii), we need change in the total length of the pointers to the stems, and this has actually already been computed, during the computation of (9).37 Finally in (10c.iii), the set of pointers from certain stem positions to words consists of pointers to all of the words that we have already labeled as being in Wc, and we can compute the length of these pointers by adding counts to these words; the length of the pointers to these words needs to be computed anyway in determining the compressed length of the corpus.This completes the computations needed to compare two states of the morphology.In addition, we must compute the difference in the compressed length of the corpus in the two states, and this is given in (11).37 The equivalence between the number computed in (9) and the number needed here is not exactly fortuitous, but it is not an error either.The figure computed in (9) describes an aspect of the complexity of the morphology as a whole, whereas the computation described here in the text is what it is because we have made the assumption that each stem occurs in exactly one signature.That assumption is not, strictly speaking, correct in natural language; we could well imagine an analysis that permitted the same stem to appear in several distinction signatures, and in that case, the computation here would not reduce to (9).But the assumption made in the text is entirely reasonable, and simplifies the construction for us.
This article documents a large set of heretofore unpublished details Collins used in his parser, such that, along with Collins’ (1999) thesis, this article contains all information necessary to duplicate Collins’ benchmark results.Indeed, these as-yet-unpublished details account for an 11% relative increase in error from an implementation including all details to a clean-room implementation of Collins’ model.We also show a cleaner and equally well-performing method for the handling of punctuation and conjunction and reveal certain other probabilistic oddities about Collins’ parser.We not only analyze the effect of the unpublished details, but also reanalyze the effect of certain well-known details, revealing that bilexical dependencies are barely used by the model and that head choice is not nearly as important to overall parsing performance as once thought.Finally, we perform experiments that show that the true discriminative power of lexicalization appears to lie in the fact that unlexicalized syntactic structures are generated conditioning on the headword and its part of speech.Michael Collins’ (1996, 1997, 1999) parsing models have been quite influential in the field of natural language processing.Not only did they achieve new performance benchmarks on parsing the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), and not only did they serve as the basis of Collins’ own future work (Collins 2000; Collins and Duffy 2002), but they also served as the basis of important work on parser selection (Henderson and Brill 1999), an investigation of corpus variation and the effectiveness of bilexical dependencies (Gildea 2001), sample selection (Hwa 2001), bootstrapping non-English parsers (Hwa, Resnik, and Weinberg 2002), and the automatic labeling of semantic roles and predicate-argument extraction (Gildea and Jurafsky 2000; Gildea and Palmer 2002), as well as that of other research efforts.Recently, in order to continue our work combining word sense with parsing (Bikel 2000) and the study of language-dependent and -independent parsing features (Bikel and Chiang 2000), we built a multilingual parsing engine that is capable of instantiating a wide variety of generative statistical parsing models (Bikel 2002).1 As an appropriate baseline model, we chose to instantiate the parameters of Collins’ Model 2.This task proved more difficult than it initially appeared.Starting with Collins’ (1999) thesis, we reproduced all the parameters described but did not achieve nearly the same high performance on the well-established development test set of Section 00 of the Penn Treebank.Together with Collins’ thesis, this article contains all the information necessary to replicate Collins’ parsing results.2 Specifically, this article describes all the as-yet-unpublished details and features of Collins’ model and some analysis of the effect of these features with respect to parsing performance, as well as some comparative analysis of the effects of published features.3 In particular, implementing Collins’ model using only the published details causes an 11% increase in relative error over Collins’ own published results.That is, taken together, all the unpublished details have a significant effect on overall parsing performance.In addition to the effects of the unpublished details, we also have new evidence to show that the discriminative power of Collins’ model does not lie where once thought: Bilexical dependencies play an extremely small role in Collins’ models (Gildea 2001), and head choice is not nearly as critical as once thought.This article also discusses the rationale for various parameter choices.In general, we will limit our discussion to Collins’ Model 2, but we make occasional reference to Model 3, as well.There are three primary motivations for this work.First, Collins’ parsing model represents a widely used and cited parsing model.As such, if it is not desirable to use it as a black box (it has only recently been made publicly available), then it should be possible to replicate the model in full, providing a necessary consistency among research efforts employing it.Careful examination of its intricacies will also allow researchers to deviate from the original model when they think it is warranted and accurately document those deviations, as well as understand the implications of doing so.The second motivation is related to the first: science dictates that experiments be replicable, for this is the way we may test and validate them.The work described here comes in the wake of several previous efforts to replicate this particular model, but this is the first such effort to provide a faithful and equally well-performing emulation of the original.The third motivation is that a deep understanding of an existing model—its intricacies, the interplay of its many features—provides the necessary platform for advancement to newer, “better” models.This is especially true in an area like statistical parsing that has seen rapid maturation followed by a soft “plateau” in performance.Rather than simply throwing features into a new model and measuring their effect in a crude way using standard evaluation metrics, this work aims to provide a more thorough understanding of the nature of a model’s features.This understanding not only is useful in its own right but should help point the way toward newer features to model or better modeling techniques, for we are in the best position for advancement when we understand existing strengths and limitations.2 In the course of replicating Collins’ results, it was brought to our attention that several other researchers had also tried to do this and had also gotten performance that fell short of Collins’ published results.For example, Gildea (2001) reimplemented Collins’ Model 1 but obtained results with roughly 16.7% more relative error than Collins’ reported results using that model.The Collins parsing model decomposes the generation of a parse tree into many small steps, using reasonable independence assumptions to make the parameter estimation problem tractable.Even though decoding proceeds bottom-up, the model is defined in a top-down manner.Every nonterminal label in every tree is lexicalized: the label is augmented to include a unique headword (and that headword’s part of speech) that the node dominates.The lexicalized PCFG that sits behind Model 2 has rules of the form where P, L;, R;, and H are all lexicalized nonterminals, and P inherits its lexical head from its distinguished head-child, H. In this generative model, first P is generated, then its head-child H, then each of the left- and right-modifying nonterminals are generated from the head outward.The modifying nonterminals L; and R; are generated conditioning on P and H, as well as a distance metric (based on what material intervenes between the currently generated modifying nonterminal and H) and an incremental subcategorization frame feature (a multiset containing the arguments of H that have yet to be generated on the side of H in which the currently generated nonterminal falls).Note that if the modifying nonterminals were generated completely independently, the model would be very impoverished, but in actuality, because it includes the distance and subcategorization frame features, the model captures a crucial bit of linguistic reality, namely, that words often have well-defined sets of complements and adjuncts, occurring with some well-defined distribution in the right-hand sides of a (context-free) rewriting system.The process proceeds recursively, treating each newly generated modifier as a parent and then generating its head and modifier children; the process terminates when (lexicalized) preterminals are generated.As a way to guarantee the consistency of the model, the model also generates two hidden +STOP+ nonterminals as the leftmost and rightmost children of every parent (see Figure 7).To the casual reader of Collins’ thesis, it may not be immediately apparent that there are quite a few preprocessing steps for each annotated training tree and that these steps are crucial to the performance of the parser.We identified 11 preprocessing steps necessary to prepare training trees when using Collins’ parsing model: The order of presentation in the foregoing list is not arbitrary, as some of the steps depend on results produced in previous steps.Also, we have separated the steps into their functional units; an implementation could combine steps that are independent of one another (for clarity, our implementation does not, however).Finally, we note that the final step, head-finding, is actually required by some of the previous steps in certain cases; in our implementation, we selectively employ a head-finding module during the first 10 steps where necessary.A few of the preprocessing steps rely on the notion of a coordinated phrase.In this article, the conditions under which a phrase is considered coordinated are slightly more detailed than is described in Collins’ thesis.A node represents a coordinated phrase if In the Penn Treebank, a coordinating conjunction is any preterminal node with the label CC.This definition essentially picks out all phrases in which the head-child is truly conjoined to some other phrase, as opposed to a phrase in which, say, there is an initial CC, such as an S that begins with the conjunction but.As a preprocessing step, pruning of unnecessary nodes simply removes preterminals that should have little or no bearing on parser performance.In the case of the English Treebank, the pruned subtrees are all preterminal subtrees whose root label is one of {‘‘, ’’, .}.There are two reasons to remove these types of subtrees when parsing the English Treebank: First, in the treebanking guidelines (Bies 1995), quotation marks were given the lowest possible priority and thus cannot be expected to appear within constituent boundaries in any kind of consistent way, and second, neither of these types of preterminals—nor any punctuation marks, for that matter—counts towards the parsing score.An NP is basal when it does not itself dominate an NP; such NP nodes are relabeled NPB.More accurately, an NP is basal when it dominates no other NPs except possessive NPs, where a possessive NP is an NP that dominates POS, the preterminal possessive A nonhead NPB child of NP requires insertion of extra NP. marker for the Penn Treebank.These possessive NPs are almost always themselves base NPs and are therefore (almost always) relabeled NPB.For consistency’s sake, when an NP has been relabeled as NPB, a normal NP node is often inserted as a parent nonterminal.This insertion ensures that NPB nodes are always dominated by NP nodes.The conditions for inserting this “extra” NP level are slightly more detailed than is described in Collins’ thesis, however.The extra NP level is added if one of the following conditions holds: In postprocessing, when an NPB is an only child of an NP node, the extra NP level is removed by merging the two nodes into a single NP node, and all remaining NPB nodes are relabeled NP.The insertion of extra NP levels above certain NPB nodes achieves a degree of consistency for NPs, effectively causing the portion of the model that generates children of NP nodes to have less perplexity.Collins appears to have made a similar effort to improve the consistency of the NPB model.NPB nodes that have sentential nodes as their final (rightmost) child are “repaired”: The sentential child is raised so that it becomes a new right-sibling of the NPB node (see Figure 3).6 While such a transformation is reasonable, it is interesting to note that Collins’ parser performs no equivalent detransformation when parsing is complete, meaning that when the parser produces the “repaired” structure during testing, there is a spurious NP bracket.7 The gap feature is discussed extensively in chapter 7 of Collins’ thesis and is applicable only to his Model 3.The preprocessing step in which gap information is added locates every null element preterminal, finds its co-indexed WHNP antecedent higher up in the tree, replaces the null element preterminal with a special trace tag, and threads the gap feature in every nonterminal in the chain between the common ancestor of the antecedent and the trace.The threaded-gap feature is represented by appending -g to every node label in the chain.The only detail we would like to highlight here is that an implementation of this preprocessing step should check for cases in which threading is impossible, such as when two filler-gap dependencies cross.An implementation should be able to handle nested filler-gap dependencies, however.The node labels of sentences with no subjects are transformed from S to SG.This step enables the parsing model to be sensitive to the different contexts in which such subjectless sentences occur as compared to normal S nodes, since the subjectless sentences are functionally acting as noun phrases.Collins’ example of illustrates the utility of this transformation.However, the conditions under which an S may be relabeled are not spelled out; one might assume that every S whose subject (identified in the Penn Treebank with the -SBJ function tag) dominates a null element should be relabeled SG.In actuality, the conditions are much stricter.An S is relabeled SG when the following conditions hold: The latter two conditions appear to be an effort to capture only those subjectless sentences that are based around gerunds, as in the flying planes example.8 Removing null elements simply involves pruning the tree to eliminate any subtree that dominates only null elements.The special trace tag that is inserted in the step that adds gap information (Section 4.5) is excluded, as it is specifically chosen to be something other than the null-element preterminal marker (which is -NONE- in the Penn Treebank).The step in which punctuation is raised is discussed in detail in chapter 7 of Collins’ thesis.The main idea is to raise punctuation—which is any preterminal subtree in which the part of speech is either a comma or a colon—to the highest possible point in the tree, so that it always sits between two other nonterminals.Punctuation that occurs at the very beginning or end of a sentence is “raised away,” that is, pruned.In addition, any implementation of this step should handle the case in which multiple punctuation elements appear as the initial or final children of some node, as well as the more pathological case in which multiple punctuation elements appear along the left or right frontier of a subtree (see Figure 4).Finally, it is not clear what to do with nodes that dominate only punctuation preterminals.Our implementation simply issues a warning in such cases and leaves the punctuation symbols untouched.Head-children are not exempt from being relabeled as arguments.Collins employs a small set of heuristics to mark certain nonterminals as arguments, by appending -A to the nonterminal label.This section reveals three unpublished details about Collins’ argument finding: This step simply involves stripping away all nonterminal augmentations, except those that have been added from other preprocessing steps (such as the -A augmentation for argument labels).This includes the stripping away of all function tags and indices marked by the Treebank annotators.Head moves from right to left conjunct in a coordinated phrase, except when the parent nonterminal is NPB.With arguments identified as described in Section 4.9, if a subjectless sentence is found to have an argument prior to its head, this step detransforms the SG so that it reverts to being an S. Head-finding is discussed at length in Collins’ thesis, and the head-finding rules used are included in his Appendix A.There are a few unpublished details worth mentioning, however.There is no head-finding rule for NX nonterminals, so the default rule of picking the leftmost child is used.10 NX nodes roughly represent the N’ level of syntax and in practice often denote base NPs.As such, the default rule often picks out a less-thanideal head-child, such as an adjective that is the leftmost child in a base NP.Collins’ thesis discusses a case in which the initial head is modified when it is found to denote the right conjunct in a coordinated phrase.That is, if the head rules pick out a head that is preceded by a CC that is non-initial, the head should be modified to be the nonterminal immediately to the left of the CC (see Figure 6).An important detail is that such “head movement” does not occur inside base NPs.That is, a phrase headed by NPB may indeed look as though it constitutes a coordinated phrase—it has a CC that is noninitial but to the left of the currently chosen head—but the currently chosen head should remain chosen.11 As we shall see, there is exceptional behavior for base NPs in almost every part of the Collins parser.10 In our first attempt at replicating Collins’ results, we simply employed the same head-finding rule for NX nodes as for NP nodes.This choice yields different—but not necessarily inferior—results.11 In Section 4.1, we defined coordinated phrases in terms of heads, but here we are discussing how the head-finder itself needs to determine whether a phrase is coordinated.It does this by considering the potential new choice of head: If the head-finding rules pick out a head that is preceded by a noninitial CC (Jane), will moving the head to be a child to the left of the CC (John) yield a coordinated phrase?If so, then the head should be moved—except when the parent is NPB. vi feature is true when generating right-hand +STOP+ nonterminal, because the NP the will to continue contains a verb.The trainer’s job is to decompose annotated training trees into a series of head- and modifier-generation steps, recording the counts of each of these steps.Referring to (1), each H, Li, and Ri are generated conditioning on previously generated items, and each of these events consisting of a generated item and some maximal history context is counted.Even with all this decomposition, sparse data are still a problem, and so each probability estimate for some generated item given a maximal context is smoothed with coarser distributions using less context, whose counts are derived from these “top-level” head- and modifier-generation counts.As mentioned in Section 3, instead of generating each modifier independently, the model conditions the generation of modifiers on certain aspects of the history.One such function of the history is the distance metric.One of the two components of this distance metric is what we will call the “verb intervening” feature, which is a predicate vi that is true if a verb has been generated somewhere in the surface string of the previously generated modifiers on the current side of the head.For example, in Figure 7, when generating the right-hand +STOP+ nonterminal child of the VP, the vi predicate is true, because one of the previously generated modifiers on the right side of the head dominates a verb, continue.12 More formally, this feature is most easily defined in terms of a recursively defined cv (“contains verb”) predicate, which is true if and only if a node dominates a verb: Bikel Intricacies of Collins’ Parsing Model Referring to (2), we define the verb-intervening predicate recursively on the first-order Markov process generating modifying nonterminals: and similarly for right modifiers.What is considered to be a verb?While this is not spelled out, as it happens, a verb is any word whose part-of-speech tag is one of {VB, VBD, VBG, VBN, VBP, VBZ}.That is, the cv predicate returns true only for these preterminals and false for all other preterminals.Crucially, this set omits MD, which is the marker for modal verbs.Another crucial point about the vi predicate is that it does not include verbs that appear within base NPs.Put another way, in order to emulate Collins’ model, we need to amend the definition of cv by stipulating that cv(NPB) = false.One oddity of Collins’ trainer that we mention here for the sake of completeness is that it skips certain training trees.For “odd historical reasons,”13 the trainer skips all trees with more than 500 tokens, where a token is considered in this context to be a word, a nonterminal label, or a parenthesis.This oddity entails that even some relatively short sentences get skipped because they have lots of tree structure.In the standard Wall Street Journal training corpus, Sections 02–21 of the Penn Treebank, there are 120 such sentences that are skipped.Unless there is something inherently wrong with these trees, one would predict that adding them to the training set would improve a parser’s performance.As it happens, there is actually a minuscule (and probably statistically insignificant) drop in performance (see Table 5) when these trees are included.5.3.1 The Threshold Problem.Collins mentions in chapter 7 of his thesis that “[a]ll words occurring less than 5 times in training data, and words in test data which have never been seen in training, are replaced with the ‘UNKNOWN’ token (page 186).” The frequency below which words are considered unknown is often called the unknownword threshold.Unfortunately, this term can also refer to the frequency above which words are considered known.As it happens, the unknown-word threshold Collins uses in his parser for English is six, not five.14 To be absolutely unambiguous, words that occur fewer than six times, which is to say, words that occur five times or fewer, in the data are considered “unknown.” words into the parsing model, then, is simply to map all low-frequency words in the training data to some special +UNKNOWN+ token before counting top-level events for parameter estimation (where “low-frequency” means “below the unknown-word threshold”).Collins’ trainer actually does not do this.Instead, it does not directly modify any of the words in the original training trees and proceeds to break up these unmodified trees into the top-level events.After these events have been collected 13 This phrase was taken from a comment in one of Collins’ preprocessing Perl scripts.14 As with many of the discovered discrepancies between the thesis and the implementation, we determined the different unknown-word threshold through reverse engineering, in this case, through an analysis of the events output by Collins’ trainer. and counted, the trainer selectively maps low-frequency words when deriving counts for the various context (back-off) levels of the parameters that make use of bilexical statistics.If this mapping were performed uniformly, then it would be identical to mapping low-frequency words prior to top-level event counting; this is not the case, however.We describe the details of this unknown-word mapping in Section 6.9.2.While there is a negligible yet detrimental effect on overall parsing performance when one uses an unknown-word threshold of five instead of six, when this change is combined with the “obvious” method for handling unknown words, there is actually a minuscule improvement in overall parsing performance (see Table 5).All parameters that generate trees in Collins’ model are estimates of conditional probabilities.Even though the following overview of parameter classes presents only the maximal contexts of the conditional probability estimates, it is important to bear in mind that the model always makes use of smoothed probability estimates that are the linear interpolation of several raw maximum-likelihood estimates, using various amounts of context (we explore smoothing in detail in Section 6.8).In Sections 4.5 and 4.9, we saw how the raw Treebank nonterminal set is expanded to include nonterminals augmented with -A and -g. Although it is not made explicit in Collins’ thesis, Collins’ model uses two mapping functions to remove these augmentations when including nonterminals in the history contexts of conditional probabilities.Presumably this was done to help alleviate sparse-data problems.We denote the “argument removal” mapping function as alpha and the “gap removal” mapping function as gamma.For example: Since gap augmentations are present only in Model 3, the gamma function effectively is the identity function in the context of Models 1 and 2.The head nonterminal is generated conditioning on its parent nonterminal label, as well as the headword and head tag which they share, since parents inherit their lexical head information from their head-children.More specifically, an unlexicalized head nonterminal label is generated conditioning on the fully lexicalized parent nonterminal.We denote the parameter class as follows: When the model generates a head-child nonterminal for some lexicalized parent nonterminal, it also generates a kind of subcategorization frame (subcat) on either side of the head-child, with the following maximal context: A fully lexicalized tree.The VP node is the head-child of S. Probabilistically, it is as though these subcats are generated with the head-child, via application of the chain rule, but they are conditionally independent.15 These subcats may be thought of as lists of requirements on a particular side of a head.For example, in Figure 8, after the root node of the tree has been generated (see Section 6.10), the head child VP is generated, conditioning on both the parent label S and the headword of that parent, sat–VBD.Before any modifiers of the head-child are generated, both a left- and right-subcat frame are generated.In this case, the left subcat is {NP-A} and the right subcat is {}, meaning that there are no required elements to be generated on the right side of the head.Subcats do not specify the order of the required arguments.They are dynamically updated multisets: When a requirement has been generated, it is removed from the multiset, and subsequent modifiers are generated conditioning on the updated multiset.16 The implementation of subcats in Collins’ parser is even more specific: Subcats are multisets containing various numbers of precisely six types of items: NP-A, S-A, SBAR-A, VP-A, g, and miscellaneous.The g indicates that a gap must be generated and is applicable only to Model 3.Miscellaneous items include all nonterminals that were marked as arguments in the training data that were not any of the other named types.There are rules for determining whether NPs, Ss, SBARs, and VPs are arguments, and the miscellaneous arguments occur as the result of the argument-finding rule for PPs, which states that the first non-PRN, non-part-of-speech tag that occurs after the head of a PP should be marked as an argument, and therefore nodes that are not one of the four named types can be marked.As mentioned above, after a head-child and its left and right subcats are generated, modifiers are generated from the head outward, as indicated by the modifier nonterminal indices in Figure 1.A fully lexicalized nonterminal has three components: the nonterminal label, the headword, and the headword’s part of speech.Fully lexicalized modifying nonterminals are generated in two steps to allow for the parameters to be independently smoothed, which, in turn, is done to avoid sparse-data problems.These two steps estimate the joint event of all three components using the chain rule.In the A tree containing both punctuation and conjunction. first step, a partially lexicalized version of the nonterminal is generated, consisting of the unlexicalized label plus the part of speech of its headword.These partially lexicalized modifying nonterminals are generated conditioning on the parent label, the head label, the headword, the head tag, the current state of the dynamic subcat, and a distance metric.Symbolically, the parameter classes are where ∆ denotes the distance metric.17 As discussed above, one of the two components of this distance metric is the vi predicate.The other is a predicate that simply reports whether the current modifier is the first modifier being generated, that is, whether i = 1.The second step is to generate the headword itself, where, because of the chain rule, the conditioning context consists of everything in the histories of expressions (7) and (8) plus the partially lexicalized modifier.As there are some interesting idiosyncrasies with these headword-generation parameters, we describe them in more detail in Section 6.9.6.5.1 Inconsistent Model.As discussed in Section 4.8, punctuation is raised to the highest position in the tree.This means that in some sense, punctuation acts very much like a coordinating conjunction, in that it “conjoins” the two siblings between which it sits.Observing that it might be helpful for conjunctions to be generated conditioning on both of their conjuncts, Collins introduced two new parameter classes in his thesis parser, Ppunc and PCC.18 As per the definition of a coordinated phrase in Section 4.1, conjunction via a CC node or a punctuation node always occurs posthead (i.e., as a right-sibling of the head).Put another way, if a conjunction or punctuation mark occurs prehead, it is 17 Throughout this article we use the notation L(w, t)i to refer to the three items that constitute a fully lexicalized left-modifying nonterminal, which are the unlexicalized label Li, its headword wLi, and its part of speech tLi, and similarly for right modifiers.We use L(t)i to refer to the two items Li and tLi of a partially lexicalized nonterminal.Finally, when we do not wish to distinguish between a left and right modifier, we use M(w, t)i, M(t)i, and Mi. not generated via this mechanism.19 Furthermore, even if there is arbitrary material between the right conjunct and the head, the parameters effectively assume that the left conjunct is always the head-child.For example, in Figure 9, the rightmost NP (bushy bushes) is considered to be conjoined to the leftmost NP (short grass), which is the head-child, even though there is an intervening NP (tall trees).The new parameters are incorporated into the model by requiring that all modifying nonterminals be generated with two boolean flags: coord, indicating that the nonterminal is conjoined to the head via a CC, and punc, indicating that the nonterminal is conjoined to the head via a punctuation mark.When either or both of these flags is true, the intervening punctuation or conjunction is generated via appropriate instances of the Ppun,/PCC parameter classes.For example, the model generates the five children in Figure 9 in the following order: first, the head-child is generated, which is the leftmost NP (short grass), conditioning on the parent label and the headword and tag.Then, since modifiers are always generated from the head outward, the right-sibling of the head, which is the tall trees NP, is generated with both the punc and CC flags false.Then, the rightmost NP (bushy bushes) is generated with both the punc and CC booleans true, since it is considered to be conjoined to the head-child and requires the generation of an intervening punctuation mark and conjunction.Finally, the intervening punctuation is generated conditioning on the parent, the head, and the right conjunct, including the headwords of the two conjoined phrases, and the intervening CC is similarly generated.A simplified version of the probability of generating all these children is summarized as follows: The idea is that using the chain rule, the generation of two conjuncts and that which conjoins them is estimated as one large joint event.20 This scheme of using flags to trigger the Ppun, and PCC parameters is problematic, at least from a theoretical standpoint, as it causes the model to be inconsistent.Figure 10 shows three different trees that would all receive the same probability from Collins’ model.The problem is that coordinating conjunctions and punctuation are not generated as first-class words, but only as triggered from these punc and coord flags, meaning that the number of such intervening conjunctive items (and the order in which they are to be generated) is not specified.So for a given sentence/tree pair containing a conjunction and/or a punctuation mark, there is an infinite number of similar sentence/tree pairs with arbitrary amounts of “conjunctive” material between the same two nodes.Because all of these trees have the same, nonzero probability, the sum ETP(T), where T is a possible tree generated by the model, diverges, meaning the model is inconsistent (Booth and Thompson 1973).Another consequence of not generating posthead conjunctions and punctuation as first-class words is that they The Collins model assigns equal probability to these three trees. do not count when calculating the head-adjacency component of Collins’ distance metric.When emulating Collins’ model, instead of reproducing the Ppun, and PCC parameter classes directly in our parsing engine, we chose to use a different mechanism that does not yield an inconsistent model but still estimates the large joint event that was the motivation behind these parameters in the first place.6.5.2 History Mechanism.In our emulation of Collins’ model, we use the history, rather than the dedicated parameter classes PCC and Ppun,, to estimate the joint event of generating a conjunction (or punctuation mark) and its two conjuncts.The first big change that results is that we treat punctuation preterminals and CCs as first-class objects, meaning that they are generated in the same way as any other modifying nonterminal.The second change is a little more involved.First, we redefine the distance metric to consist solely of the vi predicate.Then, we add to the conditioning context a mapped version of the previously generated modifier according to the following where Mi is some modifier Li or Ri.21 So, the maximal context for our modifying nonterminal parameter class is now defined as follows: where side is a boolean-valued event that indicates whether the modifier is on the left or right side of the head.By treating CC and punctuation nodes as first-class nonterminals and by adding the mapped version of the previously generated modifier, we have, in one fell swoop, incorporated the “no intervening” component of Collins’ distance metric (the i = 0 case of the delta function) and achieved an estimate of the joint event of a conjunction and its conjuncts, albeit with different dependencies, that is, a different application of the chain rule.To put this parameterization change in sharp relief, consider the abstract tree structure To a first approximation, under the old parameterization, the conjunction of some node R1 with a head H and a parent P looked like this: ˆpH(H  |P) · ˆpR(R1, coord=1  |P,H) · ˆpCC(CC  |P,H,R1) whereas under the new parameterization, it looks like this: Either way, the probability of the joint conditional event {H, CC, R1  |P} is being estimated, but with the new method, there is no need to add two new specialized parameter classes, and the new method does not introduce inconsistency into the model.Using less simplification, the probability of generating the five children of Figure 9 is now 21 Originally, we had an additional mechanism that attempted to generate punctuation and conjunctions with conditional independence.One of our reviewers astutely pointed out that the mechanism led to a deficient model (the very thing we have been trying to avoid), and so we have subsequently removed it from our model.The removal leads to a 0.05% absolute reduction in F-measure (which in this case is also a 0.05% relative increase in error) on sentences of length ≤ 40 words in Section 00 of the Penn Treebank.As this difference is not at all statistically significant (according to a randomized stratified shuffling test [Cohen 1995]), all evaluations reported in this article are with the original model.As shown in Section 8.1, this new parameterization yields virtually identical performance to that of the Collins model.22 As we have already seen, there are several ways in which base NPs are exceptional in Collins’ parsing model.This is partly because the flat structure of base NPs in the Penn Treebank suggested the use of a completely different model by which to generate them.Essentially, the model for generating children of NPB nodes is a “bigrams of nonterminals” model.That is, it looks a great deal like a bigram language model, except that the items being generated are not words, but lexicalized nonterminals.Heads of NPB nodes are generated using the normal head-generation parameter, but modifiers are always generated conditioning not on the head, but on the previously generated modifier.That is, we modify expressions (7) and (8) to be Though it is not entirely spelled out in his thesis, Collins considers the previously generated modifier to be the head-child, for all intents and purposes.Thus, the subcat and distance metrics are always irrelevant, since it is as though the current modifier is right next to the head.23 Another consequence of this is that NPBs are never considered to be coordinated phrases (as mentioned in Section 4.12), and thus CCs dominated by NPB are never generated using a PCC parameter; instead, they are generated using a normal modifying-nonterminal parameter.Punctuation dominated by NPB, on the other hand, is still, as always, generated via Ppunc parameters, but crucially, the modifier is always conjoined (via the punctuation mark) to the “pseudohead” that is the previously generated modifier.Consequently, when some right modifier Ri is generated, the previously generated modifier on the right side of the head, Ri−1, is never a punctuation preterminal, but always the previous “real” (i.e., nonpunctuation) preterminal.24 Base NPs are also exceptional with respect to determining chart item equality, the comma-pruning rule, and general beam pruning (see Section 7.2 for details).Two parameter classes that make their appearance only in Appendix E of Collins’ thesis are those that compute priors on lexicalized nonterminals.These priors are used as a crude proxy for the outside probability of a chart item (see Baker [1979] and Lari and Young [1990] for full descriptions of the Inside–Outside algorithm).Previous work (Goodman 1997) has shown that the inside probability alone is an insufficient scoring metric when comparing chart items covering the same span during decoding and that some estimate of the outside probability of a chart item should be factored into the score.A prior on the root (lexicalized) nonterminal label of the derivation forest represented by a particular chart item is used for this purpose in Collins’ parser.22 As described in Bikel (2002), our parsing engine allows easy experimentation with a wide variety of different generative models, including the ability to construct history contexts from arbitrary numbers of previously generated modifiers.The mapping function delta and the transition function tau presented in this section are just two examples of this capability.The prior of a lexicalized nonterminal M(w, t) is broken down into two separate estimates using parameters from two new classes, Ppriorw and PpriorNT: where ˆp(M  |w, t) is smoothed with ˆp(M  |t) and estimates using the parameters of the Ppriorw class are unsmoothed.Many of the parameter classes in Collins’ model—and indeed, in most statistical parsing models—define conditional probabilities with very large conditioning contexts.In this case, the conditioning contexts represent some subset of the history of the generative process.Even if there were orders of magnitude more training data available, the large size of these contexts would cause horrendous sparse-data problems.The solution is to smooth these distributions that are made rough primarily by the abundance of zeros.Collins uses the technique of deleted interpolation, which smoothes the distributions based on full contexts with those from coarser models that use less of the context, by successively deleting elements from the context at each back-off level.As a simple example, the head parameter class smoothes PH0(H  |P, wh, th) with PH1(H  |P, th) and PH2(H  |P).For some conditional probability p(A  |B), let us call the reduced context at the ith back-off level Oi(B), where typically O0(B) = B.Each estimate in the back-off chain is computed via maximum-likelihood (ML) estimation, and the overall smoothed estimate with n back-off levels is computed using n −1 smoothing weights, denoted A0, ... , An−2.These weights are used in a recursive fashion: The smoothed version ˜ei = ˜pi(A  |Oi(B)) of an unsmoothed ML estimate ei = ˆpi(A  |Oi(B)) at back-off level i is computed via the formula So, for example, with three levels of back-off, the overall smoothed estimate would be defined as Each smoothing weight can be conceptualized as the confidence in the estimate with which it is being multiplied.These confidence values can be derived in a number of sensible ways; the technique used by Collins was adapted from that used in Bikel et al. (1997), which makes use of a quantity called the diversity of the history context (Witten and Bell 1991), which is equal to the number of unique futures observed in training for that history context.6.8.1 Deficient Model.As previously mentioned, n back-off levels require n−1 smoothing weights.Collins’ parser effectively uses n weights, because the estimator always adds an extra, constant-valued estimate to the back-off chain.Collins’ parser hardcodes this extra value to be a vanishingly small (but nonzero) “probability” of 10−19, resulting in smoothed estimates of the form when there are three levels of back-off.The addition of this constant-valued en = 10−19 causes all estimates in the parser to be deficient, as it ends up throwing away probability mass.More formally, the proof leading to equation (17) no longer holds: The “distribution” sums to less than one (there is no history context in the model for which there are 1019 possible outcomes).25 for computing smoothing weights is where ci is the count of the history context Oi(B) and ui is the diversity of that context.26 The multiplicative constant five is used to give less weight to the back-off levels with more context and was optimized by looking at overall parsing performance on the development test set, Section 00 of the Penn Treebank.We call this constant the smoothing factor and denote it as ff.As it happens, the actual formula for computing smoothing weights in Collins’ implementation is where ft is an unmentioned smoothing term.For every parameter class except the subcat parameter class and Ppriorw, ft = 0 and ff = 5.0.For the subcat parameter class, ft = 5.0 and ff = 0.For Ppriorw, ft = 1.0 and ff = 0.0.This curiously means that diversity is not used at all when smoothing subcat-generation probabilities.27 The second case in (19) handles the situation in which the history context was never observed in training, that is, where ci = ui = 0, which would yield an undefined value 25 Collins used this technique to ensure that even futures that were never seen with an observed history context would still have some probability mass, albeit a vanishingly small one (Collins, personal communication, January 2003).Another commonly used technique would be to back off to the uniform distribution, which has the desirable property of not producing deficient estimates.As with all of the treebank- or model-specific aspects of the Collins parser, our engine uses equation (16) or (18) depending on the value of a particular run-time setting.26 The smoothing weights can be viewed as confidence values for the probability estimates with which they are multiplied.The Witten-Bell technique crucially makes use of the quantity ni = ui , the average number of transitions from the history context Oi(B) to a possible future.With a little algebraic manipulation, we have a quantity that is at its maximum when ni = ci and at its minimum when ni = 1, that is, when every future observed in training was unique.This latter case represents when the model is most “uncertain,” in that the transition distribution from Oi(B) is uniform and poorly trained (one observation per possible transition).Because these smoothing weights measure, in some sense, the closeness of the observed distribution to uniform, they can be viewed as proxies for the entropy of the distribution p(·  |Oi(B)).Back-off levels for PLw/PRw, the modifier headword generation parameter classes. wLiand tLi are, respectively, the headword and its part of speech of the nonterminal Li.This table is basically a reproduction of the last column of Table 7.1 in Collins’ thesis.Our new parameter class for the generation of headwords of modifying nonterminals. when ft = 0.In such situations, making λi = 0 throws all remaining probability mass to the smoothed back-off estimate, ˜ei+1.This is a crucial part of the way smoothing is done: If a particular history context φi(B) has never been observed in training, the smoothed estimate using less context, φi+1(B), is simply substituted as the “best guess” for the estimate using more context; that is, ˜ei = ˜ei+1.28 As mentioned in Section 6.4, fully lexicalized modifying nonterminals are generated in two steps.First, the label and part-of-speech tag are generated with an instance of PL or PR.Next, the headword is generated via an instance of one of two parameter classes, PLw or PRw.The back-off contexts for the smoothed estimates of these parameters are specified in Table 1.Notice how the last level of back-off is markedly different from the previous two levels in that it removes nearly all the elements of the history: In the face of sparse data, the probability of generating the headword of a modifying nonterminal is conditioned only on its part of speech. order to capture the most data for the crucial last level of back-off, Collins uses words that occur on either side of the headword, resulting in a general estimate ˆp(w I t), as opposed to ˆpLw(wLi I tLi).Accordingly, in our emulation of Collins’ model, we replace the left- and right-word parameter classes with a single modifier headword generation parameter class that, as with (11), includes a boolean side component that is deleted from the last level of back-off (see Table 2).Even with this change, there is still a problem.Every headword in a lexicalized parse tree is the modifier of some other headword—except the word that is the head of the entire sentence (i.e., the headword of the root nonterminal).In order to properly duplicate Collins’ model, an implementation must take care that the P(w I t) model includes counts for these important headwords.29 The low-frequency word Fido is mapped to +UNKNOWN+, but only when it is generated, not when it is conditioned upon.All the nonterminals have been lexicalized (except for preterminals) to show where the heads are.6.9.2 Unknown-Word Mapping.As mentioned above, instead of mapping every lowfrequency word in the training data to some special +UNKNOWN+ token, Collins’ trainer instead leaves the training data untouched and selectively maps words that appear in the back-off levels of the parameters from the PL. and PR. parameter classes.Rather curiously, the trainer maps only words that appear in the futures of these parameters, but never in the histories.Put another way, low-frequency words are generated as +UNKNOWN+ but are left unchanged when they are conditioned upon.For example, in Figure 11, where we assume Fido is a low-frequency word, the trainer would derive counts for the smoothed parameter the word would not be mapped.This strange mapping scheme has some interesting consequences.First, imagine what happens to words that are truly unknown, that never occurred in the training data.Such words are mapped to the +UNKNOWN+ token outright before parsing.Whenever the parser estimates a probability with such a truly unknown word in the history, it will necessarily throw all probability mass to the backed-off estimate (˜e1 in our earlier notation), since +UNKNOWN+ effectively never occurred in a history context during training.The second consequence is that the mapping scheme yields a “superficient”30 model, if all other parts of the model are probabilistically sound (which is actually Back-off structure for PTOPNT and PTOPw, which estimate the probability of generating H(w, t) as the root nonterminal of a parse tree.PTOPNT is unsmoothed. n/a: not applicable. not the case here).With a parsing model such as Collins’ that uses bilexical dependencies, generating words in the course of parsing is done very much as it is in a bigram language model: Every word is generated conditioning on some previously generated word, as well as some hidden material.The only difference is that the word being conditioned upon is often not the immediately preceding word in the sentence.However, one could plausibly construct a consistent bigram language model that generates words with the same dependencies as those in a statistical parser that uses bilexical dependencies derived from head-lexicalization.Collins (personal communication, January 2003) notes that his parser’s unknownword-mapping scheme could be made consistent if one were to add a parameter class that estimated ˆp(w  |+UNKNOWN+), where w E VL U {+UNKNOWN+}.The values of these estimates for a given sentence would be constant across all parses, meaning that the “superficiency” of the model would be irrelevant when determining arg max P(T  |S).It is assumed that all trees that can be generated by the model have an implicit nonterminal +TOP+ that is the parent of the observed root.The observed lexicalized root nonterminal is generated conditioning on +TOP+ (which has a prior probability of 1.0) using a parameter from the class PTOP.This special parameter class is mentioned in a footnote in chapter 7 of Collins’ thesis.There are actually two parameter classes used to generated observed roots, one for generating the partially lexicalized root nonterminal, which we call PTOPNT, and the other for generating the headword of the entire sentence, which we call PTOPw.Table 3 gives the unpublished back-off structure of these two additional parameter classes.Note that PTOPw backs off to simply estimating ˆp(w  |t).Technically, it should be estimating ˆpNT(w  |t), which is to say the probability of a word’s occurring with a tag in the space of lexicalized nonterminals.This is different from the last level of back-off in the modifier headword parameter classes, which is effectively estimating ˆp(w  |t) in the space of lexicalized preterminals.The difference is that in the same sentence, the same headword can occur with the same tag in multiple nodes, such as sat in Figure 8, which occurs with the tag VBD three times (instead of just once) in the tree shown there.Despite this difference, Collins’ parser uses counts from the (shared) last level of back-off of the PLw and PRw parameters when delivering e1 estimates for the PTOPw parameters.Our parsing engine emulates this “count sharing” for PTOPw by default, by sharing counts from our PMw parameter class.Parsing, or decoding, is performed via a probabilistic version of the CKY chartparsing algorithm.As with normal CKY, even though the model is defined in a topdown, generative manner, decoding proceeds bottom-up.Collins’ thesis gives a pseuSince the goal of the decoding process is to determine the maximally likely theory, if during decoding a proposed chart item is equal (or, technically, equivalent) to an item that is already in the chart, the one with the greater score survives.Chart item equality is closely tied to the generative parameters used to construct theories: We want to treat two chart items as unequal if they represent derivation forests that would be considered unequal according to the output elements and conditioning contexts of the parameters used to generate them, subject to the independence assumptions of the model.For example, for two chart items to be considered equal, they must have the same label (the label of the root of their respective derivation forests’ subtrees), the same headword and tag, and the same left and right subcat.They must also have the same head label (that is, label of the head-child).If a chart item’s root label is an NP node, its head label is most often an NPB node, given the “extra” NP levels that are added during preprocessing to ensure that NPB nodes are always dominated by NP nodes.In such cases, the chart item will contain a back pointer to the chart item that represents the base NP.Curiously, however, Collins’ implementation considers the head label of the NP chart item not to be NPB, but rather the head label of the NPB chart item.In other words, to get the head label of an NP chart item, one must “peek through” the NPB and get at the NPB’s head label.Presumably, this was done as a consideration for the NPB nodes’ being “extra” nodes, in some sense.It appears to have little effect on overall parsing accuracy, however.Ideally, every parse theory could be kept in the chart, and when the root symbol has been generated for all theories, the top-ranked one would “win.” In order to speed things up, Collins employs three different types of pruning.The first form of pruning is to use a beam: The chart memoizes the highest-scoring theory in each span, and if a proposed chart item for that span is not within a certain factor of the top-scoring item, it is not added to the chart.Collins reports in his thesis that he uses a beam width of 105.As it happens, the beam width for his thesis experiments was 104.Interestingly, there is a negligible difference in overall parsing accuracy when this wider beam is used (see Table 5).An interesting modification to the standard beam in Collins’ parser is that for chart items representing NP or NP-A derivations with more than one child, the beam is expanded to be 104 · e3.We suspect that Collins made this modification after he added the base NP model, to handle the greater perplexity associated with NPs.The second form of pruning employed is a comma constraint.Collins observed that in the Penn Treebank data, 96% of the time, when a constituent contained a comma, the word immediately following the end of the constituent’s span was either a comma or the end of the sentence.So for speed reasons, the decoder rejects all theories that would generate constituents that violate this comma constraint.31 There is a subtlety to Collins’ implementation of this form of pruning, however.Commas are quite common within parenthetical phrases.Accordingly, if a comma in an input Overall parsing results using only details found in Collins (1997, 1999).The first two lines show the results of Collins’ parser and those of our parser in its “complete” emulation mode (i.e., including unpublished details).All reported scores are for sentences of length < 40 words.LR (labeled recall) and LP (labeled precision) are the primary scoring metrics.CBs is the number of crossing brackets.0 CBs and < 2 CBs are the percentages of sentences with 0 and < 2 crossing brackets, respectively.F (the F-measure) is the evenly weighted harmonic mean of precision and recall, or 1 LP·LR sentence occurs after an opening parenthesis and before a closing parenthesis or the end of the sentence, it is not considered a comma for the purposes of the comma constraint.Another subtlety is that the comma constraint should effectively not be employed when pursuing theories of an NPB subtree.As it turns out, using the comma constraint also affects accuracy, as shown in Section 8.1.The final form of pruning employed is rather subtle: Within each cell of the chart that contains items covering some span of the sentence, Collins’ parser uses buckets of items that share the same root nonterminal label for their respective derivations.Only 100 of the top-scoring items covering the same span with the same nonterminal label are kept in a particular bucket, meaning that if a new item is proposed and there are already 100 items covering the same span with the same label in the chart, then it will be compared to the lowest-scoring item in the bucket.If it has a higher score, it will be added to the bucket and the lowest-scoring item will be removed; otherwise, it will not be added.Apparently, this type of pruning has little effect, and so we have not duplicated it in our engine.32 When the parser encounters an unknown word, the first-best tag delivered by Ratnaparkhi’s (1996) tagger is used.As it happens, the tag dictionary built up when training contains entries for every word observed, even low-frequency words.This means that during decoding, the output of the tagger is used only for those words that are truly unknown, that is, that were never observed in training.For all other words, the chart is seeded with a separate item for each tag observed with that word in training.In this section we present the results of effectively doing a “clean-room” implementation of Collins’ parsing model, that is, using only information available in (Collins 1997, 1999), as shown in Table 4.The clean-room model has a 10.6% increase in F-measure error compared to Collins’ parser and an 11.0% increase in F-measure error compared to our engine in its complete emulation of Collins’ Model 2.This is comparable to the increase in error seen when removing such published features as the verb-intervening component of the distance metric, which results in an F-measure error increase of 9.86%, or the subcat feature, which results in a 7.62% increase in F-measure error.33 Therefore, while the collection of unpublished details presented in Sections 4–7 is disparate, in toto those details are every bit as important to overall parsing performance as certain of the published features.This does not mean that all the details are equally important.Table 5 shows the effect on overall parsing performance of independently removing or changing certain of the more than 30 unpublished details.34 Often, the detrimental effect of a particular change is quite insignificant, even by the standards of the performance-obsessed world of statistical parsing, and occasionally, the effect of a change is not even detrimental at all.That is why we do not claim the importance of any single unpublished detail, but rather that of their totality, given that several of the unpublished details are, most likely, interacting.However, we note that certain individual details, such as the universal p(w It) model, do appear to have a much more marked effect on overall parsing accuracy than others.The previous section accounts for the noticeable effects of all the unpublished details of Collins’ model.But what of the details that were published?In chapter 8 of his thesis, Collins gives an account on the motivation of various features of his model, including the distance metric, the model’s use of subcats (and their interaction with the distance metric), and structural versus semantic preferences.In the discussion of this last issue, Collins points to the fact that structural preferences—which, in his model, are 33 These F-measures and the differences between them were calculated from experiments presented in Collins (1999, page 201); these experiments, unlike those on which our reported numbers are based, were on all sentences, not just those of length ≤ 40 words.As Collins notes, removing both the distance metric and subcat features results in a gigantic drop in performance, since without both of these features, the model has no way to encode the fact that flatter structures should be avoided in several crucial cases, such as for PPs, which tend to prefer one argument to the right of their head-children.34 As a reviewer pointed out, the use of the comma constraint is a “published” detail.However, the specifics of how certain commas do not apply to the constraint is an “unpublished detail,” as mentioned in Section 7.2.Number of times our parsing engine was able to deliver a probability for the various levels of back-off of the modifier-word generation model, PMw, when testing on Section 00, having trained on Sections 02–21.In other words, this table reports how often a context in the back-off chain of PMw that was needed during decoding was observed in training. modeled primarily by the PL and PR parameters—often provide the right information for disambiguating competing analyses, but that these structural preferences may be “overridden” by semantic preferences.Bilexical statistics (Eisner 1996), as represented by the maximal context of the PLw and PRw parameters, serve as a proxy for such semantic preferences, where the actual modifier word (as opposed to, say, merely its part of speech) indicates the particular semantics of its head.Indeed, such bilexical statistics were widely assumed for some time to be a source of great discriminative power for several different parsing models, including that of Collins.However, Gildea (2001) reimplemented Collins’ Model 1 (essentially Model 2 but without subcats) and altered the PLw and PRw parameters so that they no longer had the top level of context that included the headword (he removed back-off level 0, as depicted in Table 1).In other words, Gildea removed all bilexical statistics from the overall model.Surprisingly, this resulted in only a 0.45% absolute reduction in F-measure (3.3% relative increase in error).Unfortunately, this result was not entirely conclusive, in that Gildea was able to reimplement Collins’ baseline model only partially, and the performance of his partial reimplementation was not quite as good as that of Collins’ parser.35 Training on Sections 02–21, we have duplicated Gildea’s bigram-removal experiment, except that our chosen test set is Section 00 instead of Section 23 and our chosen model is the more widely used Model 2.Using the mode that most closely emulates Collins’ Model 2, with bigrams, our engine obtains a recall of 89.89% and a precision of 90.14% on sentences of length ≤ 40 words (see Table 8, Model Mtw,tw).Without bigrams, performance drops only to 89.49% on recall, 89.95% on precision— an exceedingly small drop in performance (see Table 8, Model Mtw,t).In an additional experiment, we have examined the number of times that the parser is able, while decoding Section 00, to deliver a requested probability for the modifier-word generation model using the increasingly less-specific contexts of the three back-off levels.The results are presented in Table 6.Back-off level 0 indicates the use of the full history context, which contains the head-child’s headword.Note that probabilities making use of this full context, that is, making use of bilexical dependencies, are available only 1.49% of the time.Combined with the results from the previous experiment, this suggests rather convincingly that such statistics are far less significant than once thought to the overall discriminative power of Collins’ models, confirming Gildea’s result for Model 2.36 If not bilexical statistics, then surely, one might think, head-choice is critical to the performance of a head-driven lexicalized statistical parsing model.Partly to this end, in Chiang and Bikel (2002), we explored methods for recovering latent information in treebanks.The second half of that paper focused on a use of the Inside–Outside algorithm to reestimate the parameters of a model defined over an augmented tree space, where the observed data were considered to be the gold-standard labeled bracketings found in the treebank, and the hidden data were considered to be the headlexicalizations, one of the most notable tree augmentations performed by modern statistical parsers.These expectation maximization (EM) experiments were motivated by the desire to overcome the limitations imposed by the heuristics that have been heretofore used to perform head-lexicalization in treebanks.In particular, it appeared that the head rules used in Collins’ parser had been tweaked specifically for the English Penn Treebank.Using EM would mean that very little effort would need to be spent on developing head rules, since EM could take an initial model that used simple heuristics and optimize it appropriately to maximize the likelihood of the unlexicalized (observed) training trees.To test this, we performed experiments with an initial model trained using an extremely simplified head-rule set in which all rules were of the form “if the parent is X, then choose the left/rightmost child.” A surprising side result was that even with this simplified set of head-rules, overall parsing performance still remained quite high.Using our simplified head-rule set for English, our engine in its “Model 2 emulation mode” achieved a recall of 88.55% and a precision of 88.80% for sentences of length ≤40 words in Section 00 (see Table 7).So contrary to our expectations, the lack of careful head-choice is not crippling in allowing the parser to disambiguate competing theories and is a further indication that semantic preferences, as represented by conditioning on a headword, rarely override structural ones.Given that bilexical dependencies are almost never used and have a surprisingly small effect on overall parsing performance, and given that the choice of head is not terribly critical either, one might wonder what power, if any, head-lexicalization is providing.The answer is that even when one removes bilexical dependencies from the model, there are still plenty of lexico-structural dependencies, that is, structures being generated conditioning on headwords and headwords being generated conditioning on structures.To test the effect of such lexicostructural dependencies in our lexicalized PCFGstyle formalism, we experimented with the removal of the head tag th and/or the head word wh from the conditioning contexts of the PMw and PM parameters.The recertainly points to the utility of caching probabilities (the 219 million are tokens, not types).Parsing performance with various models on Section 00 of the Penn Treebank.PM is the parameter class for generating partially lexicalized modifying nonterminals (a nonterminal label and part of speech).PMw is the parameter class that generates the headword of a modifying nonterminal.Together, PM and PMw generate a fully lexicalized modifying nonterminal.The check marks indicate the inclusion of the headword wh and its part of speech th of the lexicalized head nonterminal H(th,wh) in the conditioning contexts of PM and PMw.See Table 4 for definitions of the remaining column headings. sults are shown in Table 8.Model Mtw,tw shows our baseline, and Model Mφ,φ shows the effect of removing all dependence on the headword and its part of speech, with the other models illustrating varying degrees of removing elements from the two parameter classes’ conditioning contexts.Notably, including the headword wh in or removing it from the PM contexts appears to have a significant effect on overall performance, as shown by moving from Model Mtw,t to Model Mt,t and from Model Mtw,φ to Model Mt,φ.This reinforces the notion that particular headwords have structural preferences, so that making the PM parameters dependent on headwords would capture such preferences.As for effects involving dependence on the head tag th, observe that moving from Model Mtw,t to Model Mtw,φ results in a small drop in both recall and precision, whereas making an analogous move from Model Mt,t to Model Mt,φ results in a drop in recall, but a slight gain in precision (the two moves are analogous in that in both cases, th is dropped from the context of PMw).It is not evident why these two moves do not produce similar performance losses, but in both cases, the performance drops are small relative to those observed when eliminating wh from the conditioning contexts, indicating that headwords matter far more than parts of speech for determining structural preferences, as one would expect.We have documented what we believe is the complete set of heretofore unpublished details Collins used in his parser, such that, along with Collins’ (1999) thesis, thi s article contains all information necessary to duplicate Collins’ benchmark results.Indeed, these as-yet-unpublished details account for an 11% relative increase in error from an implementation including all details to a clean-room implementation of Collins’ model.We have also shown a cleaner and equally well-performing method for the handling of punctuation and conjunction, and we have revealed certain other probabilistic oddities about Collins’ parser.We have not only analyzed the effect of the unpublished details but also reanalyzed the effect of certain well-known details, revealing that bilexical dependencies are barely used by the model and that head choice is not nearly as important to overall parsing performance as once thought.Finally, we have performed experiments that show that the true discriminative power of lexicalization appears to lie in the fact that unlexicalized syntactic structures are generated conditioning on the headword and head tag.These results regarding the lack of reliance on bilexical statistics suggest that generative models still have room for improvement through the employment of bilexical-class statistics, that is, dependencies among head-modifier word classes, where such classes may be defined by, say, WordNet synsets.Such dependencies might finally be able to capture the semantic preferences that were thought to be captured by standard bilexical statistics, as well as to alleviate the sparse-data problems associated with standard bilexical statistics.This is the subject of our current research.This section contains tables for all parameter classes in Collins’ Model 3, with appropriate modifications and additions from the tables presented in Collins’ thesis.The notation is that used throughout this article.In particular, for notational brevity we use M(w, t)i to refer to the three items Mi, tMi, and wMi that constitute some fully lexicalized modifying nonterminal and similarly M(t)i to refer to the two items Mi and tMi that constitute some partially lexicalized modifying nonterminal.The (unlexicalized) nonterminal-mapping functions alpha and gamma are defined in Section 6.1.As a shorthand, y(M(t)i) = y(Mi),tMi.The head-generation parameter class, PH, gap-generation parameter class, PG, and subcat-generation parameter classes, PsubcatL and PsubcatR, have back-off structures as follows: The two parameter classes for generating modifying nonterminals that are not dominated by a base NP, PM and PMw, have the following back-off structures.Recall that back-off level 2 of the PMw parameters includes words that are the heads of the observed roots of sentences (that is, the headword of the entire sentence).The two parameter classes for generating modifying nonterminals that are children of base NPs (NPB nodes), PM,NPB and PMw,NPB, have the following back-off structures.Back-off level 2 of the PMw,NPB parameters includes words that are the heads of the observed roots of sentences (that is, the headword of the entire sentence).Also, note that there is no coord flag, as coordinating conjunctions are generated in the same way as regular modifying nonterminals when they are dominated by NPB.Finally, we define M0 = H, that is, the head nonterminal label of the base NP that was generated using a PH parameter.The two parameter classes for generating punctuation and coordinating conjunctions, Ppunc and Pcoord, have the following back-off structures (Collins, personal communication, October 2001), where 2 type ttype The parameter classes for generating fully lexicalized root nonterminals given the hidden root +TOP+, PTOP and PTOPw, have the following back-off structures (identical to Table 3; n/a: not applicable).The parameter classes for generating prior probabilities on lexicalized nonterminals M(w, t), Ppriorw and PpriorNT, have the following back-off structures, where prior is a dummy variable to indicate that Ppriorwis not smoothed (although the Ppriorw parameters still have an associated smoothing weight; see note 27).I would especially like to thank Mike Collins for his invaluable assistance and great generosity while I was replicating his thesis results and for his comments on a prerelease draft of this article.Many thanks to David Chiang and Dan Gildea for the many valuable discussions during the course of this work.Also, thanks to the anonymous reviewers for their helpful and astute observations.Finally, thanks to my Ph.D. advisor Mitch Marcus, who during the course of this work was, as ever, a source of keen insight and unbridled optimism.This work was supported in part by NSF grant no.SBR-89-20239 and DARPA grant no.N66001-00-1-8915.
This article considers approaches which rerank the output of an existing probabilistic parser.The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses.A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence.The strength of our approach is that it allows a tree to be represented as an arbitrary set offeatures, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account.We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998).We apply the boosting method to parsing the Wall Street Journal treebank.The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model.The new model achieved 89.75% F-measure, a 13% relative decrease in Fmeasure error over the baseline model’s score of 88.2%.The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data.Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach.We argue that the method is an appealing alternative—in terms of both simplicity and efficiency—to work on feature selection methods within log-linear (maximum-entropy) models.Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation.Machine-learning approaches to natural language parsing have recently shown some success in complex domains such as news wire text.Many of these methods fall into the general category of history-based models, in which a parse tree is represented as a derivation (sequence of decisions) and the probability of the tree is then calculated as a product of decision probabilities.While these approaches have many advantages, it can be awkward to encode some constraints within this framework.In the ideal case, the designer of a statistical parser would be able to easily add features to the model that are believed to be useful in discriminating among candidate trees for a sentence.In practice, however, adding new features to a generative or history-based model can be awkward: The derivation in the model must be altered to take the new features into account, and this can be an intricate task.This article considers approaches which rerank the output of an existing probabilistic parser.The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses.A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence.The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation which takes these features into account.We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998).The algorithm can be viewed as a feature selection method, optimizing a particular loss function (the exponential loss function) that has been studied in the boosting literature.We applied the boosting method to parsing the Wall Street Journal (WSJ) treebank (Marcus, Santorini, and Marcinkiewicz 1993).The method combines the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model.The baseline model achieved 88.2% F-measure on this task.The new model achieves 89.75% Fmeasure, a 13% relative decrease in F-measure error.Although the experiments in this article are on natural language parsing, the approach should be applicable to many other natural language processing (NLP) problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation.See Collins (2002a) for an application of the boosting approach to named entity recognition, and Walker, Rambow, and Rogati (2001) for the application of boosting techniques for ranking in the context of natural language generation.The article also introduces a new, more efficient algorithm for the boosting approach which takes advantage of the sparse nature of the feature space in the parsing data.Other NLP tasks are likely to have similar characteristics in terms of sparsity.Experiments show an efficiency gain of a factor of 2,600 for the new algorithm over the obvious implementation of the boosting approach.Efficiency issues are important, because the parsing task is a fairly large problem, involving around one million parse trees and over 500,000 features.The improved algorithm can perform 100,000 rounds of feature selection on our task in a few hours with current processing speeds.The 100,000 rounds of feature selection require computation equivalent to around 40 passes over the entire training set (as opposed to 100,000 passes for the “naive”implementation).The problems with history-based models and the desire to be able to specify features as arbitrary predicates of the entire tree have been noted before.In particular, previous work (Ratnaparkhi, Roukos, and Ward 1994; Abney 1997; Della Pietra, Della Pietra, and Lafferty 1997; Johnson et al. 1999; Riezler et al.2002) has investigated the use of Markov random fields (MRFs) or log-linear models as probabilistic models with global features for parsing and other NLP tasks.(Log-linear models are often referred to as maximum-entropy models in the NLP literature.)Similar methods have also been proposed for machine translation (Och and Ney 2002) and language understanding in dialogue systems (Papineni, Roukos, and Ward 1997, 1998).Previous work (Friedman, Hastie, and Tibshirani 1998) has drawn connections between log-linear models and boosting for classification problems.One contribution of our research is to draw similar connections between the two approaches to ranking problems.We argue that the efficient boosting algorithm introduced in this article is an attractive alternative to maximum-entropy models, in particular, feature selection methods that have been proposed in the literature on maximum-entropy models.The earlier methods for maximum-entropy feature selection methods (Ratnaparkhi, Roukos, and Ward 1994; Berger, Della Pietra, and Della Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997; Papineni, Roukos, and Ward 1997, 1998) require several full passes over the training set for each round of feature selection, suggesting that at least for the parsing data, the improved boosting algorithm is several orders of magnitude more efficient.1 In section 6.4 we discuss our approach in comparison to these earlier methods for feature selection, as well as the more recent work of McCallum (2003); Zhou et al. (2003); and Riezler and Vasserman (2004).The remainder of this article is structured as follows.Section 2 reviews historybased models for NLP and highlights the perceived shortcomings of history-based models which motivate the reranking approaches described in the remainder of the article.Section 3 describes previous work (Friedman, Hastie, and Tibshirani 2000; Duffy and Helmbold 1999; Mason, Bartlett, and Baxter 1999; Lebanon and Lafferty 2001; Collins, Schapire, and Singer 2002) that derives connections between boosting and maximum-entropy models for the simpler case of classification problems; this work forms the basis for the reranking methods.Section 4 describes how these approaches can be generalized to ranking problems.We introduce loss functions for boosting and MRF approaches and discuss optimization methods.We also derive the efficient algorithm for boosting in this section.Section 5 gives experimental results, investigating the performance improvements on parsing, efficiency issues, and the effect of various parameters of the boosting algorithm.Section 6 discusses related work in more detail.Finally, section 7 gives conclusions.The reranking models in this article were originally introduced in Collins (2000).In this article we give considerably more detail in terms of the algorithms involved, their justification, and their performance in experiments on natural language parsing.Before discussing the reranking approaches, we describe history-based models (Black et al. 1992).They are important for a few reasons.First, several of the best-performing parsers on the WSJ treebank (e.g., Ratnaparkhi 1997; Charniak 1997, 2000; Collins 1997, 1999; Henderson 2003) are cases of history-based models.Many systems applied to part-of-speech tagging, speech recognition, and other language or speech tasks also fall into this class of model.Second, a particular history-based model (that of Collins [1999]) is used as the initial model for our approach.Finally, it is important to describe history-based models—and to explain their limitations—to motivate our departure from them.Parsing can be framed as a supervised learning task, to induce a function f : X--+Y given training examples (xi, yi), where xi Z X, yi Z Y.We define GEN(x)ÎY to be the set of candidates for a given input x.In the parsing problem x is a sentence, and GEN(x) is a set of candidate trees for that sentence.A particular characteristic of the problem is the complexity of GEN(x) : GEN(x) can be very large, and each member of GEN(x) has a rich internal structure.This contrasts with “typical”classification problems in which GEN(x) is a fixed, small set, for example, f-1,+11 in binary classification problems.In probabilistic approaches, a model is defined which assigns a probability P(x, y) to each (x, y) pair.2 The most likely parse for each sentence x is then arg maxyEGEN(x) P(x, y).This leaves the question of how to define P(x,y).In history-based approaches, a one-to-one mapping is defined between each pair (x, y) and a decision sequence (d1... dn).The sequence (d1... dn) can be thought of as the sequence of moves that build (x, y) in some canonical order.Given this mapping, the probability of a tree can be written as Here, (d1... di-1) is the history for the ith decision.F is a function which groups histories into equivalence classes, thereby making independence assumptions in the model.Probabilistic context-free grammars (PCFGs) are one example of a history-based model.The decision sequence (d1... dn) is defined as the sequence of rule expansions in a top-down, leftmost derivation of the tree.The history is equivalent to a partially built tree, and F picks out the nonterminal being expanded (i.e., the leftmost nonterminal in the fringe of this tree), making the assumption that P(diId1... di-1) depends only on the nonterminal being expanded.In the resulting model a tree with rule expansions Our base model, that of Collins (1999), is also a history-based model.It can be considered to be a type of PCFG, where the rules are lexicalized.An example rule would be Lexicalization leads to a very large number of rules; to make the number of parameters manageable, the generation of the right-hand side of a rule is broken down into a number of decisions, as follows: 2 To be more precise, generative probabilistic models assign joint probabilities P(x,y) to each (x,y) pair.Similar arguments apply to conditional history-based models, which define conditional probabilities P(y I x) through a definition where d1... dn are again the decisions made in building a parse, and F is a function that groups histories into equivalence classes.Note that x is added to the domain of F (the context on which decisions are conditioned).See Ratnaparkhi (1997) for one example of a method using this approach.Figure 1 illustrates this process.Each of the above decisions has an associated probability conditioned on the left-hand side of the rule (VP(saw)) and other information in some cases.History-based approaches lead to models in which the log-probability of a parse tree can be written as a linear sum of parameters ak multiplied by features hk.Each feature hk(x, y) is the count of a different “event”or fragment within the tree.As an example, consider a PCFG with rules (Ak—>Ok) for 1 < k < m. If hk(x,y) is the number of times (Ak—>Ok) is seen in the tree, and ak = log P(OklAk) is the parameter associated with that rule, then All models considered in this article take this form, although in the boosting models the score for a parse is not a log-probability.The features hk define an m-dimensional vector of counts which represent the tree.The parameters ak represent the influence of each feature on the score of a tree.A drawback of history-based models is that the choice of derivation has a profound influence on the parameterization of the model.(Similar observations have been made in the related cases of belief networks [Pearl 1988], and language models for speech recognition [Rosenfeld 1997].)When designing a model, it would be desirable to have a framework in which features can be easily added to the model.Unfortunately, with history-based models adding new features often requires a modification of the underlying derivations in the model.Modifying the derivation to include a new feature type can be a laborious task.In an ideal situation we would be able to encode arbitrary features hk, without having to worry about formulating a derivation that included these features.To take a concrete example, consider part-of-speech tagging using a hidden Markov model (HMM).We might have the intuition that almost every sentence has at least one verb and therefore that sequences including at least one verb should have increased scores under the model.Encoding this constraint in a compact way in an HMM takes some ingenuity.The obvious approach—to add to each state the information about whether or not a verb has been generated in the history—doubles The sequence of decisions involved in generating the right-hand side of a lexical rule. the number of states (and parameters) in the model.In contrast, it would be trivial to implement a feature hk(x,y) which is 1 if y contains a verb, 0 otherwise.We now turn to machine-learning methods for the ranking task.In this section we review two methods for binary classification problems: logistic regression (or maximum-entropy) models and boosting.These methods form the basis for the reranking approaches described in later sections of the article.Maximum-entropy models are a very popular method within the computational linguistics community; see, for example, Berger, Della Pietra, and Della Pietra (1996) for an early article which introduces the models and motivates them.Boosting approaches to classification have received considerable attention in the machine-learning community since the introduction of AdaBoost by Freund and Schapire (1997).Boosting algorithms, and in particular the relationship between boosting algorithms and maximum-entropy models, are perhaps not familiar topics in the NLP literature.However there has recently been much work drawing connections between the two methods (Friedman, Hastie, and Tibshirani 2000; Lafferty 1999; Duffy and Helmbold 1999; Mason, Bartlett, and Baxter 1999; Lebanon and Lafferty 2001; Collins, Schapire, and Singer 2002); in this section we review this work.Much of this work has focused on binary classification problems, and this section is also restricted to problems of this type.Later in the article we show how several of the ideas can be carried across to reranking problems.The general setup for binary classification problems is as follows: where each ak E R, hence a¯ is an m-dimensional real-valued vector.We show that both logistic regression and boosting implement a linear, or hyperplane, classifier.This means that given an input example x and parameter values ¯a, the output from the classifier is Collins and Koo Discriminative Reranking for NLP where hyperplane which passes through the origin4 of the space and has a¯ as its normal.Points lying on one side of this hyperplane are classified as +1; points on the other side are classified as —1.The central question in learning is how to set the parameters ¯a, given the training examples bðx1, y1Þ, ðx2, y2Þ, ... ,ðxn, ynÞÀ.Logistic regression and boosting involve different algorithms and criteria for training the parameters ¯a, but recent work (Friedman, Hastie, and Tibshirani 2000; Lafferty 1999; Duffy and Helmbold 1999; Mason, Bartlett, and Baxter 1999; Lebanon and Lafferty 2001; Collins, Schapire, and Singer 2002) has shown that the methods have strong similarities.The next section describes parameter estimation methods.A central idea in both logistic regression and boosting is that of a loss function, which drives the parameter estimation methods of the two approaches.This section describes loss functions for binary classification.Later in the article, we introduce loss functions for reranking tasks which are closely related to the loss functions for classification tasks.First, consider a logistic regression model.The parameters of the model a¯ are used to define a conditional probability where Fðx, ¯aÞ is as defined in equation (2).Some form of maximum-likelihood estimation is often used for parameter estimation.The parameters are chosen to maximize the log-likelihood of the training set; equivalently: we talk (to emphasize the similarities to the boosting approach) about minimizing the negative log-likelihood.The negative log-likelihood, LogLoss(¯a), is defined as There are many methods in the literature for minimizing LogLoss(¯a) with respect to ¯a, for example, generalized or improved iterative scaling (Berger, Della Pietra, and Della Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997), or conjugate gradient methods (Malouf 2002).In the next section we describe feature selection methods, as described in Berger, Della Pietra, and Della Pietra (1996) and Della Pietra, Della Pietra, and Lafferty (1997).Once the parameters a¯ are estimated on training examples, the output for an example x is the most likely label under the model, where as before, sign (z) = 1 if z > 0, sign (z) = —1 otherwise.Thus we see that the logistic regression model implements a hyperplane classifier.In boosting, a different loss function is used, namely, ExpLoss(¯a), which is defined as This loss function is minimized using a feature selection method, which we describe in the next section.There are strong similarities between LogLoss (equation (4)) and ExpLoss (equation (6)).In making connections between the two functions, it is useful to consider a third function of the parameters and training examples, where gpÄ is one if p is true, zero otherwise.Error(¯a) is the number of incorrectly classified training examples under parameter values ¯a.Finally, it will be useful to define the margin on the ith training example, given parameter values ¯a, as The three loss functions differ only in their choice of an underlying “potential function”of the margins, f(z).This function is f(z) = log (1 + e—z), f(z) = e—z, or f (z) = Qz < 01 for LogLoss, ExpLoss, and Error, respectively.The f(z) functions penalize nonpositive margins on training examples.The simplest function, f (z) = Qz < 01, gives a cost of one if a margin is negative (an error is made), zero otherwise.ExpLoss and LogLoss involve definitions for f(z) which quickly tend to zero as z Y oo but heavily penalize increasingly negative margins.Figure 2 shows plots for the three definitions of f (z).The functions f (z) = e—z and f (z) = log (1 + e—z) are both upper bounds on the error function, so that minimizing either LogLoss or ExpLoss can be seen as minimizing an upper bound on the number of training errors.(Note that minimizing Error(¯�) itself is known to be at least NP-hard if no parameter settings can achieve zero errors on the training set; see, for example, Hoffgen, van Horn, and Simon [1995].)As z Y oo, the functions f (z) = e—z and f (z) = log(1 + e—z) become increasingly similar, because log (1 + e—z) Y e—z as e—z Y 0.For negative z, the two functions behave quite differently. f (z) = e—z shows an exponentially growing cost function as z Y — oo.In contrast, as z Y —oo it can be seen that log(1 + e—z) Y log(e—z) = —z, so this function shows asymptotically linear growth for negative z.As a final remark, note that both f (z) = e—z and f (z) = log(1 + e—z) are convex in z, with the result that LogLoss(¯�) and ExpLoss(¯�) are convex in the parameters ¯�.This means that there are no problems with local minima when optimizing these two loss functions.In this article we concentrate on feature selection methods: algorithms which aim to make progress in minimizing the loss functions LogLoss(¯�) and ExpLoss(¯�) while using a small number of features (equivalently, ensuring that most parameter values in Potential functions underlying ExpLoss, LogLoss, and Error.The graph labeled ExpLoss is a plot of f (z) = e—z for z = [—1.5...1.5]; LogLoss shows a similar plot for f (z) = log(1 + e—z); Error is a plot of f (z) = Qz < 01. a¯ are zero).Roughly speaking, the motivation for using a small number of features is the hope that this will prevent overfitting in the models.Feature selection methods have been proposed in the maximum-entropy literature by several authors (Ratnaparkhi, Roukos, and Ward 1994; Berger, Della Pietra, and Della Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997; Papineni, Roukos, and Ward 1997, 1998; McCallum 2003; Zhou et al. 2003; Riezler and Vasserman 2004).The most basic approach—for example see Ratnaparkhi, Roukos, and Ward (1994) and Berger, Della Pietra, and Della Pietra (1996)—involves selection of a single feature at each iteration, followed by an update to the entire model, as follows: Step 3: Update the parameter for the feature chosen at Step 2 in such a way as to minimize ExpLoss(¯a) with respect to this one parameter.All other parameter values are left fixed.Return to Step 2.The difference with this latter “boosting”approach is that in Step 3, only one parameter value is adjusted, namely, the parameter corresponding to the newly chosen feature.Note that in this framework, the same feature may be chosen at more than one iteration.5 The maximum-entropy feature selection method can be quite inefficient, as the entire model is updated at each step.For example, Ratnaparkhi (1998) quotes times of around 30 hours for 500 rounds of feature selection on a prepositionalphrase attachment task.These experiments were performed in 1998, when processors were no doubt considerably slower than those available today.However, the PP attachment task is much smaller than the parsing task that we are addressing: Our task involves around 1,000,000 examples, with perhaps a few hundred features per example, and 100,000 rounds of feature selection; this compares to 20,000 examples, 16 features per example, and 500 rounds of feature selection for the PP attachment task in Ratnaparkhi (1998).As an estimate, assuming that computational complexity scales linearly in these factors,6 our task is 1,000,000 as large as the PP attachment task.These figures suggest that the maximum-entropy feature selection approach may be infeasible for large-scale tasks such as the one in this article.The fact that the boosting approach does not update the entire model at each round of feature selection may be a disadvantage in terms of the number of features or the test data accuracy of the final model.There is reason for concern that Step 2 will at some iterations mistakenly choose features which are apparently useful in reducing the loss function, but which would have little utility if the entire model had been optimized at the previous iteration of Step 3.However, previous empirical results for boosting have shown that it is a highly effective learning method, suggesting that this is not in fact a problem for the approach.Given the previous strong results for the boosting approach, and for reasons of computational efficiency, we pursue the boosting approach to feature selection in this article.Minimization of LogLoss is most often justified as a parametric, maximum-likelihood (ML) approach to estimation.Thus this approach benefits from the usual guarantees for ML estimation: If the distribution generating examples is within the class of distributions specified by the log-linear form, then in the limit as the sample size goes to infinity, the model will be optimal in the sense of convergence to the true underlying distribution generating examples.As far as we are aware, behavior of the models for finite sample sizes is less well understood.In particular, while feature selection methods have often been proposed for maximum-entropy models, little theoretical justification (in terms of guarantees about generalization) has been given for them.It seems intuitive that a model with a smaller number of parameters will require fewer samples for convergence, but this is not necessarily the case, and at present this intuition lacks a theoretical basis.Feature selection methods can probably be motivated either from a Bayesian perspective (through a prior favoring models with a smaller number of nonzero parameters) or from a frequentist/goodness-of-fit perspective (models with fewer parameters are less likely to fit the data by chance), but this requires additional research.The statistical justification for boosting approaches is quite different.Boosting algorithms were originally developed within the PAC framework (Valiant 1984) for machine learning, specifically to address questions regarding the equivalence of weak and strong learning.Freund and Schapire (1997) originally introduced AdaBoost and gave a first set of statistical guarantees for the algorithm.Schapire et al. (1998) gave a second set of guarantees based on the analysis of margins on training examples.Both papers assume that a fixed distribution D(x, y) is generating both training and test examples and that the goal is to find a hypothesis with a small number of expected errors with respect to this distribution.The form of the distribution is not assumed to be known, and in this sense the guarantees are nonparametric, or “distribution free.”Freund and Schapire (1997) show that if the weak learning assumption holds (i.e., roughly speaking, a feature with error rate better than chance can be found for any distribution over the sample space X x {-1, +1}), then the training error for the ExpLoss method decreases rapidly enough for there to be good generalization to test examples.Schapire et al. (1998) show that under the same assumption, minimization of ExpLoss using the feature selection method ensures that the distribution of margins on training data develops in such a way that good generalization performance on test examples is guaranteed.Thus far in this article we have presented boosting as a feature selection approach.In this section, we note that there is an alternative view of boosting in which it is described as a method for combining multiple models, for example, as a method for forming a linear combination of decision trees.We consider only the simpler, feature selection view of boosting in this article.This section is included for completeness and because the more general view of boosting may be relevant to future work on boosting approaches for parse reranking (note, however, that the discussion in this section is not essential to the rest of the article, so the reader may safely skip this section if she or he wishes to do so).In feature selection approaches, as described in this article, the set of possible features hkðxÞ for k = 1, ... , m is taken to be a fixed set of relatively simple functions.In particular, we have assumed that m is relatively small (for example, small enough for algorithms that require O(m) time or space to be feasible).More generally, however, boosting can be applied in more complex settings.For example, a common use of boosting is to form a linear combination of decision trees.In this case each example x is represented as a number of attribute-value pairs, and each “feature”hk(x) is a complete decision tree built on predicates over the attribute values in x.In this case the number of features m is huge: There are as many features as there are decision trees over the given set of attributes, thus m grows exponentially quickly with the number of attributes that are used to represent an example x.Boosting may even be applied in situations in which the number of features is infinite.For example, it may be used to form a linear combination of neural networks.In this case each feature hk(x) corresponds to a different parameter setting within the (infinite) set of possible parameter settings for the neural network.In more complex settings such as boosting of decision trees or neural networks, it is generally not feasible to perform an exhaustive search (with O(m) time complexity) for the feature which has the greatest impact on the exponential7 loss function.Instead, an approximate search is performed.In boosting approaches, this approximate search is achieved through a protocol in which at each round of boosting, a “distribution”over the training examples is maintained.The distribution can be interpreted as assigning an importance weight to each training example, most importantly giving higher weight to examples which are incorrectly classified.At each round of boosting the distribution is passed to an algorithm such as a decision tree or neural network learning method, which attempts to return a feature (a decision tree, or a neural network parameter setting) which has a relatively low error rate with respect to the distribution.The feature that is returned is then incorporated into the linear combination of features.The algorithm which generates a classifier given a distribution over the examples (for example, the decision tree induction method) is usually referred to as “the weak learner.”The weak learner generally uses an approximate (for example, greedy) method to find a function with a low error rate with respect to the distribution.Freund and Schapire (1997) show that provided that at each round of boosting the weak learner returns a feature with greater than (50 + e) % accuracy for some fixed e, the number of training errors falls exponentially quickly with the number of rounds of boosting.This fast drop in training errors translates to statistical bounds on generalization performance (Freund and Schapire 1997).7 Note that it is also possible to apply these methods to the LogLoss function; see, for example, Friedman et al. (2000) and Duffy and Helmbold (1999).Under this view of boosting, the feature selection methods in this article are a particularly simple case in which the weak learner can afford to exhaustively search through the space of possible features.Future work on reranking approaches might consider other approaches—such as boosting of decision trees—which can effectively consider more complex features.This section describes how the ideas from classification problems can be extended to reranking tasks.A baseline statistical parser is used to generate N-best output both for its training set and for test data sentences.Each candidate parse for a sentence is represented as a feature vector which includes the log-likelihood under the baseline model, as well as a large number of additional features.The additional features can in principle be any predicates over sentence/tree pairs.Evidence from the initial loglikelihood and the additional features is combined using a linear model.Parameter estimation becomes a problem of learning how to combine these different sources of information.The boosting algorithm we use is related to the generalization of boosting methods to ranking problems in Freund et al. (1998); we also introduce an approach related to the conditional log-linear models of Ratnaparkhi, Roukos, and Ward (1994), Papineni, Roukos, and Ward (1997, 1998), Johnson et al.(1999), Riezler et al. (2002), and Och and Ney (2002).Section 4.1 gives a formal definition of the reranking problem.Section 4.2 introduces loss functions for reranking that are analogous to the LogLoss and ExpLoss functions in section 3.2.Section 4.3 describes a general approach to feature selection methods with these loss functions.Section 4.4 describes a first algorithm for the exponential loss (ExpLoss) function; section 4.5 introduces a more efficient algorithm for the case of ExpLoss.Finally, section 4.6 describes issues in feature selection algorithms for the LogLoss function.We use the following notation in the rest of this article: tree and the underlying sentence (i.e., each xi,j is a pair bsi, ti,jÀ, where si is the ith sentence in the training data, and ti,j is the jth tree for this sentence).We assume that the parses are distinct, that is, that xi,j 0 xi,j¶ for j 0 j¶.Thus our training data consist of a set of parses, {xi,j : i = 1, ... , n, j = 1, ... , niJ, together with scores Score(xi,j) and log-probabilities L(xi,j).We represent candidate parse trees through m features, hk for k = 1,. .., m. Each hk is an indicator function, for example, We show that the restriction to binary-valued features is important for the simplicity and efficiency of the algorithms.10 We also assume a vector of m + 1 parameters, a¯ = {a0, a1, ... , am}.Each ai can take any value in the reals.The ranking function for a parse tree x implied by a parameter vector a¯ is defined as Given a new test sentence s, with parses xj for j = 1, ... , N, the output of the model is the highest-scoring tree under the ranking function arg max Thus F(x, ¯a) can be interpreted as a measure of how plausible a parse x is, with higher scores meaning that x is more plausible.Competing parses for the same sentence are ranked in order of plausibility by this function.We can recover the base ranking function—the log-likelihood L(x)—by setting a0 to a positive constant and setting all other parameter values to be zero.Our intention is to use the training examples to pick parameter values which improve upon this initial ranking.We now discuss how to set these parameters.First we discuss loss functions Loss(¯a) which can be used to drive the training process.We then go on to describe feature selection methods for the different loss functions.8 In the event that multiple parses get the (same) highest score, the parse with the highest value of loglikelihood L under the baseline model is taken as xi,1.In the event that two parses have the same score and the same log-likelihood—which occurred rarely if ever in our experiments—we make a random choice between the two parses.9 This is not necessarily a significant issue if an application using the output from the parser is sensitive to improvements in evaluation measures such as precision and recall that give credit for partial matches between the parser’s output and the correct parse.In this case, it is important only that the precision/ recall for xi,1 is significantly higher than that of the baseline parser, that is, that there is some “head room”for the reranking module in terms of precision and recall.10 In particular, this restriction allows closed-form parameter updates for the models based on ExpLoss that we consider.Note that features tracking the counts of different rules can be simulated through several features which take value one if a rule is seen > 1 time, > 2 times > 3 times, and so on.4.2.1 Ranking Errors and Margins.The loss functions we consider are all related to the number of ranking errors a function F makes on the training set.The ranking error rate is the number of times a lower-scoring parse is (incorrectly) ranked above the best parse: where again, gpÄ is one if p is true, zero otherwise.In the ranking problem we define the margin for each example xi,j such that i = 1,. .., n, j = 2,. .., ni, as Thus Mij(¯a) is the difference in ranking score between the correct parse of a sentence and a competing parse xi,j.It follows that The ranking error is zero if all margins are positive.The loss functions we discuss all turn out to be direct functions of the margins on training examples.4.2.2 Log-Likelihood.The first loss function is that suggested by Markov random fields.As suggested by Ratnaparkhi, Roukos, and Ward (1994) and Johnson et al. (1999), the conditional probability of xi,q being the correct parse for the ith sentence is defined as Hence once the parameters are trained, the ranking function is used to order candidate trees for test examples.The log-likelihood of the training data is Under maximum-likelihood estimation, the parameters a¯ would be set to maximize the log-likelihood.Equivalently, we again talk about minimizing the negative log-likelihood.Some manipulation shows that the negative log-likelihood is a function of the margins on training data: Note the similarity of equation (9) to the LogLoss function for classification in equation (4). described in Schapire and Singer (1999).It is a special case of the general ranking methods described in Freund et al. (1998), with the ranking “feedback”being a simple binary distinction between the highest-scoring parse and the other parses.Again, the loss function is a function of the margins on training data: Note the similarity of equation (10) to the ExpLoss function for classification in equation (6).It can be shown that ExpLossð¯aÞ > Errorð¯aÞ, so that minimizing ExpLossð¯aÞ is closely related to minimizing the number of ranking errors.11 This follows from the fact that for any x, e�x > gx < 01, and therefore that We generalize the ExpLoss function slightly, by allowing a weight for each example xi,j, for i = 1, ... , n, j = 2, ... , ni.We use Si,j to refer to this weight.In particular, in some experiments in this article, we use the following definition: where, as defined in section 4.1, Score(xi,j) is some measure of the “goodness”of a parse, such as the F-measure (see section 5 for the exact definition of Score used in our experiments).The definition for ExpLoss is modified to be This definition now takes into account the importance, Si,j, of each example.It is an upper bound on the following quantity: which is the number of errors weighted by the factors Si,j.The original definition of ExpLoss in equation (10) can be recovered by setting Si,j = 1 for all i, j (i.e., by giving equal weight to all examples).In our experiments we found that a definition of Si,j such as that in equation (11) gave improved performance on development data, presumably because it takes into account the relative cost of different ranking errors in trainingdata examples.At this point we have definitions for ExpLoss and LogLoss which are analogous to the definitions in section 3.2 for binary classification tasks.Section 3.3 introduced the idea of feature selection methods; the current section gives a more concrete description of the methods used in our experiments.The goal of feature selection methods is to find a small subset of the features that contribute most to reducing the loss function.The methods we consider are greedy, at each iteration picking the feature hk with additive weight d which has the most impact on the loss function.In general, a separate set of instances is used in cross-validation to choose the stopping point, that is, to decide on the number of features in the model.At this point we introduce some notation concerning feature selection methods.We define Upd(¯a,k,d) to be an updated parameter vector, with the same parameter values as a¯ with the exception of ak, which is incremented by d: The d parameter can potentially take any value in the reals.The loss for the updated model is Loss(Upd(¯a, k,d)).Assuming we greedily pick a single feature with some weight to update the model, and given that the current parameter settings are ¯a, the optimal feature/weight pair (k*, d*) is Note that this is essentially the idea behind the “boosting”approach to feature selection introduced in section 3.3.In contrast, the feature selection method of Berger, Della Pietra, and Della Pietra (1996), also described in section 3.3, would involve updating parameter values for all selected features at step 2b.The main computation for both loss functions involves searching for the optimal feature/weight pair (k*, d*).In both cases we take a two-step approach to solving this problem.In the first step the optimal update for each feature hk is calculated.We define BestWt(k, ¯a) as the optimal update for the kth feature (it must be calculated for all features k = 1, ... , m): The next step is to calculate the Loss for each feature with its optimal update, which we will call BestLoss(k, ¯a) = min Loss(Upd(¯a, k, d)) = Loss(Upd(¯a, k, BestWt(k, ¯a))) d BestWt and BestLoss for each feature having been computed, the optimal feature/ weight pair can be found: k* =arg min kBestLoss(k, ¯a), d* = BestWt(k*, ¯a) The next sections describe how BestWt and BestLoss can be computed for the two loss functions.At the first iteration, a0 is set to optimize ExpLoss (recall that L(xi,j) is the loglikelihood for parse xi,j under the base parsing model): contribution of the log-likelihood feature is well-calibrated with respect to the exponential loss function.In our implementation a0 was optimized using simple bruteforce search.All values of a0 between 0.001 and 10 at increments of 0.001 were tested, and the value which minimized the function in equation (12) was chosen.12 Feature selection then proceeds to search for values of the remaining parameters, a1, ... , am.(Note that it might be preferable to also allow a0 to be adjusted as features are added; we leave this to future work.)This requires calculation of the terms BestWt(k, ¯a) and BestLoss(k, ¯a) for each feature.For binary-valued features these values have closed-form solutions, which is computationally very convenient.We now describe the form of these updates.See appendix A for how the updates can be derived (the derivation is essentially the same as that in Schapire and Singer [1999]).First, we note that for any feature, [hk(xi,1) — hk(xi,j)] can take on three values: +1, —1, or 0 (this follows from our assumption of binary-valued feature values).For each k we define the following sets: Thus A+k is the set of training examples in which the kth feature is seen in the correct parse but not in the competing parse; A—k is the set in which the kth feature is seen in the incorrect but not the correct parse.Based on these definitions, we next define Wk and W—k as follows: and BestLoss(k, ¯a) = Z — (rWk+ Wk ~2 (16) where Z 1/4 Ei Enij1/42 SId - e Mi,jð6Þ 1/4 ExpLossðaÞ is a constant (for fixed a) which appears in the BestLoss for all features and therefore does not affect their ranking.As Schapire and Singer (1999) point out, the updates in equation (15) can be problematic, as they are undefined (infinite) when either Wþk or Wk is zero.Following Schapire and Singer (1999), we introduce smoothing through a parameter E and the following new definition of BestWt: BestWtðk, aÞ 1/4 1 log Wþk þ EZ ð17Þ 2 Wk þ EZ The smoothing parameter E is chosen through optimization on a development set.See Figure 3 for a direct implementation of the feature selection method for ExpLoss.We use an array of values to indicate the gain of each feature (i.e., the impact that choosing this feature will have on the ExpLoss function).The features are ranked by this quantity.It can be seen that almost all of the computation involves the calculation of Z and Wþk and Wk for each feature hk.Once these values have been computed, the optimal feature and its update can be chosen.4.5 A New, More Efficient Algorithm for ExpLoss This section presents a new algorithm which is equivalent to the ExpLoss algorithm in Figure 3, but can be vastly more efficient for problems with sparse feature spaces.In the experimental section of this article we show that it is almost 2,700 times more efficient for our task than the algorithm in Figure 3.The efficiency of the different algorithms is important in the parsing problem.The training data we eventually used contained around 36,000 sentences, with an average of 27 parses per sentence, giving around 1,000,000 parse trees in total.There were over 500,000 different features.The new algorithm is also applicable, with minor modifications, to boosting approaches for classification problems in which the representation also involves sparse binary features (for example, the text classification problems in Schapire and Singer [2000]).As far as we are aware, the new algorithm has not appeared elsewhere in the boosting literature.Figure 4 shows the improved boosting algorithm.Inspection of the algorithm in Figure 3 shows that only margins on examples in the sets Aþk~ and A- are modified when a feature k* is selected.The feature space in many NLP problems is very sparse (most features only appear on relatively few training examples, or equivalently, most training examples will have only a few nonzero features).It follows that in many cases, the sets Aþk~ and A- will be much smaller than the overall size of the training set.Therefore when updating the model from a to Updð¯�, k*, S*Þ, the values Wþk and Wk remain unchanged for many features and do not need to be recalculated.In fact, only A naive algorithm for the boosting loss function. features which co-occur with k* on some example must be updated.The algorithm in Figure 4 recalculates the values of A k and A—k only for those features which co-occur with the selected feature k*.To achieve this, the algorithm relies on a second pair of indices.For all i, 2 < j < ni, we define Figure 4 An improved algorithm for the boosting loss function.In contrast, the naive algorithm requires a pass over the entire training set, which requires the following number of steps: The relative efficiency of the two algorithms depends on the value of C/T at each iteration.In the worst case, when every feature chosen appears on every training example, then C/T = 1, and the two algorithms essentially have the same running time.However in sparse feature spaces there is reason to believe that C/T will be small for most iterations.In section 5.4.3 we show that this is the case for our experiments.4.6 Feature Selection for LogLoss We now describe an approach that was implemented for LogLoss.At the first iteration, a0 is set to one.Feature selection then searches for values of the remaining parameters, a1, ... , am.We now describe how to calculate the optimal update for a feature k with the LogLoss function.First we recap the definition of the probability of a particular parse xi,q given parameter settings ¯a: Unfortunately, unlike the case of ExpLoss, in general an analytic solution for BestWt does not exist.However, we can define an iterative solution using techniques from iterative scaling (Della Pietra, Della Pietra, and Lafferty 1997).We first define ˜ number of times that feature k is seen in the best parse, and ˜pkð¯aÞ, the expected number of times under the model that feature k is seen: Given this method for calculating BestWt(k, ¯a), BestLoss(k, ¯a) can be calculated as Loss(k, BestWt(k, ¯a)).Note that this is only one of a number of methods for finding BestWt(k, ¯a): Given that this is a one-parameter, convex optimization problem, it is a fairly simple task, and there are many methods which could be used.Unfortunately there does not appear to be an efficient algorithm for LogLoss that is analogous to the ExpLoss algorithm in Figure 4 (at least if the feature selection method is required to pick the feature with highest impact on the loss function at each iteration).A similar observation for LogLoss can be made, in that when the model is updated with a feature/weight pair (k*, d*), many features will have their values for BestWt and BestLoss unchanged.Only those features which co-occur with k* on some example will need to have their values of BestWt and BestLoss updated.However, this observation does not lead to an efficient algorithm: Updating these values is much more expensive than in the ExpLoss case.The procedure for finding the optimal value BestWt(k, ¯a) must be applied for each feature which co-occurs with the chosen feature k*.For example, the iterative scaling procedure described above must be applied for a number of features.For each feature, this will involve recalculation of the distribution {P(xi,1 1 si),P(xi,2 I si), ...,P(xi,ni I si)I for each example i on which the feature occurs.13 It takes only one feature that is seen on all training examples for the algorithm to involve recalculation of P(xi,j I si) for the entire training set.This contrasts with the simple updates in the improved boosting algorithm (W+k = W+k + D and Wk = Wk + D).In fact in the parsing experiments, we were forced to give up on the LogLoss feature selection methods because of their inefficiency (see section 6.4 for more discussion about efficiency).˜hk ˜pk(¯a').Note, however, that approximate methods for finding the best feature and updating its weight may lead to efficient algorithms.Appendix B gives a sketch of one such approach, which is based on results from Collins, Schapire, and Singer (2002).We did not test this method; we leave this to future work.We used the Penn Wall Street Journal treebank (Marcus, Santorini, and Marcinkiewicz 1993) as training and test data.Sections 2–21 inclusive (around 40,000 sentences) were used as training data, section 23 was used as the final test set.Of the 40,000 training sentences, the first 36,000 were used as the main training set.The remaining 4,000 sentences were used as development data and to cross-validate the number of rounds (features) in the model.Model 2 of Collins (1999) was used to parse both the training and test data, producing multiple hypotheses for each sentence.We achieved this by disabling dynamic programming in the parser and choosing a relatively narrow beam width of 1,000.The resulting parser returns all parses that fall within the beam.The number of such parses varies sentence by sentence.In order to gain a representative set of training data, the 36,000 training sentences were parsed in 2,000 sentence chunks, each chunk being parsed with a model trained on the remaining 34,000 sentences (this prevented the initial model from being unrealistically “good”on the training sentences).The 4,000 development sentences were parsed with a model trained on the 36,000 training sentences.Section 23 was parsed with a model trained on all 40,000 sentences.In the experiments we used the following definition for the Score of the parse: where F-measure(xi,j) is the F1 score14 of the parse when compared to the goldstandard parse (a value between 0 and 100), and Size(xi,j) is the number of constituents in the gold-standard parse for the ith sentence.Hence the Score function is sensitive to both the accuracy of the parse, and also the number of constituents in the goldstandard parse.The following types of features were included in the model.We will use the rule VP —> PP VBD NP NP SBAR with head VBD as an example.Note that the output of our baseline parser produces syntactic trees with headword annotations (see Collins [1999]) for a description of the rules used to find headwords).Two-level rules.Same as Rules, but also including the entire rule above the rule.Two-level bigrams.Same as Bigrams, but also including the entire rule above the rule.Trigrams.All trigrams within the rule.The example rule would contribute the trigrams (VP, STOP, PP, VBD!), (VP, PP, VBD!, NP), (VP, VBD!, NP, NP), (VP, NP, NP, SBAR), and (VP,NP, SBAR, STOP) (! is used to mark the head of the rule).Grandparent bigrams.Same as Bigrams, but also including the nonterminal above the bigrams.Lexical bigrams.Same as Bigrams, but with the lexical heads of the two nonterminals also included.Head Modifiers.All head-modifier pairs, with the grandparent nonterminal also included.An adj flag is also included, which is one if the modifier is adjacent to the head, zero otherwise.As an example, say the nonterminal dominating the example rule is S. The example rule would contribute (Left, S, VP, VBD, PP, adj = 1), (Right, S, VP, VBD, NP, adj = 1), (Right, S, VP, VBD, NP, adj = 0), and (Right, S, VP, VBD, SBAR, adj = 0).PPs.Lexical trigrams involving the heads of arguments of prepositional phrases.The example shown at right would contribute the trigram (NP, NP, PP, NP, president, of, U.S.), in addition to the relation (NP, NP, PP, NP, of, U.S.), which ignores the headword of the constituent being modified by the PP.The three nonterminals (for example, NP, NP, PP) identify the parent of the entire phrase, the nonterminal of the head of the phrase, and the nonterminal label for the PP.Distance head modifiers.Features involving the distance between headwords.For example, assume dist is the number of words between the headwords of the VBD and SBAR in the (VP, VBD, SBAR) head-modifier relation in the above rule.This relation would then generate features (VP, VBD, SBAR, = dist), and (VP, VBD, SBAR, < x) for all dist < x < 9 and (VP, VBD, SBAR, > x) for all 1 < x < dist.Further lexicalization.In order to generate more features, a second pass was made in which all nonterminals were augmented with their lexical heads when these headwords were closed-class words.All features apart from head modifiers, PPs, and distance head modifiers were then generated with these augmented nonterminals.All of these features were initially generated, but only features seen on at least one parse for at least five different sentences were included in the final model (this count cutoff was implemented to keep the number of features down to a tractable number).The ExpLoss method was trained with several values for the smoothing parameter e: {0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.0025, 0.005, 0.0075}.For each value of e, the method was run for 100,000 rounds on the training data.The implementation was such that the feature updates for all 100,000 rounds for each training run were recorded in a file.This made it simple to test the model on development data for all values of N between 0 and 100,000.The different values of & and N were compared on development data through the following criterion: where Score is as defined above, and zi is the output of the model on the ith development set example.The &, N values which maximized this quantity were used to define the final model applied to the test data (section 23 of the treebank).The optimal values were & = 0.0025 and N = 90,386, at which point 11,673 features had nonzero values (note that the feature selection techniques may result in a given feature being updated more than once).The computation took roughly 3–4 hours on a machine with a 1.6 GHz pentium processor and around 2 GB of memory.Table 1 shows results for the method.The model of Collins (1999) was the base model; the ExpLoss model gave a 1.5% absolute improvement over this method.The method gives very similar accuracy to the model of Charniak (2000), which also uses a rich set of initial features in addition to Charniak’s (1997) original model.The LogLoss method was too inefficient to run on the full data set.Instead we made some tests on a smaller subset of the data (5,934 sentences, giving 200,000 parse trees) and 52,294 features.15 On an older machine (an order of magnitude or more slower than the machine used for the final tests) the boosting method took 40 minutes for 10,000 rounds on this data set.The LogLoss method took 20 hours to complete 3,500 rounds (a factor of about 85 times slower).This was in spite of various heuristics that were implemented in an attempt to speed up LogLoss: for example, selecting multiple features at each round or recalculating the statistics for only the best K features for some small K at the previous round of feature selection.In initial experiments we found ExpLoss to give similar, perhaps slightly better, accuracy than LogLoss.This section describes further experiments investigating various aspects of the boosting algorithm: the effect of the & and N parameters, learning curves, the choice of the Si,j weights, and efficiency issues.5.4.1 The Effect of the a and N Parameters.Figure 5 shows the learning curve on development data for the optimal value of & (0.0025).The accuracy shown is the performance relative to the baseline method of using the probability from the generative model alone in ranking parses, where the measure in equation (21) is used to measure performance.For example, a score of 101.5 indicates a 1.5% increase in this score.The learning curve is initially steep, eventually flattening off, but reaching its peak value after a large number (90,386) of rounds of feature selection.Table 2 indicates how the peak performance varies with the smoothing parameter &.Figure 6 shows learning curves for various values of &.It can be seen that values other than & = 0.0025 can lead to undertraining or overtraining of the model.Results on section 23 of the WSJ Treebank.“LR”is labeled recall; “LP”is labeled precision; “CBs”is the average number of crossing brackets per sentence; “0 CBs”is the percentage of sentences with 0 crossing brackets; “2 CBs”is the percentage of sentences with two or more crossing brackets.All the results in this table are for models trained and tested on the same data, using the same evaluation metric.Note that the ExpLoss results are very slightly different from the original results published in Collins (2000).We recently reimplemented the boosting code and reran the experiments, and minor differences in the code and a values tested on development data led to minor improvements in the results.Learning curve on development data for the optimal value for a (0.0025).The y-axis is the level of accuracy (100 is the baseline score), and the x-axis is the number of rounds of boosting. idea of weights Si,j representing the importance of examples.Thus far, in the experiments in this article, we have used the definition thereby weighting examples in proportion to their difference in score from the correct parse for the sentence in question.In this section we compare this approach to a default definition of Si,j, namely, Si,j 1/4 1 ð23Þ Using this definition, we trained the ExpLoss method on the same training set for several values of the smoothing parameter a and evaluated the performance on development data.Table 3 compares the peak performance achieved under the two definitions of Si,j on the development set.It can be seen that the definition in equation (22) outperforms the simpler method in equation (23).Figure 7 shows the learning curves for the optimal values of a for the two methods.It can be seen that the learning curve for the definition of Si,j in equation (22) consistently dominates the curve for the simpler definition.5.4.3 Efficiency Gains.Section 4.5 introduced an efficient algorithm for optimizing ExpLoss.In this section we explore the empirical gains in efficiency seen on the parsing data sets in this article.We first define the quantity T as follows: Learning curves on development data for various values of &.In each case the y-axis is the level of accuracy (100 is the baseline score), and the x-axis is the number of rounds of boosting.The three graphs compare the curve for & = 0.0025 (the optimal value) to (from top to bottom) & = 0.0001, & = 0.0075, and & = 0.001.The top graph shows that & = 0.0001 leads to undersmoothing (overtraining).Initially the graph is higher than that for & = 0.0025, but on later rounds the performance starts to decrease.The middle graph shows that & = 0.0075 leads to oversmoothing (undertraining).The graph shows consistently lower performance than that for & = 0.0025.The bottom graph shows that there is little difference in performance for & = 0.001 versus & = 0.0025.This is a measure of the number of updates to the Wþk and Wk variables required in making a pass over the entire training set.Thus it is a measure of the amount of computation that the naive algorithm for ExpLoss, presented in Figure 3, requires for each round of feature selection.Next, say the improved algorithm in Figure 4 selects feature k* on the t th round of feature selection.Then we define the following quantity: We are now in a position to compare the running times of the two algorithms.We define the following quantities: Here, Work(n) is the computation required for n rounds of feature selection, where a single unit of computation corresponds to a pass over the entire training set.Savings(n) tracks the relative efficiency of the two algorithms as a function of the number of features, n. For example, if Savings(100) = 1,200, this signifies that for the first 100 rounds of feature selection, the improved algorithm is 1,200 times as efficient as the naive algorithm.Finally, Savings(a, b) indicates the relative efficiency between rounds a and b, inclusive, of feature selection.For example, Savings(11, 100) = 83 signifies that between rounds 11 and 100 inclusive of the algorithm, the improved algorithm was 83 times as efficient.Figures 8 and 9 show graphs of Work(n) and Savings(n) versus n. The savings from the improved algorithm are dramatic.In 100,000 rounds of feature selection, the improved algorithm requires total computation that is equivalent to a mere 37.1 passes over the training set.This is a saving of a factor of 2,692 over the naive algorithm.Table 4 shows the value of Savings(a,b) for various values of (a,b).It can be seen that the performance gains are significantly larger in later rounds of feature selection, presumably because in later stages relatively infrequent features are being selected.Even so, there are still savings of a factor of almost 50 in the early stages of the method.Charniak (2000) describes a parser which incorporates additional features into a previously developed parser, that of Charniak (1997).The method gives substantial improvements over the original parser and results which are very close to the results of the boosting method we have described in this article (see section 5 for experimental results comparing the two methods).Our features are in many ways similar to those of Charniak (2000).The model in Charniak (2000) is quite different, however.The additional features are incorporated using a method inspired by maximum-entropy models (e.g., the model of Ratnaparkhi [1997]).Ratnaparkhi (1997) describes the use of maximum-entropy techniques applied to parsing.Log-linear models are used to estimate the conditional probabilities P(di I (D (d1,...,di_1)) in a history-based parser.As a result the model can take into account quite a rich set of features in the history.Savings(n)(y-axis) versus n(x-axis).Both approaches still rely on decomposing a parse tree into a sequence of decisions, and we would argue that the techniques described in this article have more flexibility in terms of the features that can be included in the model.Abney (1997) describes the application of log-linear models to stochastic headdriven phrase structure grammars (HPSGs).Della Pietra, Della Pietra, and Lafferty (1997) describe feature selection methods for log-linear models, and Rosenfeld (1997) describes application of these methods to language modeling for speech recognition.These methods all emphasize models which define a joint probability over the space of all parse trees (or structures in question): For this reason we describe these approaches as “Joint log-linear models.”The probability of a tree xi,j is Here Z is the (infinite) set of possible trees, and the denominator cannot be calculated explicitly.This is a problem for parameter estimation, in which an estimate of the denominator is required, and Monte Carlo methods have been proposed (Della Pietra, Della Pietra, and Lafferty 1997; Abney 1997; Rosenfeld 1997) as a technique for estimation of this value.Our sense is that these methods can be computationally expensive.Notice that the joint likelihood in equation (27) is not a direct function of the margins on training examples, and its relation to error rate is therefore not so clear as in the discriminative approaches described in this article.Ratnaparkhi, Roukos, and Ward (1994), Johnson et al. (1999), and Riezler et al.(2002) suggest training log-linear models (i.e., the LogLoss function in equation (9)) for parsing problems.Ratnaparkhi, Roukos, and Ward (1994) use feature selection techniques for the task.Johnson et al. (1999) and Riezler et al.(2002) do not use a feature selection technique, employing instead an objective function which includes a Gaussian prior on the parameter values, thereby penalizing parameter values which become too large: Closed-form updates under iterative scaling are not possible with this objective function; instead, optimization algorithms such as gradient descent or conjugate gradient methods are used to estimate parameter values.In more recent work, Lafferty, McCallum, and Pereira (2001) describe the use of conditional Markov random fields (CRFs) for tagging tasks such as named entity recognition or part-of-speech tagging (hidden Markov models are a common method applied to these tasks).CRFs employ the objective function in equation (28).A key insight of Lafferty, McCallum, and Pereira (2001) is that when features are of a significantly local nature, the gradient of the function in equation (28) can be calculated efficiently using dynamic programming, even in cases in which the set of candidates involves all possible tagged sequences and is therefore exponential in size.See also Sha and Pereira (2003) for more recent work on CRFs.Optimizing a log-linear model with a Gaussian prior (i.e., choosing parameter values which achieve the global minimum of the objective function in equation (28)) is a plausible alternative to the feature selection approaches described in the current article or to the feature selection methods previously applied to log-linear models.The Gaussian prior (i.e., the Pk a2k/72k penalty) has been found in practice to be very effective in combating overfitting of the parameters to the training data (Chen and Rosenfeld 1999; Johnson et al. 1999; Lafferty, McCallum, and Pereira 2001; Riezler et al.2002).The function in equation (28) can be optimized using variants of gradient descent, which in practice require tens or at most hundreds of passes over the training data (see, e.g., Sha and Pereira 2003).Thus log-linear models with a Gaussian prior are likely to be comparable in terms of efficiency to the feature selection approach described in this article (in the experimental section, we showed that for the parsereranking task, the efficient boosting algorithm requires computation that is equivalent to around 40 passes over the training data).Note, however, that the two methods will differ considerably in terms of the sparsity of the resulting reranker.Whereas the feature selection approach leads to around 11,000 (2%) of the features in our model having nonzero parameter values, log-linear models with Gaussian priors typically have very few nonzero parameters (see, e.g., Riezler and Vasserman 2004).This may be important in some domains, for example, those in which there are a very large number of features and this large number leads to difficulties in terms of memory requirements or computation time.A number of previous papers (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1998; Della Pietra, Della Pietra, and Lafferty 1997; McCallum 2003; Zhou et al. 2003; Riezler and Vasserman 2004) describe feature selection approaches for log-linear models applied to NLP problems.Earlier work (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1998; Della Pietra, Della Pietra, and Lafferty 1997) suggested methods that added a feature at a time to the model and updated all parameters in the current model at each step (for more detail, see section 3.3).Assuming that selection of a feature takes one pass over the training set and that fitting a model takes p passes over the training set, these methods require f x (p + 1) passes over the training set, where f is the number of features selected.In our experiments, f z 10,000.It is difficult to estimate the value for p, but assuming (very conservatively) that p = 2, selecting 10,000 features would require 30,000 passes over the training set.This is around 1,000 times as much computation as that required for the efficient boosting algorithm applied to our data, suggesting that the feature selection methods in Berger, Della Pietra, and Della Pietra (1996), Ratnaparkhi (1998), and Della Pietra, Della Pietra, and Lafferty (1997) are not sufficiently efficient for the parsing task.More recent work (McCallum 2003; Zhou et al. 2003; Riezler and Vasserman 2004) has considered methods for speeding up the feature selection methods described in Berger, Della Pietra, and Della Pietra (1996), Ratnaparkhi (1998), and Della Pietra, Della Pietra, and Lafferty (1997).McCallum (2003) and Riezler and Vasserman (2004) describe approaches that add k features at each step, where k is some constant greater than one.The running time for these methods is therefore O(f x (p + 1)1k).Riezler and Vasserman (2004) test a variety of values for k, finding that k = 100 gives optimal performance.McCallum (2003) uses a value of k = 1,000.Zhou et al. (2003) use a different heuristic that avoids having to recompute the gain for every feature at every iteration.We would argue that the alternative feature selection methods in the current article may be preferable on the grounds of both efficiency and simplicity.Even with large values of k in the approach of McCallum (2003) and Riezler and Vasserman (2004) (e.g., k = 1,000), the approach we describe is likely to be at least as efficient as these alternative approaches.In terms of simplicity, the methods in McCallum (2003) and Riezler and Vasserman (2004) require selection of a number of free parameters governing the behavior of the algorithm: the value for k, the value for a regularizer constant (used in both McCallum [2003] and Riezler and Vasserman [2004]), and the precision with which the model is optimized at each stage of feature selection (McCallum [2003] describes using “just a few BFGS iterations”at each stage).In contrast, our method requires a single parameter to be chosen (the value for the e smoothing parameter) and makes a single approximation (that only a single feature is updated at each round of feature selection).The latter approximation is particularly important, as it leads to the efficient algorithm in Figure 4, which avoids a pass over the training set at each iteration of feature selection (note that in sparse feature spaces, f rounds of feature selection in our approach can take considerably fewer than f passes over the training set, in contrast to other work on feature selection within log-linear models).Note that there are other important differences among the approaches.Both Della Pietra, Della Pietra, and Lafferty (1997) and McCallum (2003) describe methods that induce conjunctions of “base”features, in a way similar to decision tree learners.Thus a relatively small number of base features can lead to a very large number of possible conjoined features.In future work it might be interesting to consider these kinds of approaches for the parsing problem.Another difference is that both McCallum, and Riezler and Vasserman, describe approaches that use a regularizer in addition to feature selection: McCallum uses a two-norm regularizer; Riezler and Vasserman use a one-norm regularizer.Finally, note that other feature selection methods have been proposed within the machine-learning community: for example, “filter”methods, in which feature selection is performed as a preprocessing step before applying a learning method, and backward selection methods (Koller and Sahami 1996), in which initially all features are added to the model and features are then incrementally removed from the model.6.5 Boosting, Perceptron, and Support Vector Machine Approaches for Ranking Problems Freund et al. (1998) introduced a formulation of boosting for ranking problems.The problem we have considered is a special case of the problem in Freund et al. (1998), in that we have considered a binary distinction between candidates (i.e., the best parse vs. other parses), whereas Freund et al. consider learning full or partial orderings over candidates.The improved algorithm that we introduced in Figure 4 is, however, a new algorithm that could perhaps be generalized to the full problem of Freund et al. (1998); we leave this to future research.Altun, Hofmann, and Johnson (2003) and Altun, Johnson, and Hofmann (2003) describe experiments on tagging tasks using the ExpLoss function, in contrast to the LogLoss function used in Lafferty, McCallum, and Pereira (2001).Altun, Hofmann, and Johnson (2003) describe how dynamic programming methods can be used to calculate gradients of the ExpLoss function even in cases in which the set of candidates again includes all possible tagged sequences, a set which grows exponentially in size with the length of the sentence being tagged.Results in Altun, Johnson, and Hofmann (2003) suggest that the choice of ExpLoss versus LogLoss does not have a major impact on accuracy for the tagging task in question.Perceptron-based algorithms, or the voted perceptron approach of Freund and Schapire (1999), are another alternative to boosting and LogLoss methods.See Collins (2002a, 2002b) and Collins and Duffy (2001, 2002) for applications of the perceptron algorithm.Collins (2002b) gives convergence proofs for the methods; Collins (2002a) directly compares the boosting and perceptron approaches on a named entity task; and Collins and Duffy (2001, 2002) use a reranking approach with kernels, which allow representations of parse trees or labeled sequences in very-high-dimensional spaces.Shen, Sarkar, and Joshi (2003) describe support vector machine approaches to ranking problems and apply support vector machines (SVMs) using tree-adjoining grammar (Joshi, Levy, and Takahashi 1975) features to the parsing data sets we have described in this article, with good empirical results.See Collins (2004) for a discussion of many of these methods, including an overview of statistical bounds for the boosting, perceptron, and SVM methods, as well as a discussion of the computational issues involved in the different algorithms.This article has introduced a new algorithm, based on boosting approaches in machine learning, for ranking problems in natural language processing.The approach gives a 13% relative reduction in error on parsing Wall Street Journal data.While in this article the experimental focus has been on parsing, many other problems in natural language processing or speech recognition can also be framed as reranking problems, so the methods described should be quite broadly applicable.The boosting approach to ranking has been applied to named entity segmentation (Collins 2002a) and natural language generation (Walker, Rambow, and Rogati 2001).The key characteristics of the approach are the use of global features and of a training criterion (optimization problem) that is discriminative and closely related to the task at hand (i.e., parse accuracy).In addition, the article introduced a new algorithm for the boosting approach which takes advantage of the sparse nature of the feature space in the parsing data that we use.Other NLP tasks are likely to have similar characteristics in terms of sparsity.Experiments show an efficiency gain of a factor of over 2,600 on the parsing data for the new algorithm over the obvious implementation of the boosting approach.We would argue that the improved boosting algorithm is a natural alternative to maximum-entropy or (conditional) log-linear models.The article has drawn connections between boosting and maximum-entropy models in terms of the optimization problems that they involve, the algorithms used, their relative efficiency, and their performance in empirical tests.This appendix gives a derivation of the optimal updates for ExpLoss.The derivation is very close to that in Schapire and Singer (1999).Recall that for parameter values ¯a, we need to compute BestWtðk, ¯aÞ and BestLossðk, ¯aÞ for k 1/4 1, ... , m, where BestWtðk, ¯aÞ 1/4 arg min ExpLossðUpdð¯a, k, dÞÞ d and BestLossðk, ¯aÞ 1/4 ExpLossðUpdð¯a, k, BestWtðk, ¯aÞÞÞ The first thing to note is that an update in parameters from a¯ to Updð¯a, k,dÞÞ results in a simple additive update to the ranking function F: Fðxi,j, Updð¯a, k, dÞÞ 1/4 Fðxi,j, aÞ þ dhkðxi,jÞ It follows that the margin on example ði, jÞ also has a simple update: Next, we note that 1/2hkðxi,1Þ — hkðxi,jÞ] can take on three values: +1, —1, or 0.We split the training sample into three sets depending on this value: Aþk 1/4 fði,jÞ : 1/2hkðxi,1Þ — hkðxi,jÞ] 1/4 1g To find the value of d that minimizes this loss, we set the derivative of (A.1) with respect to d to zero, giving the following solution: where Z = ExpLoss(¯a) = Pi Pni 2 Si,je—Mi,j(¯a) is a constant (for constant ¯a) which appears in the BestLoss for all features and therefore does not affect their ranking.Appendix B: An Alternative Method for LogLoss In this appendix we sketch an alternative approach for feature selection in LogLoss that is potentially an efficient method, at the cost of introducing an approximation in the feature selection method.Until now, we have defined BestLossðk, ¯aÞ to be the minimum of the loss given that the kth feature is updated an optimal amount: BestLossðk, ¯aÞ 1/4 min LogLossðUpdð¯a,k, dÞÞ d In this section we sketch a different approach, based on results from Collins, Schapire, and Singer (2002), which leads to an algorithm very similar to that for ExpLoss in Figures 3 and 4.Take the following definitions (note the similarity to the definitions in equations (13), (14), (15), and (16), with only the definitions for Wk+ and Wk~ being altered): Note that the ExpLoss computations can be recovered by replacing qi,j in equation (B.1) with qi,j 1/4 emi,jð¯aÞ.This is the only essential difference between the new algorithm and the ExpLoss method.Results from Collins, Schapire and Singer (2002) show that under these definitions the following guarantee holds: LogLossðUpdð¯a,k, BestWtðk, ¯aÞÞÞ < BestLossðk, ¯aÞ So it can be seen that the update from a¯ to Updð¯a, k, BestWtðk, ¯aÞÞ is guaranteed to decrease LogLoss by at least ffiffiffiffiffiffiffi (Wk -W, )2.From these results, the algorithms in Figures 3 and 4 could be altered to take the revised definitions of Wþk and Wk into account.Selecting the feature with the minimum value of BestLossðk, ¯aÞ at each iteration leads to the largest guaranteed decrease in LogLoss.Note that this is now an approximation, in that BestLossðk, ¯a) is an upper bound on the log-likelihood which may or may not be tight.There are convergence guarantees for the method, however, in that as the number of rounds of feature selection goes to infinity, the LogLoss approaches its minimum value.The algorithms in Figures 3 and 4 could be modified to take the alternative definitions of Wþk and Wk into account, thereby being modified to optimize LogLoss instead of ExpLoss.The denominator terms in the qi,j definitions in equation (B.1) may complicate the algorithms somewhat, but it should still be possible to derive relatively efficient algorithms using the technique.For a full derivation of the modified updates and for quite technical convergence proofs, see Collins, Schapire and Singer (2002).We give a sketch of the argument here.First, we show that LogLossðUpdð¯a, k, dÞÞ < LogLossð¯a — Wþk — Wk þ Wþk e~d þ Wk edÞ ðB.4Þ Equation (B.6) can be derived from equation (B.5) through the bound logð1 + xÞ < x for all x.The second step is to minimize the right-hand side of the bound in equation (B.4) with respect to d. It can be verified that the minimum is found at at which value the right-hand side of equation (B.4) is equal toThanks to Rob Schapire and Yoram Singer for useful discussions on boosting algorithms and to Mark Johnson for useful discussions about linear models for parse ranking.Steve Abney and Fernando Pereira gave useful feedback on earlier drafts of this work.Finally, thanks to the anonymous reviewers for several useful comments.
We describe work toward the construction of a very wide-coverage probabilistic parsing system for natural language (NL), based on LR parsing techniques.The system is intended to rank the large number of syntactic analyses produced by NL grammars according to the frequency of occurrence of the individual rules deployed in each analysis.We discuss a fully automatic procedure for constructing an LR parse table from a unification-based grammar formalism, and consider the suitability of alternative LALR(1) parse table construction methods for large grammars.The parse table is used as the basis for two parsers; a user-driven interactive system that provides a computationally tractable and labor-efficient method of supervised training of the statistical information required to drive the probabilistic parser.The latter is constructed by associating probabilities with the LR parse table directly.This technique is superior to parsers based on probabilistic lexical tagging or probabilistic context-free grammar because it allows for a more context-dependent probabilistic language model, as well as use of a more linguistically adequate grammar formalism.We compare the performance of an optimized variant of Tomita's (1987) generalized LR parsing algorithm to an (efficiently indexed and optimized) chart parser.We report promising results of a pilot study training on 150 noun definitions from the Longman Dictionary of Contemporary English (LDOCE) and retesting on these plus a further 55 definitions.Finally, we discuss limitations of the current system and possible extensions to deal with lexical (syntactic and semantic) frequency of occurrence.The task of syntactically analyzing substantial corpora of naturally occurring text and transcribed speech has become a focus of recent work.Analyzed corpora would be of great benefit in the gathering of statistical data regarding language use, for example to train speech recognition devices, in more general linguistic research, and as a first step toward robust wide-coverage semantic interpretation.The Alvey Natural Language Tools (ANLT) system is a wide-coverage lexical, morphological, and syntactic analysis system for English (Briscoe et al. 1987).Previous work has demonstrated that the ANLT system is, in principle, able to assign the correct parse to a high proportion of English noun phrases drawn from a variety of corpora.The goal of the work reported here is to develop a practical parser capable of returning probabilistically highly ranked analyses (from the usually large number of syntactically legitimate possibilities) for material drawn from a specific corpus on the basis of minimal (supervised) training and manual modification.The first issue to consider is what the analysis will be used for and what constraints this places on its form.The corpus analysis literature contains a variety of proposals, ranging from part-of-speech tagging to assignment of a unique, sophisticated syntactic analysis.Our eventual goal is to recover a semantically and pragmatically appropriate syntactic analysis capable of supporting semantic interpretation.Two stringent requirements follow immediately: firstly, the analyses assigned must determinately represent the syntactic relations that hold between all constituents in the input; secondly, they must be drawn from an a priori defined, well-formed set of possible syntactic analyses (such as the set defined by a generative grammar).Otherwise, semantic interpretation of the resultant analyses cannot be guaranteed to be (structurally) unambiguous, and the semantic operations defined (over syntactic configurations) cannot be guaranteed to match and yield an interpretation.These requirements immediately suggest that approaches that recover only lexical tags (e.g. de Rose 1988) or a syntactic analysis that is the 'closest fit' to some previously defined set of possible analyses (e.g.Sampson, Haigh, and Atwell 1989), are inadequate (taken alone).Pioneering approaches to corpus analysis proceeded on the assumption that computationally tractable generative grammars of sufficiently general coverage could not be developed (see, for example, papers in Garside, Leech, and Sampson 1987).However, the development of wide-coverage declarative and computationally tractable grammars makes this assumption questionable.For example, the ANLT word and sentence grammar (Grover et al. 1989; Carroll and Grover 1989) consists of an English lexicon of approximately 40,000 lexemes and a 'compiled' fixed-arity term unification grammar containing around 700 phrase structure rules.Taylor, Grover, and Briscoe (1989) demonstrate that an earlier version of this grammar was capable of assigning the correct analysis to 96.8% of a corpus of 10,000 noun phrases extracted (without regard for their internal form) from a variety of corpora.However, although Taylor, Grover, and Briscoe show that the ANLT grammar has very wide coverage, they abstract away from issues of lexical idiosyncrasy by formimg equivalence classes of noun phrases and parsing a single token of each class, and they do not address the issues of 1) tuning a grammar to a particular corpus or sublanguage 2) selecting the correct analysis from the set licensed by the grammar and 3) providing reliable analyses of input outside the coverage of the grammar.Firstly, it is clear that vocabulary, idiom, and conventionalized constructions used in, say, legal language and dictionary definitions, will differ both in terms of the range and frequency of words and constructions deployed.Secondly, Church and Patil (1982) demonstrate that for a realistic grammar parsing realistic input, the set of possible analyses licensed by the grammar can be in the thousands.Finally, it is extremely unlikely that any generative grammar will ever be capable of correctly analyzing all naturally occurring input, even when tuned for a particular corpus or sublanguage (if only because of the synchronic idealization implicit in the assumption that the set of grammatical sentences of a language is well formed.)In this paper, we describe our approach to the first and second problems and make some preliminary remarks concerning the third (far harder) problem.Our approach to grammar tuning is based on a semi-automatic parsing phase during which additions to the grammar are made manually and statistical information concerning the frequency of use of grammar rules is acquired.Using this statistical information and modified grammar, a breadth-first probabilistic parser is constructed.The latter is capable of ranking the possible parses identified by the grammar in a useful (and efficient) manner.However, (unseen) sentences whose correct analysis is outside the coverage of the grammar reri:ain a problem.The feasibility and usefulness of our approach has been investigated in a preliminary way by analyzing a small corpus of noun definitions drawn from the Longman Dictionary of Contemporary English (LDOCE) (Procter 1978).This corpus was chosen because the vocabulary employed is restricted (to approximately 2,000 morphemes), average definition length is about 10 words (with a maximum of around 30), and each definition is independent, allowing us to ignore phenomena such as ellipsis.In addition, the language of definitions represents a recognizable sublanguage, allowing us to explore the task of tuning a general purpose grammar.The results reported below suggest that probabilistic information concerning the frequency of occurrence of syntactic rules correlates in a useful (though not absolute) way with the semantically and pragmatically most plausible analysis.In Section 2, we briefly review extant work on probabilistic approaches to corpus analysis and parsing and argue the need for a more refined probabilistic model to distinguish distinct derivations.Section 3 discusses work on LR parsing of natural language and presents our technique for automatic construction of LR parsers for unification-based grammars.Section 4 presents the method and results for constructing a LALR(1) parse table for the ANLT grammar and discusses these in the light of both computational complexity and other empirical results concerning parse table size and construction time.Section 5 motivates our interactive and incremental approach to semi-automatic production of a disambiguated training corpus and describes the variant of the LR parser used for this task.Section 6 describes our implementation of a breadth-first LR parser and compares its performance empirically to a highly optimized chart parser for the same grammar, suggesting that (optimized) LR parsing is more efficient in practice for the ANLT grammar despite exponential worst case complexity results.Section 7 explains the technique we employ for deriving a probabilistic version of the LR parse table from the training corpus, and demonstrates that this leads to a more refined and parse-context—dependent probabilistic model capable of distinguishing derivations that in a probabilistic context-free model would be equally probable.Section 8 describes and presents the results of our first experiment parsing LDOCE noun definitions, and Section 9 draws some preliminary conclusions and outlines ways in which the work described should be modified and extended.In the field of speech recognition, statistical techniques based on hidden Markov modeling are well established (see e.g.Holmes 1988:129f for an introduction).The two main algorithms utilized are the Viterbi (1967) algorithm and the Baum-Welch algorithm (Baum 1972).These algorithms provide polynomial solutions to the tasks of finding the most probable derivation for a given input and a stochastic regular grammar, and of performing iterative re-estimation of the parameters of a (hidden) stochastic regular grammar by considering all possible derivations over a corpus of inputs, respectively.Baker (1982) demonstrates that Baum-Welch re-estimation can be extended to context-free grammars (CFGs) in Chomsky Normal Form (CNF).Fujisaki et al. (1989) demonstrate that the Viterbi algorithm can be used in conjunction with the CYK parsing algorithm and a CFG in CNF to efficiently select the most probable derivation of a given input.Kupiec (1991) extends Baum-Welch re-estimation to arbitrary (nonCNF) CFGs.Baum-Welch re-estimation can be used with restricted or unrestricted grammars/models in the sense that some of the parameters corresponding to possible productions over a given (non-)terminal category set/set of states can be given an initial probability of zero.Unrestricted grammars/models quickly become impractical because the number of parameters requiring estimation becomes large and these algorithms are polynomial in the length of the input and number of free parameters.Typically, in applications of Markov modeling in speech recognition, the derivation used to analyze a given input is not of interest; rather what is sought is the best (most likely) model of the input.In any application of these or similar techniques to parsing, though, the derivation selected is of prime interest.Baum (1972) proves that Baum-Welch re-estimation will converge to a local optimum in the sense that the initial probabilities will be modified to increase the likelihood of the corpus given the grammar and 'stabilize' within some threshold after a number of iterations over the training corpus.However, there is no guarantee that the global optimum will be found, and the a priori initial probabilities chosen are critical for convergence on useful probabilities (e.g.Lan i and Young 1990).The main application of these techniques to written input has been in the robust, lexical tagging of corpora with part-of-speech labels (e.g.Garside, Leech, and Sampson 1987; de Rose 1988; Meteer, Schwartz, and Weischedel 1991; Cutting et al. 1992).Fujisaki et al. (1989) describe a corpus analysis experiment using a probabilistic CNF CFG containing 7550 rules on a corpus of 4206 sentences (with an average sentence length of approximately 11 words).The unsupervised training process involved automatically assigning probabilities to each CF rule on the basis of their frequency of occurrence in all possible analyses of each sentence of the corpus.These probabilities were iteratively re-estimated using a variant of the Baum-Welch algorithm, and the Viterbi algorithm was used in conjunction with the CYK parsing algorithm to efficiently select the most probable analysis after training.Thus the model was restricted in that many of the possible parameters (rules) defined over the (non-)terminal category set were initially set to zero and training was used only to estimate new probabilities for a set of predefined rules.Fujisaki et al. suggest that the stable probabilities will model semantic and pragmatic constraints in the corpus, but this will only be so if these correlate with the frequency of rules in correct analyses, and also if the 'noise' in the training data created by the incorrect parses is effectively factored out.Whether this is so will depend on the number of 'false positive' examples with only incorrect analyses, the degree of heterogeneity in the training corpus, and so forth.Fujisaki et al. report some results based on testing the parser on the corpus used for training.In 72 out of 84 sentences examined, the most probable analysis was also the correct analysis.Of the remainder, 6 were false positives and did not receive a correct parse, while the other 6 did but it was not the most probable.A success rate (per sentence) of 85% is apparently impressive, but it is difficult to evaluate properly in the absence of full details concerning the nature of the corpus.For example, if the corpus contains many simple and similar constructions, unsupervised training is more likely to converge quickly on a useful set of probabilities.Sharman, Jelinek, and Mercer (1990) conducted a similar experiment with a grammar in ID/LP format (Gazdar et al. 1985; Sharman 1989).ID/LP grammars separate the two types of information encoded in CF rules—immediate dominance and immediate precedence—into two rule types that together define a CFG.This allows probabilities concerning dominance, associated with ID rules, to be factored out from those concerning precedence, associated with LP rules.In this experiment, a supervised training regime was employed.A grammar containing 100 terminals and 16 nonterminals and initial probabilities based on the frequency of ID and LP relations was extracted from a manually parsed corpus of about one million words of text.The resulting probabilistic ID/LP grammar was used to parse 42 sentences of 30 words or less drawn from the same corpus.In addition, lexical syntactic probabilities were integrated with the probability of the ID/LP relations to rank parses.Eighteen of the parses were identical to the original manual analyses, while a further 19 were 'similar,' yielding a success rate of 88%.What is noticeable about this experiment is that the results are no better than Fujisaki et al. 's unsupervised training experiment discussed above, despite the use of supervised training and a more sophisticated grammatical model.It is likely that these differences derive from the corpus material used for training and testing, and that the results reported by Fujisaki et al. will not be achieved with all corpora.Pereira and Schabes (1992) report an experiment using Baum-Welch re-estimation to infer a grammar and associated rule probabilities from a category set containing 15 nonterminals and 48 terminals, corresponding to the Penn Treebank lexical tagset (Santorini 1990).The training data was 770 sentences, represented as tag sequences, drawn from the treebank.They trained the system in an unsupervised mode and also in a 'semi-supervised' mode, in which the manually parsed version of the corpus was used to constrain the set of analyses used during re-estimation.In supervised training analyses were accepted if they produced bracketings consistent but not necessarily identical with those assigned manually.They demonstrate that in supervised mode, training not only converges faster but also results in a grammar in which the most probable analysis is compatible with the manually assigned analysis of further test sentences drawn from the tree bank in a much greater percentage of cases-78% as opposed to 35%.This result indicates very clearly the importance of supervised training, particularly in a context where the grammar itself is being inferred in addition to the probability of individual rules.In our work, we are concerned to utilize the existing wide-coverage ANLT grammar; therefore, we have concentrated initially on exploring how an adequate probabilistic model can be derived for a unification-based grammar and trained in a supervised mode to effectively select useful analyses from the large space of syntactically legitimate possibilities.There are several inherent problems with probabilistic CFG (including ID/LP)-based systems.Firstly, although CFG is an adequate model of the majority of constructions occurring in natural language (Gazdar and Mellish 1989), it is clear that wide-coverage CFGs will need to be very large indeed, and this will lead to difficulties of (manual) development of consistent grammars and, possibly, to computational intractability at parse time (particularly during the already computationally expensive training phase).Secondly, associating probabilities with CF rules means that information about the probability of a rule applying at a particular point in a parse derivation is lost.This leads to complications distinguishing the probability of different derivations when the same rule can be applied several times in more than one way.Grammar 1 below is an example of a probabilistic CFG, in which each production is associated with a probability and the probabilities of all rules expanding a given nonterminal category sum to one.Grammar 1 The probability of a particular parse is the product of the probabilities of each rule used in the derivation.Thus the probability of parse a) in Figure 1 is 0.0336.The probability of parse b) or c) must be identical though (0.09), because the same rule is applied twice in each case.Similarly, the probability of d) and e) is also identical (0.09) for essentially the same reason.However, these rules are natural treatments of noun compounding and prepositional phrase (PP) attachment in English, and the different derivations correlate with different interpretations.For example, b) would be an appropriate analysis for toy coffee grinder, while c) would be appropriate for cat food tin, and each of d) and e) yields one of the two possible interpretations of the man in the park with the telescope.We want to keep these structural configurations probabilistically distinct in case there are structurally conditioned differences in their frequency of occurrence; as would be predicted, for example, by the theory of parsing strategies (e.g.Frazier 1988).Fujisaki et al. (1989) propose a rather inelegant solution for the noun compound case, which involves creating 5582 instances of 4 morphosyntactically identical rules for classes of word forms with distinct bracketing behavior in noun—noun compounds.However, we would like to avoid enlarging the grammar and eventually to integrate probabilistic lexical information with probabilistic structural information in a more modular fashion.Probabilistic CFGs also will not model the context dependence of rule use; for example, an NP is more likely to be expanded as a pronoun in subject position than elsewhere (e.g.Magerman and Marcus 1991), but only one global probability can be associated with the relevant CF production.Thus the probabilistic CFG model predicts (incorrectly) that a) and f) will have the same probability of occurrence.These considerations suggest that we need a technique that allows use of a more adequate grammatical formalism than CFG and a more context-dependent probabilistic model.Our approach is to use the LR parsing technique as a natural way to obtain a finitestate representation of a non-finite—state grammar incorporating information about parse context.In the following sections, we introduce the LR parser and in Section 8 Ted Briscoe and John Carroll Generalized Probabilistic LR Parsing we demonstrate that LR parse tables do provide an appropriate amount of contextual information to solve the problems described above.The heart of the LR parsing technique is the parse table construction algorithm, which is the most complex and computationally expensive aspect of LR parsing.Much of the attraction of the technique stems from the fact that the real work takes place in a precompilation phase and the run time behavior of the resulting parser is relatively simple and directed.An LR parser finds the 'rightmost derivation in reverse,' for a given string and CF grammar.The precompilation process results in a parser control mechanism that enables the parser to identify the 'handle,' or appropriate substring in the input to reduce, and the appropriate rule of the grammar with which to perform the reduction.The control information is standardly encoded as a parse table with rows representing parse states, and columns terminal and nonterminal symbols of the grammar.This representation defines a finite-state automaton.Figure 2 gives the LALR(1) parse table for Grammar 1.(LALR(1) is the most commonly used variant of LR since it usually provides the best trade-off between directed rule invocation and parse table size.)If the grammar is in the appropriate LR class (a stronger restriction than being an unambiguous CFG), the automaton will be deterministic; however, some algorithms for parse table construction are also able to build nondeterministic automata containing action conflicts for ambiguous CFGs.Parse table construction is discussed further in Section 4.Tomita (1987) describes a system for nondeterministic LR parsing of context-free grammars consisting of atomic categories, in which each CF production may be augmented with a set of tests (which perform similar types of operations to those available in a unification grammar).At parse time, whenever a sequence of constituents is about to be reduced into a higher-level constituent using a production, the augmentation associated with the production is invoked to check syntactic or semantic constraints such as agreement, pass attribute values between constituents, and construct a representation of the higher-level constituent.(This is the standard approach to parsing with attribute grammars).The parser is driven by an LR parse table; however, the table is constructed solely from the CF portion of the grammar, and so none of the extra information embodied in the augmentations is taken into account during its construction.Thus the predictive power of the parser to select the appropriate rule given a specific parse history is limited to the CF portion of the grammar, which must be defined manually by the grammar writer.This requirement places a greater load on the grammar writer and is inconsistent with most recent unification-based grammar formalisms, which represent grammatical categories entirely as feature bundles (e.g.Gazdar et al. 1985; Pollard and Sag 1987; Zeevat, Calder, and Klein 1987).In addition, it violates the principle that grammatical formalisms should be declarative and defined independently of parsing procedure, since different definitions of the CF portion of the grammar will, at least, effect the efficiency of the resulting parser and might, in principle, lead to nontermination on certain inputs in a manner similar to that described by Shieber (1985).In what follows, we will assume that the unification-based grammars we are considering are represented in the ANLT object grammar formalism (Briscoe et al. 1987).This formalism is a notational variant of Definite Clause Grammar (e.g.Pereira and Warren 1980), in which rules consist of a mother category and one or more daughter categories, defining possible phrase structure configurations.Categories consist of sets of feature name-value pairs, with the possibility of variable values, which may be bound within a rule, and of category-valued features.Categories are combined using fixed-arity term unification (Prolog-style).The results and techniques we report below should generalize to many other unification-based formalisms.An example of a possible ANLT object grammar rule is: This rule provides a (simple) analysis of the structure of English clauses, corresponding to S --+ NP VP, using a feature system based loosely on that of GPSG (Gazdar et al. 1985).In Tomita's LR parsing framework, each such rule must be manually converted into a rule of the following form in which some subpart of each category has been replaced by an atomic symbol.Vb[BAR 2, PER x, PLU y, VFORM z] -4 Nn [BAR 2, PER x, PLU y, CASE Nom] Vb [BAR 1, PER x, PLU y, VFORM z] However, it is not obvious which features should be so replaced—why not include BAR and CASE?It will be difficult for the grammar writer to make such substitutions in a consistent way, and still more difficult to make them in an optimal way for the purposes of LR parsing, since both steps involve consideration and comparison of all the categories mentioned in each rule of the grammar.Ted Briscoe and John Carroll Generalized Probabilistic LR Parsing Constructing the LR parse table directly and automatically from a unification grammar would avoid these drawbacks.In this case, the LR parse table would be based on complex categories, with unification of complex categories taking the place of equality of atomic ones in the standard LR parse table construction algorithm (Osborne 1990; Nakazawa 1991).However, this approach is computationally prohibitively expensive: Osborne (1990:26) reports that his implementation (in HP Common Lisp on a Hewlett Packard 9000/350) takes almost 24 hours to construct the LR(0) states for a unification grammar of just 75 productions.Our approach, described below, not only extracts unification information from complex categories, but is computationally tractable for realistic sized grammars and also safe from inconsistency.We start with a unification grammar and automatically construct a CF 'backbone' of rules containing categories with atomic names and an associated 'residue' of feature name-value pairs.Each backbone grammar rule is generally in direct one-to-one correspondence with a single unification grammar rule.The LR parse table is then constructed from the CF backbone grammar.The parser is driven by this table, but in addition when reducing a sequence of constituents the parser performs the unifications specified in the relevant unification grammar rule to form the category representing the higher-level constituent, and the derivation fails if one of the unifications fails.Our parser is thus similar to Tomita's (1987), except that it performs unifications rather than invoking CF rule augmentations; however, the main difference between our approach and Tomita's is the way in which the CF grammar that drives the parser comes into being.Even though a unification grammar will be, at best, equivalent to a very large (and at worst, if features are employed in recursive or cyclic ways, possibly infinite) set of atomic-category CF productions, in practice we have obtained LR parsers that perform well from backbone grammars containing only about 30% more productions than the original unification grammar.The construction method ensures that for any given grammar the CF backbone captures at least as much information as the optimal CFG that contains the same number of rules as the unification grammar.Thus the construction method guarantees that the resulting LR parser will terminate and will be as predictive as the source grammar in principle allows.Building the backbone grammar is a two-stage process: Backbone grammar corresponding to object grammar.2.For each unification grammar rule, create a backbone grammar rule containing atomic categories, each atomic category being the name assigned to the category in the disjoint category set that unifies with the corresponding category in the unification grammar rule: for each rule R of form Cl -4 C2 Cn in unification grammar add a rule B of form 51 -4 B2 Bn to backbone grammar where Bi is the name assigned to the (single) category in disjoint-set which unifies with Ci, for i=1, n. For example, for the rules in Figure 3 (corresponding loosely to S NP VP, NP Vi and VP --> Vt NP), step 1 would create the disjoint-set shown in Figure 4.(Note that the value for CASE on the NP categories in the grammar has 'collapsed' down to a Backbone parse tree for either kim or lee or sandy using rule N2 --> N2 [CONJ EITHER], N2[CONJ OR] +. variable, but that the two V categories remain distinct).Figure 5 shows the backbone rules that would be built in step 2.Algorithms for creating LR parse tables assume that the terminal vocabulary of the grammar is distinct from the nonterminal one, so the procedure described above will not deal properly with a unification grammar rule whose mother category is assumed elsewhere in the grammar to be a lexical category.The modification we make is to automatically associate two different atomic categories, one terminal and one nonterminal, with such categories, and to augment the backbone grammar with a unary rule expanding the nonterminal category to the terminal.Two other aspects of the ANLT grammar formalism require further minor elaborations to the basic algorithm: firstly, a rule may introduce a gap by including the feature specification [NULL +] on the gapped daughter—for each such daughter an extra rule is added to the backbone grammar expanding the gap category to the null string; secondly, the formalism allows Kleene star and plus operators (Gazdar et al. 1985)— in the ANLT grammar these operators are utilized in rules for coordination.A rule containing Kleene star daughters is treated as two rules: one omitting the daughters concerned and one with the daughters being Kleene plus.A new nonterminal category is created for each distinct Kleene plus category, and two extra rules are added to the backbone grammar to form a right-branching binary tree structure for it; a parser can easily be modified to flatten this out during processing into the intended flat sequence of categories.Figure 6 gives an example of what such a backbone tree looks like.Grammars written in other, more low-level unification grammar formalisms, such as PATR-II (Shieber 1984), commonly employ treatments of the type just described to deal with phenomena such as gapping, coordination, and compounding.However, this method both allows the grammar writer to continue to use the full facilities of the ANLT formalism and allows the algorithmic derivation of an appropriate backbone grammar to support LR parsing.The major task of the backbone grammar is to encode sufficient information (in the atomic categoried CF rules) from the unification grammar to constrain the application of the latter's rules at parse time.The nearly one-to-one mapping of unification grammar rules to backbone grammar rules described above works quite well for the ANLT grammar, with only a couple of exceptions that create spurious shift-reduce conflicts during parsing, resulting in an unacceptable degradation in performance.The phenomena concerned are coordination and unbounded dependency constructions.In the ANLT grammar three very general rules are used to form nominal, adjectival, and prepositional phrases following a conjunction; the categories in these rules lead to otherwise disjoint categories for conjuncts being merged, giving rise to a set of overly general backbone grammar rules.For example, the rule in the ANLT grammar for forming a noun phrase conjunct introduced by a conjunction is N2 [CONJ @con] --> [SUBCAT @con , CONJN +3, H2.The variable value for the CONJ feature in the mother means that all N2 categories specified for this feature (e.g.N2 [CONJ EITHER], N2 [CONJ NULL] ) are generalized to the same category.This results in the backbone rules, when parsing either kim or lee helps, being unable, after forming a N2 [CONJ EITHER] for either kim, to discriminate between the alternatives of preparing to iterate this constituent (as in the phrase kim, lee, or sandy helps where kim would be N2 [CONJ NULL] ), or shifting the next word or to start a new constituent.We solve this problem by declaring CONJ to be a feature that may not have a variable value in an element of the disjoint category set.This directs the system to expand out each unification grammar rule that has a category containing this feature with a variable value into a number of rules fully specified for the feature, and to create backbone rules for each of these.There are eight possible values for CONJ in the grammar, so the general rule for forming a nominal conjunct given above, for example, ends up being represented by a set of eight specialized backbone grammar rules.In the grammar, unbounded dependency constructions (UBCs) are analyzed by propagating the preposed constituent through the parse tree as the value of the SLASH feature, to link it with the 'gap' that appears in the constituent's normal position.All nonlexical major categories contain the feature, rules in the grammar propagating it between mother and a single daughter; other daughters are marked [SLASH [NOSLASH +1] indicating that the daughter is not 'gapped.'Backbone grammar construction would normally lose the information in the unification grammar about where gaps are allowed to occur, significantly degrading the performance of a parser.To carry the information over into the backbone we declare that wherever SLASH occurs with a variable value, the value should be expanded out into two values: [NOSLASH +], and a notional value unifying with anything except [NOSLASH +3.We have also experimented with a smaller grammar employing 'gap threading' (e.g.Pereira and Shieber 1987), an alternative treatment of UBCs.We were able to use the same techniques for expanding out and inference on the values of the (in this case atomic) features used for threading the gaps to produce a backbone grammar (and parse table) that had the same constraining power with respect to gaps as the original grammar.To date, we have not attempted to compute CF backbones for grammars written in formalisms with minimal phrase structure components and (almost) completely general categories, such as HPSG (Pollard and Sag 1987) and UCG (Zeevat, Calder, and Klein 1987); more extensive inference on patterns of possible unification within nested categories and appropriate expanding-out of the categories concerned would be necessary for an LR parser to work effectively.This and other areas of complexity in unification-based formalisms need further investigation before we can claim to have developed a system capable of producing a useful LR parse table for any unificationbased grammar.In particular, declaring certain category-valued features so that they cannot take variable values may lead to nontermination in the backbone construction for some grammars.However, it should be possible to restrict the set of features that are considered in category-valued features in an analogous way to Shieber's (1985) restrictors for Earley's (1970) algorithm, so that a parse table can still be constructed.Ted Briscoe and John Carroll Generalized Probabilistic LR Parsing The backbone grammar generated from the ANLT grammar is large: it contains almost 500 distinct categories and more than 1600 productions.When we construct the LALR(1) parse table, we therefore require an algorithm with practical time and space requirements.In the LR parsing literature there are essentially two approaches to constructing LALR(1) parse tables.One approach is graph-based (DeRemer and Pennello 1982), transforming the parse table construction problem to a set of wellknown directed graph problems, which in turn are solvable by efficient algorithms.Unfortunately this approach does not work for grammars that are not LR(k) for any k (DeRemer and Pennello 1982:633), for example, ambiguous grammars.We therefore broadly follow the alternative approach of Aho, Sethi, and Ullman (1986), but with a number of optimizations: pairs), and 670,000 reduce actions (terminal—rule-number pairs); however, of the goto entries only 2,600 are distinct and of the shift actions only 1,100 are distinct; most states contain just reduce or just shift actions, and in any one state very few different rules are involved in reduce actions.'The majority of states contain just reduce or just shift actions, and in any one state very few different rules are involved in reduce actions.Taking advantage of the characteristics of this distribution, in each state we represent (in Common Lisp) For the grammars we have investigated, this representation achieves a similar order of space saving to the comb vector representation suggested by Aho, Sethi, and Ullman (1986:244ff) for unambiguous grammars (see Klein and Martin [19891 for a survey of representation techniques).The parse table for the ANLT grammar occupies approximately 360 Kbytes of memory, and so represents each action (shift, reduce, or goto) in an average of less than 2.3 bits.In contrast to conventional techniques, though, we maintain a faithful representation of the parse table, not replacing error entries with more convenient nonerror ones in order to save extra space.Our parsers are thus able to detect failures as soon as theoretically possible, an important efficiency feature when parsing nondeterministically with ambiguous grammars, and a time-saving feature when parsing interactively with them (see next section).Table 1 compares the size of the LALR(1) parse table for the ANLT grammar with others reported in the literature.From these figures, the ANLT grammar is more than twice the size of Tomita's (combined morphological and syntactic) grammar for Japanese (Tomita 1987:45).The grammar itself is about one order of magnitude bigger than that of a typical programming language, but the LALR(1) parse table, in terms of number of actions, is two orders of magnitude bigger.Although Tomita (1984:357) anticipates LR parsing techniques being applied to large NL grammars written in formalisms such as GPSG, the sizes of parse tables for such grammars grow more rapidly than he predicts.However, for large real-world NL grammars such as the ANLT, the table size is still quite manageable despite Johnson's (1989) worst-case complexity result of the number of LR(0) states being exponential on grammar size (leading to a parser with exponentially bad time performance).We have, therefore, not found it necessary to use Schabes' (1991a) LR-like tables (with number of states guaranteed to be polynomial even in the worst case).As might be expected, and Table 2 illustrates, parse table construction for large grammars is CPU-intensive.As a rough guide, Grosch (1990) quotes LALR(1) table construction for a grammar for Modula-2 taking from about 5 to 50 seconds, so scaling up two orders of magnitude, our timings for the ANLT grammar fall in the expected region.The major problem with attempting to employ a disambiguated training corpus is to find a way of constructing this corpus in an error-free and resource-efficient fashion.Even manual assignment of lexical categories is slow, labor-intensive, and error-prone.The greater complexity of constructing a complete parse makes the totally manual approach very unattractive, if not impractical.Sampson (1987:83) reports that it took 2 person-years to produce the 'LOB tree bank' of 50,000 words.Furthermore, in that project, no attempt was made to ensure that the analyses were well formed with respect to a generative grammar.Attempting to manually construct analyses consistent with a grammar of any size and sophistication would place an enormous additional load on the analyst.Leech and Garside (1991) discuss the problems that arise in manual parsing of corpora concerning accuracy and consistency of analyses across time and analyst, the labor-intensive nature of producing detailed analyses, and so forth.They advocate an approach in which simple 'skeleton' parses are produced by hand from previously tagged material, with checking for consistency between analysts.These skeleton analyses can then be augmented automatically with further information implicit in the lexical tags.While this approach may well be the best that can be achieved with fully manual techniques, it is still unsatisfactory in several respects.Firstly, the analyses are crude, while we would like to automatically parse with a grammar capable of assigning sophisticated semantically interpretable ones; but it is not clear how to train an existing grammar with such unrelated analyses.Secondly, the quality of any grammar obtained automatically from the parsed corpus is likely to be poor because of the lack of any rigorous checks on the form of the skeleton parses.Such a grammar might, in principle, be trained from the parsed corpus, but there are still likely to be small mismatches between the actual analysis assigned manually and any assigned automatically.For these reasons, we decided to attempt to produce a training corpus using the grammar that we wished ultimately to train.As long as the method employed ensured that any analysis assigned was a member of the set defined by the grammar, these problems during training should not arise.Following our experience of constructing a substantial lexicon for the ANLT grammar from unreliable and indeterminate data (Carroll and Grover 1989), we decided to construct the disambiguated training corpus semi-automatically, restricting manual interaction to selection between alternatives defined by the ANLT grammar.One obvious technique would be to generate all possible parses with a conventional parser and to have the analyst select the correct parse from the set returned (or reject them all).However, this approach places a great load on the analyst, who will routinely need to examine large numbers of parses for given sentences.In addition, computation of all possible analyses is likely to be expensive and, in the limit, intractable.Briscoe (1987) demonstrates that the structure of the search space in parse derivations makes a left-to-right, incremental mode of parse selection most efficient.For example, in noun compounds analyzed using a recursive binary-branching rule (N N N) the number of analyses correlates with the Catalan series (Church and Patil, 1982), so a 3-word compound has 2 analyses, 4 has 5, 5 has 14, 9 has 1430, and so forth.However, Briscoe (1987:154f) shows that with a simple bounded context parser (with one word lookahead) set up to request help whenever a parse indeterminacy arises, it is possible to select any of the 14 analyses of a 5-word compound with a maximum of 5 interactions and any of the 1430 analyses of a 9-word compound with around 13 interactions.In general, resolution of the first indeterminacy in the input will rule out approximately half the potential analyses, resolution of the next, half of the remaining ones, and so on.For 'worst case' CF ambiguities (with 0(n3) complexity) this approach to parse selection appears empirically to involve numbers of interactions that increase at little more than linear rate with respect to the length of the input.It is possible to exploit this insight in two ways.One method would be to compute all possible analyses represented as a (packed) parse forest and ask the user to select between competing subanalyses that have been incorporated into a successful analysis of the input.In this way, only genuine global syntactic ambiguities would need to be considered by the user.However, the disadvantage of this approach is that it relies on a prior (and perhaps CPU-intensive) on-line computation of the full set of analyses.The second method involves incremental interaction with the parser during the parse to guide it through the search space of possibilities.This has the advantage of being guaranteed to be computationally tractable but the potential disadvantage of requiring the user to resolve many local syntactic ambiguities that will not be incorporated into a successful analysis.Nevertheless, using LR techniques this problem can be minimized and, because we do not wish to develop a system that must be able to compute all possible analyses (at some stage) in order to return the most plausible one, we have chosen the latter incremental method.The interactive incremental parsing system that we implemented asks the user for a decision at each choice point during the parse.However, to be usable in practice, such a system must avoid, as far as possible, presenting the user with spurious choices that could be ruled out either by using more of the left context or by looking at words yet to be parsed.Our approach goes some way to addressing these points, since the parser is as predictive as the backbone grammar and LR technique allow, and the LALR(1) parse table allows one word lookahead to resolve some ambiguities (although, of course, the resolution of a local ambiguity may potentially involve an unlimited amount of lookahead; e.g.Briscoe 1987:125ff).In fact, LR parsing is the most effectively predictive parsing technique for which an automatic compilation procedure is known, but this is somewhat undermined by our use of features, which will block some derivations so that the valid prefix property will no longer hold (e.g.Schabes 1991b).Extensions to the LR technique, for example those using LR-regular grammars (Culic and Cohen 1973; Bermudez 1991), might be used to further cut down on interactions; however, computation of the parse tables to drive such extended LR parsers may prove intractable for large NL grammars (Hektoen 1991).An LR parser faces an indeterminacy when it enters a state in which there is more than one possible action, given the current lookahead.In a particular state there cannot be more than one shift or accept action, but there can be several reduce actions, each specifying a reduction with a different rule.When parsing, each shift or reduce choice must lead to a different final structure, and so the indeterminacy represents a point of syntactic ambiguity (although it may not correspond to a genuinely global syntactic ambiguity in the input, on account of the limited amount of lookahead).In the ANLT grammar and lexicon, lexical ambiguity is at least as pervasive as structural ambiguity.A naive implementation of an interactive LR parser would ask the user the correct category for each ambiguous word as it was shifted; many open-class words are assigned upwards of twenty lexical categories by the ANLT lexicon with comparatively fine distinctions between them, so this strategy would be completely impracticable.To avoid asking the user about lexical ambiguity, we use the technique of preterminal delaying (Shieber 1983), in which the assignment of an atomic preterminal category to a lexical item is not made until the choice is forced by the use of a particular production in a later reduce action.After shifting an ambiguous lexical item, the parser enters a state corresponding to the union of states that would be entered on shifting the individual lexical categories.(Each union of states will in practice be small, since it being otherwise would imply that the current context was completely failing to constrain the following input).Since, in general, several unification grammar categories for a single word may be subsumed by a single atomic preterminal category, we extend Shieber's technique so that it deals with a grammar containing complex categories by associating a set of alternative analyses with each state (not just one), and letting the choice between them be forced by later reduce actions, just as with atomic preterminal categories.In order not to overload the user with spurious choices concerning local ambiguities, the parser does not request help immediately after it reaches a parse action conflict.Instead the parser pursues each option in a limited breadth-first fashion and only requests help with analysis paths that remain active.In our current system this type of lookahead is limited to up to four indeterminacies ahead.Such checking is cheap in terms of machine resources and very effective in cutting down both the number of choice points the user is forced to consider and also the average number of options in each one.Table 3 shows the reduction in user interaction achieved by increasing the amount of lookahead in our system.Computation of the backbone grammar generates extra rules (as previously described to deal with lexical categories used as rule mothers and daughters specified to be repeatable an indefinite number of times) that do not correspond directly to single unification grammar rules.At choice points, reductions involving these rules are not presented to the user; instead the system applies the reductions automatically, proceeding until the next shift action or choice point is reached, including these options in those presented to the user.The final set of measures taken to reduce the amount of interaction required with the user is to ask if the phrase being parsed contains one or more gaps or instances of coordination before presenting choices involving either of these phenomena, blocking consideration of rules on the basis of the presence of particular feature-value pairs.Figure 7 shows the system parsing a phrase with a four-choice lookahead.The resulting parse tree is displayed with category aliases substituted for the actual complex categories.The requests for manual selection of the analysis path are displayed to the analyst in as terse a manner as possible, and require knowledge of the ANLT grammar and lexicon to be resolved effectively.Figure 8 summarizes the amount of interaction required in the experiment reported below for parsing a set of 150 LDOCE noun definitions with the ANLT grammar.To date, the largest number of interactions we have observed for a single phrase is 55 for the (30-word) LDOCE definition for youth hostel: Achieving the correct analysis interactively took the first author about 40 minutes (including the addition of two lexical entries).Definitions of this length will often have many hundreds or even thousands of parses; computing just the parse forest for this definition takes of the order of two hours of CPU time (on a DEC 3100 Unix workstation).Since in a more general corpus of written material the average sentence length is likely to be 30-40 words, this example illustrates clearly the problems with any approach based on post hoc on-line selection of the correct parse.However, using Numbers of definitions requiring particular amounts of interaction. the incremental approach to semi-automatic parsing we have been able to demonstrate that the correct analysis is among this set.Furthermore, a probabilistic parser such as the one described later may well be able to compute this analysis in a tractable fashion by extracting it from the parse forest.(To date, the largest example for which we have been able to compute all analyses had approximately 2500).The parse histories resulting from semi-automatic parsing are automatically stored and can be used to derive the probabilistic information that will guide the parser after training.We return to a discussion of the manner in which this information is utilized in Section 7.As well as building an interactive parsing system incorporating the ANLT grammar (described above), we have implemented a breadth-first, nondeterministic LR parser for unification grammars.This parser is integrated with the Grammar Development Environment (GDE; Carroll et al. 1988) in the ANLT system, and provided as an alternative parser for use with stable grammars for batch parsing of large bodies of text.The existing chart parser, although slower, has been retained since it is more suited to grammar development, because of the speed with which modifications to the grammar can be compiled and its better debugging facilities (Boguraev et al. 1988).Our nondeterministic LR parser is based on Kipps' (1989) reformulation of Tomita's (1987) parsing algorithm and uses a graph-structured stack in the same way.Our parser is driven by the LALR(1) state table computed from the backbone grammar, but in addition on each reduction the parser performs the unifications appropriate to the unification grammar version of the backbone rule involved.The analysis being pursued fails if one of the unifications fails.The parser performs sub-analysis sharing (where if two or more trees have a common sub-analysis, that sub-analysis is represented only once), and local ambiguity packing (in which sub-analyses that have the same top node and cover the same input have their top nodes merged, being treated by higher level structures as a single sub-analysis).However, we generalize the technique of atomic category packing described by Tomita, driven by atomic category names, to complex feature-based categories following Alshawi (1992): the packing of sub-analyses is driven by the subsumption relationship between the feature values in their top nodes.An analysis is only packed into one that has already been found if its top node is subsumed by, or is equal to that of the one already found.An analysis, once packed, will thus never need to be unpacked during parsing (as in Tomita's system) since the value of each feature will always be uniquely determined.Our use of local ambiguity packing does not in practice seem to result in exponentially bad performance with respect to sentence length (cf.Johnson 1989) since we have been able to generate packed parse forests for sentences of over 30 words having many thousands of parses.We have implemented a unification version of Schabes' (1991a) chart-based LR-like parser (which is polynomial in sentence length for CF grammars), but experiments with the ANLT grammar suggest that it offers no practical advantages over our Tomita-style parser, and Schabes' table construction algorithm yields less fine-grained and, therefore, less predictive parse tables.Nevertheless, searching the parse forest exhaustively to recover each distinct analysis proved computationally intractable for sentences over about 22 words in length.Wright, Wrigley, and Sharman (1991) describe a Viterbi-like algorithm for unpacking parse forests containing probabilities of (sub-)analyses to find the n-best analyses, but this approach does not generalize (except in a heuristic way) to our approach in which unification failure on the different extensions of packed nodes (resulting from differing super- or subanalyses) cannot be computed 'locally.'In subsequent work (Carroll and Briscoe 1992) we have developed such a heuristic technique for best-first search of the parse forest which, in practice, makes the recovery of the most probable analyses much more efficient (allowing analysis of sentences containing over 30 words).We noticed during preliminary experiments with our unification LR parser that it was often the case that the same unifications were being performed repeatedly, even during the course of a single reduce action.The duplication was happening in cases where two or more pairs of states in the graph-structured stack had identical complex categories between them (for example due to backbone grammar ambiguity).During a reduction with a given rule, the categories between each pair of states in a backwards traversal of the stack are collected and unified with the appropriate daughters of the rule.Identical categories appearing here between traversed pairs of states leads to duplication of unifications.By caching unification results we eliminated this wasted effort and improved the initially poor performance of the parser by a factor of about three.As for actual parse times, Table 4 compares those for the GDE chart parser, the semi-automatic, user-directed LR parser, and the nondeterministic LR parser.Our general experience is that although the nondeterministic LR parser is only around 30-50% faster than the chart parser, it often generates as little as a third the amount of garbage.(The relatively modest speed advantage compared with the substantial space saving appears to be due to the larger overheads involved in LR parsing).Efficient use of space is obviously an important factor for practical parsing of long and ambiguous texts.7.LR Parsing with Probabilistic Disambiguation Several researchers (Wright and Wrigley 1989; Wright 1990; Ng and Tomita 1991; Wright, Wrigley, and Sharman 1991) have proposed using LR parsers as a practical method of parsing with a probabilistic context-free grammar.This approach assumes that probabilities are already associated with a CFG and describes techniques for distributing those probabilities around the LR parse table in such a way that a probabilistic ranking of alternative analyses can be computed quickly at parse time, and probabilities assigned to analyses will be identical to those defined by the original probabilistic CFG.However, our method of constructing the training corpus allows us to associate probabilities with an LR parse table directly, rather than simply with rules of the grammar.An LR parse state encodes information about the left and right context of the current parse.Deriving probabilities relative to the parse context will allow the probabilistic parser to distinguish situations in which identical rules reapply in different ways across different derivations or apply with differing probabilities in different contexts.Semi-automatic parsing of the training corpus yields a set of LR parse histories that are used to construct the probabilistic version of the LALR(1) parse table.The parse table is a nondeterministic finite-state automaton so it is possible to apply Markov modeling techniques to the parse table (in a way analogous to their application to lexical tagging or CFGs).Each row of the parse table corresponds to the possible transitions out of the state represented by that row, and each transition is associated with a particular lookahead item and a parse action.Nondeterminism arises when more than one action, and hence transition, is possible given a particular lookahead item.The most straightforward technique for associating probabilities with the parse table is to assign a probability to each action in the action part of the table (e.g.Wright 1990).5 If probabilities are associated directly with the parse table rather than derived from a probabilistic CFG or equivalent global pairing of probabilities to rules, then the resulting probabilistic model will be more sensitive to parse context.For example, in a derivation for the sentence he loves her using Grammar 1, the distinction between reducing the first pronoun and second pronoun to NP—using rule 5 (NP --> ProNP)— can be maintained in terms of the different lookahead items paired with the reduce actions relating to this rule (in state 5 of the parse table in Figure 2); in the first case, the lookahead item will be Vi, and in the second $ (the end of sentence marker).However, this approach does not make maximal use of the context encoded into a transition in the parse table, and it is possible to devise situations in which the reduction of a pronoun in subject position and elsewhere would be indistinguishable in terms of lookahead alone; for example, if we added appropriate rules for adverbs to Grammar 1, then this reduction would be possible with lookahead Adv in sentences such as he passionately loves her and he loves her passionately.A slightly less obvious approach is to further subdivide reduce actions according to the state reached after the reduce action has applied.This state is used together with the resultant nonterminal to define the state transition in the goto part of the parse table.Thus, this move corresponds to associating probabilities with transitions in the automaton rather than with actions in the action part of the table.For example, a reduction of pronoun to NP in subject position in the parse table for Grammar 1 in Figure 2 always results in the parser returning to state 0 (from which the goto table deterministically prescribes a transition to state 7 with nonterminal NP).Reduction to NP of a pronoun in object position always results in the parser returning to state 11.Thus training on a corpus with more subject than nonsubject pronominal NPs will now result in a probabilistic preference for reductions that return to 'pre-subject' states with 'post-subject' lookaheads.Of course, this does not mean that it will be impossible to devise grammars in which reductions cannot be kept distinct that might, in principle, have different frequencies of occurrence.However, this approach appears to be the natural stochastic, probabilistic model that emerges when using a LALR(1) table.Any further sensitivity to context would require sensitivity to patterns in larger sections of a parse derivation than can be defined in terms of such a table.The probabilities required to create the probabilistic version of the parse table can be derived from the set of parse histories resulting from the training phase described in Section 5, by computing the frequency with which each transition from a particular state has been taken and converting these to probabilities such that the probabilities A probabilistic version of the parse table for Grammar 1. assigned to each transition from a given state sum to one.In Figure 9 we show a probabilistic LALR(1) parse table for Grammar 1 derived from a simple, partial (and artificial) training phase.In this version of the table a probability is associated with each shift action in the standard way, but separate probabilities are associated with reduce Parse derivations for the winter holiday camp closed. actions, depending on the state reached after the action; for example, in state 4 with lookahead N@ the probability of reducing with rule 10 is 0.17 if the state reached is 3 and 0.22 if the state reached is 5.The actions that have no associated probabilities are ones that have not been utilized during the training phase; each is assigned a smoothed probability that is the reciprocal of the result of adding one to the total number of observations of actions actually taken in that state.Differential probabilities are thus assigned to unseen events in a manner analogous to the Good-Turing technique.For this reason, the explicit probabilities for each row add up to less than one.The goto part of the table is not shown because it is always deterministic and, therefore, we do not associate probabilities with goto transitions.The difference between our approach and one based on probabilistic CFG can be brought out by considering various probabilistic derivations using the probabilistic parse table for Grammar 1.Assuming that we are using probabilities simply to rank parses, we can compute the total probability of an analysis by multiplying together the probabilities of each transition we take during its derivation.In Figure 10, we give the two possible complete derivations for a sentence such as the winter holiday camp closed consisting of a determiner, three nouns, and an intransitive verb.The ambiguity concerns whether the noun compound is left- or right-branching, and, as we saw in Section 2, a probabilistic CFG cannot distinguish these two derivations.The probability of each step can be read off the action table and is shown after the lookahead item in the figure.In step 8 a shift-reduce conflict occurs so the stack 'splits' while the left- and rightbranching analyses of the noun compound are constructed.The a) branch corresponds Ted Briscoe and John Carroll Generalized Probabilistic LR Parsing to the right-branching derivation and the product of the probabilities is 4.6 x 10-8, while the product for the left-branching b) derivation is 5.1 x 10-7.Since the table was constructed from parse histories with a preponderance of left-branching structures this is the desired result.In practice, this technique is able to distinguish and train accurately on 3 of the 5 possible structures for a 4-word noun-noun compound; but it inaccurately prefers a completely left-branching analysis over structures of the form ((n n)(n n)) and ((n (nn)) n).Once we move to 5-word noun-noun compounds, performance degrades further.However, this level of performance on such structural configurations is likely to be adequate, because correct resolution of most ambiguity in such constructions is likely to be dominated by the actual lexical items that occur in individual texts.Nevertheless, if there are systematic structural tendencies evident in corpora (for example, Frazier's [19881 parsing strategies predict a preference for left-branching analyses of such compounds), then the probabilistic model is sensitive enough to discriminate them.'In practice, we take the geometric mean of the probabilities rather than their product to rank parse derivations.Otherwise, it would be difficult to prevent the system from always developing a bias in favor of analyses involving fewer rules or equivalently 'smaller' trees, almost regardless of the training material.Of course, the need for this step reflects the fact that, although the model is more context-dependent than probabilistic CFG, it is by no means a perfect probabilistic model of NL.7 For example, the stochastic nature of the model and the fact that the entire left context of a parse derivation is not encoded in LR state information means that the probabilistic model cannot take account of, say, the pattern of resolution of earlier conflicts in the current derivation.Another respect in which the model is approximate is that we are associating probabilities with the context-free backbone of the unification grammar.Successful unification of features at parse time does not affect the probability of a (partial) analysis, while unification failure, in effect, sets the probability of any such analysis to zero.As long as we only use the probabilistic model to rank successful analyses, this is not particularly problematic.However, parser control regimes that attempt some form of best-first search using probabilistic information associated with transitions might not yield the desired result given this property.For example, it is not possible to use Viterbi-style optimization of search for the maximally probable parse because this derivation may contain a sub-analysis that will be pruned locally before a subsequent unification failure renders the current most probable analysis impossible.In general, the current breadth-first probabilistic parser is more efficient than its nonprobabilistic counterpart described in the previous section.In contrast to the parser described by Ng and Tomita (1991), our probabilistic parser is able to merge (state and stack) configurations and in all cases still maintain a full record of all the probabilities computed up to that point, since it associates probabilities with partial analyses of the input so far rather than with nodes in the graph-structured stack.We are currently experimenting with techniques for probabilistically unpacking the packed parse forest to recover the first few most probable derivations without the need for exhaustive search or full expansion.In order to test the techniques and ideas described in previous sections, we undertook a preliminary experiment using a subset of LDOCE noun definitions as our test corpus.(The reasons for choosing this corpus are discussed in the introduction.)A corpus of approximately 32,000 noun definitions was created from LDOCE by extracting the definition fields and normalizing the definitions to remove punctuation, font control information, and so forth.'A lexicon was created for this corpus by extracting the appropriate lemmas and matching these against entries in the ANLT lexicon.The 10,600 resultant entries were loaded into the ANLT morphological system (Ritchie et al. 1987) and this sublexicon and the full ANLT grammar formed the starting point for the training process.A total of 246 definitions, selected without regard for their syntactic form, were parsed semi-automatically using the parser described in Section 5.During this process, further rules and lexical entries were created for some definitions that failed to parse.Of the total number, 150 were successfully parsed and 63 lexical entries and 14 rules were added.Some of the rules required reflected general inadequacies in the ANLT grammar; for example, we added rules to deal with new partitives and prepositional phrase and verb complementation.However, 7 of these rules cover relatively idiosyncratic properties of the definition sublanguage; for example, the postmodification of pronouns by relative clause and prepositional phrase in definitions beginning something that..., that of..., parenthetical phrases headed by adverbs, such as the period... esp the period, and coordinations without explicit conjunctions ending with etc., and so forth.Further special rules will be required to deal with brackets in definitions to cover conventions such as a man (monk) or woman (nun) who lives in a monastery, which we ignored for this test.Nevertheless, the number of new rules required is not great and the need for most was identified very early in the training process.Lexical entries are more problematic, since there is little sign that the number of new entries required will tail off.However, many of the entries required reflect systematic inadequacies in the ANLT lexicon rather than idiosyncrasies of the corpus.It took approximately one person-month to produce this training corpus.As a rough guide, it takes an average of 15 seconds to resolve a single interaction with the parser.However, the time a parse takes can often be lengthened by incorrect choices (and the consequent need to back up manually) and by the process of adding lexical entries and occasional rules.The resultant parse histories were used to construct the probabilistic parser (as described in the previous section).This parser was then used to reparse the training corpus, and the most highly ranked analyses were automatically compared with the original parse histories.We have been able to reparse in a breadth-first fashion all but 3 of the 150 definitions that were parsed manually.'(These three are each over 8 The corpus contains about 17,000 unique headwords and 13,500 distinct word forms in the definitions.Its perplexity (PP) measures based on bigram and trigram word models and an estimate of an infinite model were PP(2) = 104, PP(3) = 41, and PP(inf) = 8 (Sharman 1991).25 words in length.)There are 22 definitions one word in length: all of these trivially receive correct analyses.There are 89 definitions between two and ten words in length inclusive (mean length 6.2).Of these, in 68 cases the correct analysis (as defined by the training corpus) is also the most highly ranked.In 13 of the 21 remaining cases the correct analysis is the second or third most highly ranked analysis.Looking at these 21 cases in more detail, in 8 there is an inappropriate structural preference for 'low' or 'local' attachment (see Kimball 1973), in 4, an inappropriate preference for compounds, and in 6 of the remaining 9 cases, the highest ranked result contains a misanalysis of a single constituent two or three words in length.If these results are interpreted in terms of a goodness of fit measure such as that of Sampson, Haigh, and Atwell (1989), the measure would be better than 96%.If we take correct parse/sentence as our measure then the result is 76%.For definitions longer than 10 words this latter figure tails off, mainly due to misapplication of such statistically induced, but nevertheless structural, attachment preferences.Figure 11 summarizes these results.We also parsed a further 55 LDOCE noun definitions not drawn from the training corpus, each containing up to 10 words (mean length 5.7).Of these, in 41 cases the correct parse is the most highly ranked, in 6 cases it is the second or third most highly ranked, and in the remaining 8 cases it is not in the first three analyses.This yields a correct parse/sentence measure of 75%.Examination of the failures again reveals that a preference for local attachment of postmodifiers accounts for 5 cases, a preference for compounds for 1, and the misanalysis of a single constituent for 2.The others are mostly caused by the lack of lexical entries with appropriate SUBCAT features.In Figure 12 we show the analysis for the unseen definition of affectation, which has 20 parses of which the most highly ranked is correct.Parse tree for a person or thing that supports or helps.Figure 13 shows the highest-ranked analysis assigned to one definition of aid.This is an example of a false positive which, in this case, is caused by the lack of a lexical entry for support as an intransitive verb.Consequently, the parser finds, and ranks highest, an analysis in which supports and helps are treated as transitive verbs forming verb phrases with object NP gaps, and that supports or helps as a zero relative clause with that analyzed as a prenominal subject—compare a person or thing that that supports or helps.It is difficult to fault this analysis and the same is true for the other false positives we have looked at.Such false positives present the biggest challenge to the type of system we are attempting to develop.One hopeful sign is that the analyses assigned such examples appear to have low probabilities relative to most probable correct analyses of other examples.However, considerably more data will be required before we can decide whether this trend is robust enough to provide the basis for automatic identification of false positives.Using a manually disambiguated training corpus and manually tuned grammar appears feasible with the definitions sublanguage.Results comparable to those obtained by Fujisaki et al. (1989) and Sharman, Jelinek, and Mercer (1990) are possible on the basis of a quite modest amount of manual effort and a very much smaller training corpus, because the parse histories contain little 'noise' and usefully reflect the semantically and pragmatically appropriate analysis in the training corpus, and because the number of failures of coverage were reduced to some extent by adding the rules specifically motivated by the training corpus.Unlike Fujisalci et al. or Sharman, Jelinek, and Mercer, we did not integrate information about lexemes into the rule probabilities or make use of lexical syntactic probability.It seems likely that the structural preference for local attachment might be overruled in appropriate contexts if lexeme (or better, word sense) information were taken into account.The slightly worse results (relative to mean definition length) obtained for the unseen data appear to be caused more by the nonexistence of a correct analysis in a number of cases, rather than by a marked decline in the usefulness of the rule probabilities.This again highlights the need to deal effectively with examples outside the coverage of the grammar.The system that we have developed offers partial and practical solutions to two of the three problems of corpus analysis we identified in the introduction.The problem of tuning an existing grammar to a particular corpus or sublanguage is addressed partly by manual extensions to the grammar and lexicon during the semi-automatic training phase and partly by use of statistical information regarding frequency of rule use gathered during this phase.The results of the experiment reported in the last section suggest that syntactic peculiarities of a sublanguage or corpus surface quite rapidly, so that manual additions to the grammar during the training phase are practical.However, lexical idiosyncrasies are far less likely to be exhausted during the training phase, suggesting that it will be necessary to develop an automatic method of dealing with them.In addition, the current system does not take account of differing frequencies of occurrence of lexical entries; for example, in the LOB corpus the verb believe occurs with a finite sentential complement in 90% of citations, although it is grammatical with at least five further patterns of complementation.This type of lexical information, which will very likely vary between sublanguages, should be integrated into the probabilistic model.This will be straightforward in terms of the model, since it merely involves associating probabilities with each distinct lexical entry for a lexeme and carrying these forward in the computation of the likelihood of each parse.However, the acquisition of the statistical information from which these probabilities can be derived is more problematic.Existing lexical taggers are unable to assign tags that reliably encode subcategorization information.It seems likely that automatic acquisition of such information must await successful techniques for robust parsing of, at least, phrases in corpora (though Brent [1991] claims to be able to recognize some subcategorization patterns using large quantities of untagged text).The task of selecting the correct analysis from the set licensed by the grammar is also partially solved by the system.It is clear from the results of the preliminary experiment reported in the previous section that it is possible to make the semantically and pragmatically correct analysis highly ranked, and even most highly ranked in many cases, just by exploiting the frequency of occurrence of the syntactic rules in the training data.However, it is also clear that this approach will not succeed in all cases; for example, in the experiment the system appears to have developed a preference for local attachment of prepositional phrases (PPs), which is inappropriate in a significant number of cases.It is not surprising that probabilities based solely on the frequency of syntactic rules are not capable of resolving this type of ambiguity; in an example such as John saw the man on Monday again it is the temporal interpretation of Monday that favors the adverbial interpretation (and thus nonlocal attachment).Such examples are syntactically identical to ones such as John saw the man on the bus again, in which the possibility of a locative interpretation creates a mild preference for the adjectival reading and local attachment.To select the correct analysis in such cases it will be necessary to integrate information concerning word sense collocations into the probabilistic analysis.In this case, we are interested in collocations between the head of a PP complement, a preposition and the head of the phrase being postmodified.In general, these words will not be adjacent in the text, so it will not be possible to use existing approaches unmodified (e.g.Church and Hanks 1989), because these apply to adjacent words in unanalyzed text.Hindle and Rooth (1991) report good results using a mutual information measure of collocation applied within such a structurally defined context, and their approach should carry over to our framework straightforwardly.One way of integrating 'structural' collocational information into the system presented above would be to make use of the semantic component of the (ANLT) grammar.This component pairs logical forms with each distinct syntactic analysis that represent, among other things, the predicate-argument structure of the input.In the resolution of PP attachment and similar ambiguities, it is 'collocation' at this level of representation that appears to be most relevant.Integrating a probabilistic ranking of the resultant logical forms with the probabilistic ranking of the distinct syntactic analyses presents no problems, in principle.However, once again, the acquisition of the relevant statistical information will be difficult, because it will require considerable quantities of analyzed text as training material.One way to ameliorate the problem might be to reduce the size of the 'vocabulary' for which statistics need to be gathered by replacing lexical items with their superordinate terms (or a disjunction of such terms in the case of ambiguity).Copestake (1990, 1992) describes a program capable of extracting the genus term of a definition from an LDOCE definition, resolving the sense of such terms, and constructing hierarchical taxonomies of the resulting word senses.Taxonomies of this form might be used to replace PP complement heads and postmodified heads in corpus data with a smaller number of superordinate concepts.This would make the statistical data concerning trigrams of head—preposition—head less sparse (cf.Gale and Church 1990) and easier to gather from a corpus.Nevertheless, it will only be possible to gather such data from determinately syntactically analyzed material.The third problem of dealing usefully with examples outside the coverage of the grammar even after training is not addressed by the system we have developed.Nevertheless, the results of the preliminary experiment for unseen examples indicate that it is a significant problem, at least with respect to lexical entries.A large part of the problem with such examples is identifying them automatically.Some such examples will not receive any parse and will, therefore, be easy to spot.Many, though, will receive incorrect parses (one of which will be automatically ranked as the most probable) and can, therefore, only be identified manually (or perhaps on the basis of relative improbability).Jensen et al. (1983) describe an approach to parsing such examples based on parse 'fitting' or rule 'relaxation' to deal with 'ill-formed' input.An approach of this type might work with input that receives no parse, but cannot help with the identification of those that only receive an incorrect one.In addition, it involves annotating each grammar rule about what should be relaxed and requires that semantic interpretation can be extended to 'fitted' or partial parses (e.g.Pollack and Pereira 1988).Sampson, Haigh, and Atwell (1989) propose a more thorough-going probabilistic approach in which the parser uses a statistically defined measure of 'closest fit' to the set of analyses contained in a 'tree bank' of training data to assign an analysis.This approach attempts to ensure that analyses of new data will conform as closely as possible to existing ones, but does not require that analyses assigned are well formed with respect to any given generative grammar implicit in the tree bank analyses.Sampson, Haigh, and Atwell report some preliminary results for a parser of this type that uses the technique of simulated annealing to assign the closest fitting analysis on the basis of initial training on the LOB treebank and automatic updating of its statistical data on the basis of further parsed examples.Sampson, Haigh, and Atwell give their results in terms of a similarity measure with respect to correct analyses assigned by hand.For a 13-sentence sample the mean similarity measure was 80%, and only one example received a fully correct analysis.These results suggest that the technique is not reliable enough for practical corpus analysis, to date.In addition, the analyses assigned, on the basis of the LOB treebank scheme, are not syntactically determinate (for example, syntactic relations in unbounded dependency constructions are not represented).A more promising approach with similar potential robustness would be to infer a probabilistic grammar using Baum-Welch re-estimation from a given training corpus and predefined category set, following Lan i and Young (1990) and Pereira and Schabes (1992).This approach has the advantage that the resulting grammar defines a well-defined set of analyses for which rules of compositional interpretation might be developed.However, the technique is limited in several ways; firstly, such grammars are restricted to small (maximum about 15 nonterminal) CNF CFGs because of the computational cost of iterative re-estimation with an algorithm polynomial in sentence length and nonterminal category size; and secondly, because some form of supervised training will be essential if the analyses assigned by the grammar are to be linguistically motivated.Immediate prospects for applying such techniques to realistic NL grammars do not seem promising—the ANLT backbone grammar discussed in Section 4 contains almost 500 categories.However, Briscoe and Waegner (1992) describe an experiment in which, firstly, Baum-Welch re-estimation was used in conjunction with other more linguistically motivated constraints on the class of grammars that could be inferred, such as 'headedness'; and secondly, initial probabilities were heavily biased in favor of manually coded, linguistically highly plausible rules.This approach resulted in a simple tag sequence grammar often able to assign coherent and semantically/pragmatically plausible analyses to tag sequences drawn from the Spoken English Corpus.By combining such techniques and relaxing the CNF constraint, for example, by adopting the trellis algorithm version of Baum-Welch re-estimation (Kupiec 1991), it might be possible to create a computationally tractable system operating with a realistic NL grammar that would only infer a new rule from a finite space of linguistically motivated possibilities in the face of parse failure or improbability.In the shorter term, such techniques combined with simple tag sequence grammars might yield robust phrase-level 'skeleton' parsers that could be used as corpus analysis tools.The utility of the system reported here would be considerably improved by a more tractable approach to probabilistically unpacking the packed parse forest than exhaustive search.Finding the n-best analyses would allow us to recover analyses for longer sentences where a parse forest is constructed and would make the approach generally more efficient.Carroll and Briscoe (1992) present a heuristic algorithm for parse forest unpacking that interleaves normalization of competing sub-analyses with best-first extraction of the n most probable analyses.Normalization of competing sub-analyses with respect to the longest derivation both allows us to prune the search probabilistically and to treat the probability of analyses as the product of the probability of their sub-analyses, without biasing the system in favor of shorter derivations.This modified version of the system presented here is able to return analyses for sentences over 31 words in length, yields slightly better results on a replication of the experiment reported in Section 8, and the resultant parser is approximately three times faster at returning the three highest-ranked parsers than that presented here.In conclusion, the main positive points of the paper are that 1) LR parse tables can be used to define a more context-dependent and adequate probabilistic model of NL, 2) predictive LR parse tables can be constructed automatically from unification-based grammars in standard notation, 3) effective parse table construction and representation techniques can be defined for realistically sized ambiguous NL grammars, 4) semiautomatic LR based parse techniques can be used to efficiently construct training corpora, and 5) the LR parser and ANLT grammar jointly define a useful probabilistic model into which probabilities concerning lexical subcategorization and structurally defined word sense collocations could be integrated.This research is supported by SERC/DTI-IED project 4/1/1261 'Extensions to the Alvey Natural Language Tools' and by ESPRIT BRA 3030 'Acquisition of Lexical Information from Machine-Readable Dictionaries.'We would like to thank Longman Group Ltd. for allowing us access to the LDOCE MRD and Ann Copestake and Antonio Sanfilippo for considerable help in the analysis of the LDOCE noun definition corpus.Richard Sharman kindly calculated the perplexity measures for this corpus.In addition, Hiyan Alshawi, David Weir, and Steve Young have helped clarify our thinking and made several suggestions that have influenced the way this research has developed.Alex Lascarides and four anonymous reviewers' comments on earlier drafts were very helpful to us in preparing the final version.All errors and mistakes remain our responsibility.
Institut EURECOM In this paper we present some experiments on the use of a probabilistic model to tag English text, i.e. to assign to each word the correct tag (part of speech) in the context of the sentence.The main novelty of these experiments is the use of untagged text in the training of the model.We have used a simple triclass Markov model and are looking for the best way to estimate the parameters of this model, depending on the kind and amount of training data provided.Two approaches in particular are compared and combined: Experiments show that the best training is obtained by using as much tagged text as possible.They also show that Maximum Likelihood training, the procedure that is routinely used to estimate hidden Markov models parameters from training data, will not necessarily improve the tagging accuracy.In fact, it will generally degrade this accuracy, except when only a limited amount of hand-tagged text is available.A lot of effort has been devoted in the past to the problem of tagging text, i.e. assigning to each word the correct tag (part of speech) in the context of the sentence.Two main approaches have generally been considered: Derouault and Merialdo 1986; DeRose 1988; Church 1989; Beale 1988; Marcken 1990; Merialdo 1991; Cutting et al. 1992).More recently, some work has been proposed using neural networks (Benello, Mackie, and Anderson 1989; Nakamura and Shikano 1989).Through these different approaches, some common points have emerged: These kinds of considerations fit nicely inside a probabilistic formulation of the problem (Beale 1985; Garside and Leech 1985), which offers the following advantages: In this paper we present a particular probabilistic model, the triclass model, and results from experiments involving different ways to estimate its parameters, with the intention of maximizing the ability of the model to tag text accurately.In particular, we are interested in a way to make the best use of untagged text in the training of the model.We suppose that the user has defined a set of tags (attached to words).Consider a sentence W = w1w2 wn, and a sequence of tags T =-- tit2 tn, of the same length.We call the pair (W, T) an alignment.We say that word w, has been assigned the tag t, in this alignment.We assume that the tags have some linguistic meaning for the user, so that among all possible alignments for a sentence there is a single one that is correct from a grammatical point of view.A tagging procedure is a procedure 0 that selects a sequence of tags (and so defines an alignment) for each sentence.0:W--+T=0(W) There are (at least) two measures for the quality of a tagging procedure: In practice, performance at sentence level is generally lower than performance at word level, since all the words have to be tagged correctly for the sentence to be tagged correctly.The standard measure used in the literature is performance at word level, and this is the one considered here.In the probabilistic formulation of the tagging problem we assume that the alignments are generated by a probabilistic model according to a probability distribution: p(W, T) In this case, depending on the criterion that we choose for evaluation, the optimal tagging procedure is as follows: We call this procedure Viterbi tagging.It is achieved using a dynamic programming scheme. where 0(W), is the tag assigned to word w, by the tagging procedure in the context of the sentence W. We call this procedure Maximum It is interesting to note that the most commonly used method is Viterbi tagging (see DeRose 1988; Church 1989) although it is not the optimal method for evaluation at word level.The reasons for this preference are presumably that: However, in our experiments, we will show that Viterbi and ML tagging result in very similar performance.Of course, the real tags have not been generated by a probabilistic model and, even if they had been, we would not be able to determine this model exactly because of practical limitations.Therefore the models that we construct will only be approximations of an ideal model that does not exist.It so happens that despite these assumptions and approximations, these models are still able to perform reasonably well.We have the mathematical expression: The triclass (or tri-POS Perouault 19861, or tri-Ggram Kodogno et al. 19871, or HK) model is based on the following approximations: (the name HK model comes from the notation chosen for these probabilities).In order to define the model completely we have to specify the values of all h and k probabilities.If Nw is the size of the vocabulary and NT the number of different tags, then there are: The total number of free parameters is then: Note that this number grows only linearly with respect to the size of the vocabulary, which makes this model attractive for vocabularies of a very large size.The triclass model by itself allows any word to have any tag.However, if we have a dictionary that specifies the list of possible tags for each word, we can use this information to constrain the model: if t is not a valid tag for the word w, then we are sure that There are thus at most as many nonzero values for the k probabilities as there are possible pairs (word, tag) allowed in the dictionary.If we have some tagged text available we can compute the number of times N(w,t) a given word w appears with the tag t, and the number of times N(ti, t2, t3) the sequence (t1, t2, t3) appears in this text.We can then estimate the probabilities h and k by computing the relative frequencies of the corresponding events on this data: These estimates assign a probability of zero to any sequence of tags that did not occur in the training data.But such sequences may occur if we consider other texts.A probability of zero for a sequence creates problems because any alignment that contains this sequence will get a probability of zero.Therefore, it may happen that, for some sequences of words, all alignments get a probability of zero and the model becomes useless for such sentences.To avoid this, we interpolate these distributions with uniform distributions, i.e.:onsider the interpolated model defined by: where number of words that have the tag t The interpolation coefficient A is computed using the deleted interpolation algorithm (Jelinek and Mercer 1980) (it would also be possible to use two coefficients, one for the interpolation on h, one for the interpolation on k).The value of this coefficient is expected to increase if we increase the size of the training text, since the relative frequencies should be more reliable.This interpolation procedure is also called &quot;smoothing.&quot; Smoothing is performed as follows: It can be noted that more complicated interpolation schemes are possible.For example, different coefficients can be used depending on the count of (t1, t2), with the intuition that relative frequencies can be trusted more when this count is high.Another possibilitity is to interpolate also with models of different orders, such as hrf (t3/t2) or hrf (t3).Smoothing can also be achieved with procedures other than interpolation.One example is the &quot;backing-off&quot; strategy proposed by Katz (1987).Using a triclass model M it is possible to compute the probability of any sequence of words W according to this model: where the sum is taken over all possible alignments.The Maximum Likelihood (ML) training finds the model M that maximizes the probability of the training text: where the product is taken over all the sentences W in the training text.This is the problem of training a hidden Markov model (it is hidden because the sequence of tags is hidden).A well-known solution to this problem is the Forward-Backward (FB) or Baum—Welch algorithm (Baum and Eagon 1967; Jelinek 1976; Bahl, Jelinek, and Mercer 1983), which iteratively constructs a sequence of models that improve the probability of the training data.The advantage of this approach is that it does not require any tagging of the text, but makes the assumption that the correct model is the one in which tags are used to best predict the word sequence.The Viterbi algorithm is easily implemented using a dynamic programming scheme (Bellman 1957).The Maximum Likelihood algorithm appears more complex at first glance, because it involves computing the sum of the probabilities of a large number of alignments.However, in the case of a hidden Markov model, these computations can be arranged in a way similar to the one used during the FB algorithm, so that the overall amount of computation needed becomes linear in the length of the sentence (Baum and Eagon 1967).The main objective of this paper is to compare RF and ML training.This is done in Section 7.2.We also take advantage of the environment that we have set up to perform other experiments, described in Section 7.3, that have some theoretical interest, but did not bring any improvement in practice.One concerns the difference between Viterbi and ML tagging, and the other concerns the use of constraints during training.We shall begin by describing the textual data that we are using, before presenting the different tagging experiments using these various training and tagging methods.We use the &quot;treebank&quot; data described in Beale (1988).It contains 42,186 sentences (about one million words) from the Associated Press.These sentences have been tagged manually at the Unit for Computer Research on the English Language (University of Lancaster, U.K.), in collaboration with IBM U.K. (Winchester) and the IBM Speech Recognition group in Yorktown Heights (USA).In fact, these sentences are not only tagged but also parsed.However, we do not use the information contained in the parse.In the treebank 159 different tags are used.These tags were projected on a smaller system of 76 tags designed by Evelyne Tzoukermann and Peter Brown (see Appendix).The results quoted in this paper all refer to this smaller system.We built a dictionary that indicates the list of possible tags for each word, by taking all the words that occur in this text and, for each word, all the tags that are assigned to it somewhere in the text.In some sense, this is an optimal dictionary for this data, since a word will not have all its possible tags (in the language), but only the tags that it actually had within the text.We separated this data into two parts: In this experiment, we extracted N tagged sentences from the training data.We then computed the relative frequencies on these sentences and built a &quot;smoothed&quot; model using the procedure previously described.This model was then used to tag the 2,000 test sentences.We experimented with different values of N, for each of which we indicate the value of the interpolation coefficient and the number and percentage of correctly tagged words.Results are indicated in Table 1.As expected, as the size of the training increases, the interpolation coefficient increases and the quality of the tagging improves.When N = 0, the model is made up of uniform distributions.In this case, all alignments for a sentence are equally probable, so that the choice of the correct tag is just a choice at random.However, the percentage of correct tags is relatively high (more than three out of four) because: Note that this behavior is obviously very dependent on the system of tags that is used.It can be noted that reasonable results are obtained quite rapidly.Using 2,000 tagged sentences (less than 50,000 words), the tagging error rate is already less than 5%.Using 10 times as much data (20,000 tagged sentences) provides an improvement of only 1.5%.ML training, Viterbi tagging In ML training we take all the training data available (40,186 sentences) but we only use the word sequences, not the associated tags (except to compute the initial model, as will be described later).This is possible since the FB algorithm is able to train the model using the word sequence only.In the first experiment we took the model made up of uniform distributions as the initial one.The only constraints in this model came from the values k(w It) that were set to zero when the tag t was not possible for the word w (as found in the dictionary).We then ran the FB algorithm and evaluated the quality of the tagging.The results are shown in Figure 1.(Perplexity is a measure of the average branching factor for probabilistic models.)This figure shows that ML training both improves the perplexity of the model and reduces the tagging error rate.However, this error rate remains at a relatively high level—higher than that obtained with a RF training on 100 tagged sentences.Having shown that ML training is able to improve the uniform model, we then wanted to know if it was also able to improve more accurate models.We therefore took as the initial model each of the models obtained previously by RF training and, for each one, performed ML training using all of the training word sequences.The results are shown graphically in Figure 2 and numerically in Table 2.These results show that, when we use few tagged data, the model obtained by relative frequency is not very good and Maximum Likelihood training is able to improve it.However, as the amount of tagged data increases, the models obtained by Relative Frequency are more accurate and Maximum Likelihood training improves on the initial iterations only, but after deteriorates.If we use more than 5,000 tagged sentences, even the first iteration of ML training degrades the tagging.(This number is of course dependent on both the particular system of tags and the kind of text used in this experiment).These results call for some comments.ML training is a theoretically sound procedure, and one that is routinely and successfully used in speech recognition to estimate the parameters of hidden Markov models that describe the relations between sequences of phonemes and the speech signal.Although ML training is guaranteed to improve perplexity, perplexity is not necessarily related to tagging accuracy, and it is possible to improve one while degrading the other.Also, in the case of tagging, ML training from various initial points (top line corresponds to N=100, bottom line to N=a11). the relations between words and tags are much more precise than the relations between phonemes and speech signals (where the correct correspondence is harder to define precisely).Some characteristics of ML training, such as the effect of smoothing probabilities, are probably more suited to speech than to tagging.For this experiment we considered the initial model built by RF training over the whole training data and all the successive models created by the iterations of ML training.For each of these models we performed Viterbi tagging and ML tagging on the same test data, then evaluated and compared the number of tagging errors produced by these two methods.The results are shown in Table 3.The models obtained at different iterations are related, so one should not draw strong conclusions about the definite superiority of one tagging procedure.However, the difference in error rate is very small, and shows that the choice of the tagging procedure is not as critical as the kind of training material.Following a suggestion made by F. Jelinek, we investigated the effect of constraining the ML training by imposing constraints on the probabilities.This idea comes from the observation that the amount of training data needed to properly estimate the model increases with the number of free parameters of the model.In the case of little training data, adding reasonable constraints on the shape of the models that are looked for reduces the number of free parameters and should improve the quality of the estimates.We tried two different constraints: The tw-constrained ML training is similar to the standard ML training, except that the probabilities p(t/w) are not changed at the end of an iteration.The results in Table 4 show the number of tagging errors when the model is trained with the standard or tw-constrained ML training.They show that the tw-constrained ML training still degrades the RF training, but not as quickly as the standard ML.We have not tested what happens when smaller training data is used to build the initial model. t-constraint This constraint is more difficult to implement than the previous one because the probabilities p(t) are not the parameters of the model, but a combination of these parameters.With the help of R. Polyak we have designed an iterative procedure that allows the likelihood to be improved while preserving the values of p(t).We do not have sufficient space to describe this procedure here.Because of its greater computational complexity, we have only applied it to a biclass model, i.e. a model where The initial model is estimated by relative frequency on the whole training data and Viterbi tagging is used.As in the previous experiment, the results in Table 5 show the number of tagging errors when the model is trained with the standard or t-constrained ML training.They show that the t-constrained ML training still degrades the RF training, but not as quickly as the standard ML.Again, we have not tested what happens when smaller training data is used to build the initial model.8.Conclusion The results presented in this paper show that estimating the parameters of the model by counting relative frequencies over a very large amount of hand-tagged text lead to the best tagging accuracy.Maximum Likelihood training is guaranteed to improve perplexity, but will not necessarily improve tagging accuracy.In our experiments, ML training degrades the performance unless the initial model is already very bad.The preceding results suggest that the optimal strategy to build the best possible model for tagging is the following: whichever occurs first.I would like to thank Peter Brown, Fred Jelinek, John Lafferty, Robert Mercer, Salim Roukos, and other members of the Continuous Speech Recognition group for the fruitful discussions I had with them throughout this work.I also want to thank one of the referees for his judicious comments.
It is challenging to translate names and technical terms across languages with different alphabets and sound inventories.These items are commonly transliterated, i.e., replaced with approximate phonetic equivalents.For example, &quot;computer&quot; in English comes out as &quot;konpyuutaa&quot; in Japanese.Translating such items from Japanese back to English is even more challenging, and of practical interest, as transliterated items make up the bulk of text phrases not found in bilingual dictionaries.We describe and evaluate a method for performing backwards transliterations by machine.This method uses a generative model, incorporating several distinct stages in the transliteration process.One of the most frequent problems translators must deal with is translating proper names and technical terms.For language pairs like Spanish/English, this presents no great challenge: a phrase like Antonio Gil usually gets translated as Antonio Gil.However, the situation is more complicated for language pairs that employ very different alphabets and sound systems, such as Japanese/English and Arabic/English.Phonetic translation across these pairs is called transliteration.We will look at Japanese/English transliteration in this article.Japanese frequently imports vocabulary from other languages, primarily (but not exclusively) from English.It has a special phonetic alphabet called katakana, which is used primarily (but not exclusively) to write down foreign names and loanwords.The katakana symbols are shown in Figure 1, with their Japanese pronunciations.The two symbols shown in the lower right corner ( —, ) are used to lengthen any Japanese vowel or consonant.To write a word like golfbag in katakana, some compromises must be made.For example, Japanese has no distinct L and R. sounds: the two English sounds collapse onto the same Japanese sound.A similar compromise must be struck for English H and F. Also, Japanese generally uses an alternating consonant-vowel structure, making it impossible to pronounce LFB without intervening vowels.Katakana writing is a syllabary rather than an alphabet—there is one symbol for ga (If), another for gi eV ), another for gu ( 7 ), etc.So the way to write golfbag in katakana is ''''-' 7 'Z Y , roughly pronounced go-ru-hu-ba-ggu.Here are a few more examples: Katakana symbols and their Japanese pronunciations.Angela Johnson New York Times ice cream Notice how the transliteration is more phonetic than orthographic; the letter h in Johnson does not produce any katakana.Also, a dot-separator (o) is used to separate words, but not consistently.And transliteration is clearly an information-losing operation: ranpu could come from either lamp or ramp, while aisukuriimu loses the distinction between ice cream and I scream.Transliteration is not trivial to automate, but we will be concerned with an even more challenging problem—going from katakana back to English, i.e., back-transliteration.Human translators can often &quot;sound out&quot; a katakana phrase to guess an appropriate translation.Automating this process has great practical importance in Japanese/English machine translation.Katakana phrases are the largest source of text phrases that do not appear in bilingual dictionaries or training corpora (a.k.a.&quot;notfound words&quot;), but very little computational work has been done in this area.Yamron et al. (1994) briefly mention a pattern-matching approach, while Arbabi et al.(1994) discuss a hybrid neural-net/expert-system approach to (forward) transliteration.The information-losing aspect of transliteration makes it hard to invert.Here are some problem instances, taken from actual newspaper articles: English translations appear later in this article.Here are a few observations about back-transliteration that give an idea of the difficulty of the task: The most desirable feature of an automatic back-transliterator is accuracy.If possible, our techniques should also be: Like most problems in computational linguistics, this one requires full world knowledge for a 100% solution.Choosing between Katarina and Catalina (both good guesses for 9 ) might even require detailed knowledge of geography and figure skating.At that level, human translators find the problem quite difficult as well, so we only aim to match or possibly exceed their performance.Bilingual glossaries contain many entries mapping katakana phrases onto English phrases, e.g., (aircraft carrier 2 7 is t &quot;I' 9 7 ).It is possible to automatically analyze such pairs to gain enough knowledge to accurately map new katakana phrases that come along, and this learning approach travels well to other language pairs.A naive approach to finding direct correspondences between English letters and katakana symbols, however, suffers from a number of problems.One can easily wind up with a system that proposes iskrym as a back-transliteration of aisukuriimu.Taking letter frequencies into account improves this to a more plausible-looking isclim.Moving to real words may give is crime: the i corresponds to ai, the s corresponds to su, etc.Unfortunately, the correct answer here is ice cream.After initial experiments along these lines, we stepped back and built a generative model of the transliteration process, which goes like this: This divides our problem into five subproblems.Fortunately, there are techniques for coordinating solutions to such subproblems, and for using generative models in the reverse direction.These techniques rely on probabilities and Bayes' theorem.Suppose we build an English phrase generator that produces word sequences according to some probability distribution P(w).And suppose we build an English pronouncer that takes a word sequence and assigns it a set of pronunciations, again probabilistically, according to some P(pi w).Given a pronunciation p, we may want to search for the word sequence w that maximizes P(w Ip).Bayes' theorem lets us equivalently maximize P(w) • P(plw), exactly the two distributions we have modeled.Extending this notion, we settled down to build five probability distributions: Given a katakana string o observed by OCR, we want to find the English word sequence w that maximizes the sum, over all e, j, and k, of of the models in turn.The result is a large WFSA containing all possible English translations.We have implemented two algorithms for extracting the best translations.The first is Dijkstra's shortest-path graph algorithm (Dijkstra 1959).The second is a recently discovered k-shortest-paths algorithm (Eppstein 1994) that makes it possible for us to identify the top k translations in efficient 0(m + n log n + kn) time, where the WFSA contains n states and m arcs.The approach is modular.We can test each engine independently and be confident that their results are combined correctly.We do no pruning, so the final WFSA contains every solution, however unlikely.The only approximation is the Viterbi one, which searches for the best path through a WFSA instead of the best sequence (i.e., the same sequence does not receive bonus points for appearing more than once).This section describes how we designed and built each of our five models.For consistency, we continue to print written English word sequences in italics (golf ball), English sound sequences in all capitals (G AA L F B AO L), Japanese sound sequences in lower case (goruhubooru) and katakana sequences naturally The first model generates scored word sequences, the idea being that ice cream should score higher than ice creme, which should score higher than aice kreem.We adopted a simple unigram scoring method that multiplies the scores of the known words and phrases in a sequence.Our 262,000-entry frequency list draws its words and phrases from the Wall Street Journal corpus, an on-line English name list, and an on-line gazetteer of place names.'A portion of the WFSA looks like this: los / 0.000087 month I 0.000992 An ideal word sequence model would look a bit different.It would prefer exactly those strings which are actually grist for Japanese transliterators.For example, people rarely transliterate auxiliary verbs, but surnames are often transliterated.We have approximated such a model by removing high-frequency words like has, an, are, am, were, their, and does, plus unlikely words corresponding to Japanese sound bites, like coup and oh.We also built a separate word sequence model containing only English first and last names.If we know (from context) that the transliterated phrase is a personal name, this model is more precise.The next WFST converts English word sequences into English sound sequences.We use the English phoneme inventory from the on-line CMU Pronunciation Dictiofederal I 0.0013 nary, minus the stress marks.2 This gives a total of 40 sounds, including 14 vowel sounds (e.g., AA, AE, UW), 25 consonant sounds (e.g., K, HH, R), plus one special symbol (PAUSE).The dictionary has pronunciations for 110,000 words, and we organized a tree-based WFST from it: Note that we insert an optional PAUSE between word pronunciations.We originally thought to build a general letter-to-sound WFST (Divay and Vitale 1997), on the theory that while wrong (overgeneralized) pronunciations might occasionally be generated, Japanese transliterators also mispronounce words.However, our letter-to-sound WFST did not match the performance of Japanese transliterators, and it turns out that mispronunciations are modeled adequately in the next stage of the cascade.Next, we map English sound sequences onto Japanese sound sequences.This is an inherently information-losing process, as English R and L sounds collapse onto Japanese r, the 14 English vowel sounds collapse onto the 5 Japanese vowel sounds, etc.We face two immediate problems: An obvious target inventory is the Japanese syllabary itself, written down in katakana (e.g., -= ) or a roman equivalent (e.g., ni).With this approach, the English sound K corresponds to one of t (ka), (ki), (ku), (ke), or (ko), depending on its context.Unfortunately, because katakana is a syllabary, we would be unable to express an obvious and useful generalization, namely that English K usually corresponds to Japanese k, independent of context.Moreover, the correspondence of Japanese katakana writing to Japanese sound sequences is not perfectly one-to-one (see Section 3.4), so an independent sound inventory is well-motivated in any case.Our Japanese sound inventory includes 39 symbols: 5 vowel sounds, 33 consonant sounds (including doubled consonants like kk), and one special symbol (pause).An English sound sequence like (P R OW PAUSE S AA K ER) might map onto a Japanese sound sequence like (p u r o pause s a kk a a).Note that long Japanese vowel sounds Knight and Graehl Machine Transliteration are written with two symbols (a a) instead of just one (aa).This scheme is attractive because Japanese sequences are almost always longer than English sequences.Our WFST is learned automatically from 8,000 pairs of English/Japanese sound sequences, e.g., ((S AA K ER) (s a kk a a)).We were able to produce these pairs by manipulating a small English-katakana glossary.For each glossary entry, we converted English words into English sounds using the model described in the previous section, and we converted katakana words into Japanese sounds using the model we describe in the next section.We then applied the estimation-maximization (EM) algorithm (Baum 1972; Dempster, Laird, and Rubin 1977) to generate symbol-mapping probabilities, shown in Figure 2.Our EM training goes like this: alignments between their elements.In our case, an alignment is a drawing that connects each English sound with one or more Japanese sounds, such that all Japanese sounds are covered and no lines cross.For example, there are two ways to align the pair ( (L OW) <-> (r o 0)): In this case, the alignment on the left is intuitively preferable.The algorithm learns such preferences.2.For each pair, assign an equal weight to each of its alignments, such that those weights sum to 1.In the case above, each alignment gets a weight of 0.5.PAUSE:pause Our WFST has 99 states and 283 arcs.English sounds (in capitals) with probabilistic mappings to Japanese sound sequences (in lower case), as learned by estimation-maximization.Only mappings with conditional probabilities greater than 1% are shown, so the figures may not sum to 1.We have also built models that allow individual English sounds to be &quot;swallowed&quot; (i.e., produce zero Japanese sounds).However, these models are expensive to compute (many more alignments) and lead to a vast number of hypotheses during WFST composition.Furthermore, in disallowing &quot;swallowing,&quot; we were able to automatically remove hundreds of potentially harmful pairs from our training set, e.g., ( (B AA R B ER SH AA P) 4-* (b aab a a) ).Because no alignments are possible, such pairs are skipped by the learning algorithm; cases like these must be solved by dictionary Alignments between English and Japanese sound sequences, as determined by EM training.Best alignments are shown for the English words biscuit, divider, and filter. lookup anyway.Only two pairs failed to align when we wished they had—both involved turning English Y UW into Japanese u, as in ((Y UW K AH L EY L 1Y) +-4 (u k urere)).Note also that our model translates each English sound without regard to context.We have also built context-based models, using decision trees recoded as WFSTs.For example, at the end of a word, English T is likely to come out as (t o) rather than (t).However, context-based models proved unnecessary for back-transliteration.They are more useful for English-to-Japanese forward transliteration.To map Japanese sound sequences like (m o ot a a) onto katakana sequences like ), we manually constructed two WFSTs.Composed together, they yield an integrated WFST with 53 states and 303 arcs, producing a katakana inventory containing 81 symbols, including the dot-separator (.).The first WFST simply merges long Japanese vowel sounds into new symbols aa, ii, uu, ee, and oo.The second WFST maps Japanese sounds onto katakana symbols.The basic idea is to consume a whole syllable worth of sounds before producing any katakana.For example: This fragment shows one kind of spelling variation in Japanese: long vowel sounds (00) are usually written with a long vowel mark ( 21&quot; ) but are sometimes written with repeated katakana ( 71- 71.).We combined corpus analysis with guidelines from a Japanese textbook (Jorden and Chaplin 1976) to turn up many spelling variations and unusual katakana symbols: and so on.Spelling variation is clearest in cases where an English word like switch shows up transliterated variously ( 4 ‘2 4 'Y , 7 9 4 'Y ) in different dictionaries.Treating these variations as an equivalence class enables us to learn general sound mappings even if our bilingual glossary adheres to a single narrow spelling convention.We do not, however, generate all katakana sequences with this model; for example, we do not output strings that begin with a subscripted vowel katakana.So this model also serves to filter out some ill-formed katakana sequences, possibly proposed by optical character recognition.Perhaps uncharitably, we can view optical character recognition (OCR) as a device that garbles perfectly good katakana sequences.Typical confusions made by our commercial OCR system include t: for 71. for , 7 for 7, and 7 for I.To generate pre-OCR text, we collected 19,500 characters worth of katakana words, stored them in a file, and printed them out.To generate post-OCR text, we OCR'd the printouts.We then ran the EM algorithm to determine symbol-mapping (&quot;garbling&quot;) probabilities.Here is part of that table: This model outputs a superset of the 81 katakana symbols, including spurious quote marks, alphabetic symbols, and the numeral 7.3 We can now use the models to do a sample back-transliteration.We start with a katakana phrase as observed by OCR.We then serially compose it with the models, in reverse order.Each intermediate stage is a WFSA that encodes many possibilities.The final stage contains all back-transliterations suggested by the models, and we finally extract the best one.We start with the masutaazutoonamento problem from Section 1.Our OCR observes: This string has two recognition errors: (ku) for (t a), and (chi) for (ha).We turn the string into a chained 12-state/11-arc WFSA and compose it with the P(Iclo) model.This yields a fatter 12-state/15-arc WFSA, which accepts the correct spelling at a lower probability.Next comes the POO model, which produces a 28-state/31-arc WFSA whose highest-scoring sequence is: masutaazutoochimento Next comes P(elj), yielding a 62-state/241-arc WFSA whose best sequence is: Next to last comes P(w le), which results in a 2982-state/4601-arc WFSA whose best sequence (out of roughly three hundred million) is: masters tone am ent awe This English string is closest phonetically to the Japanese, but we are willing to trade phonetic proximity for more sensical English; we rescore this WFSA by composing it with P(w) and extract the best translation: Other Section 1 examples (aasudee and robaato shyoon renaado) are translated correctly as earth day and robert sean leonard.We may also be interested in the k best translations.In fact, after any composition, we can inspect several high-scoring sequences using the algorithm of Eppstein (1994).Given the following katakana input phrase: Inspecting the k-best list is useful for diagnosing problems with the models.If the right answer appears low in the list, then some numbers are probably off somewhere.If the right answer does not appear at all, then one of the models may be missing a word or suffer from some kind of brittleness.A k-best list can also be used as input to a later context-based disambiguator, or as an aid to a human translator.We have performed two large-scale experiments, one using a full-language P(w) model, and one using a personal name language model.In the first experiment, we extracted 1,449 unique katakana phrases from a corpus of 100 short news articles.Of these, 222 were missing from an on-line 100,000-entry bilingual dictionary.We back-transliterated these 222 phrases.Many of the translations are perfect: technical program, sex scandal, omaha beach, new york times, ramon diaz.Others are close: tanya harding, nickel simpson, danger washington, world cap.Some miss the mark: nancy care again, plus occur, patriot miss rea1.4 While it is difficult to judge overall accuracy—some of the phrases are onomatopoetic, and others are simply too hard even for good human translators—it is easier to identify system weaknesses, and most of these lie in the P(w) model.For example, nancy kerrigan should be preferred over nancy care again.In a second experiment, we took (non-OCR) katakana versions of the names of 100 U.S. politicians, e.g.: ' 1 (jyon.buroo), 7)1&quot;1&quot; 7 (aruhonsu. damatto), and -Q.4 ' 7 .s/ (maiku.dewain).We back-transliterated these by machine and asked four human subjects to do the same.These subjects were native English speakers and news-aware; we gave them brief instructions.The results were as in Table 1.There is room for improvement on both sides.Being English speakers, the human subjects were good at English name spelling and U.S. politics, but not at Japanese phonetics.A native Japanese speaker might be expert at the latter but not the former.People who are expert in all of these areas, however, are rare.On the automatic side, many errors can be corrected.A first-name/last-name model would rank richard bryan more highly than richard brian.A bigram model would prefer orren hatch over olin hatch.Other errors are due to unigram training problems, or more rarely, incorrect or brittle phonetic models.For example, Long occurs much more often than Ron in newspaper text, and our word selection does not exclude phrases like Long Island.So we get long wyden instead of ron wyden.One way to fix these problems is by manually changing unigram probabilities.Reducing P(long) by a factor of ten solves the problem while maintaining a high score for P(long rongu).Despite these problems, the machine's performance is impressive.When word separators (p) are removed from the katakana phrases, rendering the task exceedingly difficult for people, the machine's performance is unchanged.In other words, it offers the same top-scoring translations whether or not the separators are present; however, their presence significantly cuts down on the number of alternatives considered, improving efficiency.When we use OCR, 7% of katakana tokens are misrecognized, affecting 50% of test strings, but translation accuracy only drops from 64% to 52%.In a 1947 memorandum, Weaver (1955) wrote: One naturally wonders if the problem of translation could conceivably be treated as a problem of cryptography.When I look at an article in Russian, I say: &quot;This is really written in English, but it has been coded in some strange symbols.I will now proceed to decode.&quot; (p. 18) Whether this is a useful perspective for machine translation is debatable (Brown et al. 1993; Knoblock 1996)—however, it is a dead-on description of transliteration.Most katakana phrases really are English, ready to be decoded.We have presented a method for automatic back-transliteration which, while far from perfect, is highly competitive.It also achieves the objectives outlined in Section 1.It ports easily to new language pairs; the P(w) and P(ejw) models are entirely reusable, while other models are learned automatically.It is robust against OCR noise, in a rare example of high-level language processing being useful (necessary, even) in improving low-level OCR.There are several directions for improving accuracy.The biggest problem is that raw English frequency counts are not the best indication of whether a word is a possible source for transliteration.Alternative data collection methods must be considered.We may also consider changes to the model sequence itself.As we have presented it, our hypothetical human transliterator produces Japanese sounds from English sounds only, without regard for the original English spelling.This means that English homonyms will produce exactly the same katakana strings.In reality, though, transliterators will sometimes key off spelling, so that tonya and tanya produce toonya and taanya.It might pay to carry along some spelling information in the English pronunciation lattices.Sentential context should be useful for determining correct translations.It is often clear from a Japanese sentence whether a katakana phrase is a person, an institution, or a place.In many cases it is possible to narrow things further—given the phrase &quot;such-and-such, Arizona,&quot; we can restrict our P(w) model to include only those cities and towns in Arizona.It is also interesting to consider transliteration for other languages.In Arabic, for example, it is more difficult to identify candidates for transliteration because there is no distinct, explicit alphabet that marks them.Furthermore, Arabic is usually written without vowels, so we must generate vowel sounds from scratch in order to produce correct English.Finally, it may be possible to embed phonetic-shift models inside speech recognizers, to explicitly adjust for heavy foreign accents.We would like to thank Alton Earl Ingram, Yolanda Gil, Bonnie Glover Stalls, Richard Whitney, Kenji Yamada, and the anonymous reviewers for their helpful comments.We would also like to thank our sponsors at the Department of Defense.
Semantic inference is a key component for advanced natural language understanding.Several important applications are already relying heavily on inference, including question answering (Moldovan et al. 2003; Harabagiu and Hickl 2006), information extraction (Romano et al.2006), and textual entailment (Szpektor et al. 2004).In response, several researchers have created resources for enabling semantic inference.Among manual resources used for this task are WordNet (Fellbaum 1998) and Cyc (Lenat 1995).Although important and useful, these resources primarily contain prescriptive inference rules such as “X divorces Y ⇒ X married Y”.In practical NLP applications, however, plausible inference rules such as “X married Y” ⇒ “X dated Y” are very useful.This, along with the difficulty and labor-intensiveness of generating exhaustive lists of rules, has led researchers to focus on automatic methods for building inference resources such as inference rule collections (Lin and Pantel 2001; Szpektor et al. 2004) and paraphrase collections (Barzilay and McKeown 2001).Using these resources in applications has been hindered by the large amount of incorrect inferences they generate, either because of altogether incorrect rules or because of blind application of plausible rules without considering the context of the relations or the senses of the words.For example, consider the following sentence: Terry Nichols was charged by federal prosecutors for murder and conspiracy in the Oklahoma City bombing. and an inference rule such as: Using this rule, we can infer that “federal prosecutors announced the arrest of Terry Nichols”.However, given the sentence: Fraud was suspected when accounts were charged by CCM telemarketers without obtaining consumer authorization. the plausible inference rule (1) would incorrectly infer that “CCM telemarketers announced the arrest of accounts”.This example depicts a major obstacle to the effective use of automatically learned inference rules.What is missing is knowledge about the admissible argument values for which an inference rule holds, which we call Inferential Selectional Preferences.For example, inference rule (1) should only be applied if X is a Person and Y is a Law Enforcement Agent or a Law Enforcement Agency.This knowledge does not guarantee that the inference rule will hold, but, as we show in this paper, goes a long way toward filtering out erroneous applications of rules.In this paper, we propose ISP, a collection of methods for learning inferential selectional preferences and filtering out incorrect inferences.The presented algorithms apply to any collection of inference rules between binary semantic relations, such as example (1).ISP derives inferential selectional preferences by aggregating statistics of inference rule instantiations over a large corpus of text.Within ISP, we explore different probabilistic models of selectional preference to accept or reject specific inferences.We present empirical evidence to support the following main contribution: Claim: Inferential selectional preferences can be automatically learned and used for effectively filtering out incorrect inferences.Selectional preference (SP) as a foundation for computational semantics is one of the earliest topics in AI and NLP, and has its roots in (Katz and Fodor 1963).Overviews of NLP research on this theme are (Wilks and Fass 1992), which includes the influential theory of Preference Semantics by Wilks, and more recently (Light and Greiff 2002).Rather than venture into learning inferential SPs, much previous work has focused on learning SPs for simpler structures.Resnik (1996), the seminal paper on this topic, introduced a statistical model for learning SPs for predicates using an unsupervised method.Learning SPs often relies on an underlying set of semantic classes, as in both Resnik’s and our approach.Semantic classes can be specified manually or derived automatically.Manual collections of semantic classes include the hierarchies of WordNet (Fellbaum 1998), Levin verb classes (Levin 1993), and FrameNet (Baker et al. 1998).Automatic derivation of semantic classes can take a variety of approaches, but often uses corpus methods and the Distributional Hypothesis (Harris 1964) to automatically cluster similar entities into classes, e.g.CBC (Pantel and Lin 2002).In this paper, we experiment with two sets of semantic classes, one from WordNet and one from CBC.Another thread related to our work includes extracting from text corpora paraphrases (Barzilay and McKeown 2001) and inference rules, e.g.TEASE1 (Szpektor et al. 2004) and DIRT (Lin and Pantel 2001).While these systems differ in their approaches, neither provides for the extracted inference rules to hold or fail based on SPs.Zanzotto et al. (2006) recently explored a different interplay between SPs and inferences.Rather than examine the role of SPs in inferences, they use SPs of a particular type to derive inferences.For instance the preference of win for the subject player, a nominalization of play, is used to derive that “win => play”.Our work can be viewed as complementary to the work on extracting semantic inferences and paraphrases, since we seek to refine when a given inference applies, filtering out incorrect inferences.The aim of this paper is to learn inferential selectional preferences for filtering inference rules.Let pi => pj be an inference rule where p is a binary semantic relation between two entities x and y.Let (x, p, y) be an instance of relation p. Formal task definition: Given an inference rule pi => pj and the instance (x, pi, y), our task is to determine if (x, pj, y) is valid.Consider the example in Section 1 where we have the inference rule “X is charged by Y” => “Y announced the arrest of X”.Our task is to automatically determine that “federal prosecutors announced the arrest of Terry Nichols” (i.e., (Terry Nichols, pj, federal prosecutors)) is valid but that “CCM telemarketers announced the arrest of accounts” is invalid.Because the semantic relations p are binary, the selectional preferences on their two arguments may be either considered jointly or independently.For example, the relation p = “X is charged by Y” could have joint SPs: This distinction between joint and independent selectional preferences constitutes the difference between the two models we present in this section.The remainder of this section describes the ISP approach.In Section 3.1, we describe methods for automatically determining the semantic contexts of each single relation’s selectional preferences.Section 3.2 uses these for developing our inferential selectional preference models.Finally, we propose inference filtering algorithms in Section 3.3. cx Resnik (1996) defined the selectional preferences of a predicate as the semantic classes of the words that appear as its arguments.Similarly, we define the relational selectional preferences of a binary semantic relation pi as the semantic classes C(x) of the words that can be instantiated for x and as the semantic classes C(y) of the words that can be instantiated for y.The semantic classes C(x) and C(y) can be obtained from a conceptual taxonomy as proposed in (Resnik 1996), such as WordNet, or from the classes extracted from a word clustering algorithm such as CBC (Pantel and Lin 2002).For example, given the relation “X is charged by Y”, its relational selection preferences from WordNet could be {social group, organism, state...} for X and {authority, state, section...} for Y.Below we propose joint and independent models, based on a corpus analysis, for automatically determining relational selectional preferences.Model 1: Joint Relational Model (JRM) Our joint model uses a corpus analysis to learn SPs for binary semantic relations by considering their arguments jointly, as in example (2).Given a large corpus of English text, we first find the occurrences of each semantic relation p. For each instance 〈x, p, y〉, we retrieve the sets C(x) and C(y) of the semantic classes that x and y belong to and accumulate the frequencies of the triples 〈c(x), p, c(y)〉, where c(x) ∈ C(x) and c(y) ∈ C(y)2.Each triple 〈c(x), p, c(y)〉 is a candidate selectional preference for p. Candidates can be incorrect when: a) they were generated from the incorrect sense of a polysemous word; or b) p does not hold for the other words in the semantic class.Intuitively, we have more confidence in a particular candidate if its semantic classes are closely associated given the relation p. Pointwise mutual information (Cover and Thomas 1991) is a commonly used metric for measuring this association strength between two events e1 and e2: 2 In this paper, the semantic classes C(x) and C(y) are extracted from WordNet and CBC (described in Section 4.2).We define our ranking function as the strength of association between two semantic classes, cx and cy3, given the relation p: Let |cx, p, cy |denote the frequency of observing the instance 〈c(x), p, c(y)〉.We estimate the probabilities of Equation 3.2 using maximum likelihood estimates over our corpus: Similarly to (Resnik 1996), we estimate the above frequencies using: these classes co-occurring even though they would form a valid relational selectional preference.To alleviate this problem, we propose a second model that is less strict by considering the arguments of the binary semantic relations independently, as in example (3).Similarly to JRM, we extract each instance p, of each semantic relation p and retrieve the set of semantic classes C(x) and C(y) that x and y belong to, accumulating the frequencies of the triples p, and p, where tic class given the relation p, according to Equations 3.3. where p, denotes the frequency of observing cy d c(y) in our equations.The intersection of the two sets of SPs forms the candidate inferential SPs for the inference pi => pj: (Law Enforcement Agent, *) (*, Person) We use the same minimum, maximum, and average ranking strategies as in JIM.Whereas in Section 3.1 we learned selectional preferences for the arguments of a relation p, in this section we learn selectional preferences for the arguments of an inference rule pi => pj.Model 1: Joint Inferential Model (JIM) Given an inference rule pi => pj, our joint model defines the set of inferential SPs as the intersection of the relational SPs for pi and pj, as defined in the Joint Relational Model (JRM).For example, suppose relation pi = “X is charged by Y” gives the following SP scores under the JRM: and that pj = “Y announced the arrest of X” gives the following SP scores under the JRM: The intersection of the two sets of SPs forms the candidate inferential SPs for the inference pi => pj: We rank the candidate inferential SPs according to three ways to combine their relational SP scores, using the minimum, maximum, and average of the SPs.For example, for (Law Enforcement Agent, Person), the respective scores would be 1.45, 2.01, and 1.73.These different ranking strategies produced nearly identical results in our experiments, as discussed in Section 5.Model 2: Independent Inferential Model (IIM) Our independent model is the same as the joint model above except that it computes candidate inferential SPs using the Independent Relational Model (IRM) instead of the JRM.Consider the same example relations pi and pj from the joint model and suppose that the IRM gives the following relational SP scores for pi: and the following relational SP scores for pj: Given an inference rule pi => pj and the instance (x, pi, y), the system’s task is to determine whether (x, pj, y) is valid.Let C(w) be the set of semantic classes c(w) to which word w belongs.Below we present three filtering algorithms which range from the least to the most permissive: Since both JIM and IIM use a ranking score in their inferential SPs, each filtering algorithm can be tuned to be more or less strict by setting an acceptance threshold on the ranking scores or by selecting only the top i percent highest ranking SPs.In our experiments, reported in Section 5, we tested each model using various values of i.This section describes the methodology for testing our claim that inferential selectional preferences can be learned to filter incorrect inferences.Given a collection of inference rules of the form pi => pj, our task is to determine whether a particular instance (x, pj, y) holds given that (x, pi, y) holds4.In the next sections, we describe our collection of inference rules, the semantic classes used for forming selectional preferences, and evaluation criteria for measuring the filtering quality.Our models for learning inferential selectional preferences can be applied to any collection of inference rules between binary semantic relations.In this paper, we focus on the inference rules contained in the DIRT resource (Lin and Pantel 2001).DIRT consists of over 12 million rules which were extracted from a 1GB newspaper corpus (San Jose Mercury, Wall Street Journal and AP Newswire from the TREC-9 collection).For example, here are DIRT’s top 3 inference rules for “X solves Y”: “Y is solved by X”, “X resolves Y”, “X finds a solution to Y” The choice of semantic classes is of great importance for selectional preference.One important aspect is the granularity of the classes.Too general a class will provide no discriminatory power while too fine-grained a class will offer little generalization and apply in only extremely few cases.The absence of an attested high-quality set of semantic classes for this task makes discovering preferences difficult.Since many of the criteria for developing such a set are not even known, we decided to experiment with two very different sets of semantic classes, in the hope that in addition to learning semantic preferences, we might also uncover some clues for the eventual decisions about what makes good semantic classes in general.Our first set of semantic classes was directly extracted from the output of the CBC clustering algorithm (Pantel and Lin 2002).We applied CBC to the TREC-9 and TREC-2002 (Aquaint) newswire collections consisting of over 600 million words.CBC generated 1628 noun concepts and these were used as our semantic classes for SPs.Secondly, we extracted semantic classes from WordNet 2.1 (Fellbaum 1998).In the absence of any externally motivated distinguishing features (for example, the Basic Level categories from Prototype Theory, developed by Eleanor Rosch (1978)), we used the simple but effective method of manually truncating the noun synset hierarchy5 and considering all synsets below each cut point as part of the semantic class at that node.To select the cut points, we inspected several different hierarchy levels and found the synsets at a depth of 4 5 Only nouns are considered since DIRT semantic relations connect only nouns. to form the most natural semantic classes.Since the noun hierarchy in WordNet has an average depth of 12, our truncation created a set of concepts considerably coarser-grained than WordNet itself.The cut produced 1287 semantic classes, a number similar to the classes in CBC.To properly test WordNet as a source of semantic classes for our selectional preferences, we would need to experiment with different extraction algorithms.The goal of the filtering task is to minimize false positives (incorrectly accepted inferences) and false negatives (incorrectly rejected inferences).A standard methodology for evaluating such tasks is to compare system filtering results with a gold standard using a confusion matrix.A confusion matrix captures the filtering performance on both correct and incorrect inferences: where A represents the number of correct instances correctly identified by the system, D represents the number of incorrect instances correctly identified by the system, B represents the number of false positives and C represents the number of false negatives.To compare systems, three key measures are used to summarize confusion matrices: probability of a filter being correct.In this section, we provide empirical evidence to support the main claim of this paper.Given a collection of DIRT inference rules of the form pi => pj, our experiments, using the methodology of Section 4, evaluate the capability of our ISP models for determining if (x, pj, y) holds given that (x, pi, y) holds.For each filtering algorithm in Section 3.3, ISP.JIM, ISP.IIM.∧, and ISP.IIM.v, we trained their probabilistic models using corpus statistics extracted from the 1999 AP newswire collection (part of the TREC-2002 Aquaint collection) consisting of approximately 31 million words.We used the Minipar parser (Lin 1993) to match DIRT patterns in the text.This permits exact matches since DIRT inference rules are built from Minipar parse trees.For each system, we experimented with the different ways of combining relational SP scores: minimum, maximum, and average (see Section 3.2).Also, we experimented with various values for the i parameter described in Section 3.3.In order to compute the confusion matrices described in Section 4.3, we must first construct a representative set of inferences and manually annotate them as correct or incorrect.We randomly selected 100 inference rules of the form pi => pj from DIRT.For each pattern pi, we then extracted its instances from the Aquaint 1999 AP newswire collection (approximately 22 million words), and randomly selected 10 distinct instances, resulting in a total of 1000 instances.For each instance of pi, applying DIRT’s inference rule would assert the instance (x, pj, y).Our evaluation tests how well our models can filter these so that only correct inferences are made.To form the gold standard, two human judges were asked to tag each instance (x, pj, y) as correct or incorrect.For example, given a randomly selected inference rule “X is charged by Y => Y announced the arrest of X” and the instance “Terry Nichols was charged by federal prosecutors”, the judges must determine if the instance (federal prosecutors, Y announced the arrest of X, Terry Nichols) is correct.The judges were asked to consider the following two criteria for their decision: Judges found that annotation decisions can range from trivial to difficult.The differences often were in the instances for which one of the judges fails to see the right context under which the inference could hold.To minimize disagreements, the judges went through an extensive round of training.To that end, the 1000 instances (x, pj, y) were split into DEV and TEST sets, 500 in each.The two judges trained themselves by annotating DEV together.The TEST set was then annotated separately to verify the inter-annotator agreement and to verify whether the task is well-defined.The kappa statistic (Siegel and Castellan Jr. 1988) was x = 0.72.For the 70 disagreements between the judges, a third judge acted as an adjudicator.We compare our ISP algorithms to the following baselines: One alternative to our approach is admit instances on the Web using literal search queries.We investigated this technique but discarded it due to subtle yet critical issues with pattern canonicalization that resulted in rejecting nearly all inferences.However, we are investigating other ways of using Web corpora for this task.For each ISP algorithm and parameter combination, we constructed a confusion matrix on the development set and computed the system sensitivity, specificity and accuracy as described in Section 4.3.This resulted in 180 experiments on the development set.For each ISP algorithm and semantic class source, we selected the best parameter combinations according to the following criteria: textual entailment researchers have commented that inference rule collections like DIRT are difficult to use due to low precision.Many have asked for filtered versions that remove incorrect inferences even at the cost of removing correct inferences.In response, we show results for the system achieving the best sensitivity while maintaining at least 90% specificity on the DEV set.We evaluated the selected systems on the TEST set.Table 1 summarizes the quality of the systems selected according to the Accuracy criterion.The best performing system, ISP.IIM.v, performed statistically significantly better than all three baselines.The best system according to the 90%Specificity criteria was ISP.JIM, which coincidentally has the highest accuracy for that model as shown in Table 16.This result is very promising for researchers that require highly accurate inference rules since they can use ISP.JIM and expect to recall 17% of the correct inferences by only accepting false positives 12% of the time.Figures 1a) and 1b) present the full confusion matrices for the most accurate and highly specific systems, with both systems selected on the DEV set.The most accurate system was ISP.IIM.v, which is the most permissive of the algorithms.This suggests that a larger corpus for learning SPs may be needed to support stronger performance on the more restrictive methods.The system in Figure 1b), selected for maximizing sensitivity while maintaining high specificity, was 70% correct in predicting correct inferences.Figure 2 illustrates the ROC curve for all our systems and parameter combinations on the TEST set.ROC curves plot the true positive rate against the false positive rate.The near-diagonal line plots the three baseline systems.Several trends can be observed from this figure.First, systems using the semantic classes from WordNet tend to perform less well than systems using CBC classes.As discussed in Section 4.2, we used a very simplistic extraction of semantic classes from WordNet.The results in Figure 2 serve as a lower bound on what could be achieved with a better extraction from WordNet.Upon inspection of instances that WordNet got incorrect but CBC got correct, it seemed that CBC had a much higher lexical coverage than WordNet.For example, several of the instances contained proper names as either the X or Y argument (WordNet has poor proper name coverage).When an argument is not covered by any class, the inference is rejected.Figure 2 also illustrates how our three different ISP algorithms behave.The strictest filters, ISP.JIM and ISP.IIM.n, have the poorest overall performance but, as expected, have a generally very low rate of false positives.ISP.IIM.v, which is a much more permissive filter because it does not require both arguments of a relation to match, has generally many more false positives but has an overall better performance.We did not include in Figure 2 an analysis of the minimum, maximum, and average ranking strategies presented in Section 3.2 since they generally produced nearly identical results.For the most accurate system, ISP.IIM.v, we explored the impact of the cutoff threshold i on the sensitivity, specificity, and accuracy, as shown in Figure 3.Rather than step the values by 10% as we did on the DEV set, here we stepped the threshold value by 2% on the TEST set.The more permissive values of i increase sensitivity at the expense of specificity.Interestingly, the overall accuracy remained fairly constant across the entire range of i, staying within 0.05 of the maximum of 0.62 achieved at i=30%.Finally, we manually inspected several incorrect inferences that were missed by our filters.A common source of errors was due to the many incorrect “antonymy” inference rules generated by DIRT, such as “X is rejected in Y”=>“X is accepted in Y”.This recognized problem in DIRT occurs because of the distributional hypothesis assumption used to form the inference rules.Our ISP algorithms suffer from a similar quandary since, typically, antonymous relations take the same sets of arguments for X (and Y).For these cases, ISP algorithms learn many selectional preferences that accept the same types of entities as those that made DIRT learn the inference rule in the first place, hence ISP will not filter out many incorrect inferences.We presented algorithms for learning what we call inferential selectional preferences, and presented evidence that learning selectional preferences can be useful in filtering out incorrect inferences.Future work in this direction includes further exploration of the appropriate inventory of semantic classes used as SP’s.This work constitutes a step towards better understanding of the interaction of selectional preferences and inferences, bridging these two aspects of semantics.
Dependency parsing has been a topic of active research in natural language processing in the last several years.An important part of this research effort are the CoNLL 2006 and 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), which allowed for a comparison of many algorithms and approaches for this task on many languages.Current dependency parsers can be categorized into three families: local-and-greedy transitionbased parsers (e.g., MALTPARSER (Nivre et al., 2006)), globally optimized graph-based parsers (e.g., MSTPARSER (McDonald et al., 2005)), and hybrid systems (e.g., (Sagae and Lavie, 2006b; Nivre and McDonald, 2008)), which combine the output of various parsers into a new and improved parse, and which are orthogonal to our approach.Transition-based parsers scan the input from left to right, are fast (O(n)), and can make use of rich feature sets, which are based on all the previously derived structures.However, all of their decisions are very local, and the strict left-to-right order implies that, while the feature set can use rich structural information from the left of the current attachment point, it is also very restricted in information to the right of the attachment point: traditionally, only the next two or three input tokens are available to the parser.This limited look-ahead window leads to error propagation and worse performance on root and long distant dependencies relative to graphbased parsers (McDonald and Nivre, 2007).Graph-based parsers, on the other hand, are globally optimized.They perform an exhaustive search over all possible parse trees for a sentence, and find the highest scoring tree.In order to make the search tractable, the feature set needs to be restricted to features over single edges (first-order models) or edges pairs (higher-order models, e.g.(McDonald and Pereira, 2006; Carreras, 2007)).There are several attempts at incorporating arbitrary tree-based features but these involve either solving an ILP problem (Riedel and Clarke, 2006) or using computationally intensive sampling-based methods (Nakagawa, 2007).As a result, these models, while accurate, are slow (O(n3) for projective, first-order models, higher polynomials for higher-order models, and worse for richer tree-feature models).We propose a new category of dependency parsing algorithms, inspired by (Shen et al., 2007): nondirectional easy-first parsing.This is a greedy, deterministic parsing approach, which relaxes the leftto-right processing order of transition-based parsing algorithms.By doing so, we allow the explicit incorporation of rich structural features derived from both sides of the attachment point, and implicitly take into account the entire previously derived structure of the whole sentence.This extension allows the incorporation of much richer features than those available to transition- and especially to graph-based parsers, and greatly reduces the locality of transition-based algorithm decisions.On the other hand, it is still a greedy, best-first algorithm leading to an efficient implementation.We present a concrete O(nlogn) parsing algorithm, which significantly outperforms state-of-theart transition-based parsers, while closing the gap to graph-based parsers.When humans comprehend a natural language sentence, they arguably do it in an incremental, left-toright manner.However, when humans consciously annotate a sentence with syntactic structure, they hardly ever work in fixed left-to-right order.Rather, they start by building several isolated constituents by making easy and local attachment decisions and only then combine these constituents into bigger constituents, jumping back-and-forth over the sentence and proceeding from easy to harder phenomena to analyze.When getting to the harder decisions a lot of structure is already in place, and this structure can be used in deciding a correct attachment.Our parser follows a similar kind of annotation process: starting from easy attachment decisions, and proceeding to harder and harder ones.When making later decisions, the parser has access to the entire structure built in earlier stages.During the training process, the parser learns its own notion of easy and hard, and learns to defer specific kinds of decisions until more structure is available.Our (projective) parsing algorithm builds the parse tree bottom up, using two kinds of actions: ATTACHLEFT(i) and ATTACHRIGHT(i) .These actions are applied to a list of partial structures p1, ... , pk, called pending, which is initialized with the n words of the sentence w1, ... , wn.Each action connects the heads of two neighbouring structures, making one of them the parent of the other, and removing the daughter from the list of partial structures.ATTACHLEFT(i) adds a dependency edge (pi, pi+1) and removes pi+1 from the list.ATTACHRIGHT(i) adds a dependency edge (pi+1, pi) and removes pi from the list.Each action shortens the list of partial structures by 1, and after n−1 such actions, the list contains the root of a connected projective tree over the sentence.Figure 1 shows an example of parsing the sentence “a brown fox jumped with joy”.The pseudocode of the algorithm is given in Algorithm 1.At each step the algorithm chooses a specific action/location pair using a function score(ACTION(i)), which assign scores to action/location pairs based on the partially built structures headed by pi and pi+1, as well as neighbouring structures.The score() function is learned from data.This scoring function reflects not only the correctness of an attachment, but also the order in which attachments should be made.For example, consider the attachments (brown,fox) and (joy,with) in Figure (1.1).While both are correct, the scoring function prefers the (adjective,noun) attachment over the (prep,noun) attachment.Moreover, the attachment (jumped,with), while correct, receives a negative score for the bare preposition “with” (Fig.(1.1) - (1.4) ), and a high score once the verb has its subject and the PP “with joy” is built (Fig.(1.5) ).Ideally, we would like to score easy and reliable attachments higher than harder less likely attachments, thus performing attachments in order of confidence.This strategy allows us both to limit the extent of error propagation, and to make use of richer contextual information in the later, harder attachments.Unfortunately, this kind of ordering information is not directly encoded in the data.We must, therefore, learn how to order the decisions.We first describe the learning algorithm (Section 4) and a feature representation (Section 5) which enables us to learn an effective scoring function.We use a linear model score(x) = w� · O(x), where O(x) is a feature representation and w� is a weight vector.We write Oact(i) to denote the feature representation extracted for action act at location i.The model is trained using a variant of the structured perceptron (Collins, 2002), similar to the algorithm of (Shen et al., 2007; Shen and Joshi, 2008).As usual, we use parameter averaging to prevent the perceptron from overfitting.The training algorithm is initialized with a zero parameter vector w. The algorithm makes several passes over the data.At each pass, we apply the training procedure given in Algorithm 2 to every sentence in the training set.At training time, each sentence is parsed using the parsing algorithm and the current w. Whenever an invalid action is chosen by the parsing algorithm, it is not performed (line 6).Instead, we update the parameter vector w� by decreasing the weights of the features associated with the invalid action, and increasing the weights for the currently highest scoring valid action.1 We then proceed to parse the sentence with the updated values.The process repeats until a valid action is chosen.Note that each single update does not guarantee that the next chosen action is valid, or even different than the previously selected action.Yet, this is still an aggressive update procedure: we do not leave a sentence until our parameters vector parses it cor1We considered 3 variants of this scheme: (1) using the highest scoring valid action, (2) using the leftmost valid action, and (3) using a random valid action.The 3 variants achieved nearly identical accuracy, while (1) converged somewhat faster than the other two. rectly, and we do not proceed from one partial parse to the next until w� predicts a correct location/action pair.However, as the best ordering, and hence the best attachment point is not known to us, we do not perform a single aggressive update step.Instead, our aggressive update is performed incrementally in a series of smaller steps, each pushing w� away from invalid attachments and toward valid ones.This way we integrate the search of confident attachments into the learning process.The function isValid(act(i), gold, arcs) (line 4) is used to decide if the chosen action/location pair is valid.It returns True if two conditions apply: (a) (pi, pj) is present in gold, (b) all edges (❑, pj) in gold are also in arcs.In words, the function verifies that the proposed edge is indeed present in the gold parse and that the suggested daughter already found all its own daughters.2The feature representation for an action can take into account the original sentence, as well as the entire parse history: Oact(i) above is actually O(act(i), sentence, Arcs, pending).We use binary valued features, and each feature is conjoined with the type of action.When designing the feature representation, we keep in mind that our features should not only direct the parser toward desired actions and away from undesired actions, but also provide the parser with means of choosing between several desired actions.We want the parser to be able to defer some desired actions until more structure is available and a more informed prediction can be made.This desire is reflected in our choice of features: some of our features are designed to signal to the parser the presence of possibly “incomplete” structures, such as an incomplete phrase, a coordinator without conjuncts, and so on.When considering an action ACTION(i), we limit ourselves to features of partial structures around the attachment point: pi−2, pi−1, pi, pi+1, pi+2, pi+s, that is the two structures which are to be attached by the action (pi and pi+1), and the two neighbouring structures on each side3.While these features encode local context, it is local in terms of syntactic structure, and not purely in terms of sentence surface form.This let us capture some, though not all, long-distance relations.For a partial structure p, we use wp to refer to the head word form, tp to the head word POS tag, and lcp and rcp to the POS tags of the left-most and right-most child of p respectively.All our prepositions (IN) and coordinators (CC) are lexicalized: for them, tp is in fact wptp.We define structural, unigram, bigram and ppattachment features.The structural features are: the length of the structures (lenp), whether the structure is a word (contains no children: ncp), and the surface distance between structure heads (Apipj).The unigram and bigram features are adapted from the feature set for left-to-right Arc-Standard dependency parsing described in (Huang et al., 2009).We extended that feature set to include the structure on both sides of the proposed attachment point.In the case of unigram features, we added features that specify the POS of a word and its left-most and right-most children.These features provide the nondirectional model with means to prefer some attachment points over others based on the types of structures already built.In English, the left- and rightmost POS-tags are good indicators of constituency.The pp-attachment features are similar to the bigram features, but fire only when one of the structures is headed by a preposition (IN).These features are more lexicalized than the regular bigram features, and include also the word-form of the rightmost child of the PP (rcwp).This should help the model learn lexicalized attachment preferences such as (hit, with-bat).Figure 2 enumerate the feature templates we use.The parsing algorithm (Algorithm 1) begins with n+1 disjoint structures (the words of the sentence + ROOT symbol), and terminates with one connected structure.Each iteration of the main loop connects two structures and removes one of them, and so the loop repeats for exactly n times.The argmax in line 5 selects the maximal scoring action/location pair.At iteration i, there are n − i locations to choose from, and a naive computation of the argmax is O(n), resulting in an O(n2) algorithm.Each performed action changes the partial structures and with it the extracted features and the computed scores.However, these changes are limited to a fixed local context around the attachment point of the action.Thus, we observe that the feature extraction and score calculation can be performed once for each action/location pair in a given sentence, and reused throughout all the iterations.After each iteration we need to update the extracted features and calculated scores for only k locations, where k is a fixed number depending on the window size used in the feature extraction, and usually k « n. Using this technique, we perform only (k + 1)n feature extractions and score calculations for each sentence, that is O(n) feature-extraction operations per sentence.Given the scores for each location, the argmax can then be computed in O(logn) time using a heap, resulting in an O(nlogn) algorithm: n iterations, where the first iteration involves n feature extraction operations and n heap insertions, and each subsequent iteration involves k feature extractions and heap updates.We note that the dominating factor in polynomialtime discriminative parsers, is by far the featureextraction and score calculation.It makes sense to compare parser complexity in terms of these operations only.4 Table 1 compares the complexity of our 4Indeed, in our implementation we do not use a heap, and opt instead to find the argmax using a simple O(n) max operation.This O(n2) algorithm is faster in practice than the heap based one, as both are dominated by the O(n) feature extraction, while the cost of the O(n) max calculationis negligible compared to the constants involved in heap maintenance.In terms of feature extraction and score calculation operations, our algorithm has the same cost as traditional shift-reduce (MALT) parsers, and is an order of magnitude more efficient than graph-based (MST) parsers.Beam-search decoding for left-toright parsers (Zhang and Clark, 2008) is also linear, but has an additional linear dependence on the beamsize.The reported results in (Zhang and Clark, 2008) use a beam size of 64, compared to our constant of k = 6.Our Python-based implementation5 (the perceptron is implemented in a C extension module) parses about 40 tagged sentences per second on an Intel based MacBook laptop.We evaluate the parser using the WSJ Treebank.The trees were converted to dependency structures with the Penn2Malt conversion program,6 using the headfinding rules from (Yamada and Matsumoto, 2003).7 We use Sections 2-21 for training, Section 22 for development, and Section 23 as the final test set.The text is automatically POS tagged using a trigram HMM based POS tagger prior to training and parsing.Each section is tagged after training the tagger on all other sections.The tagging accuracy of the tagger is 96.5 for the training set and 96.8 for the test set.While better taggers exist, we believe that the simpler HMM tagger overfits less, and is more representative of the tagging performance on nonWSJ corpus texts.Parsers We evaluate our parser against the transition-based MALT parser and the graph-based MST parser.We use version 1.2 of MALT parser8, with the settings used for parsing English in the CoNLL 2007 shared task.For the MST parser9, we use the default first-order, projective parser settings, which provide state-of-the-art results for English.All parsers are trained and tested on the same data.Our parser is trained for 20 iterations.Evaluation Measures We evaluate the parsers using three common measures: (unlabeled) Accuracy: percentage of tokens which got assigned their correct parent.Root: The percentage of sentences in which the ROOT attachment is correct.Complete: the percentage of sentences in which all tokens were assigned their correct parent.Unlike most previous work on English dependency parsing, we do not exclude punctuation marks from the evaluation.Results are presented in Table 2.Our nondirectional easy-first parser significantly outperforms the left-to-right greedy MALT parser in terms of accuracy and root prediction, and significantly outperforms both parsers in terms of exact match.The globally optimized MST parser is better in rootprediction, and slightly better in terms of accuracy.We evaluated the parsers also on the English dataset from the CoNLL 2007 shared task.While this dataset is also derived from the WSJ Treebank, it differs from the previous dataset in two important aspects: it is much smaller in size, and it is created using a different conversion procedure, which is more linguistically adequate.For these experiments, we use the dataset POS tags, and the same parameters as in the previous set of experiments: we train the nondirectional parser for 20 iterations, with the same feature set.The CoNLL dataset contains some nonprojective constructions.MALT and MST deal with non-projectivity.For the non-directional parser, we projectivize the training set prior to training using the procedure described in (Carreras, 2007).Results are presented in Table 3.While all models suffer from the move to the smaller dataset and the more challenging annotation scheme, the overall story remains the same: the nondirectional parser is better than MALT but not as good as MST in terms of parent-accuracy and root prediction, and is better than both MALT and MST in terms of producing complete correct parses.That the non-directional parser has lower accuracy but more exact matches than the MST parser can be explained by it being a deterministic parser, and hence still vulnerable to error propagation: once it erred once, it is likely to do so again, resulting in low accuracies for some sentences.However, due to the easy-first policy, it manages to parse many sentences without a single error, which lead to higher exact-match scores.The non-directional parser avoids error propagation by not making the initial error.On average, the non-directional parser manages to assign correct heads to over 60% of the tokens before making its first error.The MST parser would have ranked 5th in the shared task, and NONDIR would have ranked 7th.The better ranking systems in the shared task are either higher-order global models, beam-search based systems, or ensemble-based systems, all of which are more complex and less efficient than the NONDIR parser.Parse Diversity The parses produced by the nondirectional parser are different than the parses produced by the graph-based and left-to-right parsers.To demonstrate this difference, we performed an Oracle experiment, in which we combine the output of several parsers by choosing, for each sentence, the parse with the highest score.Results are presented in Table 4.A non-oracle blending of MALT+MST+NONDIR using Sagae and Lavie’s (2006) simplest combination method assigning each component the same weight, yield an accuracy of 90.8 on the CoNLL 2007 English dataset, making it the highest scoring system among the participants.When we investigate the POS category of mistaken instances, we see that for all parsers, nodes with structures of depth 2 and more which are assigned an incorrect head are predominantly PPs (headed by ’IN’), followed by NPs (headed by ’NN’).All parsers have a hard time dealing with PP attachment, but MST parser is better at it than NONDIR, and both are better than MALT.Looking further at the mistaken instances, we notice a tendency of the PP mistakes of the NONDIR parser to involve, before the PP, an NP embedded in a relative clause.This reveals a limitation of our parser: recall that for an edge to be built, the child must first acquire all its own children.This means that in case of relative clauses such as “I saw the boy [who ate the pizza] with my eyes”, the parser must decide if the PP “with my eyes” should be attached to “the pizza” or not before it is allowed to build parts of the outer NP (“the boy who... ”).In this case, the verb “saw” and the noun “boy” are both outside of the sight of the parser when deciding on the PP attachment, and it is forced to make a decision in ignorance, which, in many cases, leads to mistakes.The globally optimized MST does not suffer as much from such cases.We plan to address this deficiency in future work.Deterministic shift-reduce parsers are restricted by a strict left-to-right processing order.Such parsers can rely on rich syntactic information on the left, but not on the right, of the decision point.They are forced to commit early, and suffer from error propagation.Our non-directional parser addresses these deficiencies by discarding the strict left-to-right processing order, and attempting to make easier decisions before harder ones.Other methods of dealing with these deficiencies were proposed over the years: Several Passes Yamada and Matsumoto’s (2003) pioneering work introduces a shift-reduce parser which makes several left-to-right passes over a sentence.Each pass adds structure, which can then be used in subsequent passes.Sagae and Lavie (2006b) extend this model to alternate between left-to-right and right-to-left passes.This model is similar to ours, in that it attempts to defer harder decisions to later passes over the sentence, and allows late decisions to make use of rich syntactic information (built in earlier passes) on both sides of the decision point.However, the model is not explicitly trained to optimize attachment ordering, has an O(n2) runtime complexity, and produces results which are inferior to current single-pass shift-reduce parsers.Beam Search Several researchers dealt with the early-commitment and error propagation of deterministic parsers by extending the greedy decisions with various flavors of beam-search (Sagae and Lavie, 2006a; Zhang and Clark, 2008; Titov and Henderson, 2007).This approach works well and produces highly competitive results.Beam search can be incorporated into our parser as well.We leave this investigation to future work.Strict left-to-right ordering is also prevalent in sequence tagging.Indeed, one major influence on our work is Shen et.al.’s bi-directional POS-tagging algorithm (Shen et al., 2007), which combines a perceptron learning procedure similar to our own with beam search to produce a state-of-the-art POStagger, which does not rely on left-to-right processing.Shen and Joshi (2008) extends the bidirectional tagging algorithm to LTAG parsing, with good results.We build on top of that work and present a concrete and efficient greedy non-directional dependency parsing algorithm.Structure Restrictions Eisner and Smith (2005) propose to improve the efficiency of a globally optimized parser by posing hard constraints on the lengths of arcs it can produce.Such constraints pose an explicit upper bound on parser accuracy.10 Our parsing model does not pose such restrictions.Shorter edges are arguably easier to predict, and our parses builds them early in time.However, it is also capable of producing long dependencies at later stages in the parsing process.Indeed, the distribution of arc lengths produced by our parser is similar to those produced by the MALT and MST parsers.We presented a non-directional deterministic dependency parsing algorithm, which is not restricted by the left-to-right parsing order of other deterministic parsers.Instead, it works in an easy-first order.This strategy allows using more context at each decision.The parser learns both what and when to connect.We show that this parsing algorithm significantly outperforms a left-to-right deterministic algorithm.While it still lags behind globally optimized parsing algorithms in terms of accuracy and root prediction, it is much better in terms of exact match, and much faster.As our parsing framework can easily and efficiently utilize more structural information than globally optimized parsers, we believe that with some enhancements and better features, it can outperform globally optimized algorithms, especially when more structural information is needed, such as for morphologically rich languages.Moreover, we show that our parser produces different structures than those produced by both left-to-right and globally optimized parsers, making it a good candidate for inclusion in an ensemble system.Indeed, a simple combination scheme of graph-based, left-to-right and non-directional parsers yields state-of-the-art results on English dependency parsing on the CoNLL 2007 dataset.We hope that further work on this non-directional parsing framework will pave the way to better understanding of an interesting cognitive question: which kinds of parsing decisions are hard to make, and which linguistic constructs are hard to analyze?
A defining feature of neural network language models is their representation of words as high dimensional real valued vectors.In these models (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010), words are converted via a learned lookuptable into real valued vectors which are used as the inputs to a neural network.As pointed out by the original proposers, one of the main advantages of these models is that the distributed representation achieves a level of generalization that is not possible with classical n-gram language models; whereas a n-gram model works in terms of discrete units that have no inherent relationship to one another, a continuous space model works in terms of word vectors where similar words are likely to have similar vectors.Thus, when the model parameters are adjusted in response to a particular word or word-sequence, the improvements will carry over to occurrences of similar words and sequences.By training a neural network language model, one obtains not just the model itself, but also the learned word representations, which may be used for other, potentially unrelated, tasks.This has been used to good effect, for example in (Collobert and Weston, 2008; Turian et al., 2010) where induced word representations are used with sophisticated classifiers to improve performance in many NLP tasks.In this work, we find that the learned word representations in fact capture meaningful syntactic and semantic regularities in a very simple way.Specifically, the regularities are observed as constant vector offsets between pairs of words sharing a particular relationship.For example, if we denote the vector for word i as xi, and focus on the singular/plural relation, we observe that x apple−xapples ≈ xcar−xcars, xfamily−xfamilies ≈ xcar−xcars, and so on.Perhaps more surprisingly, we find that this is also the case for a variety of semantic relations, as measured by the SemEval 2012 task of measuring relation similarity.Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics The remainder of this paper is organized as follows.In Section 2, we discuss related work; Section 3 describes the recurrent neural network language model we used to obtain word vectors; Section 4 discusses the test sets; Section 5 describes our proposed vector offset method; Section 6 summarizes our experiments, and we conclude in Section 7.Distributed word representations have a long history, with early proposals including (Hinton, 1986; Pollack, 1990; Elman, 1991; Deerwester et al., 1990).More recently, neural network language models have been proposed for the classical language modeling task of predicting a probability distribution over the “next” word, given some preceding words.These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b).This early work demonstrated outstanding performance in terms of word-prediction, but also the need for more computationally efficient models.This has been addressed by subsequent work using hierarchical prediction (Morin and Bengio, 2005; Mnih and Hinton, 2009; Le et al., 2011; Mikolov et al., 2011b; Mikolov et al., 2011a).Also of note, the use of distributed topic representations has been studied in (Hinton and Salakhutdinov, 2006; Hinton and Salakhutdinov, 2010), and (Bordes et al., 2012) presents a semantically driven method for obtaining word representations.The word representations we study are learned by a recurrent neural network language model (Mikolov et al., 2010), as illustrated in Figure 1.This architecture consists of an input layer, a hidden layer with recurrent connections, plus the corresponding weight matrices.The input vector w(t) represents input word at time t encoded using 1-of-N coding, and the output layer y(t) produces a probability distribution over words.The hidden layer s(t) maintains a representation of the sentence history.The input vector w(t) and the output vector y(t) have dimensionality of the vocabulary.The values in the hidden and output layers are computed as follows: where In this framework, the word representations are found in the columns of U, with each column representing a word.The RNN is trained with backpropagation to maximize the data log-likelihood under the model.The model itself has no knowledge of syntax or morphology or semantics.Remarkably, training such a purely lexical model to maximize likelihood will induce word representations with striking syntactic and semantic properties.To understand better the syntactic regularities which are inherent in the learned representation, we created a test set of analogy questions of the form “a is to b as c is to ” testing base/comparative/superlative forms of adjectives; singular/plural forms of common nouns; possessive/non-possessive forms of common nouns; and base, past and 3rd person present tense forms of verbs.More precisely, we tagged 267M words of newspaper text with Penn Treebank POS tags (Marcus et al., 1993).We then selected 100 of the most frequent comparative adjectives (words labeled JJR); 100 of the most frequent plural nouns (NNS); 100 of the most frequent possessive nouns (NN POS); and 100 of the most frequent base form verbs (VB).We then systematically generated analogy questions by randomly matching each of the 100 words with 5 other words from the same category, and creating variants as indicated in Table 1.The total test set size is 8000.The test set is available online.1 In addition to syntactic analogy questions, we used the SemEval-2012 Task 2, Measuring Relation Similarity (Jurgens et al., 2012), to estimate the extent to which RNNLM word vectors contain semantic information.The dataset contains 79 fine-grained word relations, where 10 are used for training and 69 testing.Each relation is exemplified by 3 or 4 gold word pairs.Given a group of word pairs that supposedly have the same relation, the task is to order the target pairs according to the degree to which this relation holds.This can be viewed as another analogy problem.For example, take the ClassInclusion:Singular Collective relation with the prototypical word pair clothing:shirt.To measure the degree that a target word pair dish:bowl has the same relation, we form the analogy “clothing is to shirt as dish is to bowl,” and ask how valid it is.As we have seen, both the syntactic and semantic tasks have been formulated as analogy questions.We have found that a simple vector offset method based on cosine distance is remarkably effective in solving these questions.In this method, we assume relationships are present as vector offsets, so that in the embedding space, all pairs of words sharing a particular relation are related by the same constant offset.This is illustrated in Figure 2.In this model, to answer the analogy question a:b c:d where d is unknown, we find the embedding vectors xa, xb, xc (all normalized to unit norm), and compute y = xb − xa + xc. y is the continuous space representation of the word we expect to be the best answer.Of course, no word might exist at that exact position, so we then search for the word whose embedding vector has the greatest cosine similarity to y and output it: provided.We have explored several related methods and found that the proposed method performs well for both syntactic and semantic relations.We note that this measure is qualitatively similar to relational similarity model of (Turney, 2012), which predicts similarity between members of the word pairs (xb, xd), (x,-, xd) and dis-similarity for (xa, xd).To evaluate the vector offset method, we used vectors generated by the RNN toolkit of Mikolov (2012).Vectors of dimensionality 80, 320, and 640 were generated, along with a composite of several systems, with total dimensionality 1600.The systems were trained with 320M words of Broadcast News data as described in (Mikolov et al., 2011a), and had an 82k vocabulary.Table 2 shows results for both RNNLM and LSA vectors on the syntactic task.LSA was trained on the same data as the RNN.We see that the RNN vectors capture significantly more syntactic regularity than the LSA vectors, and do remarkably well in an absolute sense, answering more than one in three questions correctly.2 In Table 3 we compare the RNN vectors with those based on the methods of Collobert and Weston (2008) and Mnih and Hinton (2009), as implemented by (Turian et al., 2010) and available online 3 Since different words are present in these datasets, we computed the intersection of the vocabularies of the RNN vectors and the new vectors, and restricted the test set and word vectors to those.This resulted in a 36k word vocabulary, and a test set with 6632 questions.Turian’s Collobert and Weston based vectors do poorly on this task, whereas the Hierarchical Log-Bilinear Model vectors of (Mnih and Hinton, 2009) do essentially as well as the RNN vectors.These representations were trained on 37M words of data and this may indicate a greater robustness of the HLBL method.We conducted similar experiments with the semantic test set.For each target word pair in a relation category, the model measures its relational similarity to each of the prototypical word pairs, and then uses the average as the final score.The results are evaluated using the two standard metrics defined in the task, Spearman’s rank correlation coefficient p and MaxDiff accuracy.In both cases, larger values are better.To compare to previous systems, we report the average over all 69 relations in the test set.From Table 4, we see that as with the syntactic regularity study, the RNN-based representations perform best.In this case, however, Turian’s CW vectors are comparable in performance to the HLBL vectors.With the RNN vectors, the performance improves as the number of dimensions increases.Surprisingly, we found that even though the RNN vectors are not trained or tuned specifically for this task, the model achieves better results (RNN-320, RNN640 & RNN-1600) than the previously best performing system, UTD-NB (Rink and Harabagiu, 2012).We have presented a generally applicable vector offset method for identifying linguistic regularities in continuous space word representations.We have shown that the word representations learned by a RNNLM do an especially good job in capturing these regularities.We present a new dataset for measuring syntactic performance, and achieve almost 40% correct.We also evaluate semantic generalization on the SemEval 2012 task, and outperform the previous state-of-the-art.Surprisingly, both results are the byproducts of an unsupervised maximum likelihood training criterion that simply operates on a large amount of text data.
Over the past decade, most work in the field of information extraction has shifted from complex rule-based, systems designed to handle a wide variety of semantic phenomena including quantification, anaphora, aspect and modality (e.g.Alshawi (1992)), to simpler finite-state or statistical systems such as Hobbs et al. (1997) and Miller et al.(1998).Much of the evaluation of these systems has been conducted on extracting relations for specific semantic domains such as corporate acquisitions or terrorist events in the framework of the DARPA Message Understanding Conferences.Recently, attention has turned to creating corpora annotated for argument structure for a broader range of predicates.The Propbank project at the University of Pennsylvania (Kingsbury and Palmer, 2002) and the FrameNet project at the International Computer Science Institute (Baker et al., 1998) share the goal of documenting the syntactic realization of arguments of the predicates of the general English lexicon by annotating a corpus with semantic roles.Even for a single predicate, semantic arguments often have multiple syntactic realizations, as shown by the following paraphrases: Correctly identifying the semantic roles of the sentence constituents is a crucial part of interpreting text, and in addition to forming an important part of the information extraction problem, can serve as an intermediate step in machine translation or automatic summarization.In this paper, we examine how the information provided by modern statistical parsers such as Collins (1997) and Charniak (1997) contributes to solving this problem.We measure the effect of parser accuracy on semantic role prediction from parse trees, and determine whether a complete tree is indeed necessary for accurate role prediction.Gildea and Jurafsky (2002) describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles.The system first passed sentences through an automatic parser, extracted syntactic features from the parses, and estimated probabilities for semantic roles from the syntactic and lexical features.Both training and test sentences were automatically parsed, as no hand-annotated parse trees were available for the corpus.While the errors introduced by the parser no doubt negatively affected the results obtained, there was no direct way of quantifying this effect.Of the systems evaluated for the Message Understanding Conference task, Miller et al. (1998) made use of an integrated syntactic and semantic model producing a full parse tree, and achieved results comparable to other systems that did not make use of a complete parse.As in the FrameNet case, the parser was not trained on the corpus for which semantic annotations were available, and the effect of better, or even perfect, parses could not be measured.One of the differences between the two semantic annotation projects is that the sentences chosen for annotation for Propbank are from the same Wall Street Journal corpus chosen for annotation for the original Penn Treebank project, and thus hand-checked syntactic parse trees are available for the entire dataset.In this paper, we compare the performance of a system based on goldstandard parses with one using automatically generated parser output.We also examine whether it is possible that the additional information contained in a full parse tree is negated by the errors present in automatic parser output, by testing a role-labeling system based on a flat or &quot;chunked&quot; representation of the input.The results in this paper are primarily derived from the Propbank corpus, and will be compared to earlier results from the FrameNet corpus.Before proceeding to the experiments, this section will briefly describe the similarities and differences between the two sets of data.While the goals of the two projects are similar in many respects, their methodologies are quite different.FrameNet is focused on semantic frames, which are defined as schematic representation of situations involving various participants, props, and other conceptual roles (Fillmore, 1976).The project methodology has proceeded on a frameby-frame basis, that is by first choosing a semantic frame, defining the frame and its participants or frame elements, and listing the various lexical predicates which invoke the frame, and then finding example sentences of each predicate in the corpus (the British National Corpus was used) and annotating each frame element.The example sentences were chosen primarily for coverage of all the syntactic realizations of the frame elements, and simple examples of these realizations were preferred over those involving complex syntactic structure not immediate relevant to the lexical predicate itself.From the perspective of an automatic classification system, the overrepresentation of rare syntactic realizations may cause the system to perform more poorly than it might on more statistically representative data.On the other hand, the exclusion of complex examples may make the task artificially easy.Only sentences where the lexical predicate was used &quot;in frame&quot; were annotated.A word with multiple distinct senses would generally be analyzed as belonging to different frames in each sense, but may only be found in the FrameNet corpus in the sense for which a frame has been defined.It is interesting to note that the semantic frames are a helpful way of generalizing between predicates; words in the same frame have been found frequently to share the same syntactic argument structure.A more complete description of the FrameNet project can be found in (Baker et al., 1998; Johnson et al., 2001), and the ramifications for automatic classification are discussed more thoroughly in (Gildea and Jurafsky, 2002).The philosophy of the Propbank project can be likened to FrameNet without frames.While the semantic roles of FrameNet are defined at the level of the frame, in Propbank, roles are defined on a per-predicate basis.The core arguments of each predicate are simply numbered, while remaining arguments are given labels such as &quot;temporal&quot; or &quot;locative&quot;.While the two types of label names are reminiscent of the traditional argument/adjunct distinction, this is primarily as a convenience in defining roles, and no claims are intended as to optionality or other traditional argument/adjunct tests.To date, Propbank has addressed only verbs, where FrameNet includes nouns and adjectives.Propbank's annotation process has proceeded from the most to least common verbs, and all examples of each verb from the corpus are annotated.Thus, the data for each predicate are statistically representative of the corpus, as are the frequencies of the predicates themselves.Annotation takes place with reference to the Penn Treebank trees not only are annotators shown the trees when analyzing a sentence, they are constrained to assign the semantic labels to portions of the sentence corresponding to nodes in the tree.Propbank annotators tag all examples of a given verb, regardless of word sense.The tagging guidelines for a verb may contain many &quot;rolesets&quot;, corresponding to word sense at a relatively coarsegrained level.The need for multiple rolesets is determined by the roles themselves, that is, uses of the verb with different arguments are given separate rolesets.However, the preliminary version of the data used in the experiments below are not tagged for word sense, or for the roleset used.Sense tagging is planned for a second pass through the data.In many cases the roleset can be determined from the argument annotations themselves.However, we did not make any attempt to distinguish sense in our experiments, and simply attempted to predict argument labels based on the identity of the lexical predicate.In previous work using the FrameNet corpus, Gildea and Jurafsky (2002) developed a system to predict semantic roles from sentences and their parse trees as determined by the statistical parser of Collins (1997).We will briefly review their probability model before adapting the system to handle unparsed data.Probabilities of a parse constituent belonging to a given semantic role were calculated from the following features: Phrase Type: This feature indicates the syntactic type of the phrase expressing the semantic roles: examples include noun phrase (NP), verb phrase (VP), and clause (S).Phrase types were derived automatically from parse trees generated by the parser, as shown in Figure 1.The parse constituent spanning each set of words annotated as an argument was found, and the constituent's nonterminal label was taken as the phrase type.As an example of how this feature is useful, in communication frames, the SpEAKER is likely to appear as a noun phrase, Topic as a prepositional phrase or noun phrase, and MEDiUM as a prepositional phrase, as in: &quot;We talked about the proposal over the phone.&quot; When no parse constituent was found with boundaries matching those of an argument during testing, the largest constituent beginning at the argument's left boundary and lying entirely within the element was used to calculate the features.Parse Tree Path: This feature is designed to capture the syntactic relation of a constituent to the predicate.It is defined as the path from the predicate through the parse tree to the constituent in question, represented as a string of parse tree nonterminals linked by symbols indicating upward or downward movement through the tree, as shown in Figure 2.Although the path is composed as a string of symbols, our systems will treat the string as an atomic value.The path includes, as the first element of the string, the part of speech of the predicate, and, as the last element, the phrase type or syntactic category of the sentence constituent marked as an argument.Position: This feature simply indicates whether the constituent to be labeled occurs before or after the predicate defining the semantic frame.This feature is highly correlated with grammatical function, since subjects will generally appear before a verb, and objects after.This feature may overcome the shortcomings of reading grammatical function from the parse tree, as well as errors in the parser output.He ate some pancakes Figure 2: In this example, the path from the predicate ate to the argument He can be represented as VBfVPfS�NP, with f indicating upward movement in the parse tree and � downward movement.Voice: The distinction between active and passive verbs plays an important role in the connection between semantic role and grammatical function, since direct objects of active verbs correspond to subjects of passive verbs.From the parser output, verbs were classified as active or passive by building a set of 10 passive-identifying patterns.Each of the patterns requires both a passive auxiliary (some form of &quot;to be&quot; or &quot;to get&quot;) and a past participle.Head Word: Lexical dependencies provide important information in labeling semantic roles, as one might expect from their use in statistical models for parsing.Since the parser used assigns each constituent a head word as an integral part of the parsing model, the head words of the constituents can be read from the parser output.For example, in a communication frame, noun phrases headed by &quot;Bill&quot;, &quot;brother&quot;, or &quot;he&quot; are more likely to be the SpEAKER, while those headed by &quot;proposal&quot;, &quot;story&quot;, or &quot;question&quot; are more likely to be the Topic.To predict argument roles in new data, we wish to estimate the probability of each role given these five features and the predicate p: P(rlpt, path, position, voice, hw, p).Due to the sparsity of the data, it is not possible to estimate this probability from the counts in the training.Instead, we estimate probabilities from various subsets of the features, and interpolate a linear combination of the resulting distributions.The interpolation is performed over the most specific distributions for which data are available, which can be thought of as choosing the topmost distributions available from a backoff lattice, shown in We applied the same system, using the same features to a preliminary release of the Propbank data.The dataset used contained annotations for 26,138 predicate-argument structures containing 65,364 individual arguments and containing examples from 1,527 lexical predicates (types).In order to provide results comparable with the statistical parsing literature, annotations from Section 23 of the Treebank were used as the test set; all other sections were included in the training set.The system was tested under two conditions, one in which it is given the constituents which are arguments to the predicate and merely has to predict the correct role, and one in which it has to both find the arguments in the sentence and label them correctly.Results are shown in Tables 1 and 2.Although results for Propbank are lower than for FrameNet, this appears to be primarily due to the smaller number of training examples for each predicate, rather than the difference in annotation style between the two corpora.The FrameNet data contained at least ten examples from each predicate, while 17% of the Propbank data had fewer than ten training examples.Removing these examples from the test set gives 84.1% accuracy with gold-standard parses and 80.5% accuracy with automatic parses.As our path feature is a somewhat unusual way of looking at parse trees, its behavior in the system warrants a closer look.The path feature is most useful as a way of finding arguments in the unknown boundary condition.Removing the path feature from the known-boundary system results in only a small degradation in performance, from 82.3% to 81.7%.One reason for the relatively small impact may be sparseness of the feature � 7% of paths in the test set are unseen in training data.The most common values of the feature are shown in Table 3, where the first two rows correspond to standard subject and object positions.One reason for sparsity is seen in the third row: in the Treebank, the adjunction of an adverbial phrase or modal verb can cause an additional VP node to appear in our path feature.We tried two variations of the path feature to address this problem.The first collapses sequences of nodes with the same label, for example combining rows 2 and 3 of Table 3.The second variation uses only two values for the feature: NP under S (subject position), and NP under VP (object position).Neither variation improved performance in the known boundary condition.As a gauge of how closely the Propbank argument labels correspond to the path feature overall, we note that by always assigning the most common role for each path, for example always assigning ARG0 to the subject position, and using no other features, we obtain the correct role 69.4% of the time, vs. 82.3% for the complete system.Many recent information extraction systems for limited domains have relied on finite-state systems that do not build a full parse tree for the sentence being analyzed.Among such systems, (Hobbs et al., 1997) built finite-state recognizers for various entities, which were then cascaded to form recognizers for higher-level relations, while (Ray and Craven, 2001) used low-level &quot;chunks&quot; from a general-purpose syntactic analyzer as observations in a trained Hidden Markov Model.Such an approach has a large advantage in speed, as the extensive search of modern statistical parsers is avoided.It is also possible that this approach may be more robust to error than parsers.Although we expect the attachment decisions made by a parser to be relevant to determining whether a constituent of a sentence is an argument of a particular predicate, and what its relation to the predicate is, those decisions may be so frequently incorrect that a much simpler system can do just as well.In this section we test this hypothesis by comparing a system which is given only a flat, &quot;chunked&quot; representation of the input sentence to the parse-tree-based systems described above.In this representation, base-level constituent boundaries and labels are present, but there are no dependencies between constituents, as shown by the following sample sentence: (3) [NP Big investment banks] [V P refused to step] [ADVP up] [PP to] [NP the plate] [V P to support] [NP the beleaguered floor traders] [PP by] [V P buying] [NP big blocks] [PP of] [NP stock] , [NP traders] [V P say] .Our chunks were derived from the Treebank trees using the conversion described by Tjong Kim Sang and Buchholz (2000).Thus, the experiments were carried out using &quot;goldstandard&quot; rather than automatically derived chunk boundaries, which we believe will provide an upper bound on the performance of a chunkbased system.The information provided by the parse tree can be decomposed into three pieces: the constituent boundaries, the grammatical relationship between predicate and argument, expressed by our path feature, and the head word of each candidate constituent.We will examine the contribution of each of these information sources, beginning with the problem of assigning the correct role in the case where the boundaries of the arguments in the sentence are known, and then turning to the problem of finding arguments in the sentence.When the argument boundaries are known, the grammatical relationship of the the constituent to the predicate turns out to be of little value.Removing the path feature from the system described above results in only a small degradation in performance, from 82.3% to 81.7%.While the path feature serves to distinguish subjects from objects, the combination of the constituent position before or after the predicate and the active/passive voice feature serves the same purpose.However, this result still makes use of the parser output for finding the constituent's head word.We implemented a simple algorithm to guess the argument's head word from the chunked output: if the argument begins at a chunk boundary, taking the last word of the chunk, and in all other cases, taking the first word of the argument.This heuristic matches the head word read from the parse tree 77% of the the time, as it correctly identifies the final word of simple noun phrases as the head, the preposition as the head of prepositional phrases, and the complementizer as the head of sentential complements.Using this process for determining head words, the system drops to 77.0% accuracy, indicating that identifying the relevant head word from semantic role prediction is in itself an important function of the parser.This chunker-based result is only slightly lower than the 79.2% obtained using automatic parses in the known boundary condition.These results for the known boundary condition are summarized in Table 4. gold parse auto parse gold parse chunks Table 4: Summary of results for known boundary condition We might expect the information provided by the parser to be more important in identifying the arguments in the sentence than in assigning them the correct role.While it is easy to guess whether a noun phrase is a subject or object given only its position relative to the predicate, identifying complex noun phrases and determining whether they are arguments of a verb may be more difficult without the attachment information provided by the parser.To test this, we implemented a system in which the argument labels were assigned to chunks, with the path feature used by the parse-tree-based system replaced by a number expressing the distance in chunks to the left or right of the predicate.Of the 3990 arguments in our test set, only 39.8% correspond to a single chunk in the flattened sentence representation, giving an upper bound to the performance of this system.In particular, sentential complements (which comprise 11% of the data) and prepositional phrases (which comprise 10%) always correspond to more than one chunk, and therefore cannot be correctly labeled by our system which assigns roles to single chunks.In fact, this system achieves 27.6% precision and 22.0% recall.In order to see how much of the performance degradation is caused by the difficulty of finding exact argument boundaries in the chunked representation, we can relax the scoring criteria to count as correct all cases where the system correctly identifies the first chunk belonging to an argument.For example, if the system assigns the correct label to the preposition beginning a prepositional phrase, the argument will be counted as correct, even though the system does not find the argument's righthand boundary.With this scoring regime, the chunk-based system performs at 49.5% precision and 35.1% recall, still significantly lower than the 57.7% precision/50.0% recall for exact matches using automatically generated parses.Results for the unknown boundary condition are summarized in Table 5. gold parse auto parse chunk chunk, relaxed scoring Table 5: Summary of results for unknown boundary condition As an example for comparing the behavior of the tree-based and chunk-based systems, consider the following sentence, with human annotations showing the arguments of the predicate support: (4) [ARG0 Big investment banks] refused to step up to the plate to support [ARG1 the beleaguered floor traders] [MNR by buying big blocks of stock] , traders say .Our tree-based system assigned the following analysis: floor traders] [MNR by buying big blocks of stock] , traders say .In this case, the system failed to find the predicate's ARG0 relation, because it is syntactically distant from the verb support.The original Treebank syntactic tree contains a trace which would allow one to recover this relation, co-indexing the empty subject position of support with the noun phrase &quot;Big investment banks&quot;.However, our automatic parser output does not include such traces, nor does our system make use of them.The chunk-based system assigns the following argument labels: (6) Big investment banks refused to step up to [ARG0 the plate] to support [ARG1 the beleaguered floor traders] by buying big blocks of stock , traders say .Here, as before, the true ARG0 relation is not found, and it would be difficult to imagine identifying it without building a complete syntactic parse of the sentence.But now, unlike in the tree-based output, the ARG0 label is mistakenly attached to a noun phrase immediately before the predicate.The ARG1 relation in direct object position is fairly easily identifiable in the chunked representation as a noun phrase directly following the verb.The prepositional phrase expressing the Manner relation, however, is not identified by the chunk-based system.The tree-based system's path feature for this constituent is VBTVP�PP, which identifies the prepositional phrase as attaching to the verb, and increases its probability of being assigned an argument label.The chunkbased system sees this as a prepositional phrase appearing as the second chunk after the predicate.Although this may be a typical position for the Manner relation, the fact that the preposition attaches to the predicate rather than to its direct object is not represented.In interpreting these results, it is important to keep in mind the differences between this task and other information extraction datasets.In comparison to the domain-specific relations evaluated by the Message Understanding Conference (MUC) tasks, we have a wider variety of relations but fewer training instances for each.The relations may themselves be less susceptible to finite state methods.For example, a named-entity system which indentifies corporation names can go a long way towards finding the &quot;employment&quot; relation of MUC, and similarly systems for tagging genes and proteins help a great deal for relations in the biomedical domain.Both Propbank and FrameNet tend to include longer arguments with internal syntactic structure, making parsing decisions more important in finding argument boundaries.They also involve abstract relations, with a wide variety of possible fillers for each role.Our chunk-based system takes the last word of the chunk as its head word for the purposes of predicting roles, but does not make use of the identities of the chunk's other words or the intervening words between a chunk and the predicate, unlike Hidden Markov Model-like systems such as Bikel et al. (1997), McCallum et al.(2000) and Lafferty et al. (2001).While a more elaborate finite-state system might do better, it is possible that additional features would not be helpful given the small amount of data for each predicate.By using a gold-standard chunking representation, we have obtained higher performance over what could be expected from an entirely automatic system based on a flat representation of the data.We feel that our results show that statistical parsers, although computationally expensive, do a good job of providing relevant information for semantic interpretation.Not only the constituent structure but also head word information, produced as a side product, are important features.Parsers, however, still have a long way to go.Our results using hand-annotated parse trees show that improvements in parsing should translate directly into better semantic interpretations.Acknowledgments This work was undertaken with funding from the Institute for Research in Cognitive Science at the University of Pennsylvania and from the Propbank project, DoD Grant MDA904-00C-2136.
The currently best single-model statistical parser (Charniak, 1999) achieves Parseval scores of over 89% on the Penn Treebank.However, the grammar underlying the Penn Treebank is very permissive, and a parser can do well on the standard Parsevalmeasures without committing itself on certain se mantically significant decisions, such as predicting null elements arising from deletion or movement.The potential benefit of wide-coverage parsing with CCG lies in its more constrained grammar and itssimple and semantically transparent capture of ex traction and coordination.We present a number of models over syntac tic derivations of Combinatory Categorial Grammar (CCG, see Steedman (2000) and Clark et al (2002), this conference, for introduction), estimated from and tested on a translation of the Penn Treebank to a corpus of CCG normal-form derivations.CCG grammars are characterized by much larger categorysets than standard Penn Treebank grammars, distin guishing for example between many classes of verbswith different subcategorization frames.As a re sult, the categorial lexicon extracted for this purposefrom the training corpus has 1207 categories, com pared with the 48 POS-tags of the Penn Treebank.On the other hand, grammar rules in CCG are lim ited to a small number of simple unary and binary combinatory schemata such as function application and composition.This results in a smaller and less overgenerating grammar than standard PCFGs (ca.3,000 rules when instantiated with the above cate gories in sections 02-21, instead of >12,400 in the original Treebank representation (Collins, 1999)).Since CCG produces unary and binary branching trees with a very fine-grained category set, CCG Parseval scores cannot be compared with scores of standard Treebank parsers.Therefore, we alsoevaluate performance using a dependency evaluation reported by Collins (1999), which counts word word dependencies as determined by local trees and their labels.According to this metric, a local tree with parent node P, head daughter H and non-head daughter S (and position of S relative to P, ie.leftor right, which is implicit in CCG categories) de fines a hP;H;Si dependency between the head word of S, wS, and the head word of H , wH.This measureis neutral with respect to the branching factor.Fur thermore, as noted by Hockenmaier (2001), it doesnot penalize equivalent analyses of multiple modi Computational Linguistics (ACL), Philadelphia, July 2002, pp.335-342.Proceedings of the 40th Annual Meeting of the Association for Pierre Vinken ; 61 years old ; will join the board as a nonexecutive director Nov 29 N=N N ; N=N N (S[adj]nNP)nNP ; (S[dcl]nNP)=(S[b]nNP) ((S[b]nNP)=PP)=NP NP=N N PP=NP NP=N N=N N ((SnNP)n(SnNP))=N N > > > > > N N N N (SnNP)n(SnNP) > NP NP NP NP < > > NP S[adj]nNP (S[b]nNP)=PP PP > NPnNP S[b]nNP < < NP S[b]nNP > NP S[dcl]nNP < S[dcl] Figure 1: A CCG derivation in our corpus fiers.In the unlabeled case hi (where it only matters whether word a is a dependent of word b, not whatthe label of the local tree is which defines this depen dency), scores can be compared across grammars with different sets of labels and different kinds of trees.In order to compare our performance with the parser of Clark et al (2002), we also evaluate our best model according to the dependency evaluation introduced for that parser.For further discussion we refer the reader to Clark and Hockenmaier (2002) .CCGbank is a corpus of CCG normal-form derivations obtained by translating the Penn Tree bank trees using an algorithm described by Hockenmaier and Steedman (2002).Almost alltypes of construction?with the exception of gap ping and UCP (?Unlike Coordinate Phrases?)arecovered by the translation procedure, which pro cesses 98.3% of the sentences in the training corpus (WSJ sections 02-21) and 98.5% of the sentences in the test corpus (WSJ section 23).The grammar contains a set of type-changing rules similar to the lexical rules described in Carpenter (1992).Figure1 shows a derivation taken from CCGbank.Categories, such as ((S[b]nNP)=PP)=NP, encode unsaturated subcat frames.The complement-adjunct distinction is made explicit; for instance as a nonexecutive director is marked up as PP-CLR in the Tree bank, and hence treated as a PP-complement of join, whereas Nov. 29 is marked up as an NP-TMP and therefore analyzed as VP modifier.The -CLR tag is not in fact a very reliable indicator of whether a constituent should be treated as a complement, but the translation to CCG is automatic and must do the best it can with the information in the Treebank.The verbal categories in CCGbank carry features distinguishing declarative verbs (and auxiliaries) from past participles in past tense, past par ticiples for passive, bare infinitives and ing-forms.There is a separate level for nouns and noun phrases, but, like the nonterminal NP in the Penn Treebank, noun phrases do not carry any number agreement.The derivations in CCGbank are ?normal-form?in the sense that analyses involving the combinatory rules of type-raising and composition are only used when syntactically necessary.Expansion HeadCat NonHeadCat P(exp j : : : ) P(H j : : : ) P(S j : : : ) Baseline P P;exp P;exp;H + Conj P;con jP P;exp;con jP P;exp;H ;con jP + Grandparent P;GP P;GP;exp P;GP;exp;H + ? P#?L;RP P;exp#?L;RP P;exp;H#?L;RP Table 1: The unlexicalized models The models described here are all extensions of a very simple model which models derivations by a top-down tree-generating process.This model was originally described in Hockenmaier (2001), where it was applied to a preliminary version of CCGbank, and its definition is repeated here in the top row of Table 1.Given a (parent) node with category P, choose the expansion exp of P, where exp can beleaf (for lexical categories), unary (for unary ex pansions such as type-raising), left (for binary trees where the head daughter is left) or right (binary trees, head right).If P is a leaf node, generate its head word w. Otherwise, generate the category ofits head daughter H . If P is binary branching, gen erate the category of its non-head daughter S (a complement or modifier of H).The model itself includes no prior knowledge spe cific to CCG other than that it only allows unary andbinary branching trees, and that the sets of nontermi nals and terminals are not disjoint (hence the need to include leaf as a possible expansion, which acts as a stop probability).All the experiments reported in this section were conducted using sections 02-21 of CCGbank as training corpus, and section 23 as test corpus.We replace all rare words in the training data with their POS-tag.For all experiments reported here and in section 5, the frequency threshold was set to 5.LikeCollins (1999), we assume that the test data is POS tagged, and can therefore replace unknown words inthe test data with their POS-tag, which is more ap propriate for a formalism like CCG with a large set of lexical categories than one generic token for all unknown words.The performance of the baseline model is shown in the top row of table 3.For six out of the 2379 sentences in our test corpus we do not get a parse.1The reason is that a lexicon consisting of the word category pairs observed in the training corpus does not contain all the entries required to parse the test corpus.We discuss a simple, but imperfect, solution to this problem in section 7.State-of-the-art statistical parsers use many other features, or conditioning variables, such as head words, subcategorization frames, distance measures and grandparent nodes.We too can extend the baseline model described in the previous section by including more features.Like the models of Goodman (1997), the additional features in our model are generated probabilistically, whereas in the parser of Collins (1997) distance measures are assumed to be a function of the already generated structure and are not generated explicitly.In order to estimate the conditional probabilitiesof our model, we recursively smooth empirical es timates e?i of specific conditional distributions with(possible smoothed) estimates of less specific distri butions e?i
Recent work in statistical approaches to parsing and tagging has begun to consider methods which incorporate global features of candidate structures.Examples of such techniques are Markov Random Fields (Abney 1997; Della Pietra et al. 1997; Johnson et al.1999), and boosting algorithms (Freund et al. 1998; Collins 2000; Walker et al.2001).One appeal of these methods is their flexibility in incorporating features into a model: essentially any features which might be useful in discriminating good from bad structures can be included.A second appeal of these methods is that their training criterion is often discriminative, attempting to explicitly push the score or probability of the correct structure for each training sentence above the score of competing structures.This discriminative property is shared by the methods of (Johnson et al. 1999; Collins 2000), and also the Conditional Random Field methods of (Lafferty et al.2001).In a previous paper (Collins 2000), a boosting algorithm was used to rerank the output from an existing statistical parser, giving significant improvements in parsing accuracy on Wall Street Journal data.Similar boosting algorithms have been applied to natural language generation, with good results, in (Walker et al. 2001).In this paper we apply reranking methods to named-entity extraction.A state-ofthe-art (maximum-entropy) tagger is used to generate 20 possible segmentations for each input sentence, along with their probabilities.We describe a number of additional global features of these candidate segmentations.These additional features are used as evidence in reranking the hypotheses from the max-ent tagger.We describe two learning algorithms: the boosting method of (Collins 2000), and a variant of the voted perceptron algorithm, which was initially described in (Freund & Schapire 1999).We applied the methods to a corpus of over one million words of tagged web data.The methods give significant improvements over the maximum-entropy tagger (a 17.7% relative reduction in error-rate for the voted perceptron, and a 15.6% relative improvement for the boosting method).One contribution of this paper is to show that existing reranking methods are useful for a new domain, named-entity tagging, and to suggest global features which give improvements on this task.We should stress that another contribution is to show that a new algorithm, the voted perceptron, gives very credible results on a natural language task.It is an extremely simple algorithm to implement, and is very fast to train (the testing phase is slower, but by no means sluggish).It should be a viable alternative to methods such as the boosting or Markov Random Field algorithms described in previous work.Over a period of a year or so we have had over one million words of named-entity data annotated.The data is drawn from web pages, the aim being to support a question-answering system over web data.A number of categories are annotated: the usual people, organization and location categories, as well as less frequent categories such as brand-names, scientific terms, event titles (such as concerts) and so on.From this data we created a training set of 53,609 sentences (1,047,491 words), and a test set of 14,717 sentences (291,898 words).The task we consider is to recover named-entity boundaries.We leave the recovery of the categories of entities to a separate stage of processing.1 We evaluate different methods on the task through precision and recall.If a method proposes entities on the test set, and of these are correct (i.e., an entity is marked by the annotator with exactly the same span as that proposed) then the precision of a method is .Similarly, if is the total number of entities in the human annotated version of the test set, then the recall is .The problem can be framed as a tagging task – to tag each word as being either the start of an entity, a continuation of an entity, or not to be part of an entity at all (we will use the tags S, C and N respectively for these three cases).As a baseline model we used a maximum entropy tagger, very similar to the ones described in (Ratnaparkhi 1996; Borthwick et.al 1998; McCallum et al. 2000).Max-ent taggers have been shown to be highly competitive on a number of tagging tasks, such as part-of-speech tagging (Ratnaparkhi 1996), named-entity recognition (Borthwick et.al 1998), and information extraction tasks (McCallum et al. 2000).Thus the maximumentropy tagger we used represents a serious baseline for the task.We used the following features (several of the features were inspired by the approach of (Bikel et. al 1999), an HMM model which gives excellent results on named entity extraction): The word being tagged, the previous word, and the next word.The previous tag, and the previous two tags (bigram and trigram features).A compound feature of three fields: (a) Is the word at the start of a sentence?; (b) does the word occur in a list of words which occur more frequently as lower case rather than upper case words in a large corpus of text?(c) the type of the first letter of the word, where is defined as ‘A’ if is a capitalized letter, ‘a’ if is a lower-case letter, ‘0’ if is a digit, and otherwise.For example, if the word Animal is seen at the start of a sentence, and it occurs in the list of frequent lower-cased words, then it would be mapped to the feature 1-1-A.The word with each character mapped to its .For example, G.M. would be mapped to A.A., and Animal would be mapped to Aaaaaa.The word with each character mapped to its type, but repeated consecutive character types are not repeated in the mapped string.For example, Animal would be mapped to Aa, G.M. would again be mapped to A.A..The tagger was applied and trained in the same way as described in (Ratnaparkhi 1996).The feature templates described above are used to create a set of binary features , where is the tag, and is the “history”, or context.An example is if t = S and the word being tagged = “Mr.” otherwise , defining a conditional distribution over the tags given a history as The parameters are trained using Generalized Iterative Scaling.Following (Ratnaparkhi 1996), we only include features which occur 5 times or more in training data.In decoding, we use a beam search to recover 20 candidate tag sequences for each sentence (the sentence is decoded from left to right, with the top 20 most probable hypotheses being stored at each point).As a baseline we trained a model on the full 53,609 sentences of training data, and decoded the 14,717 sentences of test data.This gave 20 candidates per The parameters of the model are for test sentence, along with their probabilities.The baseline method is to take the most probable candidate for each test data sentence, and then to calculate precision and recall figures.Our aim is to come up with strategies for reranking the test data candidates, in such a way that precision and recall is improved.In developing a reranking strategy, the 53,609 sentences of training data were split into a 41,992 sentence training portion, and a 11,617 sentence development set.The training portion was split into 5 sections, and in each case the maximum-entropy tagger was trained on 4/5 of the data, then used to decode the remaining 1/5.The top 20 hypotheses under a beam search, together with their log probabilities, were recovered for each training sentence.In a similar way, a model trained on the 41,992 sentence set was used to produce 20 hypotheses for each sentence in the development set. generator, and hashes them to integers.For example, suppose the three strings WE=Gen Xer, WE=The Day They Shot John Lennon, WE=Dougherty Arts Center were hashed to 100, 250, and 500 respectively.Conceptually, the candidate is represented by a large number of features for where is the number of distinct feature strings in training data.In this example, only take the value, all other features being zero.We now introduce some notation with which to describe the full set of global features.First, we assume the following primitives of an input candidate: for is the’th tag in the tagged sequence.The module we describe in this section generates global features for each candidate tagged sequence.As input it takes a sentence, along with a proposed segmentation (i.e., an assignment of a tag for each word in the sentence).As output, it produces a set of feature strings.We will use the following tagged sentence as a running example in this section: An example feature type is simply to list the full strings of entities that appear in the tagged input.In this example, this would give the three features WE=Gen Xer WE=The Day They Shot John Lennon WE=Dougherty Arts Center Here WE stands for “whole entity”.Throughout this section, we will write the features in this format.The start of the feature string indicates the feature type (in this case WE), followed by =.Following the type, there are generally 1 or more words or other symbols, which we will separate with the symbol .A seperate module in our implementation takes the strings produced by the global-feature for is if begins with a lowercase letter, otherwise. for is a transformation of , where the transformation is applied in the same way as the final feature type in the maximum entropy tagger.Each character in the word is mapped to its , but repeated consecutive character types are not repeated in the mapped string.For example, Animal would be mapped to Aa in this feature, G.M. would again be mapped to A.A.. for is the same as , but has an additional flag appended.The flag indicates whether or not the word appears in a dictionary of words which appeared more often lower-cased than capitalized in a large corpus of text.In our example, Animal appears in the lexicon, but G.M. does not, so the two values for would be Aa1 and A.A.0 respectively.In addition, and are all defined to be NULL if or .Most of the features we describe are anchored on entity boundaries in the candidate segmentation.We will use “feature templates” to describe the features that we used.As an example, suppose that an entity seen in a candidate.We take the entity to span words inclusive in the candidate. is seen from words to inclusive in a segmentation.Then the WE feature described in the previous section can be generated by the template WE= Applying this template to the three entities in the running example generates the three feature strings described in the previous section.As another example, consider the template FF= .This will generate a feature string for each of the entities in a candidate, this time using the values rather than .For the full set of feature templates that are anchored around entities, see figure 1.A second set of feature templates is anchored around quotation marks.In our corpus, entities (typically with long names) are often seen surrounded by quotes.For example, “The Day They Shot John Lennon”, the name of a band, appears in the running example.Define to be the index of any double quotation marks in the candidate, to be the index of the next (matching) double quotation marks if they appear in the candidate.Additionally, define to be the index of the last word beginning with a lower case letter, upper case letter, or digit within the quotation marks.The first set of feature templates tracks the values of for the words within quotes:2 Q= Q2= 2We only included these features if , to prevent an explosion in the length of feature strings., , and .The values for and would be and (these features are derived from The and Lennon, which respectively do and don’t appear in the capitalization lexicon).This would give QF= and QF2= .At this point, we have fully described the representation used as input to the reranking algorithms.The maximum-entropy tagger gives 20 proposed segmentations for each input sentence.Each candidate is represented by the log probability from the tagger, as well as the values of the global features for .In the next section we describe algorithms which blend these two sources of information, the aim being to improve upon a strategy which just takes the candidate from The next set of feature templates are sensitive to whether the entire sequence between quotes is tagged as a named entity.Define to be if S, and =C for (i.e., if the sequence of words within the quotes is tagged as a single entity).Also define to be the number of upper cased words within the quotes, to be the number of lower case words, and to be if , otherwise.Then two other templates are: QF= QF2= In the “The Day They Shot John Lennon” example we would have provided that the entire sequence within quotes was tagged as an entity.Additionally,This section introduces notation for the reranking task.The framework is derived by the transformation from ranking problems to a margin-based classification problem in (Freund et al. 1998).It is also related to the Markov Random Field methods for parsing suggested in (Johnson et al. 1999), and the boosting methods for parsing in (Collins 2000).We consider the following set-up: Training data is a set of example input/output pairs.In tagging we would have training examples where each is a sentence and each is the correct sequence of tags for that sentence.We assume some way of enumerating a set of candidates for a particular sentence.We use to denote the’th candidate for the’th sentence in training data, and to denote the set of candidates for .In this paper, the top outputs from a maximum entropy tagger are used as the set of candidates.Without loss of generality we take to be the candidate for which has the most correct tags, i.e., is closest to being correct.3 is the probability that the base model assigns to .We define We assume a set of additional features, for .The features could be arbitrary functions of the candidates; our hope is to include features which help in discriminating good candidates from bad ones.Finally, the parameters of the model are a vector of parameters, ranking function is defined as This function assigns a real-valued number to a candidate .It will be taken to be a measure of the plausibility of a candidate, higher scores meaning higher plausibility.As such, it assigns a ranking to different candidate structures for the same sentence, 3In the event that multiple candidates get the same, highest score, the candidate with the highest value of log-likelihood under the baseline model is taken as . and in particular the output on a training or test example is .In this paper we take the features to be fixed, the learning problem being to choose a good setting for the parameters .In some parts of this paper we will use vector notation.Define to be the vector .Then the ranking score can also be written as where is the dot product between vectors and .The first algorithm we consider is the boosting algorithm for ranking described in (Collins 2000).The algorithm is a modification of the method in (Freund et al. 1998).The method can be considered to be a greedy algorithm for finding the parameters that minimize the loss function where as before, .The theoretical motivation for this algorithm goes back to the PAC model of learning.Intuitively, it is useful to note that this loss function is an upper bound on the number of “ranking errors”, a ranking error being a case where an incorrect candidate gets a higher value for than a correct candidate.This follows because for all , , where we define to be for , and otherwise.Hence where .Note that the number of ranking errors is .As an initial step, is set to be and all other parameters for are set to be zero.The algorithm then proceeds for iterations ( is usually chosen by cross validation on a development set).At each iteration, a single feature is chosen, and its weight is updated.Suppose the current parameter values are , and a single feature is chosen, its weight being updated through an increment, i.e., .Then the new loss, after this parameter update, will be the tagger with the highest score for ...The where .The boosting algorithm chooses the feature/update pair which is optimal in terms of minimizing the loss function, i.e., and then makes the update .Figure 2 shows an algorithm which implements this greedy procedure.See (Collins 2000) for a full description of the method, including justification that the algorithm does in fact implement the update in Eq.1 at each iteration.4 The algorithm relies on the following arrays: Thus is an index from features to correct/incorrect candidate pairs where the ’th feature takes value on the correct candidate, and value on the incorrect candidate.The array is a similar index from features to examples.The arrays and are reverse indices from training examples to features.Figure 3 shows the training phase of the perceptron algorithm, originally introduced in (Rosenblatt 1958).The algorithm maintains a parameter vector , which is initially set to be all zeros.The algorithm then makes a pass over the training set, at each training example storing a parameter vector for .The parameter vector is only modified when a mistake is made on an example.In this case the update is very simple, involving adding the difference of the offending examples’ representations ( in the figure).See (Cristianini and Shawe-Taylor 2000) chapter 2 for discussion of the perceptron algorithm, and theory justifying this method for setting the parameters.In the most basic form of the perceptron, the parameter values are taken as the final parameter settings, and the output on a new test example with for is simply the highest .Input: A set of candidates for , A sequence of parameter vectors for Initialization: Set for ( stores the number of votes for ) scoring candidate under these parameter values, i.e., where .(Freund & Schapire 1999) describe a refinement of the perceptron, the voted perceptron.The training phase is identical to that in figure 3.Note, however, that all parameter vectors for are stored.Thus the training phase can be thought of as a way of constructing different parameter settings.Each of these parameter settings will have its own highest ranking candidate, where .The idea behind the voted perceptron is to take each of the parameter settings to “vote” for a candidate, and the candidate which gets the most votes is returned as the most likely candidate.See figure 4 for the algorithm.5We applied the voted perceptron and boosting algorithms to the data described in section 2.3.Only features occurring on 5 or more distinct training sentences were included in the model.This resulted precision, recall, F-measure.Figures in parantheses are relative improvements in error rate over the maximum-entropy model.All figures are percentages. in 93,777 distinct features.The two methods were trained on the training portion (41,992 sentences) of the training set.We used the development set to pick the best values for tunable parameters in each algorithm.For boosting, the main parameter to pick is the number of rounds, .We ran the algorithm for a total of 300,000 rounds, and found that the optimal value for F-measure on the development set occurred after 83,233 rounds.For the voted perceptron, the representation was taken to be a vector where is a parameter that influences the relative contribution of the log-likelihood term versus the other features.A value of was found to give the best results on the development set.Figure 5 shows the results for the three methods on the test set.Both of the reranking algorithms show significant improvements over the baseline: a 15.6% relative reduction in error for boosting, and a 17.7% relative error reduction for the voted perceptron.In our experiments we found the voted perceptron algorithm to be considerably more efficient in training, at some cost in computation on test examples.Another attractive property of the voted perceptron is that it can be used with kernels, for example the kernels over parse trees described in (Collins and Duffy 2001; Collins and Duffy 2002).(Collins and Duffy 2002) describe the voted perceptron applied to the named-entity data in this paper, but using kernel-based features rather than the explicit features described in this paper.See (Collins 2002) for additional work using perceptron algorithms to train tagging models, and a more thorough description of the theory underlying the perceptron algorithm applied to ranking problems.A question regarding the approaches in this paper is whether the features we have described could be incorporated in a maximum-entropy tagger, giving similar improvements in accuracy.This section discusses why this is unlikely to be the case.The problem described here is closely related to the label bias problem described in (Lafferty et al. 2001).One straightforward way to incorporate global features into the maximum-entropy model would be to introduce new features which indicated whether the tagging decision in the history creates a particular global feature.For example, we could introduce a feature if t = N and this decision creates an LWLC=1 feature otherwise As an example, this would take the value if its was tagged as N in the following context,because tagging its as N in this context would create an entity whose last word was not capitalized, i.e., University for.Similar features could be created for all of the global features introduced in this paper.This example also illustrates why this approach is unlikely to improve the performance of the maximum-entropy tagger.The parameter associated with this new feature can only affect the score for a proposed sequence by modifying at the point at which .In the example, this means that the LWLC=1 feature can only lower the score for the segmentation by lowering the probability of tagging its as N. But its has almost probably of not appearing as part of an entity, so should be almost whether is or in this context!The decision which effectively created the entity University for was the decision to tag for as C, and this has already been made.The independence assumptions in maximum-entropy taggers of this form often lead points of local ambiguity (in this example the tag for the word for) to create globally implausible structures with unreasonably high scores.See (Collins 1999) section 8.4.2 for a discussion of this problem in the context of parsing.Acknowledgements Many thanks to Jack Minisi for annotating the named-entity data used in the experiments.Thanks also to Nigel Duffy, Rob Schapire and Yoram Singer for several useful discussions.
Coreference resolution is the process of linking together multiple expressions of a given entity.The key to solve this problem is to determine the antecedent for each referring expression in a document.In coreference resolution, it is common that two or more candidates compete to be the antecedent of an anaphor (Mitkov, 1999).Whether a candidate is coreferential to an anaphor is often determined by the competition among all the candidates.So far, various algorithms have been proposed to determine the preference relationship between two candidates.Mitkov’s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators to rank the candidates.And centering algorithms (Brennan et al., 1987; Strube, 1998; Tetreault, 2001), sort the antecedent candidates based on the ranking of the forward-looking or backwardlooking centers.In recent years, supervised machine learning approaches have been widely used in coreference resolution (Aone and Bennett, 1995; McCarthy, 1996; Soon et al., 2001; Ng and Cardie, 2002a), and have achieved significant success.Normally, these approaches adopt a single-candidate model in which the classifier judges whether an antecedent candidate is coreferential to an anaphor with a confidence value.The confidence values are generally used as the competition criterion for the antecedent candidates.For example, the “Best-First” selection algorithms (Aone and Bennett, 1995; Ng and Cardie, 2002a) link the anaphor to the candidate with the maximal confidence value (above 0.5).One problem of the single-candidate model, however, is that it only takes into account the relationships between an anaphor and one individual candidate at a time, and overlooks the preference relationship between candidates.Consequently, the confidence values cannot accurately represent the true competition criterion for the candidates.In this paper, we present a competition learning approach to coreference resolution.Motivated by the research work by Connolly et al. (1997), our approach adopts a twin-candidate model to directly learn the competition criterion for the antecedent candidates.In such a model, a classifier is trained based on the instances formed by an anaphor and a pair of its antecedent candidates.The classifier is then used to determine the preference between any two candidates of an anaphor encountered in a new document.The candidate that wins the most comparisons is selected as the antecedent.In order to reduce the computational cost and data noises, our approach also employs a candidate filter to eliminate the invalid or irrelevant candidates.The layout of this paper is as follows.Section 2 briefly describes the single-candidate model and analyzes its limitation.Section 3 proposes in details the twin-candidate model and Section 4 presents our coreference resolution approach based on this model.Section 5 reports and discusses the experimental results.Section 6 describes related research work.Finally, conclusion is given in Section 7.The main idea of the single-candidate model for coreference resolution is to recast the resolution as a binary classification problem.During training, a set of training instances is generated for each anaphor in an annotated text.An instance is formed by the anaphor and one of its antecedent candidates.It is labeled as positive or negative based on whether or not the candidate is tagged in the same coreferential chain of the anaphor.After training, a classifier is ready to resolve the NPs1 encountered in a new document.For each NP under consideration, every one of its antecedent candidates is paired with it to form a test instance.The classifier returns a number between 0 and 1 that indicates the likelihood that the candidate is coreferential to the NP.The returned confidence value is commonly used as the competition criterion to rank the candidate.Normally, the candidates with confidences less than a selection threshold (e.g.0.5) are discarded.Then some algorithms are applied to choose one of the remaining candidates, if any, as the antecedent.For example, “Closest-First” (Soon et al., 2001) selects the candidate closest to the anaphor, while “Best-First” (Aone and Bennett, 1995; Ng and Cardie, 2002a) selects the candidate with the maximal confidence value.One limitation of this model, however, is that it only considers the relationships between a NP encountered and one of its candidates at a time during its training and testing procedures.The confidence value reflects the probability that the candidate is coreferential to the NP in the overall distribution2, but not the conditional probability when the candidate is concurrent with other competitors.Consequently, the confidence values are unreliable to represent the true competition criterion for the candidates.To illustrate this problem, just suppose a data set where an instance could be described with four exclusive features: F1, F2, F3 and F4.The ranking of candidates obeys the following rule: Here CSFi (1 <_ i <_ 4 ) is the set of antecedent candidates with the feature Fi on.The mark of “>>” denotes the preference relationship, that is, the candidates in CSF1 is preferred to those in CSF2, and to those in CSF3 and CSF4.Let CF2 and CF3 denote the class value of a leaf node “F2 = 1” and “F3 = 1”, respectively.It is possible that CF2 < CF3, if the anaphors whose candidates all belong to CSF3 or CSF4 take the majority in the training data set.In this case, a candidate in CSF3 would be assigned a larger confidence value than a candidate in CSF2.This nevertheless contradicts the ranking rules.If during resolution, the candidates of an anaphor all come from CSF2 or CSF3, the anaphor may be wrongly linked to a candidate in CSF3 rather than in CSF2.Different from the single-candidate model, the twin-candidate model aims to learn the competition criterion for candidates.In this section, we will introduce the structure of the model in details.Consider an anaphor ana and its candidate set candidate_set, {C1, C2, ..., Ck}, where Cj is closer to ana than Ci if j > i.Suppose positive_set is the set of candidates that occur in the coreferential chain of ana, and negative_set is the set of candidates not in the chain, that is, negative_set = candidate_set - positive_set.The set of training instances based on ana, inst_set, is defined as follows: From the above definition, an instance is formed by an anaphor, one positive candidate and one negative candidate.For each instance, inst(ci, cj, ana), the candidate at the first position, Ci, is closer to the anaphor than the candidate at the second position, Cj.A training instance inst(ci, cj , ana) is labeled as positive if Ci ∈ positive-set and Cj ∈ negative-set; or negative if Ci ∈ negative-set and Cj ∈ positiveset.See the following example: Any design to link China's accession to the WTO with the missile tests1 was doomed to failure.“If some countries2 try to block China TO accession, that will not be popular and will fail to win the support of other countries3” she said.Although no governments4 have suggested formal sanctions5 on China over the missile tests6, the United States has called them7 “provocative and reckless” and other countries said they could threaten Asian stability.In the above text segment, the antecedent candidate set of the pronoun “them?” consists of six candidates highlighted in Italics.Among the candidates, Candidate 1 and 6 are in the coreferential chain of “them?”, while Candidate 2, 3, 4, 5 are not.Thus, eight instances are formed for “them?”: Here the instances in the first line are negative, while those in the second line are all positive.A feature vector is specified for each training or testing instance.Similar to those in the singlecandidate model, the features may describe the lexical, syntactic, semantic and positional relationships of an anaphor and any one of its candidates.Besides, the feature set may also contain intercandidate features characterizing the relationships between the pair of candidates, e.g. the distance between the candidates in the number distances or paragraphs.Based on the feature vectors generated for each anaphor encountered in the training data set, a classifier can be trained using a certain machine learning algorithm, such as C4.5, RIPPER, etc.Given the feature vector of a test instance inst(ci, cj , ana ) (i > j), the classifier returns the positive class indicating that Ci is preferred to Cj as the antecedent of ana; or negative indicating that Cj is preferred.Let CR( inst(ci , cj , ana)) denote the classification result for an instance inst(ci, cj , ana ) .The antecedent of an anaphor is identified using the algorithm shown in Figure 1.Input: ana: the anaphor under consideration candidate_set: the set of antecedent candidates of ana, {C1, C2,...,Ck} for i = 1 to K do Score[ i ] = 0; for i = K downto 2 do for j = i – 1 downto 1 do if CR( inst( ci , cj, ana )) = = positive then While the realization and the structure of the twincandidate model are significantly different from the single-candidate model, the single-candidate model in fact can be regarded as a special case of the twin-candidate model.To illustrate this, just consider a virtual “blank” candidate C0 such that we could convert an instance inst(ci, ana ) in the single-candidate model to an instance inst(ci , c0, ana) in the twin-candidate model.Let inst(ci , c0, ana) have the same class label as inst(ci , ana) , that is, inst(ci , c0, ana) is positive if Ci is the antecedent of ana; or negative if not.Apparently, the classifier trained on the instance set { inst(ci , ana) }, T1, is equivalent to that trained on { inst(ci , c0, ana) }, T2.T1 and T2 would assign the same class label for the test instances inst(ci, ana ) and inst(ci, c0, ana ) , respectively.That is to say, determining whether Ci is coreferential to ana by T1 in the single-candidate model equals to determining whether Ci is better than C0 w.r.t ana by T2 in the twin-candidate model.Here we could take C0 as a “standard candidate”.While the classification in the single-candidate model can find its interpretation in the twincandidate model, it is not true vice versa.Consequently, we can safely draw the conclusion that the twin-candidate model is more powerful than the single-candidate model in characterizing the relationships among an anaphor and its candidates.Our competition learning approach adopts the twin-candidate model introduced in the Section 3.The main process of the approach is as follows: To determine the boundary of the noun phrases, a pipeline of Nature Language Processing components are applied to an input raw text: Among them, named entity recognition, part-ofspeech tagging and text chunking apply the same Hidden Markov Model (HMM) based engine with error-driven learning capability (Zhou and Su, 2000 & 2002).The named entity recognition component recognizes various types of MUC-style named entities, i.e., organization, location, person, date, time, money and percentage.For our study, in this paper we only select those features that can be obtained with low annotation cost and high reliability.All features are listed in Table 1 together with their respective possible values.For a NP under consideration, all of its preceding NPs could be the antecedent candidates.Nevertheless, since in the twin-candidate model the number of instances for a given anaphor is about the square of the number of its antecedent candidates, the computational cost would be prohibitively large if we include all the NPs in the candidate set.Moreover, many of the preceding NPs are irrelevant or even invalid with regard to the anaphor.These data noises may hamper the training of a goodperformanced classifier, and also damage the accuracy of the antecedent selection: too many comparisons are made between incorrect candidates.Therefore, in order to reduce the computational cost and data noises, an effective candidate filtering strategy must be applied in our approach.During training, we create the candidate set for each anaphor with the following filtering algorithm: Features describing the two candidates During resolution, we filter the candidates for each encountered pronoun in the same way as during training.That is, we only consider the NPs in the current and the preceding 2 sentences.Such a context window is reasonable as the distance between a pronominal anaphor and its antecedent is generally short.In the MUC-6 data set, for example, the immediate antecedents of 95% pronominal anaphors can be found within the above distance.Comparatively, candidate filtering for nonpronouns during resolution is complicated.A potential problem is that for each non-pronoun under consideration, the twin-candidate model always chooses a candidate as the antecedent, even though all of the candidates are “low-qualified”, that is, unlikely to be coreferential to the non-pronoun under consideration.In fact, the twin-candidate model in itself can identify the qualification of a candidate.We can compare every candidate with a virtual “standard candidate”, C0.Only those better than C0 are deemed qualified and allowed to enter the “round robin”, whereas the losers are eliminated.As we have discussed in Section 3.5, the classifier on the pairs of a candidate and C0 is just a singlecandidate classifier.Thus, we can safely adopt the single-candidate classifier as our candidate filter.The candidate filtering algorithm during resolution is as follows:Our coreference resolution approach is evaluated on the standard MUC-6 (1995) and MUC-7 (1998) data set.For MUC-6, 30 “dry-run” documents annotated with coreference information could be used as training data.There are also 30 annotated training documents from MUC-7.For testing, we utilize the 30 standard test documents from MUC-6 and the 20 standard test documents from MUC-7.In the experiment we compared our approach with the following research works: Among them, S-List, a version of centering algorithm, uses well-defined heuristic rules to rank the antecedent candidates; Ng and Cardie’s approach employs the standard single-candidate model and “Best-First” rule to select the antecedent; Connolly et al.’s approach also adopts the twin-candidate model, but their approach lacks of candidate filtering strategy and uses greedy linear search to select the antecedent (See “Related work” for details).We constructed three baseline systems based on the above three approaches, respectively.For comparison, in the baseline system 2 and 3, we used the similar feature set as in our system (see table 1).Table 2 and 3 show the performance of different approaches in the pronoun and non-pronoun resolution, respectively.In these tables we focus on the abilities of different approaches in resolving an anaphor to its antecedent correctly.The recall measures the number of correctly resolved anaphors over the total anaphors in the MUC test data set, and the precision measures the number of correct anaphors over the total resolved anaphors.The F-measure F=2*RP/(R+P) is the harmonic mean of precision and recall.The experimental result demonstrates that our competition learning approach achieves a better performance than the baseline approaches in resolving pronominal anaphors.As shown in Table 2, our approach outperforms Ng and Cardie’s singlecandidate based approach by 3.7 and 5.4 in Fmeasure for MUC-6 and MUC-7, respectively.Besides, compared with Strube’s S-list algorithm, our approach also achieves gains in the F-measure by 3.2 (MUC-6), and 1.6 (MUC-7).In particular, our approach obtains significant improvement (21.1 for MUC-6, and 13.1 for MUC-7) over Connolly et al.’s twin-candidate based approach.Compared with the gains in pronoun resolution, the improvement in non-pronoun resolution is slight.As shown in Table 3, our approach resolves non-pronominal anaphors with the recall of 51.3 (39.7) and the precision of 90.4 (87.6) for MUC-6 (MUC-7).In contrast to Ng and Cardie’s approach, the performance of our approach improves only 0.3 (0.6) in recall and 0.5 (1.2) in precision.The reason may be that in non-pronoun resolution, the coreference of an anaphor and its candidate is usually determined only by some strongly indicative features such as alias, apposition, string-matching, etc (this explains why we obtain a high precision but a low recall in non-pronoun resolution).Therefore, most of the positive candidates are coreferential to the anaphors even though they are not the “best”.As a result, we can only see comparatively slight difference between the performances of the two approaches.Although Connolly et al.’s approach also adopts the twin-candidate model, it achieves a poor performance for both pronoun resolution and nonpronoun resolution.The main reason is the absence of candidate filtering strategy in their approach (this is why the recall equals to the precision in the tables).Without candidate filtering, the recall may rise as the correct antecedents would not be eliminated wrongly.Nevertheless, the precision drops largely due to the numerous invalid NPs in the candidate set.As a result, a significantly low Fmeasure is obtained in their approach.Table 4 summarizes the overall performance of different approaches to coreference resolution.Different from Table 2 and 3, here we focus on whether a coreferential chain could be correctly identified.For this purpose, we obtain the recall, the precision and the F-measure using the standard MUC scoring program (Vilain et al. 1995) for the coreference resolution task.Here the recall means the correct resolved chains over the whole coreferential chains in the data set, and precision means the correct resolved chains over the whole resolved chains.In line with the previous experiments, we see reasonable improvement in the performance of the coreference resolution: compared with the baseline approach based on the single-candidate model, the F-measure of approach increases from 69.4 to 71.3 for MUC-6, and from 58.7 to 60.2 for MUC-7.A similar twin-candidate model was adopted in the anaphoric resolution system by Connolly et al. (1997).The differences between our approach and theirs are: (1) In Connolly et al.’s approach, all the preceding NPs of an anaphor are taken as the antecedent candidates, whereas in our approach we use candidate filters to eliminate invalid or irrelevant candidates.(2) The antecedent identification in Connolly et al.’s approach is to apply the classifier to successive pairs of candidates, each time retaining the better candidate.However, due to the lack of strong assumption of transitivity, the selection procedure is in fact a greedy search.By contrast, our approach evaluates a candidate according to the times it wins over the other competitors.Comparatively this algorithm could lead to a better solution.(3) Our approach makes use of more indicative features, such as Appositive, Name Alias, String-matching, etc.These features are effective especially for non-pronoun resolution.In this paper we have proposed a competition learning approach to coreference resolution.We started with the introduction of the singlecandidate model adopted by most supervised machine learning approaches.We argued that the confidence values returned by the single-candidate classifier are not reliable to be used as ranking criterion for antecedent candidates.Alternatively, we presented a twin-candidate model that learns the competition criterion for antecedent candidates directly.We introduced how to adopt the twincandidate model in our competition learning approach to resolve the coreference problem.Particularly, we proposed a candidate filtering algorithm that can effectively reduce the computational cost and data noises.The experimental results have proved the effectiveness of our approach.Compared with the baseline approach using the single-candidate model, the F-measure increases by 1.9 and 1.5 for MUC-6 and MUC-7 data set, respectively.The gains in the pronoun resolution contribute most to the overall improvement of coreference resolution.Currently, we employ the single-candidate classifier to filter the candidate set during resolution.While the filter guarantees the qualification of the candidates, it removes too many positive candidates, and thus the recall suffers.In our future work, we intend to adopt a looser filter together with an anaphoricity determination module (Bean and Riloff, 1999; Ng and Cardie, 2002b).Only if an encountered NP is determined as an anaphor, we will select an antecedent from the candidate set generated by the looser filter.Furthermore, we would like to incorporate more syntactic features into our feature set, such as grammatical role or syntactic parallelism.These features may be helpful to improve the performance of pronoun resolution.
Topic segmentation aims to automatically divide text documents, audio recordings, or video segments, into topically related units.While extensive research has targeted the problem of topic segmentation of written texts and spoken monologues, few have studied the problem of segmenting conversations with many participants (e.g., meetings).In this paper, we present an algorithm for segmenting meeting transcripts.This study uses recorded meetings of typically six to eight participants, in which the informal style includes ungrammatical sentences and overlapping speakers.These meetings generally do not have pre-set agendas, and the topics discussed in the same meeting may or may not related.The meeting segmenter comprises two components: one that capitalizes on word distribution to identify homogeneous units that are topically cohesive, and a second component that analyzes conversational features of meeting transcripts that are indicative of topic shifts, like silences, overlaps, and speaker changes.We show that integrating features from both components with a probabilistic classifier (induced with c4.5rules) is very effective in improving performance.In Section 2, we review previous approaches to the segmentation problem applied to spoken and written documents.In Section 3, we describe the corpus of recorded meetings intended to be segmented, and the annotation of its discourse structure.In Section 4, we present our text-based segmentation component.This component mainly relies on lexical cohesion, particularly term repetition, to detect topic boundaries.We evaluated this segmentation against other lexical cohesion segmentation programs and show that the performance is state-of-theart.In the subsequent section, we describe conversational features, such as silences, speaker change, and other features like cue phrases.We present a machine learning approach for integrating these conversational features with the text-based segmentation module.Experimental results show a marked improvement in meeting segmentation with the incorporation of both sets of features.We close with discussions and conclusions.Existing approaches to textual segmentation can be broadly divided into two categories.On the one hand, many algorithms exploit the fact that topic segments tend to be lexically cohesive.Embodiments of this idea include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999).Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997).In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg and Nakatani, 1998; Beeferman et al., 1999; T¨ur et al., 2001).Such indicators include long pauses, shifts in speaking rate, great range in F0 and intensity, and higher maximum accent peak.These approaches use different learning mechanisms to combine features, including decision trees (Grosz and Hirschberg, 1992; Passonneau and Litman, 1997; T¨ur et al., 2001) exponential models (Beeferman et al., 1999) or other probabilistic models (Hajime et al., 1998; Reynar, 1999).We have evaluated our segmenter on the ICSI Meeting corpus (Janin et al., 2003).This corpus is one of a growing number of corpora with human-to-human multi-party conversations.In this corpus, recordings of meetings ranged primarily over three different recurring meeting types, all of which concerned speech or language research.1 The average duration is 60 minutes, with an average of 6.5 participants.They were transcribed, and each conversation turn was marked with the speaker, start time, end time, and word content.From the corpus, we selected 25 meetings to be segmented, each by at least three subjects.We opted for a linear representation of discourse, since finer-grained discourse structures (e.g.(Grosz and Sidner, 1986)) are generally considered to be difficult to mark reliably.Subjects were asked to mark each speaker change (potential boundary) as either boundary or non-boundary.In the resulting annotation, the agreed segmentation based on majority opinion contained 7.5 segments per meeting on average, while the average number of potential boundaries is 770.We used Cochran’s Q (1950) to evaluate the agreement among annotators.Cochran’s test evaluates the null hypothesis that the number of subjects assigning a boundary at any position is randomly distributed.The test shows that the interjudge reliability is significant to the 0.05 level for 19 of the meetings, which seems to indicate that segment identification is a feasible task.2Previous work on discourse segmentation of written texts indicates that lexical cohesion is a strong indicator of discourse structure.Lexical cohesion is a linguistic property that pertains to speech as well, and is a linguistic phenomenon that can also be exploited in our case: while our data does not have the same kind of syntactic and rhetorical structure as written text, we nonetheless expect that information from the written transcription alone should provide indications about topic boundaries.In this section, we describe our work on LCseg, a topic segmenter based on lexical cohesion that can handle both speech and text, but that is especially designed to generate the lexical cohesion feature used in the feature-based segmentation described in Section 5.LCseg computes lexical chains, which are thought to mirror the discourse structure of the underlying text (Morris and Hirst, 1991).We ignore synonymy and other semantic relations, building a restricted model of lexical chains consisting of simple term repetitions, hypothesizing that major topic shifts are likely to occur where strong term repetitions start and end.While other relations between lexical items also work as cohesive factors (e.g. between a term and its super-ordinate), the work on linear topic segmentation reporting the most promising results account for term repetitions alone (Choi, 2000; Utiyama and Isahara, 2001).The preprocessing steps of LCseg are common to many segmentation algorithms.The input document is first tokenized, non-content words are removed, and remaining words are stemmed using an extension of Porter’s stemming algorithm (Xu and Croft, 1998) that conflates stems using corpus statistics.Stemming will allow our algorithm to more accurately relate terms that are semantically close.The core algorithm of LCseg has two main parts: a method to identify and weight strong term repetitions using lexical chains, and a method to hypothesize topic boundaries given the knowledge of multiple, simultaneous chains of term repetitions.A term is any stemmed content word within the text.A lexical chain is constructed to consist of all repetitions ranging from the first to the last appearance of the term in the text.The chain is divided into subchains when there is a long hiatus of h consecutive sentences with no occurrence of the term, where h is determined experimentally.For each hiatus, a new division is made and thus, we avoid creating weakly linked chains.For all chains that have been identified, we use a weighting scheme that we believe is appropriate to the task of inducing the topical or sub-topical structure of text.The weighting scheme depends on two factors: Frequency: chains containing more repeated terms receive a higher score.Compactness: shorter chains receive a higher weight than longer ones.If two chains of different lengths contain the same number of terms, we assign a higher score to the shortest one.Our assumption is that the shorter one, being more compact, seems to be a better indicator of lexical cohesion.3 We apply a variant of a metric commonly used in information retrieval, TF.IDF (Salton and Buckley, 1988), to score term repetitions.If R1 ... R,,, is the set of all term repetitions collected in the text, t1 ... t,,, the corresponding terms, L1 ... L,,, their respective lengths,4 and L the length of the text, the adapted metric is expressed as follows, combining frequency (freq(ti)) of a term ti and the compactness of its underlying chain: 3The latter parameter might seem controversial at first, and one might assume that longer chains should receive a higher score.However we point out that in a linear model of discourse, chains that almost span the entire text are barely indicative of any structure (assuming boundaries are only hypothesized where chains start and end).4All lengths are expressed in number of sentences.In the second part of the algorithm, we combine information from all term repetitions to compute a lexical cohesion score at each sentence break (or, in the case of spoken conversations, speaker turn break).This step of our algorithm is very similar in spirit to TextTiling (Hearst, 1994).The idea is to work with two adjacent analysis windows, each of fixed size k. For each sentence break, we determine a lexical cohesion function by computing the cosine similarity at the transition between the two windows.Instead of using word counts to compute similarity, we analyze lexical chains that overlap with the two windows.The similarity between windows (A and B) is computed with:5 where The similarity computed at each sentence break produces a plot that shows how lexical cohesion changes over time; an example is shown in Figure 1.The lexical cohesion function is then smoothed using a moving average filter, and minima become potential segment boundaries.Then, in a manner quite similar to (Hearst, 1994), the algorithm determines for every local minimum mi how sharp of a change there is in the lexical cohesion function.The algorithm looks on each side of mi for maxima of cohesion, and once it eventually finds one on each side (l and r), it computes the hypothesized segmentation probability: where LCF(x) is the value of the lexical cohesion function at x.This score is supposed to capture the sharpness of the change in lexical cohesion, and give probabilities close to 1 for breaks like sentence 179 in Figure 1.Finally, the algorithm selects the hypothesized boundaries with the highest computed probabilities.If the number of reference boundaries is unknown, the algorithm has to make a guess.It computes the 5Normalizing anything in these windows has little effect, since the cosine similarity is scale invariant, that is cosine(αxa, xb) = cosine(xa, xb) for α > 0. x-axis represent sentence indices, and y-axis represents the lexical cohesion function.The representative example presented here is segmented by LCseg with an error of Pk = 15.79, while the average performance of the algorithm is Pk = 15.31 on the WSJ test corpus (unknown number of segments). mean and the variance of the hypothesized probabilities of all potential boundaries (local minima).As we can see in Figure 1, there are many local minima that do not correspond to actual boundaries.Thus, we ignore all potential boundaries with a probability lower than plimit.For the remaining points, we compute the threshold using the average (µ) and standard deviation (σ) of the p(mi) values, and each potential boundary mi above the threshold µ−α·σ is hypothesized as a real boundary.We evaluate LCseg against two state-of-the-art segmentation algorithms based on lexical cohesion (Choi, 2000; Utiyama and Isahara, 2001).We use the error metric Pk proposed by Beeferman et al. (1999) to evaluate segmentation accuracy.It computes the probability that sentences k units (e.g. sentences) apart are incorrectly determined as being either in different segments or in the same one.Since it has been argued in (Pevzner and Hearst, 2002) that Pk has some weaknesses, we also include results according to the WindowDiff (WD) metric (which is described in the same work).A test corpus of concatenated6 texts extracted from the Brown corpus was built by Choi (2000) to evaluate several domain-independent segmentation algorithms.We reuse the same test corpus for our evaluation, in addition to two other test corpora we constructed to test how segmenters scale across genres and how they perform with texts with various number of segments.7 We designed two test corpora, each of 500 documents, using concatenated texts extracted from the TDT and WSJ corpora, ranging from 4 to 22 in number of segments.LCseg depends on several parameters.Parameter tuning was performed on three tuning corpora of one thousand texts each.8 We performed searches for the optimal settings of the four tunable parameters introduced above; the best performance was achieved with h = 11 (hiatus length for dividing a chain into parts), k = 2 (analysis window size), plimit = 0.1 and α= 2 (thresholding limits for the hypothesized boundaries).As shown in Table 1, our algorithm is significantly better than (Choi, 2000) (labeled C99) on all three test corpora, according to a one-sided ttest of the null hypothesis of equal mean at the 0.01 level.It is not clear whether our algorithm is better than (Utiyama and Isahara, 2001) (U00).When the number of segments is provided to the algorithms, our algorithm is significantly better than Utiyama’s on WSJ, better on Brown (but not significant), and significantly worse on TDT.When the number of boundaries is unknown, our algorithm is insignificantly worse on Brown, but significantly better on WSJ and TDT – the two corpora designed to have a varying number of segments per document.In the case of the Meeting corpus, none of the algorithms are significantly different than the others, due to the the table are the results of significance tests between U00 and LCseg.Bold-faced values are scores that are statistically significant. small test set size.In conclusion, LCseg has a performance comparable to state-of-the-art text segmentation algorithms, with the added advantage of computing a segmentation probability at each potential boundary.This information can be effectively used in the featurebased segmenter to account for lexical cohesion, as described in the next section.In the previous section, we have concentrated exclusively on the consideration of content (through lexical cohesion) to determine the structure of texts, neglecting any influence of form.In this section, we explore formal devices that are indicative of topic shifts, and explain how we use these cues to build a segmenter targeting conversational speech.Topic segmentation is reduced here to a classification problem, where each utterance break Bi is either considered a topic boundary or not.We use statistical modeling techniques to build a classifier that uses local features (e.g. cue phrases, pauses) to determine if an utterance break corresponds to a topic boundary.We chose C4.5 and C4.5rules (Quinlan, 1993), two programs to induce classification rules in the form of decision trees and production rules (respectively).C4.5 generates an unpruned decision tree, which is then analyzed by C4.5rules to generate a set of pruned production rules (it tries to find the most useful subset of them).The advantage of pruned rules over decision trees is that they are easier to analyze, and allow combination of features in the same rule (feature interactions are explicit).The greedy nature of decision rule learning algorithms implies that a large set of features can lead to bad performance and generalization capability.It is desirable to remove redundant and irrelevant features, especially in our case since we have little data labeled with topic shifts; with a large set of features, we would risk overfitting the data.We tried to restrict ourselves to features whose inclusion is motivated by previous work (pauses, speech rate) and added features that are specific to multi-speaker speech (overlap, changes in speaker activity).Cue phrases: previous work on segmentation has found that discourse particles like now, well provide valuable information about the structure of texts (Grosz and Sidner, 1986; Hirschberg and Litman, 1994; Passonneau and Litman, 1997).We analyzed the correlation between words in the meeting corpus and labeled topic boundaries, and automatically extracted utterance-initial cue phrases9 that are statistically correlated with boundaries.For every word in the meeting corpus, we counted the number of its occurrences near any topic boundary, and its number of appearances overall.Then, we performed k2 significance tests (e.g. figure 2 for okay) under the null hypothesis that no correlation exists.We selected terms whose k2 value rejected the hypothesis under a 0.01-level confidence (the rejection criterion is k2 ≥ 6.635).Finally, induced cue phrases whose usage has never been described in other work were removed (marked with ∗ in Table 3).Indeed, there is a risk that the automatically derived list of cue phrases could be too specific to the word usage in these meetings.Silences: previous work has found that major shifts in topic typically show longer silences (Passonneau and Litman, 1993; Hirschberg and Nakatani, 1996).We investigated the presence of silences in meetings and their correlation with topic boundaries, and found it necessary to make a distinction between pauses and gaps (Levinson, 1983).A pause is a silence that is attributable to a given party, for example in the middle of an adjacency pair, or when a speaker pauses in the middle of her speech.Gaps are silences not attributable to any party, and last until a speaker takes the initiative of continuing the discussion.As an approximation of this distinction, we classified a silence that follows a question or in the middle of somebody’s speech as a pause, and any other silences as a gap.While the correlation between long silences and discourse boundaries seem to be less pervasive in meetings than in other speech corpora, we have noticed that some topic boundaries are preceded (within some window) by numerous gaps.However, we found little correlation between pauses and topic boundaries.Overlaps: we also analyzed the distribution of overlapping speech by counting the average overlap rate within some window.We noticed that, many times, the beginning of segments are characterized by having little overlapping speech.Speaker change: we sometimes noticed a correlation between topic boundaries and sudden changes in speaker activity.For example, in Figure 2, it is clear that the contribution of individual speakers to the discussion can greatly change from one discourse unit to the next.We try to capture significant changes in speakership by measuring the dissimilarity between two analysis windows.For each potential boundary, we count for each speaker i the number of words that are uttered before (LZ) and after (RZ) the potential boundary (we limit our analysis to a window of fixed size).The two distributions are normalized to form two probability distributions l and r, and significant changes of speakership are detected by computing their Jensen-Shannon divergence: where D(l||r) is the KL-divergence between the two distributions.Lexical cohesion: we also incorporated the lexical cohesion function computed by LCseg as a feature of the multi-source segmenter in a manner similar to the knowledge source combination performed by (Beeferman et al., 1999) and (T¨ur et al., 2001).Note that we use both the posterior estimate computed by LCseg and the raw lexical cohesion function as features of the system.For every potential boundary BZ, the classifier analyzes features in a window surrounding BZ to decide whether it is a topic boundary or not.It is generally unclear what is the optimal window size and how features should be analyzed.Windows of various sizes can lead to different levels of prediction, and in some cases, it might be more appropriate to only extract features preceding or following BZ.We avoided making arbitrary choices of parameters; instead, for any feature F and a set F1, ... , Fn of possible ways to measure the feature (different window sizes, different directions), we picked the FZ that is in isolation the best predictor of topic boundaries (among F1, ... , Fn).Table 4 presents for each feature the analysis mode that is the most useful on the training data.We performed 25-fold cross-validation for evaluating the induced probabilistic classifier, computing the average of Pk and WD on the held-out meetings.Feature selection and decision rule learning is always performed on sets of 24 meetings, while the held-out data is used for testing.Table 5 gives some examples of the type of rules that are learned.The first rule states that if the value for the lexical cohesion (LC) function is low at the current sentence break, there is at least one CUE phrase, there is less than three seconds of silence to the left of the break,10 and a single speaker holds the floor for a longer period of time than usual to the right of the break, then we have a topic break.In general, we found that the derived rules show that lexical cohesion plays a stronger role than most other features in determining topic breaks.Nonetheless, the quantitative results summarized in table 6, which correspond to the average performance on the held-out sets, show that the integration of conversational features with the text-based segmenter outperforms either alone.We presented a domain-independent segmentation algorithm for multi-party conversation that integrates features based on content with features based on form.The learned combination of features results in a significant increase in accuracy over previous approaches to segmentation when applied to meetings.Features based on form that are likely to indicate topic shifts are automatically extracted from speech.Content based features are computed by a segmentation algorithm that utilizes a metric of lexical cohesion and that performs as well as state-ofthe-art text-based segmentation techniques.It works both with written and spoken texts.The text-based segmentation approach alone, when applied to meetings, outperforms all other segmenters, although the difference is not statistically significant.In future work, we would like to investigate the effects of adding prosodic features, such as pitch ranges, to our segmenter, as well as the effect of using errorful speech recognition transcripts as opposed to manually transcribed utterances.An implementation of our lexical cohesion segmenter is freely available for educational or research purposes.11We are grateful to Julia Hirschberg, Dan Ellis, Elizabeth Shriberg, and Mari Ostendorf for their helpful advice.We thank our ICSI project partners for granting us access to the meeting corpus and for useful discussions.This work was funded under the NSF project Mapping Meetings (IIS-012196).
Several linguistic theories, e.g.(Jackendoff, 1990) claim that semantic information in natural language texts is connected to syntactic structures.Hence, to deal with natural language semantics, the learning algorithm should be able to represent and process structured data.The classical solution adopted for such tasks is to convert syntax structures into flat feature representations which are suitable for a given learning model.The main drawback is that structures may not be properly represented by flat features.In particular, these problems affect the processing of predicate argument structures annotated in PropBank (Kingsbury and Palmer, 2002) or FrameNet (Fillmore, 1982).Figure 1 shows an example of a predicate annotation in PropBank for the sentence: &quot;Paul gives a lecture in Rome&quot;.A predicate may be a verb or a noun or an adjective and most of the time Arg 0 is the logical subject, Arg 1 is the logical object and ArgM may indicate locations, as in our example.FrameNet also describes predicate/argument structures but for this purpose it uses richer semantic structures called frames.These latter are schematic representations of situations involving various participants, properties and roles in which a word may be typically used.Frame elements or semantic roles are arguments of predicates called target words.In FrameNet, the argument names are local to a particular frame.Several machine learning approaches for argument identification and classification have been developed (Gildea and Jurasfky, 2002; Gildea and Palmer, 2002; Surdeanu et al., 2003; Hacioglu et al., 2003).Their common characteristic is the adoption of feature spaces that model predicate-argument structures in a flat representation.On the contrary, convolution kernels aim to capture structural information in term of sub-structures, providing a viable alternative to flat features.In this paper, we select portions of syntactic trees, which include predicate/argument salient sub-structures, to define convolution kernels for the task of predicate argument classification.In particular, our kernels aim to (a) represent the relation between predicate and one of its arguments and (b) to capture the overall argument structure of the target predicate.Additionally, we define novel kernels as combinations of the above two with the polynomial kernel of standard flat features.Experiments on Support Vector Machines using the above kernels show an improvement of the state-of-the-art for PropBank argument classification.On the contrary, FrameNet semantic parsing seems to not take advantage of the structural information provided by our kernels.The remainder of this paper is organized as follows: Section 2 defines the Predicate Argument Extraction problem and the standard solution to solve it.In Section 3 we present our kernels whereas in Section 4 we show comparative results among SVMs using standard features and the proposed kernels.Finally, Section 5 summarizes the conclusions.Given a sentence in natural language and the target predicates, all arguments have to be recognized.This problem can be divided into two subtasks: (a) the detection of the argument boundaries, i.e. all its compounding words and (b) the classification of the argument type, e.g.Arg0 or ArgM in PropBank or Agent and Goal in FrameNet.The standard approach to learn both detection and classification of predicate arguments is summarized by the following steps: For example, in Figure 1, for each combination of the predicate give with the nodes N, S, VP, V, NP, PP, D or IN the instances F”give”,a are generated.In case the node a exactly covers Paul, a lecture or in Rome, it will be a positive instance otherwise it will be a negative one, e.g.F”give”,”IN”.To learn the argument classifiers the T + set can be re-organized as positive T+argi and negative T−argi examples for each argument i.In this way, an individual ONE-vs-ALL classifier for each argument i can be trained.We adopted this solution as it is simple and effective (Hacioglu et al., 2003).In the classification phase, given a sentence of the test-set, all its Fp ,a are generated and classified by each individargument associated with the maximum value among the scores provided by the SVMs, i.e. argmaxiES Ci, where S is the target set of arguments.The discovery of relevant features is, as usual, a complex task, nevertheless, there is a common consensus on the basic features that should be adopted.These standard features, firstly proposed in (Gildea and Jurasfky, 2002), refer to a flat information derived from parse trees, i.e.Phrase Type, Predicate Word, Head Word, Governing Category, Position and Voice.Table 1 presents the standard features and exemplifies how they are extracted from the parse tree in Figure 1.For example, the Parse Tree Path feature represents the path in the parse-tree between a predicate node and one of its argument nodes.It is expressed as a sequence of nonterminal labels linked by direction symbols (up or down), e.g. in Figure 1, VTVPINP is the path between the predicate to give and the argument 1, a lecture.Two pairs <p1, a1> and <p2, a2> have two different Path features even if the paths differ only for a node in the parse-tree.This prevents the learning algorithm to generalize well on unseen data.In order to address this problem, the next section describes a novel kernel space for predicate argument classification.Given a vector space in Rn and a set of positive and negative points, SVMs classify vectors according to a separating hyperplane, H(x) = w�x x�+ b = 0, where w� E Rn and b E Rare learned by applying the Structural Risk Minimization principle (Vapnik, 1995).To apply the SVM algorithm to Predicate Argument Classification, we need a function O :F — Rn to map our features space F = {f1, .., f|F|} and our predicate/argument pair representation,( Fp ,a =(( Fz, into Rn, such that: Fz — O(Fz) = (01(Fz), .., On(Fz)) From the kernel theory we have that: where, Fi Vi E {1, .., l} are the training instances and the product K(Fi, Fz) =<O(Fi) · O(Fz)> is the kernel function associated with the mapping O.The simplest mapping that we can apply is O(Fz) = z� = (z1,..., zn) where zi = 1 if fi E Fz otherwise zi = 0, i.e. the characteristic vector of the set Fz with respect to F. If we choose as a kernel function the scalar product we obtain the linear kernel KL(Fx, Fz) = x�· z.Another function which is the current stateof-the-art of predicate argument classification is the polynomial kernel: Kp(Fx, Fz) = (c+x·z-)d, where c is a constant and d is the degree of the polynom.We propose two different convolution kernels associated with two different predicate argument sub-structures: the first includes the target predicate with one of its arguments.We will show that it contains almost all the standard feature information.The second relates to the sub-categorization frame of verbs.In this case, the kernel function aims to cluster together verbal predicates which have the same syntactic realizations.This provides the classification algorithm with important clues about the possible set of arguments suited for the target syntactic structure.(PAF) We consider the predicate argument structures annotated in PropBank or FrameNet as our semantic space.The smallest sub-structure which includes one predicate with only one of its arguments defines our structural feature.For example, Figure 2 illustrates the parse-tree of the sentence &quot;Paul delivers a talk in formal style&quot;.The circled substructures in (a), (b) and (c) are our semantic objects associated with the three arguments of the verb to deliver, i.e.<deliver, Arg0>, <deliver, Arg1 > and <deliver, ArgM>.Note that each predicate/argument pair is associated with only one structure, i.e.Fp,a contain only one of the circled sub-trees.Other important properties are the followings: (1) The overall semantic feature space F contains sub-structures composed of syntactic information embodied by parse-tree dependencies and semantic information under the form of predicate/argument annotation.1Fp,a was defined as the set of features of the object <p, a>.Since in our representations we have only one An example of features in Y is given in Figure 4 where the whole set of fragments, Fdeliver,Arg1, of the argument structure Fdeliver,Arg1, is shown (see also Figure 2).It is worth noting that the allowed sub-trees contain the entire (not partial) production rules.For instance, the sub-tree [NP [D a]] is excluded from the set of the Figure 4 since only a part of the production NP —* D N is used in its generation.However, this constraint does not apply to the production VP —* V NP PP along with the fragment [VP [V NP]] as the subtree [VP [PP [...]]] is not considered part of the semantic structure.Thus, in step 1, an argument structure Fp,a is mapped in a fragment set Fp,a.In step 2, this latter is mapped into x = (x1,..,x|F|) E R|F|, where xi is equal to the number of times that fi occurs in Fp,a2.In order to evaluate K((Fx), (Fz)) without evaluating the feature vector x and z we define the indicator function Ii(n) = 1 if the substructure i is rooted at node n and 0 otherwise.It follows that i(Fx) = nNx Ii(n), where Nx is the set of the Fx’s nodes.Therefore, the kernel can be written as: where Nx and Nz are the nodes in Fx and Fz, respectively.In (Collins and Duffy, 2002), it has been shown that i Ii(nx)Ii(nz) = A(nx, nz) can be computed in O(JNxJ x INzI) by the following recursive relation: guments, cannot be included one in the other.This property is important because a convolution kernel would not be effective to distinguish between an object and its sub-parts.The above object space aims to capture all the information between a predicate and one of its arguments.Its main drawback is that important structural information related to interargument dependencies is neglected.In order to solve this problem we define the SubCategorization Feature (SCF).This is the subparse tree which includes the sub-categorization frame of the target verbal predicate.For example, Figure 3 shows the parse tree of the sentence &quot;He flushed the pan and buckled his belt&quot;.The solid line describes the SCF of the predicate flush, i.e.Fflush whereas the dashed line tailors the SCF of the predicate buckle, i.e.Fbuckle.Note that SCFs are features for predicates, (i.e. they describe predicates) whereas PAF characterizes predicate/argument pairs.Once semantic representations are defined, we need to design a kernel function to estimate the similarity between our objects.As suggested in Section 2 we can map them into vectors in Rn and evaluate implicitly the scalar product among them.Given the semantic objects defined in the previous section, we design a convolution kernel in a way similar to the parse-tree kernel proposed in (Collins and Duffy, 2002).We divide our mapping  in two steps: (1) from the semantic structure space Y (i.e.PAF or SCF objects) to the set of all their possible sub-structures element in Fp,a with an abuse of notation we use it to indicate the objects themselves. where nc(nx) is the number of the children of nx and ch(n, i) is the i-th child of the node n. Note that as the productions are the same ch(nx, i) = ch(nz, i).This kind of kernel has the drawback of assigning more weight to larger structures while the argument type does not strictly depend on the size of the argument (Moschitti and Bejan, 2004).To overcome this problem we can scale the relative importance of the tree fragments using a parameter A for the cases (2) and (3), i.e.A(nx, nz) = A and respectively.It is worth noting that even if the above equations define a kernel function similar to the one proposed in (Collins and Duffy, 2002), the substructures on which it operates are different from the parse-tree kernel.For example, Figure 4 shows that structures such as [VP [V] [NP]], [VP [V delivers ] [NP]] and [VP [V] [NP [DT] [N]]] are valid features, but these fragments (and many others) are not generated by a complete production, i.e.VP —* V NP PP.As a consequence they would not be included in the parse-tree kernel of the sentence.In this section we compare standard features with the kernel based representation in order to derive useful indications for their use: First, PAK estimates a similarity between two argument structures (i.e., PAF or SCF) by counting the number of sub-structures that are in common.As an example, the similarity between the two structures in Figure 2, F”delivers”,Arg0 and F”delivers”,Arg1, is equal to 1 since they have in common only the [V delivers] substructure.Such low value depends on the fact that different arguments tend to appear in different structures.On the contrary, if two structures differ only for a few nodes (especially terminals or near terminal nodes) the similarity remains quite high.For example, if we change the tense of the verb to deliver (Figure 2) in delivered, the [VP [V delivers] [NP]] subtree will be transformed in [VP [VBD delivered] [NP]], where the NP is unchanged.Thus, the similarity with the previous structure will be quite high as: (1) the NP with all sub-parts will be matched and (2) the small difference will not highly affect the kernel norm and consequently the final score.The above property also holds for the SCF structures.For example, in Figure 3, KPAK (0(Fflush), 0(Fbuckle)) is quite high as the two verbs have the same syntactic realization of their arguments.In general, flat features do not possess this conservative property.For example, the Parse Tree Path is very sensible to small changes of parse-trees, e.g. two predicates, expressed in different tenses, generate two different Path features.Second, some information contained in the standard features is embedded in PAF: Phrase Type, Predicate Word and Head Word explicitly appear as structure fragments.For example, in Figure 4 are shown fragments like [NP [DT] [N]] or [NP [DT a] [N talk]] which explicitly encode the Phrase Type feature NP for the Arg 1 in Figure 2.b.The Predicate Word is represented by the fragment [V delivers] and the Head Word is encoded in [N talk].The same is not true for SCF since it does not contain information about a specific argument.SCF, in fact, aims to characterize the predicate with respect to the overall argument structures rather than a specific pair <p, a>.Third, Governing Category, Position and Voice features are not explicitly contained in both PAF and SCF.Nevertheless, SCF may allow the learning algorithm to detect the active/passive form of verbs.Finally, from the above observations follows that the PAF representation may be used with PAK to classify arguments.On the contrary, SCF lacks important information, thus, alone it may be used only to classify verbs in syntactic categories.This suggests that SCF should be used in conjunction with standard features to boost their classification performance.The aim of our experiments are twofold: On the one hand, we study if the PAF representation produces an accuracy higher than standard features.On the other hand, we study if SCF can be used to classify verbs according to their syntactic realization.Both the above aims can be carried out by combining PAF and SCF with the standard features.For this purpose we adopted two ways to combine kernels3: (1) K = K1 · K2 and (2) K = -yK1 + K2.The resulting set of kernels used in the experiments is the following: tween the normalized4 PAF-based kernel and the normalized polynomial kernel.KPAF·Kpd i.e. the normalized |KPAF|·|Kpd|, product between the PAF-based kernel and the polynomial kernel. tion between the normalized SCF-based kernel and the normalized polynomial kernel.KSCF·Kpd |KSCF|·|Kpd|, i.e. the normalized product between SCF-based kernel and the polynomial kernel.The above kernels were experimented over two corpora: PropBank (www.cis.upenn.edu/ace) along with Penn TreeBank5 2 (Marcus et al., 1993) and FrameNet.PropBank contains about 53,700 sentences and a fixed split between training and testing which has been used in other researches e.g., (Gildea and Palmer, 2002; Surdeanu et al., 2003; Hacioglu et al., 2003).In this split, Sections from 02 to 21 are used for training, section 23 for testing and sections 1 and 22 as developing set.We considered all PropBank arguments6 from Arg0 to Arg9, ArgA and ArgM for a total of 122,774 and 7,359 arguments in training and testing respectively.It is worth noting that in the experiments we used the gold standard parsing from Penn TreeBank, thus our kernel structures are derived with high precision.For the FrameNet corpus (www.icsi.berkeley .edu/framenet) we extracted all 24,558 sentences from the 40 frames of Senseval 3 task (www.senseval.org) for the Automatic Labeling of Semantic Roles.We considered 18 of the most frequent roles and we mapped together those having the same name.Only verbs are selected to be predicates in our evaluations.Moreover, as it does not exist a fixed split between training and testing, we selected randomly 30% of sentences for testing and 70% for training.Additionally, 30% of training was used as a validation-set.The sentences were processed using Collins’ parser (Collins, 1997) to generate parse-trees automatically.The classifier evaluations were carried out using the SVM-light software (Joachims, 1999) available at svmlight.joachims.org with the default polynomial kernel for standard feature evaluations.To process PAF and SCF, we implemented our own kernels and we used them inside SVM-light.The classification performances were evaluated using the f1 measure7 for single arguments and the accuracy for the final multi-class classifier.This latter choice allows us to compare the results with previous literature works, e.g.(Gildea and Jurasfky, 2002; Surdeanu et al., 2003; Hacioglu et al., 2003).For the evaluation of SVMs, we used the default regularization parameter (e.g., C = 1 for normalized kernels) and we tried a few costfactor values (i.e., j E {0.1,1, 2, 3, 4, 5}) to adjust the rate between Precision and Recall.We chose parameters by evaluating SVM using Kp3 kernel over the validation-set.Both A (see Section 3.3) and -y parameters were evaluated in a similar way by maximizing the performance of SVM using KPAF and -y KSCF tively.These parameters were adopted also for all the other kernels.To study the impact of our structural kernels we firstly derived the maximal accuracy reachable with standard features along with polynomial kernels.The multi-class accuracies, for PropBank and FrameNet using Kpd with d = 1,.., 5, are shown in Figure 5.We note that (a) the highest performance is reached for d = 3, (b) for PropBank our maximal accuracy (90.5%) is substantially equal to the SVM performance (88%) obtained in (Hacioglu et al., 2003) with degree 2 and (c) the accuracy on FrameNet (85.2%) is higher than the best result obtained in literature, i.e.82.0% in (Gildea and Palmer, 2002).This different outcome is due to a different task (we classify different roles) and a different classification algorithm.Moreover, we did not use the Frame information which is very important8.It is worth noting that the difference between linear and polynomial kernel is about 3-4 percent points for both PropBank and FrameNet.This remarkable difference can be easily explained by considering the meaning of standard features.For example, let us restrict the classification function CArg0 to the two features Voice and Position.Without loss of generality we can assume: (a) Voice=1 if active and 0 if passive, and (b) Position=1 when the argument is after the predicate and 0 otherwise.To simplify the example, we also assume that if an argument precedes the target predicate it is a subject, otherwise it is an object9.It follows that a constituent is Arg0, i.e.CArg0 = 1, if only one feature at a time is 1, otherwise it is not an Arg0, i.e.CArg0 = 0.In other words, CArg0 = Position XOR Voice, which is the classical example of a non-linear separable function that becomes separable in a superlinear space (Cristianini and Shawe-Taylor, 2000).After it was established that the best kernel for standard features is Kp3, we carried out all the other experiments using it in the kernel combinations.Table 2 and 3 show the single class (f1 measure) as well as multi-class classifier (accuracy) performance for PropBank and FrameNet respectively.Each column of the two tables refers to a different kernel defined in the previous section.The overall meaning is discussed in the following points: First, PAF alone has good performance, since in PropBank evaluation it outperforms the linear kernel (Kp1), 88.7% vs. 86.7% whereas in FrameNet, it shows a similar performance 79.5% vs. 82.1% (compare tables with Figure 5).This suggests that PAF generates the same information as the standard features in a linear space.However, when a degree greater than 1 is used for standard features, PAF is outperformed10.Second, SCF improves the polynomial kernel (d = 3), i.e. the current state-of-the-art, of about 3 percent points on PropBank (column SCF·P).This suggests that (a) PAK can measure the similarity between two SCF structures and (b) the sub-categorization information provides effective clues about the expected argument type.The interesting consequence is that SCF together with PAK seems suitable to automatically cluster different verbs that have the same syntactic realization.We note also that to fully exploit the SCF information it is necessary to use a kernel product (K1 · K2) combination rather than the sum (K1 + K2), e.g. column SCF+P.Finally, the FrameNet results are completely different.No kernel combinations with both PAF and SCF produce an improvement.On 10Unfortunately the use of a polynomial kernel on top the tree fragments to generate the XOR functions seems not successful. the contrary, the performance decreases, suggesting that the classifier is confused by this syntactic information.The main reason for the different outcomes is that PropBank arguments are different from semantic roles as they are an intermediate level between syntax and semantic, i.e. they are nearer to grammatical functions.In fact, in PropBank arguments are annotated consistently with syntactic alternations (see the Annotation guidelines for PropBank at www.cis.upenn.edu/ace).On the contrary FrameNet roles represent the final semantic product and they are assigned according to semantic considerations rather than syntactic aspects.For example, Cause and Agent semantic roles have identical syntactic realizations.This prevents SCF to distinguish between them.Another minor reason may be the use of automatic parse-trees to extract PAF and SCF, even if preliminary experiments on automatic semantic shallow parsing of PropBank have shown no important differences versus semantic parsing which adopts Gold Standard parse-trees.In this paper, we have experimented with SVMs using the two novel convolution kernels PAF and SCF which are designed for the semantic structures derived from PropBank and FrameNet corpora.Moreover, we have combined them with the polynomial kernel of standard features.The results have shown that: First, SVMs using the above kernels are appealing for semantically parsing both corpora.Second, PAF and SCF can be used to improve automatic classification of PropBank arguments as they provide clues about the predicate argument structure of the target verb.For example, SCF improves (a) the classification state-of-theart (i.e. the polynomial kernel) of about 3 percent points and (b) the best literature result of about 5 percent points.Third, additional work is needed to design kernels suitable to learn the deep semantic contained in FrameNet as it seems not sensible to both PAF and SCF information.Finally, an analysis of SVMs using polynomial kernels over standard features has explained why they largely outperform linear classifiers based-on standard features.In the future we plan to design other structures and combine them with SCF, PAF and standard features.In this vision the learning will be carried out on a set of structural features instead of a set of flat features.Other studies may relate to the use of SCF to generate verb clusters.
In the machine learning approaches of natural language processing (NLP), models are generally trained on large annotated corpus.However, annotating such corpus is expensive and timeconsuming, which makes it difficult to adapt an existing model to a new domain.In order to overcome this difficulty, active learning (sample selection) has been studied in more and more NLP applications such as POS tagging (Engelson and Dagan 1999), information extraction (Thompson et al. 1999), text classification (Lewis and Catlett 1994; McCallum and Nigam 1998; Schohn and Cohn 2000; Tong and Koller 2000; Brinker 2003), statistical parsing (Thompson et al.1999; Tang et al. 2002; Steedman et al.2003), noun phrase chunking (Ngai and Yarowsky 2000), etc.Active learning is based on the assumption that a small number of annotated examples and a large number of unannotated examples are available.This assumption is valid in most NLP tasks.Different from supervised learning in which the entire corpus are labeled manually, active learning is to select the most useful example for labeling and add the labeled example to training set to retrain model.This procedure is repeated until the model achieves a certain level of performance.Practically, a batch of examples are selected at a time, called batchedbased sample selection (Lewis and Catlett 1994) since it is time consuming to retrain the model if only one new example is added to the training set.Many existing work in the area focus on two approaches: certainty-based methods (Thompson et al. 1999; Tang et al.2002; Schohn and Cohn 2000; Tong and Koller 2000; Brinker 2003) and committee-based methods (McCallum and Nigam 1998; Engelson and Dagan 1999; Ngai and Yarowsky 2000) to select the most informative examples for which the current model are most uncertain.Being the first piece of work on active learning for name entity recognition (NER) task, we target to minimize the human annotation efforts yet still reaching the same level of performance as a supervised learning approach.For this purpose, we make a more comprehensive consideration on the contribution of individual examples, and more importantly maximizing the contribution of a batch based on three criteria: informativeness, representativeness and diversity.First, we propose three scoring functions to quantify the informativeness of an example, which can be used to select the most uncertain examples.Second, the representativeness measure is further proposed to choose the examples representing the majority.Third, we propose two diversity considerations (global and local) to avoid repetition among the examples of a batch.Finally, two combination strategies with the above three criteria are proposed to reach the maximum effectiveness on active learning for NER.We build our NER model using Support Vector Machines (SVM).The experiment shows that our active learning methods achieve a promising result in this NER task.The results in both MUC6 and GENIA show that the amount of the labeled training data can be reduced by at least 80% without degrading the quality of the named entity recognizer.The contributions not only come from the above measures, but also the two sample selection strategies which effectively incorporate informativeness, representativeness and diversity criteria.To our knowledge, it is the first work on considering the three criteria all together for active learning.Furthermore, such measures and strategies can be easily adapted to other active learning tasks as well.Support Vector Machines (SVM) is a powerful machine learning method, which has been applied successfully in NER tasks, such as (Kazama et al. 2002; Lee et al.2003).In this paper, we apply active learning methods to a simple and effective SVM model to recognize one class of names at a time, such as protein names, person names, etc.In NER, SVM is to classify a word into positive class “1” indicating that the word is a part of an entity, or negative class “-1” indicating that the word is not a part of an entity.Each word in SVM is represented as a high-dimensional feature vector including surface word information, orthographic features, POS feature and semantic trigger features (Shen et al. 2003).The semantic trigger features consist of some special head nouns for an entity class which is supplied by users.Furthermore, a window (size = 7), which represents the local context of the target word w, is also used to classify w. However, for active learning in NER, it is not reasonable to select a single word without context for human to label.Even if we require human to label a single word, he has to make an addition effort to refer to the context of the word.In our active learning process, we select a word sequence which consists of a machine-annotated named entity and its context rather than a single word.Therefore, all of the measures we propose for active learning should be applied to the machineannotated named entities and we have to further study how to extend the measures for words to named entities.Thus, the active learning in SVMbased NER will be more complex than that in simple classification tasks, such as text classification on which most SVM active learning works are conducted (Schohn and Cohn 2000; Tong and Koller 2000; Brinker 2003).In the next part, we will introduce informativeness, representativeness and diversity measures for the SVM-based NER.The basic idea of informativeness criterion is similar to certainty-based sample selection methods, which have been used in many previous works.In our task, we use a distance-based measure to evaluate the informativeness of a word and extend it to the measure of an entity using three scoring functions.We prefer the examples with high informative degree for which the current model are most uncertain.In the simplest linear form, training SVM is to find a hyperplane that can separate the positive and negative examples in training set with maximum margin.The margin is defined by the distance of the hyperplane to the nearest of the positive and negative examples.The training examples which are closest to the hyperplane are called support vectors.In SVM, only the support vectors are useful for the classification, which is different from statistical models.SVM training is to get these support vectors and their weights from training set by solving quadratic programming problem.The support vectors can later be used to classify the test data.Intuitively, we consider the informativeness of an example as how it can make effect on the support vectors by adding it to training set.An example may be informative for the learner if the distance of its feature vector to the hyperplane is less than that of the support vectors to the hyperplane (equal to 1).This intuition is also justified by (Schohn and Cohn 2000; Tong and Koller 2000) based on a version space analysis.They state that labeling an example that lies on or close to the hyperplane is guaranteed to have an effect on the solution.In our task, we use the distance to measure the informativeness of an example.The distance of a word’s feature vector to the hyperplane is computed as follows: where w is the feature vector of the word, ai, yi, si corresponds to the weight, the class and the feature vector of the ith support vector respectively.N is the number of the support vectors in current model.We select the example with minimal Dist, which indicates that it comes closest to the hyperplane in feature space.This example is considered most informative for current model.Based on the above informativeness measure for a word, we compute the overall informativeness degree of a named entity NE.In this paper, we propose three scoring functions as follows.Let NE = w1...wN in which wi is the feature vector of the ith word of NE. where, wi is the feature vector of the ith word in NE.In Section 4.3, we will evaluate the effectiveness of these scoring functions.In addition to the most informative example, we also prefer the most representative example.The representativeness of an example can be evaluated based on how many examples there are similar or near to it.So, the examples with high representative degree are less likely to be an outlier.Adding them to the training set will have effect on a large number of unlabeled examples.There are only a few works considering this selection criterion (McCallum and Nigam 1998; Tang et al. 2002) and both of them are specific to their tasks, viz. text classification and statistical parsing.In this section, we compute the similarity between words using a general vector-based measure, extend this measure to named entity level using dynamic time warping algorithm and quantify the representativeness of a named entity by its density.In general vector space model, the similarity between two vectors may be measured by computing the cosine value of the angle between them.The smaller the angle is, the more similar between the vectors are.This measure, called cosine-similarity measure, has been widely used in information retrieval tasks (Baeza-Yates and Ribeiro-Neto 1999).In our task, we also use it to quantify the similarity between two words.Particularly, the calculation in SVM need be projected to a higher dimensional space by using a certain kernel function K ( w i , w j ) .Therefore, we adapt the cosine-similarity measure to SVM as follows: (wi,wj)=k (wi , wi)k(wj , wj ) where, wi and wj are the feature vectors of the words i and j.This calculation is also supported by (Brinker 2003)’s work.Furthermore, if we use the linear kernel k(wi, wj) = wi ⋅ w j , the measure is the same as the traditional cosine similarity measgeneral vector-based similarity measure. tities In this part, we compute the similarity between two machine-annotated named entities given the similarities between words.Regarding an entity as a word sequence, this work is analogous to the alignment of two sequences.We employ the dynamic time warping (DTW) algorithm (Rabiner et al. 1978) to find an optimal alignment between the words in the sequences which maximize the accumulated similarity degree between the sequences.Here, we adapt it to our task.A sketch of the modified algorithm is as follows.Let NE1 = w11w12...w1n...w1N, (n = 1,..., N) and NE2 = w21w22...w2m...w2M, (m = 1,..., M) denote two word sequences to be matched.NE1 and NE2 consist of M and N words respectively.NE1(n) = w1n and NE2(m) = w2m.A similarity value Sim(w1n ,w2m) has been known for every pair of words (w1n,w2m) within NE1 and NE2.The goal of DTW is to find a path, m = map(n), which map n onto the corresponding m such that the accumulated similarity Sim* along the path is maximized. map n n= A dynamic programming method is used to determine the optimum path map(n).The accumulated similarity SimA to any grid point (n, m) can be recursively calculated as Certainly, the overall similarity measure Sim* has to be normalized as longer sequences normally give higher similarity value.So, the similarity between two sequences NE1 and NE2 is calculated as and may be regarded as a Entity Given a set of machine-annotated named entities NESet = {NE1, ..., NEN}, the representativeness of a named entity NEi in NESet is quantified by its density.The density of NEi is defined as the average similarity between NEi and all the other entities NEj in NESet as follows.If NEi has the largest density among all the entities in NESet, it can be regarded as the centroid of NESet and also the most representative examples in NESet.Diversity criterion is to maximize the training utility of a batch.We prefer the batch in which the examples have high variance to each other.For example, given the batch size 5, we try not to select five repetitious examples at a time.To our knowledge, there is only one work (Brinker 2003) exploring this criterion.In our task, we propose two methods: local and global, to make the examples diverse enough in a batch.For a global consideration, we cluster all named entities in NESet based on the similarity measure proposed in Section 2.2.2.The named entities in the same cluster may be considered similar to each other, so we will select the named entities from different clusters at one time.We employ a Kmeans clustering algorithm (Jelinek 1997), which is shown in Figure 1.The number of clusters is K Initialization: Randomly equally partition {NE1, ..., NEN} into K initial clusters Cj (j = 1, ... , K).Loop until the number of changes for the centroids of all clusters is less than a threshold In each round, we need to compute the pairwise similarities within each cluster to get the centroid of the cluster.And then, we need to compute the similarities between each example and all centroids to repartition the examples.So, the algorithm is time-consuming.Based on the assumption that N examples are uniformly distributed between the K clusters, the time complexity of the algorithm is about O(N2/K+NK) (Tang et al. 2002).In one of our experiments, the size of the NESet (N) is around 17000 and K is equal to 50, so the time complexity is about O(106).For efficiency, we may filter the entities in NESet before clustering them, which will be further discussed in Section 3.When selecting a machine-annotated named entity, we compare it with all previously selected named entities in the current batch.If the similarity between them is above a threshold 8, this example cannot be allowed to add into the batch.The order of selecting examples is based on some measure, such as informativeness measure, representativeness measure or their combination.This local selection method is shown in Figure 2.In this way, we avoid selecting too similar examples (similarity value ≥ 8) in a batch.The threshold 8 may be the average similarity between the examples in NESet.This consideration only requires O(NK+K2) computational time.In one of our experiments (N ˜ 17000 and K = 50), the time complexity is about O(105).It is more efficient than clustering algorithm described in Section 2.3.1.In this section, we will study how to combine and strike a proper balance between these criteria, viz. informativeness, representativeness and diversity, to reach the maximum effectiveness on NER active learning.We build two strategies to combine the measures proposed above.These strategies are based on the varying priorities of the criteria and the varying degrees to satisfy the criteria. most informativeness score from NESet to an intermediate set called INTERSet.By this preselecting, we make the selection process faster in the later steps since the size of INTERSet is much smaller than that of NESet.Then we cluster the examples in INTERSet and choose the centroid of each cluster into a batch called BatchSet.The centroid of a cluster is the most representative example in that cluster since it has the largest density.Furthermore, the examples in different clusters may be considered diverse to each other.By this means, we consider representativeness and diversity criteria at the same time.This strategy is shown in Figure 3.One limitation of this strategy is that clustering result may not reflect the distribution of whole sample space since we only cluster on INTERSet for efficiency.The other is that since the representativeness of an example is only evaluated on a cluster.If the cluster size is too small, the most representative example in this cluster may not be representative in the whole sample space.Given: NESet = {NE1, ..., NEN} BatchSet with the maximal size K. INTERSet with the maximal size M Steps: which the Info and Density value of NEi are normalized first.The individual importance of each criterion in this function is adjusted by the tradeoff parameter l ( 0 ≤ l ≤1 ) (set to 0.6 in our experiment).First, we select a candidate example NEi with the maximum value of this function from NESet.Second, we consider diversity criterion using the local method in Section 3.3.2.We add the candidate example NEi to a batch only if NEi is different enough from any previously selected example in the batch.The threshold ß is set to the average pair-wise similarity of the entities in NESet.In order to evaluate the effectiveness of our selection strategies, we apply them to recognize protein (PRT) names in biomedical domain using GENIA corpus V1.1 (Ohta et al. 2002) and person (PER), location (LOC), organization (ORG) names in newswire domain using MUC-6 corpus.First, we randomly split the whole corpus into three parts: an initial training set to build an initial model, a test set to evaluate the performance of the model and an unlabeled set to select examples.The size of each data set is shown in Table 1.Then, iteratively, we select a batch of examples following the selection strategies proposed, require human experts to label them and add them into the training set.The batch size K = 50 in GENIA and 10 in MUC-6.Each example is defined as a machine-recognized named entity and its context words (previous 3 words and next 3 words).Domain Class Corpus Initial Training Set Test Set Unlabeled Set Biomedical PRT GENIA1.1 10 sent.(277 words) 900 sent.(26K words) 8004 sent.(223K words) Newswire PER MUC-6 5 sent.(131 words) 602 sent.(14K words) 7809 sent.(157K words) LOC 5 sent.(130 words) 7809 sent.(157K words) ORG 5 sent.(113 words) 7809 sent.(157K words) The goal of our work is to minimize the human annotation effort to learn a named entity recognizer with the same performance level as supervised learning.The performance of our model is evaluated using “precision/recall/F-measure”.In this section, we evaluate our selection strategies by comparing them with a random selection method, in which a batch of examples is randomly selected iteratively, on GENIA and MUC-6 corpus.Table 2 shows the amount of training data needed to achieve the performance of supervised learning using various selection methods, viz.Random, Strategy1 and Strategy2.In GENIA, we find: Furthermore, when we apply our model to newswire domain (MUC-6) to recognize person, location and organization names, Strategy1 and Strategy2 show a more promising result by comparing with the supervised learning and Random, as shown in Table 2.On average, about 95% of the data can be reduced to achieve the same performance with the supervised learning in MUC-6.It is probably because NER in the newswire domain is much simpler than that in the biomedical domain (Shen et al. 2003) and named entities are less and distributed much sparser in the newswire texts than in the biomedical texts.In this section, we investigate the effectiveness of informativeness criterion in NER task.Figure 5 shows a plot of training data size versus F-measure achieved by the informativeness-based measures in Section 3.1.2: Info_Avg, Info_Min and Info_S/N as well as Random.We make the comparisons in GENIA corpus.In Figure 5, the horizontal line is the performance level (63.3 F-measure) achieved by supervised learning (223K words).We find that the three informativeness-based measures perform similarly and each of them outperforms Random.Table 3 highlights the various data sizes to achieve the peak performance using these selection methods.We find that Random (83K words) on average requires over 1.5 times as much as data to achieve the same performance as the informativeness-based selection methods (52K words).In addition to the informativeness criterion, we further incorporate representativeness and diversity criteria into active learning using two strategies described in Section 3.Comparing the two strategies with the best result of the single-criterionbased selection methods Info_Min, we are to justify that representativeness and diversity are also important factors for active learning.Figure 6 shows the learning curves for the various methods: Strategy1, Strategy2 and Info_Min.In the beginning iterations (F-measure < 60), the three methods performed similarly.But with the larger training set, the efficiencies of Stratety1 and Strategy2 begin to be evident.Table 4 highlights the final result of the three methods.In order to reach the performance of supervised learning, Strategy1 (40K words) and Strategyy2 (31K words) require about 80% and 60% of the data that Info_Min (51.9K) does.So we believe the effective combinations of informativeness, representativeness and diversity will help to learn the NER model more quickly and cost less in annotation.Since there is no study on active learning for NER task previously, we only introduce general active learning methods here.Many existing active learning methods are to select the most uncertain examples using various measures (Thompson et al. 1999; Schohn and Cohn 2000; Tong and Koller 2000; Engelson and Dagan 1999; Ngai and Yarowsky 2000).Our informativeness-based measure is similar to these works.However these works just follow a single criterion.(McCallum and Nigam 1998; Tang et al. 2002) are the only two works considering the representativeness criterion in active learning.(Tang et al. 2002) use the density information to weight the selected examples while we use it to select examples.Moreover, the representativeness measure we use is relatively general and easy to adapt to other tasks, in which the example selected is a sequence of words, such as text chunking, POS tagging, etc.On the other hand, (Brinker 2003) first incorporate diversity in active learning for text classification.Their work is similar to our local consideration in Section 2.3.2.However, he didn’t further explore how to avoid selecting outliers to a batch.So far, we haven’t found any previous work integrating the informativeness, representativeness and diversity all together.In this paper, we study the active learning in a more complex NLP task, named entity recognition.We propose a multi-criteria-based approach to select examples based on their informativeness, representativeness and diversity, which are incorporated all together by two strategies (local and global).Experiments show that, in both MUC6 and GENIA, both of the two strategies combining the three criteria outperform the single criterion (informativeness).The labeling cost can be significantly reduced by at least 80% comparing with the supervised learning.To our best knowledge, this is not only the first work to report the empirical results of active learning for NER, but also the first work to incorporate the three criteria all together for selecting examples.Although the current experiment results are very promising, some parameters in our experiment, such as the batch size K and the X in the function of strategy 2, are decided by our experience in the domain.In practical application, the optimal value of these parameters should be decided automatically based on the training process.Furthermore, we will study how to overcome the limitation of the strategy 1 discussed in Section 3 by using more effective clustering algorithm.Another interesting work is to study when to stop active learning.
Over the past decade, we have witnessed a revolution in the field of machine translation (MT) toward statistical or corpus-based methods.Yet despite this success, statistical machine translation (SMT) has many hurdles to overcome.While it excels at translating domain-specific terminology and fixed phrases, grammatical generalizations are poorly captured and often mangled during translation (Thurmair, 04).State-of-the-art phrasal SMT systems such as (Koehn et al., 03) and (Vogel et al., 03) model translations of phrases (here, strings of adjacent words, not syntactic constituents) rather than individual words.Arbitrary reordering of words is allowed within memorized phrases, but typically only a small amount of phrase reordering is allowed, modeled in terms of offset positions at the string level.This reordering model is very limited in terms of linguistic generalizations.For instance, when translating English to Japanese, an ideal system would automatically learn largescale typological differences: English SVO clauses generally become Japanese SOV clauses, English post-modifying prepositional phrases become Japanese pre-modifying postpositional phrases, etc.A phrasal SMT system may learn the internal reordering of specific common phrases, but it cannot generalize to unseen phrases that share the same linguistic structure.In addition, these systems are limited to phrases contiguous in both source and target, and thus cannot learn the generalization that English not may translate as French ne...pas except in the context of specific intervening words.The hope in the SMT community has been that the incorporation of syntax would address these issues, but that promise has yet to be realized.One simple means of incorporating syntax into SMT is by re-ranking the n-best list of a baseline SMT system using various syntactic models, but Och et al. (04) found very little positive impact with this approach.However, an n-best list of even 16,000 translations captures only a tiny fraction of the ordering possibilities of a 20 word sentence; re-ranking provides the syntactic model no opportunity to boost or prune large sections of that search space.Inversion Transduction Grammars (Wu, 97), or ITGs, treat translation as a process of parallel parsing of the source and target language via a synchronized grammar.To make this process computationally efficient, however, some severe simplifying assumptions are made, such as using a single non-terminal label.This results in the model simply learning a very high level preference regarding how often nodes should switch order without any contextual information.Also these translation models are intrinsically word-based; phrasal combinations are not modeled directly, and results have not been competitive with the top phrasal SMT systems.Along similar lines, Alshawi et al. (2000) treat translation as a process of simultaneous induction of source and target dependency trees using headtransduction; again, no separate parser is used.Yamada and Knight (01) employ a parser in the target language to train probabilities on a set of operations that convert a target language tree to a source language string.This improves fluency slightly (Charniak et al., 03), but fails to significantly impact overall translation quality.This may be because the parser is applied to MT output, which is notoriously unlike native language, and no additional insight is gained via source language analysis.Lin (04) translates dependency trees using paths.This is the first attempt to incorporate large phrasal SMT-style memorized patterns together with a separate source dependency parser and SMT models.However the phrases are limited to linear paths in the tree, the only SMT model used is a maximum likelihood channel model and there is no ordering model.Reported BLEU scores are far below the leading phrasal SMT systems.MSR-MT (Menezes & Richardson, 01) parses both source and target languages to obtain a logical form (LF), and translates source LFs using memorized aligned LF patterns to produce a target LF.It utilizes a separate sentence realization component (Ringger et al., 04) to turn this into a target sentence.As such, it does not use a target language model during decoding, relying instead on MLE channel probabilities and heuristics such as pattern size.Recently Aue et al. (04) incorporated an LF-based language model (LM) into the system for a small quality boost.A key disadvantage of this approach and related work (Ding & Palmer, 02) is that it requires a parser in both languages, which severely limits the language pairs that can be addressed.In this paper we propose a novel dependency treebased approach to phrasal SMT which uses treebased ‘phrases’ and a tree-based ordering model in combination with conventional SMT models to produce state-of-the-art translations.Our system employs a source-language dependency parser, a target language word segmentation component, and an unsupervised word alignment component to learn treelet translations from a parallel sentence-aligned corpus.We begin by parsing the source text to obtain dependency trees and word-segmenting the target side, then applying an off-the-shelf word alignment component to the bitext.The word alignments are used to project the source dependency parses onto the target sentences.From this aligned parallel dependency corpus we extract a treelet translation model incorporating source and target treelet pairs, where a treelet is defined to be an arbitrary connected subgraph of the dependency tree.A unique feature is that we allow treelets with a wildcard root, effectively allowing mappings for siblings in the dependency tree.This allows us to model important phenomena, such as not ... ne...pas.We also train a variety of statistical models on this aligned dependency tree corpus, including a channel model and an order model.To translate an input sentence, we parse the sentence, producing a dependency tree for that sentence.We then employ a decoder to find a combination and ordering of treelet translation pairs that cover the source tree and are optimal according to a set of models that are combined in a log-linear framework as in (Och, 03).This approach offers the following advantages over string-based SMT systems: Instead of limiting learned phrases to contiguous word sequences, we allow translation by all possible phrases that form connected subgraphs (treelets) in the source and target dependency trees.This is a powerful extension: the vast majority of surface-contiguous phrases are also treelets of the tree; in addition, we gain discontiguous phrases, including combinations such as verb-object, article-noun, adjective-noun etc. regardless of the number of intervening words.Another major advantage is the ability to employ more powerful models for reordering source language constituents.These models can incorporate information from the source analysis.For example, we may model directly the probability that the translation of an object of a preposition in English should precede the corresponding postposition in Japanese, or the probability that a pre-modifying adjective in English translates into a post-modifier in French.We require a source language dependency parser that produces unlabeled, ordered dependency trees and annotates each source word with a partof-speech (POS).An example dependency tree is shown in Figure 1.The arrows indicate the head annotation, and the POS for each candidate is listed underneath.For the target language we only require word segmentation.To obtain word alignments we currently use GIZA++ (Och & Ney, 03).We follow the common practice of deriving many-to-many alignments by running the IBM models in both directions and combining the results heuristically.Our heuristics differ in that they constrain manyto-one alignments to be contiguous in the source dependency tree.A detailed description of these heuristics can be found in Quirk et al. (04).Given a word aligned sentence pair and a source dependency tree, we use the alignment to project the source structure onto the target sentence.Oneto-one alignments project directly to create a target tree isomorphic to the source.Many-to-one alignments project similarly; since the ‘many’ source nodes are connected in the tree, they act as if condensed into a single node.In the case of one-to-many alignments we project the source node to the rightmost2 of the ‘many’ target words, and make the rest of the target words dependent on it.2 If the target language is Japanese, leftmost may be more appropriate. startup properties and options propriétés et options de démarrage Unaligned target words3 are attached into the dependency structure as follows: assume there is an unaligned word tj in position j.Let i < j and k > j be the target positions closest to j such that ti depends on tk or vice versa: attach tj to the lower of ti or tk.If all the nodes to the left (or right) of position j are unaligned, attach tj to the left-most (or right-most) word that is aligned.The target dependency tree created in this process may not read off in the same order as the target string, since our alignments do not enforce phrasal cohesion.For instance, consider the projection of the parse in Figure 1 using the word alignment in Figure 2a.Our algorithm produces the dependency tree in Figure 2b.If we read off the leaves in a left-to-right in-order traversal, we do not get the original input string: de démarrage appears in the wrong place.A second reattachment pass corrects this situation.For each node in the wrong order, we reattach it to the lowest of its ancestors such that it is in the correct place relative to its siblings and parent.In Figure 2c, reattaching démarrage to et suffices to produce the correct order.From the aligned pairs of dependency trees we extract all pairs of aligned source and target treelets along with word-level alignment linkages, up to a configurable maximum size.We also keep treelet counts for maximum likelihood estimation.Phrasal SMT systems often use a model to score the ordering of a set of phrases.One approach is to penalize any deviation from monotone decoding; another is to estimate the probability that a source phrase in position i translates to a target phrase in position j (Koehn et al., 03).We attempt to improve on these approaches by incorporating syntactic information.Our model assigns a probability to the order of a target tree given a source tree.Under the assumption that constituents generally move as a whole, we predict the probability of each given ordering of modifiers independently.That is, we make the following simplifying assumption (where c is a function returning the set of nodes modifying t): Furthermore, we assume that the position of each child can be modeled independently in terms of a head-relative position: Figure 3a demonstrates an aligned dependency tree pair annotated with head-relative positions; Figure 3b presents the same information in an alternate tree-like representation.We currently use a small set of features reflecting very local information in the dependency tree to model P(pos(m,t)  |S, T): As an example, consider the children of propriété in Figure 3.The head-relative positions One can also include features of siblings to produce a Markov ordering model.However, we found that this had litt of its modifiers la and Cancel are -1 and +1, respectively.Thus we try to predict as follows: Channel Models: We incorporate two distinct channel models, a maximum likelihood estimate (MLE) model and a model computed using Model-1 word-to-word alignment probabilities as in (Vogel et al., 03).The MLE model effectively captures non-literal phrasal translations such as idioms, but suffers fr om data sparsity.The wordto-word model does not typically suffer from data sparsity, but prefers more literal translations.Given a set of treelet translation pairs that cover a given input dependency tree and produce a target dependency tree, we model the probability of source given target as the product of the individual treelet translation probabilities: we assume a uniform probability distribution over the decompositions of a tree into treelets.Target Model: Given an ordered target language dependency tree, it is trivial to read off the surface string.We evaluate this string using a trigram model with modified Kneser-Ney smoothing.Miscellaneous Feature Functions: The log-linear framework allows us to incorporate other feature functions as ‘models’ in the translation process.For instance, using fewer, larger treelet translation pairs often provides better translations, since they capture more context and allow fewer possibilities for search and model error.Therefore we add a feature function that counts the number of phrases used.We also add a feature that counts the number of target words; this acts as an insertion/deletion bonus/penalty.The challenge of tree-based decoding is that the traditional left-to-right decoding approach of string-based systems is inapplicable.Additional challenges are posed by the need to handle treelets—perhaps discontiguous or overlapping— and a combinatorially explosive ordering space.Our decoding approach is influenced by ITG (Wu, 97) with several important extensions.First, we employ treelet translation pairs instead of single word translations.Second, instead of modeling rearrangements as either preserving source order or swapping source order, we allow the dependents of a node to be ordered in any arbitrary manner and use the order model described in section 2.4 to estimate probabilities.Finally, we use a log-linear framework for model combination that allows any amount of other information to be modeled.We will initially approach the decoding problem as a bottom up, exhaustive search.We define the set of all possible treelet translation pairs of the subtree rooted at each input node in the following manner: A treelet translation pair x is said to match the input dependency tree S iff there is some connected subgraph S’ that is identical to the source side of x.We say that x covers all the nodes in S’ and is rooted at source node s, where s is the root of matched subgraph S’.We first find all treelet translation pairs that match the input dependency tree.Each matched pair is placed on a list associated with the input node where the match is rooted.Moving bottomup through the input dependency tree, we compute a list of candidate translations for the input subtree rooted at each node s, as follows: Consider in turn each treelet translation pair x rooted at s. The treelet pair x may cover only a portion of the input subtree rooted at s. Find all descendents s' of s that are not covered by x, but whose parent s&quot; is covered by x.At each such node s&quot; look at all interleavings of the children of s&quot; specified by x, if any, with each translation t' from the candidate translation list5 of each child s'.Each such interleaving is scored using the models previously described and added to the candidate translation list for that input node.The resultant translation is the best scoring candidate for the root input node.As an example, see the example dependency tree in Figure 4a and treelet translation pair in 4b.This treelet translation pair covers all the nodes in 4a except the subtrees rooted at software and is.We first compute (and cache) the candidate translation lists for the subtrees rooted at software and is, then construct full translation candidates by attaching those subtree translations to installés in all possible ways.The order of sur relative to installés is fixed; it remains to place the translated subtrees for the software and is.Note that if c is the count of children specified in the mapping and r is the count of subtrees translated via recursive calls, then there are (c+r+1)!/(c+1)! orderings.Thus (1+2+1)!/(1+1)!= 12 candidate translations are produced for each combination of translations of the software and is.Converting this exhaustive search to dynamic programming relies on the observation that scoring a translation candidate at a node depends on the following information from its descendents: the order model requires features from the root of a translated subtree, and the target language model is affected by the first and last two words in each subtree.Therefore, we need to keep the best scoring translation candidate for a given subtree for each combination of (head, leading bigram, trailing bigram), which is, in the worst case, O(V5), where V is the vocabulary size.The dynamic programming approach therefore does not allow for great savings in practice because a trigram target language model forces consideration of context external to each subtree.To eliminate unnecessary ordering operations, we first check that a given set of words has not been previously ordered by the decoder.We use an order-independent hash table where two trees are considered equal if they have the same tree structure and lexical choices after sorting each child list into a canonical order.A simpler alternate approach would be to compare bags-ofwords.However since our possible orderings are bound by the induced tree structure, we might overzealously prune a candidate with a different tree structure that allows a better target order.The following optimizations do not preserve optimality, but work well in practice.Instead of keeping the full list of translation candidates for a given input node, we keep a topscoring subset of the candidates.While the decoder is no longer guaranteed to find the optimal translation, in practice the quality impact is minimal with a list size 10 (see Table 5.6).Variable-sized n-best lists: A further speedup can be obtained by noting that the number of translations using a given treelet pair is exponential in the number of subtrees of the input not covered by that pair.To limit this explosion we vary the size of the n-best list on any recursive call in inverse proportion to the number of subtrees uncovered by the current treelet.This has the intuitive appeal of allowing a more thorough exploration of large treelet translation pairs (that are likely to result in better translations) than of smaller, less promising pairs.Channel model scores and treelet size are powerful predictors of translation quality.Heuristically pruning low scoring treelet translation pairs before the search starts allows the decoder to focus on combinations and orderings of high quality treelet pairs. translation pairs rooted at that node, as ranked first by size, then by MLE channel model score, then by Model 1 score.The impact of this optimization is explored in Table 5.6.The complexity of the ordering step at each node grows with the factorial of the number of children to be ordered.This can be tamed by noting that given a fixed pre- and post-modifier count, our order model is capable of evaluating a single ordering decision independently from other ordering decisions.One version of the decoder takes advantage of this to severely limit the number of ordering possibilities considered.Instead of considering all interleavings, it considers each potential modifier position in turn, greedily picking the most probable child for that slot, moving on to the next slot, picking the most probable among the remaining children for that slot and so on.The complexity of greedy ordering is linear, but at the cost of a noticeable drop in BLEU score (see Table 5.4).Under default settings our system tries to decode a sentence with exhaustive ordering until a specified timeout, at which point it falls back to greedy ordering.We evaluated the translation quality of the system using the BLEU metric (Papineni et al., 02) under a variety of configurations.We compared against two radically different types of systems to demonstrate the competitiveness of this approach: We used a parallel English-French corpus containing 1.5 million sentences of Microsoft technical data (e.g., support articles, product documentation).We selected a cleaner subset of this data by eliminating sentences with XML or HTML tags as well as very long (>160 characters) and very short (<40 characters) sentences.We held out 2,000 sentences for development testing and parameter tuning, 10,000 sentences for testing, and 250 sentences for lambda training.We ran experiments on subsets of the training data ranging from 1,000 to 300,000 sentences.Table 4.1 presents details about this dataset.We parsed the source (English) side of the corpus using NLPWIN, a broad-coverage rule-based parser developed at Microsoft Research able to produce syntactic analyses at varying levels of depth (Heidorn, 02).For the purposes of these experiments we used a dependency tree output with part-of-speech tags and unstemmed surface words.For word alignment, we used GIZA++, following a standard training regimen of five iterations of Model 1, five iterations of the HMM Model, and five iterations of Model 4, in both directions.We then projected the dependency trees and used the aligned dependency tree pairs to extract treelet translation pairs and train the order model as described above.The target language model was trained using only the French side of the corpus; additional data may improve its performance.Finally we trained lambdas via Maximum BLEU (Och, 03) on 250 held-out sentences with a single reference translation, and tuned the decoder optimization parameters (n-best list size, timeouts etc) on the development test set.The same GIZA++ alignments as above were used in the Pharaoh decoder.We used the heuristic combination described in (Och & Ney, 03) and extracted phrasal translation pairs from this combined alignment as described in (Koehn et al., 03).Except for the order model (Pharaoh uses its own ordering approach), the same models were used: MLE channel model, Model 1 channel model, target language model, phrase count, and word count.Lambdas were trained in the same manner (Och, 03).MSR-MT used its own word alignment approach as described in (Menezes & Richardson, 01) on the same training data.MSR-MT does not use lambdas or a target language model.We present BLEU scores on an unseen 10,000 sentence test set using a single reference translation for each sentence.Speed numbers are the end-to-end translation speed in sentences per minute.All results are based on a training set size of 100,000 sentences and a phrase size of 4, except Table 5.2 which varies the phrase size and Table 5.3 which varies the training set size.Results for our system and the comparison systems are presented in Table 5.1.Pharaoh monotone refers to Pharaoh with phrase reordering disabled.The difference between Pharaoh and the Treelet system is significant at the 99% confidence level under a two-tailed paired t-test.Table 5.2 compares Pharaoh and the Treelet system at different phrase sizes.While all the differences are statistically significant at the 99% confidence level, the wide gap at smaller phrase sizes is particularly striking.We infer that whereas Pharaoh depends heavily on long phrases to encapsulate reordering, our dependency treebased ordering model enables credible performance even with single-word ‘phrases’.We conjecture that in a language pair with large-scale ordering differences, such as English-Japanese, even long phrases are unlikely to capture the necessary reorderings, whereas our tree-based ordering model may prove more robust.Table 5.3 compares the same systems at different training corpus sizes.All of the differences are statistically significant at the 99% confidence level.Noting that the gap widens at smaller corpus sizes, we suggest that our tree-based approach is more suitable than string-based phrasal SMT when translating from English into languages or domains with limited parallel data.We also ran experiments varying different system parameters.Table 5.4 explores different ordering strategies, Table 5.5 looks at the impact of discontiguous phrases and Table 5.6 looks at the impact of decoder optimizations such as treelet pruning and n-best list size.We presented a novel approach to syntacticallyinformed statistical machine translation that leverages a parsed dependency tree representation of the source language via a tree-based ordering model and treelet phrase extraction.We showed that it significantly outperforms a leading phrasal SMT system over a wide range of training set sizes and phrase sizes.Constituents vs. dependencies: Most attempts at syntactic SMT have relied on a constituency analysis rather than dependency analysis.While this is a natural starting point due to its wellunderstood nature and commonly available tools, we feel that this is not the most effective representation for syntax in MT.Dependency analysis, in contrast to constituency analysis, tends to bring semantically related elements together (e.g., verbs become adjacent to all their arguments) and is better suited to lexicalized models, such as the ones presented in this paper.The most important contribution of our system is a linguistically motivated ordering approach based on the source dependency tree, yet this paper only explores one possible model.Different model structures, machine learning techniques, and target feature representations all have the potential for significant improvements.Currently we only consider the top parse of an input sentence.One means of considering alternate possibilities is to build a packed forest of dependency trees and use this in decoding translations of each input sentence.As noted above, our approach shows particular promise for language pairs such as EnglishJapanese that exhibit large-scale reordering and have proven difficult for string-based approaches.Further experimentation with such language pairs is necessary to confirm this.Our experience has been that the quality of GIZA++ alignments for such language pairs is inadequate.Following up on ideas introduced by (Cherry & Lin, 03) we plan to explore ways to leverage the dependency tree to improve alignment quality.
Information extraction subsumes a broad range of tasks, including the extraction of entities, relations and events from various text sources, such as newswire documents and broadcast transcripts.One such task, relation detection, finds instances of predefined relations between pairs of entities, such as a Located-In relation between the entities Centre College and Danville, KY in the phrase Centre College in Danville, KY.The ‘entities’ are the individuals of selected semantic types (such as people, organizations, countries, ...) which are referred to in the text.Prior approaches to this task (Miller et al., 2000; Zelenko et al., 2003) have relied on partial or full syntactic analysis.Syntactic analysis can find relations not readily identified based on sequences of tokens alone.Even ‘deeper’ representations, such as logical syntactic relations or predicate-argument structure, can in principle capture additional generalizations and thus lead to the identification of additional instances of relations.However, a general problem in Natural Language Processing is that as the processing gets deeper, it becomes less accurate.For instance, the current accuracy of tokenization, chunking and sentence parsing for English is about 99%, 92%, and 90% respectively.Algorithms based solely on deeper representations inevitably suffer from the errors in computing these representations.On the other hand, low level processing such as tokenization will be more accurate, and may also contain useful information missed by deep processing of text.Systems based on a single level of representation are forced to choose between shallower representations, which will have fewer errors, and deeper representations, which may be more general.Based on these observations, Zhao et al. (2004) proposed a discriminative model to combine information from different syntactic sources using a kernel SVM (Support Vector Machine).We showed that adding sentence level word trigrams as global information to local dependency context boosted the performance of finding slot fillers for management succession events.This paper describes an extension of this approach to the identification of entity relations, in which syntactic information from sentence tokenization, parsing and deep dependency analysis is combined using kernel methods.At each level, kernel functions (or kernels) are developed to represent the syntactic information.Five kernels have been developed for this task, including two at the surface level, one at the parsing level and two at the deep dependency level.Our experiments show that each level of processing may contribute useful clues for this task, including surface information like word bigrams.Adding kernels one by one continuously improves performance.The experiments were carried out on the ACE RDR (Relation Detection and Recognition) task with annotated entities.Using SVM as a classifier along with the full composite kernel produced the best performance on this task.This paper will also show a comparison of SVM and KNN (k-Nearest-Neighbors) under different kernel setups.Many machine learning algorithms involve only the dot product of vectors in a feature space, in which each vector represents an object in the object domain.Kernel methods (Muller et al., 2001) can be seen as a generalization of feature-based algorithms, in which the dot product is replaced by a kernel function (or kernel) Ψ(X,Y) between two vectors, or even between two objects.Mathematically, as long as Ψ(X,Y) is symmetric and the kernel matrix formed by Ψ is positive semi-definite, it forms a valid dot product in an implicit Hilbert space.In this implicit space, a kernel can be broken down into features, although the dimension of the feature space could be infinite.Normal feature-based learning can be implemented in kernel functions, but we can do more than that with kernels.First, there are many wellknown kernels, such as polynomial and radial basis kernels, which extend normal features into a high order space with very little computational cost.This could make a linearly non-separable problem separable in the high order feature space.Second, kernel functions have many nice combination properties: for example, the sum or product of existing kernels is a valid kernel.This forms the basis for the approach described in this paper.With these combination properties, we can combine individual kernels representing information from different sources in a principled way.Many classifiers can be used with kernels.The most popular ones are SVM, KNN, and voted perceptrons.Support Vector Machines (Vapnik, 1998; Cristianini and Shawe-Taylor, 2000) are linear classifiers that produce a separating hyperplane with largest margin.This property gives it good generalization ability in high-dimensional spaces, making it a good classifier for our approach where using all the levels of linguistic clues could result in a huge number of features.Given all the levels of features incorporated in kernels and training data with target examples labeled, an SVM can pick up the features that best separate the targets from other examples, no matter which level these features are from.In cases where an error occurs in one processing result (especially deep processing) and the features related to it become noisy, SVM may pick up clues from other sources which are not so noisy.This forms the basic idea of our approach.Therefore under this scheme we can overcome errors introduced by one processing level; more particularly, we expect accurate low level information to help with less accurate deep level information.Collins et al. (1997) and Miller et al.(2000) used statistical parsing models to extract relational facts from text, which avoided pipeline processing of data.However, their results are essentially based on the output of sentence parsing, which is a deep processing of text.So their approaches are vulnerable to errors in parsing.Collins et al. (1997) addressed a simplified task within a confined context in a target sentence.Zelenko et al. (2003) described a recursive kernel based on shallow parse trees to detect personaffiliation and organization-location relations, in which a relation example is the least common subtree containing two entity nodes.The kernel matches nodes starting from the roots of two subtrees and going recursively to the leaves.For each pair of nodes, a subsequence kernel on their child nodes is invoked, which matches either contiguous or non-contiguous subsequences of node.Compared with full parsing, shallow parsing is more reliable.But this model is based solely on the output of shallow parsing so it is still vulnerable to irrecoverable parsing errors.In their experiments, incorrectly parsed sentences were eliminated.Culotta and Sorensen (2004) described a slightly generalized version of this kernel based on dependency trees.Since their kernel is a recursive match from the root of a dependency tree down to the leaves where the entity nodes reside, a successful match of two relation examples requires their entity nodes to be at the same depth of the tree.This is a strong constraint on the matching of syntax so it is not surprising that the model has good precision but very low recall.In their solution a bag-of-words kernel was used to compensate for this problem.In our approach, more flexible kernels are used to capture regularization in syntax, and more levels of syntactic information are considered.Kambhatla (2004) described a Maximum Entropy model using features from various syntactic sources, but the number of features they used is limited and the selection of features has to be a manual process.1 In our model, we use kernels to incorporate more syntactic information and let a Support Vector Machine decide which clue is crucial.Some of the kernels are extended to generate high order features.We think a discriminative classifier trained with all the available syntactic features should do better on the sparse data.ACE (Automatic Content Extraction)2 is a research and development program in information extraction sponsored by the U.S. Government.The 2004 evaluation defined seven major types of relations between seven types of entities.The entity types are PER (Person), ORG (Organization), FAC (Facility), GPE (Geo-Political Entity: countries, cities, etc.), LOC (Location), WEA (Weapon) and VEH (Vehicle).Each mention of an entity has a mention type: NAM (proper name), NOM (nominal) or 1 Kambhatla also evaluated his system on the ACE relation detection task, but the results are reported for the 2003 task, which used different relations and different training and test data, and did not use hand-annotated entities, so they cannot be readily compared to our results.PRO (pronoun); for example George W. Bush, the president and he respectively.The seven relation types are EMP-ORG (Employment/Membership/Subsidiary), PHYS (Physical), PER-SOC (Personal/Social), GPE-AFF (GPEAffiliation), Other-AFF (Person/ORG Affiliation), ART (Agent-Artifact) and DISC (Discourse).There are also 27 relation subtypes defined by ACE, but this paper only focuses on detection of relation types.Table 1 lists examples of each relation type. heads of the two entity arguments in a relation are marked.Types are listed in decreasing order of frequency of occurrence in the ACE corpus.Figure 1 shows a sample newswire sentence, in which three relations are marked.In this sentence, we expect to find a PHYS relation between Hezbollah forces and areas, a PHYS relation between Syrian troops and areas and an EMP-ORG relation between Syrian troops and Syrian.In our approach, input text is preprocessed by the Charniak sentence parser (including tokenization and POS tagging) and the GLARF (Meyers et al., 2001) dependency analyzer produced by NYU.Based on treebank parsing, GLARF produces labeled deep dependencies between words (syntactic relations such as logical subject and logical object).It handles linguistic phenomena like passives, relatives, reduced relatives, conjunctions, etc.That's because Israel was expected to retaliate against Hezbollah forces in areas controlled by Syrian troops.In our model, kernels incorporate information from tokenization, parsing and deep dependency analysis.A relation candidate R is defined as where arg1 and arg2 are the two entity arguments which may be related; seq=(t1, t2, ..., tn) is a token vector that covers the arguments and intervening words; link=(t1, t2, ..., tm) is also a token vector, generated from seq and the parse tree; path is a dependency path connecting arg1 and arg2 in the dependency graph produced by GLARF. path can be empty if no such dependency path exists.The difference between link and seq is that link only retains the “important” words in seq in terms of syntax.For example, all noun phrases occurring in seq are replaced by their heads.Words and constituent types in a stop list, such as time expressions, are also removed.A token T is defined as a string triple, where word, pos and base are strings representing the word, part-of-speech and morphological base form of T. Entity is a token augmented with other attributes, where tk is the token associated with E; type, subtype and mtype are strings representing the entity type, subtype and mention type of E. The subtype contains more specific information about an entity.For example, for a GPE entity, the subtype tells whether it is a country name, city name and so on.Mention type includes NAM, NOM and PRO.It is worth pointing out that we always treat an entity as a single token: for a nominal, it refers to its head, such as boys in the two boys; for a proper name, all the words are connected into one token, such as Bashar_Assad.So in a relation example R whose seq is (t1, t2, ..., tn), it is always true that arg1=t1 and arg2=tn.For names, the base form of an entity is its ACE type (person, organization, etc.).To introduce dependencies, we define a dependency token to be a token augmented with a vector of dependency arcs, DT=(word, pos, base, dseq), where dseq = (arc1, ... , arcn ).A dependency arc is ARC = (w, dw, label, e), where w is the current token; dw is a token connected by a dependency to w; and label and e are the role label and direction of this dependency arc respectively.From now on we upgrade the type of tk in arg1 and arg2 to be dependency tokens.Finally, path is a vector of dependency arcs, where l is the length of the path and arci (1<i<l) satisfies arc1.w=arg1.tk, arci+1.w=arci.dw and arcl.dw=arg2.tk.So path is a chain of dependencies connecting the two arguments in R. The arcs in it do not have to be in the same direction.Figure 2 shows a relation example generated from the text “... in areas controlled by Syrian troops”.In this relation example R, arg-, is ((“areas”, “NNS”, “area”, dseq), “LOC”, “Region”, “NOM”), and arg-,.dseq is ((OBJ, areas, in, 1), (OBJ, areas, controlled, 1)). arg2 is ((“troops”, “NNS”, “troop”, dseq), “ORG”, “Government”, “NOM”) and arg2.dseq = ((A-POS, troops, Syrian, 0), (SBJ, troops, controlled, 1)). path is ((OBJ, areas, controlled, 1), (SBJ, controlled, troops, 0)).The value 0 in a dependency arc indicates forward direction from w to dw, and 1 indicates backward direction.The seq and link sequences of R are shown in Figure 2.Some relations occur only between very restricted types of entities, but this is not true for every type of relation.For example, PER-SOC is a relation mainly between two person entities, while PHYS can happen between any type of entity and a GPE or LOC entity.In this section we will describe the kernels designed for different syntactic sources and explain the intuition behind them.We define two kernels to match relation examples at surface level.Using the notation just defined, we can write the two surface kernels as follows: KT is a kernel that matches two tokens.I(x, y) is a binary string match operator that gives 1 if x=y and 0 otherwise.Kernel Ψ1 matches attributes of two entity arguments respectively, such as type, subtype and lexical head of an entity.This is based on the observation that there are type constraints on the two arguments.For instance PER-SOC is a relation mostly between two person entities.So the attributes of the entities are crucial clues.Lexical information is also important to distinguish relation types.For instance, in the phrase U.S. president there is an EMP-ORG relation between president and U.S., while in a U.S. businessman there is a GPE-AFF relation between businessman and U.S. where Operator <t1, t2> concatenates all the string elements in tokens t1 and t2 to produce a new token.So Ψ2 is a kernel that simply matches unigrams and bigrams between the seq sequences of two relation examples.The information this kernel provides is faithful to the text. where min_len is the length of the shorter link sequence in R1 and R2.Ψ3 is a kernel that matches token by token between the link sequences of two relation examples.Since relations often occur in a short context, we expect many of them have similar link sequences.ψ 4 (R1 , R2)= Kpath (R1. path, R2 . path ), where Intuitively the dependency path connecting two arguments could provide a high level of syntactic regularization.However, a complete match of two dependency paths is rare.So this kernel matches the component arcs in two dependency paths in a pairwise fashion.Two arcs can match only when they are in the same direction.In cases where two paths do not match exactly, this kernel can still tell us how similar they are.In our experiments we placed an upper bound on the length of dependency paths for which we computed a non-zero kernel. where This kernel matches the local dependency context around the relation arguments.This can be helpful especially when the dependency path between arguments does not exist.We also hope the dependencies on each argument may provide some useful clues about the entity or connection of the entity to the context outside of the relation example.Having defined all the kernels representing shallow and deep processing results, we can define composite kernels to combine and extend the individual kern d Ψ3 covers the most important clues for this task: information about the two arguments and the word link between them.The polynomial extension is equivalent to adding pairs of features as new features.Intuitively this introduces new features like: the subtype of the first argument is a country name and the word of the second argument is president, which could be a good clue for an EMP-ORG relation.The polynomial kernel is down weighted by a normalization factor because we do not want the high order features to overwhelm the original ones.In our experiment, using polynomial kernels with degree higher than 2 does not produce better results.2) Full kernel This is the final kernel we used for this task, which is a combination of all the previous kernels.In our experiments, we set all the scalar factors to 1.Different values were tried, but keeping the original weight for each kernel yielded the best results for this task.All the individual kernels we designed are explicit.Each kernel can be seen as a matching of features and these features are enumerable on the given data.So it is clear that they are all valid kernels.Since the kernel function set is closed under linear combination and polynomial extension, the composite kernels are also valid.The reason we propose to use a feature-based kernel is that we can have a clear idea of what syntactic clues it represents and what kind of information it misses.This is important when developing or refining kernels, so that we can make them generate complementary information from different syntactic processing results.Experiments were carried out on the ACE RDR (Relation Detection and Recognition) task using hand-annotated entities, provided as part of the ACE evaluation.The ACE corpora contain documents from two sources: newswire (nwire) documents and broadcast news transcripts (bnews).In this section we will compare performance of different kernel setups trained with SVM, as well as different classifiers, KNN and SVM, with the same kernel setup.The SVM package we used is SVMlight.The training parameters were chosen using cross-validation.One-against-all classification was applied to each pair of entities in a sentence.When SVM predictions conflict on a relation example, the one with larger margin will be selected as the final answer.The ACE RDR training data contains 348 documents, 125K words and 4400 relations.It consists of both nwire and bnews documents.Evaluation of kernels was done on the training data using 5-fold cross-validation.We also evaluated the full kernel setup with SVM on the official test data, which is about half the size of the training data.All the data is preprocessed by the Charniak parser and GLARF dependency analyzer.Then relation examples are generated based these results.Table 2 shows the performance of the SVM on different kernel setups.The kernel setups in this experiment are incremental.From this table we can see that adding kernels continuously improves the performance, which indicates they provide additional clues to the previous setup.The argument kernel treats the two arguments as independent entities.The link sequence kernel introduces the syntactic connection between arguments, so adding it to the argument kernel boosted the performance.Setup F shows the performance of adding only dependency kernels to the argument kernel.The performance is not as good as setup B, indicating that dependency information alone is not as crucial as the link sequence. setups.Each setup adds one level of kernels to the previous one except setup F. Evaluated on the ACE training data with 5-fold cross-validation.Fscores marked by * are significantly better than the previous setup (at 95% confidence level).Another observation is that adding the bigram kernel in the presence of all other level of kernels improved both precision and recall, indicating that it helped in both correcting errors in other processing results and providing supplementary information missed by other levels of analysis.In another experiment evaluated on the nwire data only (about half of the training data), adding the bigram kernel improved F-score 0.5% and this improvement is statistically significant. different kernel setups.Types are ordered in decreasing order of frequency of occurrence in the ACE corpus.In SVM training, the same parameters were used for all 7 types.Table 3 shows the performance of SVM and KNN (k Nearest Neighbors) on different kernel setups.For KNN, k was set to 3.In the first setup of KNN, the two kernels which seem to contain most of the important information are used.It performs quite well when compared with the SVM result.The other two tests are based on the full kernel setup.For the two KNN experiments, adding more kernels (features) does not help.The reason might be that all kernels (features) were weighted equally in the composite kernel Φ2 and this may not be optimal for KNN.Another reason is that the polynomial extension of kernels does not have any benefit in KNN because it is a monotonic transformation of similarity values.So the results of KNN on kernel (Ψ1+Ψ3) and Φ1 would be exactly the same.We also tried different k for KNN and k=3 seems to be the best choice in either case.For the four major types of relations SVM does better than KNN, probably due to SVM’s generalization ability in the presence of large numbers of features.For the last three types with many fewer examples, performance of SVM is not as good as KNN.The reason we think is that training of SVM on these types is not sufficient.We tried different training parameters for the types with fewer examples, but no dramatic improvement obtained.We also evaluated our approach on the official ACE RDR test data and obtained very competitive scores.3 The primary scoring metric4 for the ACE evaluation is a 'value' score, which is computed by deducting from 100 a penalty for each missing and spurious relation; the penalty depends on the types of the arguments to the relation.The value scores produced by the ACE scorer for nwire and bnews test data are 71.7 and 68.0 repectively.The value score on all data is 70.1.5 The scorer also reports an F-score based on full or partial match of relations to the keys.The unweighted F-score for this test produced by the ACE scorer on all data is 76.0%.For this evaluation we used nearest neighbor to determine argument ordering and relation subtypes.The classification scheme in our experiments is one-against-all.It turned out there is not so much confusion between relation types.The confusion matrix of predictions is fairly clean.We also tried pairwise classification, and it did not help much.In this paper, we have shown that using kernels to combine information from different syntactic sources performed well on the entity relation detection task.Our experiments show that each level of syntactic processing contains useful information for the task.Combining them may provide complementary information to overcome errors arising from linguistic analysis.Especially, low level information obtained with high reliability helped with the other deep processing results.This design feature of our approach should be best employed when the preprocessing errors at each level are independent, namely when there is no dependency between the preprocessing modules.The model was tested on text with annotated entities, but its design is generic.It can work with noisy entity detection input from an automatic tagger.With all the existing information from other processing levels, this model can be also expected to recover from errors in entity tagging.Kernel functions have many nice properties.There are also many well known kernels, such as radial basis kernels, which have proven successful in other areas.In the work described here, only linear combinations and polynomial extensions of kernels have been evaluated.We can explore other kernel properties to integrate the existing syntactic kernels.In another direction, training data is often sparse for IE tasks.String matching is not sufficient to capture semantic similarity of words.One solution is to use general purpose corpora to create clusters of similar words; another option is to use available resources like WordNet.These word similarities can be readily incorporated into the kernel framework.To deal with sparse data, we can also use deeper text analysis to capture more regularities from the data.Such analysis may be based on newly-annotated corpora like PropBank (Kingsbury and Palmer, 2002) at the University of Pennsylvania and NomBank (Meyers et al., 2004) at New York University.Analyzers based on these resources can generate regularized semantic representations for lexically or syntactically related sentence structures.Although deeper analysis may even be less accurate, our framework is designed to handle this and still obtain some improvement in performance.This research was supported in part by the Defense Advanced Research Projects Agency under Grant N66001-04-1-8920 from SPAWAR San Diego, and by the National Science Foundation under Grant ITS-0325657.This paper does not necessarily reflect the position of the U.S. Government.We wish to thank Adam Meyers of the NYU NLP group for his help in producing deep dependency analyses.
Arabic is a morphologically complex language.1 The morphological analysis of a word consists of determining the values of a large number of (orthogonal) features, such as basic part-of-speech (i.e., noun, verb, and so on), voice, gender, number, information about the clitics, and so on.2 For Arabic, this gives us about 333,000 theoretically possible completely specified morphological analyses, i.e., morphological tags, of which about 2,200 are actually used in the first 280,000 words of the Penn Arabic Treebank (ATB).In contrast, English morphological tagsets usually have about 50 tags, which cover all morphological variation.As a consequence, morphological disambiguation of a word in context, i.e., choosing a complete 1We would like to thank Mona Diab for helpful discussions.The work reported in this paper was supported by NSF Award 0329163.The authors are listed in alphabetical order.2In this paper, we only discuss inflectional morphology.Thus, the fact that the stem is composed of a root, a pattern, and an infix vocalism is not relevant except as it affects broken plurals and verb aspect. morphological tag, cannot be done successfully using methods developed for English because of data sparseness.Hajiˇc (2000) demonstrates convincingly that morphological disambiguation can be aided by a morphological analyzer, which, given a word without any context, gives us the set of all possible morphological tags.The only work on Arabic tagging that uses a corpus for training and evaluation (that we are aware of), (Diab et al., 2004), does not use a morphological analyzer.In this paper, we show that the use of a morphological analyzer outperforms other tagging methods for Arabic; to our knowledge, we present the best-performing wide-coverage tokenizer on naturally occurring input and the bestperforming morphological tagger for Arabic.Arabic words are often ambiguous in their morphological analysis.This is due to Arabic’s rich system of affixation and clitics and the omission of disambiguating short vowels and other orthographic diacritics in standard orthography (“undiacritized orthography”).On average, a word form in the ATB has about 2 morphological analyses.An example of a word with some of its possible analyses is shown in Figure 1.Analyses 1 and 4 are both nouns.They differ in that the first noun has no affixes, while the second noun has a conjunction prefix (+ +w ‘and’) and a pronominal possessive suffix ( + +y ‘my’).In our approach, tokenizing and morphologically tagging (including part-of-speech tagging) are the same operation, which consists of three phases.First, we obtain from our morphological analyzer a list of all possible analyses for the words of a given sentence.We discuss the data and our lexicon in more detail in Section 4.Second, we apply classifiers for ten morphological features to the words of the text.The full list of features is shown in Figure 2, which also identifies possible values and which word classes (POS) can express these features.We discuss the training and decoding of these classifiers in Section 5.Third, we choose among the analyses returned by the morphological analyzer by using the output of the classifiers.This is a non-trivial task, as the classifiers may not fully disambiguate the options, or they may be contradictory, with none of them fully matching any one choice.We investigate different ways of making this choice in Section 6.As a result of this process, we have the original text, with each word augmented with values for all the features in Figure 2.These values represent a complete morphological disambiguation.Furthermore, these features contain enough information about the presence of clitics and affixes to perform tokenization, for any reasonable tokenization scheme.Finally, we can determine the POS tag, for any morphologically motivated POS tagset.Thus, we have performed tokenization, traditional POS tagging, and full morphological disambiguation in one fell swoop.Our work is inspired by Hajiˇc (2000), who convincingly shows that for five Eastern European languages with complex inflection plus English, using a morphological analyzer3 improves performance of a tagger.He concludes that for highly inflectional languages “the use of an independent morphological dictionary is the preferred choice [over] more annotated data”.Hajiˇc (2000) uses a general exponential model to predict each morphological feature separately (such as the ones we have listed in Figure 2), but he trains different models for each ambiguity left unresolved by the morphological analyzer, rather than training general models.For all languages, the use of a morphological analyzer results in tagging error reductions of at least 50%.We depart from Hajiˇc’s work in several respects.First, we work on Arabic.Second, we use this approach to also perform tokenization.Third, we use the SVM-based Yamcha (which uses Viterbi decoding) rather than an exponential model; however, we do not consider this difference crucial and do not contrast our learner with others in this paper.Fourth, and perhaps most importantly, we do not use the notion of ambiguity class in the feature classifiers; instead we investigate different ways of using the results of the individual feature classifiers in directly choosing among the options produced for the word by the morphological analyzer.While there have been many publications on computational morphological analysis for Arabic (see (Al-Sughaiyer and Al-Kharashi, 2004) for an excellent overview), to our knowledge only Diab et al. (2004) perform a large-scale corpus-based evaluation of their approach.They use the same SVMbased learner we do, Yamcha, for three different tagging tasks: word tokenization (tagging on letters of a word), which we contrast with our work in Section 7; POS tagging, which we discuss in relation to our work in Section 8; and base phrase chunking, which we do not discuss in this paper.We take the comparison between our results on POS tagging and those of Diab et al. (2004) to indicate that the use of a morphological analyzer is beneficial for Arabic as well.Several other publications deal specifically with segmentation.Lee et al. (2003) use a corpus of manually segmented words, which appears to be a subset of the first release of the ATB (110,000 words), and thus comparable to our training corpus.They obtain a list of prefixes and suffixes from this corpus, which is apparently augmented by a manually derived list of other affixes.Unfortunately, the full segmentation criteria are not given.Then a trigram model is learned from the segmented training corpus, and this is used to choose among competing segmentations for words in running text.In addition, a huge unannotated corpus (155 million words) is used to iteratively learn additional stems.Lee et al. (2003) show that the unsupervised use of the large corpus for stem identification increases accuracy.Overall, their error rates are higher than ours (2.9% vs. 0.7%), presumably because they do not use a morphological analyzer.There has been a fair amount of work on entirely unsupervised segmentation.Among this literature, Rogati et al. (2003) investigate unsupervised learning of stemming (a variant of tokenization in which only the stem is retained) using Arabic as the example language.Unsurprisingly, the results are much worse than in our resource-rich approach.Darwish (2003) discusses unsupervised identification of roots; as mentioned above, we leave root identification to future work.The data we use comes from the Penn Arabic Treebank (Maamouri et al., 2004).Like the English Penn Treebank, the corpus is a collection of news texts.Unlike the English Penn Treebank, the ATB is an ongoing effort, which is being released incrementally.As can be expected in this situation, the annotation has changed in subtle ways between the incremental releases.Even within one release (especially the first) there can be inconsistencies in the annotation.As our approach builds on linguistic knowledge, we need to carefully study how linguistic facts are represented in the ATB.In this section, we briefly summarize how we obtained the data in the representation we use for our machine learning experiments.4 We use the first two releases of the ATB, ATB1 and ATB2, which are drawn from different news sources.We divided both ATB1 and ATB2 into development, training, and test corpora with roughly 12,000 word tokens in each of the development and test corpora, and 120,000 words in each of the training corpora.We will refer to the training corpora as TR1 and TR2, and to the test corpora as, TE1 and TE2.We report results on both TE1 and TE2 because of the differences in the two parts of the ATB, both in terms of origin and in terms of data preparation.We use the ALMORGEANA morphological analyzer (Habash, 2005), a lexeme-based morphological generator and analyzer for Arabic.5 A sample output of the morphological analyzer is shown in Figure 1.ALMORGEANA uses the databases (i.e., lexicon) from the Buckwalter Arabic Morphological Analyzer, but (in analysis mode) produces an output in the lexeme-and-feature format (which we need for our approach) rather than the stem-and-affix format of the Buckwalter analyzer.We use the data from first version of the Buckwalter analyzer (Buckwalter, 2002).The first version is fully consistent with neither ATB1 nor ATB2.Our training data consists of a set of all possible morphological analyses for each word, with the unique correct analysis marked.Since we want to learn to choose the correct output using the features generated by ALMORGEANA, the training data must also be in the ALMORGEANA output format.To obtain this data, we needed to match data in the ATB to the lexeme-and-feature representation output by ALMORGEANA.The matching included the use of some heuristics, since the representations and choices are not always consistent in the ATB.For example, nHw ‘towards’ is tagged as AV, N, or V (in the same syntactic contexts).We verified whether we introduced new errors while creating our data representation by manually inspecting 400 words chosen at random from TR1 and TR2.In eight cases, our POS tag differed from that in the ATB file; all but one case were plausible changes among Noun, Adjective, Adverb and Proper Noun resulting from missing entries in the Buckwalter’s lexicon.The remaining case was a failure in the conversion process relating to the handling of broken plurals at the lexeme level.We conclude that our data representation provides an adequate basis for performing machine learning experiments.An important issue in using morphological analyzers for morphological disambiguation is what happens to unanalyzed words, i.e., words that receive no analysis from the morphological analyzer.These are frequently proper nouns; a typical example is brlwskwny ‘Berlusconi’, for which no entry exists in the Buckwalter lexicon.A backoff analysis mode in ALMORGEANA uses the morphological databases of prefixes, suffixes, and allowable combinations from the Buckwalter analyzer to hypothesize all possible stems along with feature sets.Our Berlusconi example yields 41 possible analyses, including the correct one (as a singular masculine PN).Thus, with the backoff analysis, unanalyzed words are distinguished for us only by the larger number of possible analyses (making it harder to choose the correct analysis).There are not many unanalyzed words in our corpus.In TR1, there are only 22 such words, presumably because the Buckwalter lexicon our morphological analyzer uses was developed onTR1.In TR2, we have 737 words without analysis (0.61% of the entire corpus, giving us a coverage of about 99.4% on domainsimilar text for the Buckwalter lexicon).In ATB1, and to a lesser degree in ATB2, some words have been given no morphological analysis.(These cases are not necessarily the same words that our morphological analyzer cannot analyze.)The POS tag assigned to these words is then NO FUNC.In TR1 (138,756 words), we have 3,088 NO FUNC POS labels (2.2%).In TR2 (168,296 words), the number of NO FUNC labels has been reduced to 853 (0.5%).Since for these cases, there is no meaningful solution in the data, we have removed them from the evaluation (but not from training).In contrast, Diab et al. (2004) treat NO FUNC like any other POS tag, but it is unclear whether this is meaningful.Thus, when comparing results from different approaches which make different choices about the data (for example, the NO FUNC cases), one should bear in mind that small differences in performance are probably not meaningful.We now describe how we train classifiers for the morphological features in Figure 2.We train one classifier per feature.We use Yamcha (Kudo and Matsumoto, 2003), an implementation of support vector machines which includes Viterbi decoding.6 As training features, we use two sets.These sets are based on the ten morphological features in Figure 2, plus four other “hidden” morphological features, for which we do not train classifiers, but which are represented in the analyses returned by the morphological analyzer.The reason we do not train classifiers for the hidden features is that they are only returned by the morphological analyzer when they are marked overtly in orthography, but they are not disambiguated in case they are not overtly marked.The features are indefiniteness (presence of nunation), idafa (possessed), case, and mood.First, for each of the 14 morphological features and for each possible value (including ‘NA’ if applicable), we define a binary machine learning feature which states whether in any morphological analysis for that word, the feature has that value.This gives us 58 machine learning features per word.In addition, we define a second set of features which abstracts over the first set: for all features, we state whether any morphological analysis for that word has a value other than ‘NA’.This yields a further 11 machine learning features (as 3 morphological features never have the value ‘NA’).In addition, we use the untokenized word form and a binary feature stating whether there is an analysis or not.This gives us a total of 71 machine learning features per word.We specify a window of two words preceding and following the current word, using all 71 features for each word in this 5-word window.In addition, two dynamic features are used, namely the classification made for the preceding two words.For each of the ten classifiers, Yamcha then returns a confidence value for each possible value of the classifier, and in addition it marks the value that is chosen during subsequent Viterbi decoding (which need not be the value with the highest confidence value because of the inclusion of dynamic features).We train on TR1 and report the results for the ten Yamcha classifiers on TE1 and TE2, using all simple tokens,7 including punctuation, in Figure 3.The baseline BL is the most common value associated in the training corpus TR1 with every feature for a given word form (unigram).We see that the baseline for TE1 is quite high, which we assume is due to the fact that when there is ambiguity, often one interpretation is much more prevelant than the others.The error rates on the baseline approximately double on TE2, reflecting the difference between TE2 and TR1, and the small size of TR1.The performance of our classifiers is good on TE1 (third column), and only slightly worse on TE2 (fifth column).We attribute the increase in error reduction over the baseline for TE2 to successfully learned generalizations.We investigated the performance of the classifiers on unanalyzed words.The performance is generally below the baseline BL.We attribute this to the almost complete absence of unanalyzed words in training data TR1.In future work we could attempt to improve performance in these cases; however, given their small number, this does not seem a priority.Once we have the results from the classifiers for the ten morphological features, we combine them to choose an analysis from among those returned by the morphological analyzer.We investigate several options for how to do this combination.In the following, we use two numbers for each analysis.First, the agreement is the number of classifiers agreeing with the analysis.Second, the weighted agreement is the sum, over all classifiers, of the classification confidence measure of that value that agrees with the analysis.The agreement, but not the weighted agreement, uses Yamcha’s Viterbi decoding. sifier agrees with the analysis, and with what confidence level.In addition, we use the word form.(The reason we use Ripper here is because it allows us to learn lower bounds for the confidence score features, which are real-valued.)In training, only the correct analysis is good.If exactly one analysis is classified as good, we choose that, otherwise we use Maj to choose.• The baseline (BL) chooses the analysis most commonly assigned in TR1 to the word in question.For unseen words, the choice is made randomly.In all cases, any remaining ties are resolved randomly.We present the performance in Figure 4.We see that the best performing combination algorithm on TE1 is Maj, and on TE2 it is Rip.Recall that the Yamcha classifiers are trained on TR1; in addition, Rip is trained on the output of these Yamcha classifiers on TR2.The difference in performance between TE1 and TE2 shows the difference between the ATB1 and ATB2 (different source of news, and also small differences in annotation).However, the results for Rip show that retraining the Rip classifier on a new corpus can improve the results, without the need for retraining all ten Yamcha classifiers (which takes considerable time).Figure 4 presents the accuracy of tagging using the whole complex morphological tagset.We can project this complex tagset to a simpler tagset, for example, POS.Then the minimum tagging accuracy for the simpler tagset must be greater than or equal to the accuracy of the complex morphological tagset.Even if a combining algorithm chooses the wrong analysis (and this is counted as a failure for the evaluation in this section), the chosen analysis may agree with some of the correct morphological features.We discuss our performance on the POS feature in Section 8.The term “tokenization” refers to the segmenting of a naturally occurring input sequence of orthographic symbols into elementary symbols (“tokens”) used in subsequent processing steps (such as parsing) as basic units.In our approach, we determine all morphological properties of a word at once, so we can use this information to determine tokenization.There is not a single possible or obvious tokenization scheme: a tokenization scheme is an analytical tool devised by the researcher.We evaluate in this section how well our morphological disambiguation curacy measures for each input word whether it gets tokenized correctly, independently of the number of resulting tokens; the token-based measures refer to the four token fields into which the ATB splits each word determines the ATB tokenization.The ATB starts with a simple tokenization, and then splits the word into four fields: conjunctions; particles (prepositions in the case of nouns); the word stem; and pronouns (object clitics in the case of verbs, possessive clitics in the case of nouns).The ATB does not tokenize the definite article + Al+.We compare our output to the morphologically analyzed form of the ATB, and determine if our morphological choices lead to the correct identification of those clitics that need to be stripped off.8 For our evaluation, we only choose the Maj chooser, as it performed best on TE1.We evaluate in two ways.In the first evaluation, we determine for each simple input word whether the tokenization is correct (no matter how many ATB tokens result).We report the percentage of words which are correctly tokenized in the second column in Figure 5.In the second evaluation, we report on the number of output tokens.Each word is divided into exactly four token fields, which can be either filled or empty (in the case of the three clitic token fields) or correct or incorrect (in the case of the stem token field).We report in Figure 5 accuracy over all token fields for all words in the test corpus, as well as recall, precision, and f-measure for the non-null token fields.The baseline BL is the tokenization associated with the morphological analysis most frequently chosen for the input word in training.8The ATB generates normalized forms of certain clitics and of the word stem, so that the resulting tokens are not simply the result of splitting the original words.We do not actually generate the surface token form from our deep representation, but this can be done in a deterministic, rule-based manner, given our rich morphological analysis, e.g., by using ALMORGEANA in generation mode after splitting off all separable tokens.While the token-based evaluation is identical to that performed by Diab et al. (2004), the results are not directly comparable as they did not use actual input words, but rather recreated input words from the regenerated tokens in the ATB.Sometimes this can simplify the analysis: for example, a p (ta marbuta) must be word-final in Arabic orthography, and thus a word-medial p in a recreated input word reliably signals a token boundary.The rather high baseline shows that tokenization is not a hard problem.The POS tagset Diab et al. (2004) use is a subset of the tagset for English that was introduced with the English Penn Treebank.The large set of Arabic tags has been mapped (by the Linguistic Data Consortium) to this smaller English set, and the meaning of the English tags has changed.We consider this tagset unmotivated, as it makes morphological distinctions because they are marked in English, not Arabic.The morphological distinctions that the English tagset captures represent the complete morphological variation that can be found in English.However, in Arabic, much morphological variation goes untagged.For example, verbal inflections for subject person, number, and gender are not marked; dual and plural are not distinguished on nouns; and gender is not marked on nouns at all.In Arabic nouns, arguably the gender feature is the more interesting distinction (rather than the number feature) as verbs in Arabic always agree with their nominal subjects in gender.Agreement in number occurs only when the nominal subject precedes the verb.We use the tagset here only to compare to previous work.Instead, we advocate using a reduced part-of-speech tag set,9 along with the other orthogonal linguistic features in Figure 2.We map our best solutions as chosen by the Maj model in Section 6 to the English tagset, and we furthermore assume (as do Diab et al. (2004)) the gold standard tokenization.We then evaluate against the gold standard POS tagging which we have mapped for all tokens (based on gold-standard tokenization) and only for word tokens, using the Penn Treebank (PTB) tagset as well as the smaller tagset (Smp) (see Footnote 9); BL is the baseline obtained by using the POS value from the baseline tag used in Section 6 similarly.We obtain a score for TE1 of 97.6% on all tokens.Diab et al. (2004) report a score of 95.5% for all tokens on a test corpus drawn from ATB1, thus their figure is comparable to our score of 97.6%.On our own reduced POS tagset, evaluating on TE1, we obtain an accuracy score of 98.1% on all tokens.The full dataset is shown in Figure 6.We have shown how to use a morphological analyzer for tokenization, part-of-speech tagging, and morphological disambiguation in Arabic.We have shown that the use of a morphological analyzer is beneficial in POS tagging, and we believe our results are the best published to date for tokenization of naturally occurring input (in undiacritized orthography) and POS tagging.We intend to apply our approach to Arabic dialects, for which currently no annotated corpora exist, and for which very few written corpora of any kind exist (making the dialects bad candidates even for unsupervised learning).However, there is a fair amount of descriptive work on dialectal morphology, so that dialectal morphological analyzers may be easier to come by than dialect corpora.We intend to explore to what extent we can transfer models trained on Standard Arabic to dialectal morphological disambiguation.
Word segmentation, i.e., discovering word boundaries in continuous text or speech, is of interest for both practical and theoretical reasons.It is the first step of processing orthographies without explicit word boundaries, such as Chinese.It is also one of the key problems that human language learners must solve as they are learning language.Many previous methods for unsupervised word segmentation are based on the observation that transitions between units (characters, phonemes, or syllables) within words are generally more predictable than transitions across word boundaries.Statistics that have been proposed for measuring these differences include “successor frequency” (Harris, 1954), “transitional probabilities” (Saffran et al., 1996), mutual information (Sun et al., ∗This work was partially supported by the following grants: NIH 1R01-MH60922, NIH RO1-DC000314, NSF IGERT-DGE-9870676, and the DARPA CALO project.1998), “accessor variety” (Feng et al., 2004), and boundary entropy (Cohen and Adams, 2001).While methods based on local statistics are quite successful, here we focus on approaches based on explicit probabilistic models.Formulating an explicit probabilistic model permits us to cleanly separate assumptions about the input and properties of likely segmentations from details of algorithms used to find such solutions.Specifically, this paper demonstrates the importance of contextual dependencies for word segmentation by comparing two probabilistic models that differ only in that the first assumes that the probability of a word is independent of its local context, while the second incorporates bigram dependencies between adjacent words.The algorithms we use to search for likely segmentations do differ, but so long as the segmentations they produce are close to optimal we can be confident that any differences in the segmentations reflect differences in the probabilistic models, i.e., in the kinds of dependencies between words.We are not the first to propose explicit probabilistic models of word segmentation.Two successful word segmentation systems based on explicit probabilistic models are those of Brent (1999) and Venkataraman (2001).Brent’s ModelBased Dynamic Programming (MBDP) system assumes a unigram word distribution.Venkataraman uses standard unigram, bigram, and trigram language models in three versions of his system, which we refer to as n-gram Segmentation (NGS).Despite their rather different generative structure, the MBDP and NGS segmentation accuracies are very similar.Moreover, the segmentation accuracy of the NGS unigram, bigram, and trigram models hardly differ, suggesting that contextual dependencies are irrelevant to word segmentation.However, the segmentations produced by both these methods depend crucially on properties of the search procedures they employ.We show this by exhibiting for each model a segmentation that is less accurate but more probable under that model.In this paper, we present an alternative framework for word segmentation based on the Dirichlet process, a distribution used in nonparametric Bayesian statistics.This framework allows us to develop extensible models that are amenable to standard inference procedures.We present two such models incorporating unigram and bigram word dependencies, respectively.We use Gibbs sampling to sample from the posterior distribution of possible segmentations under these models.The plan of the paper is as follows.In the next section, we describe MBDP and NGS in detail.In Section 3 we present the unigram version of our own model, the Gibbs sampling procedure we use for inference, and experimental results.Section 4 extends that model to incorporate bigram dependencies, and Section 5 concludes the paper.The NGS and MBDP systems are similar in some ways: both are designed to find utterance boundaries in a corpus of phonemically transcribed utterances, with known utterance boundaries.Both also use approximate online search procedures, choosing and fixing a segmentation for each utterance before moving onto the next.In this section, we focus on the very different probabilistic models underlying the two systems.We show that the optimal solution under the NGS model is the unsegmented corpus, and suggest that this problem stems from the fact that the model assumes a uniform prior over hypotheses.We then present the MBDP model, which uses a non-uniform prior but is difficult to extend beyond the unigram case.NGS assumes that each utterance is generated independently via a standard n-gram model.For simplicity, we will discuss the unigram version of the model here, although our argument is equally applicable to the bigram and trigram versions.The unigram model generates an utterance u according to the grammar in Figure 1, so where u consists of the words w1 ... wn and p$ is the probability of the utterance boundary marker $.This model can be used to find the highest probability segmentation hypothesis h given the data d by using Bayes’ rule: NGS assumes a uniform prior P(h) over hypotheses, so its goal is to find the solution that maximizes the likelihood P(djh).Using this model, NGS’s approximate search technique delivers competitive results.However, the true maximum likelihood solution is not competitive, since it contains no utterance-internal word boundaries.To see why not, consider the solution in which p$ = 1 and each utterance is a single ‘word’, with probability equal to the empirical probability of that utterance.Any other solution will match the empirical distribution of the data less well.In particular, a solution with additional word boundaries must have 1 − p$ > 0, which means it wastes probability mass modeling unseen data (which can now be generated by concatenating observed utterances together).Intuitively, the NGS model considers the unsegmented solution to be optimal because it ranks all hypotheses equally probable a priori.We know, however, that hypotheses that memorize the input data are unlikely to generalize to unseen data, and are therefore poor solutions.To prevent memorization, we could restrict our hypothesis space to models with fewer parameters than the number of utterances in the data.A more general and mathematically satisfactory solution is to assume a nonuniform prior, assigning higher probability to hypotheses with fewer parameters.This is in fact the route taken by Brent in his MBDP model, as we shall see in the following section.MBDP assumes a corpus of utterances is generated as a single probabilistic event with four steps: In a final deterministic step, the ordered tokens are concatenated to create an unsegmented corpus.This means that certain segmented corpora will produce the observed data with probability 1, and all others will produce it with probability 0.The posterior probability of a segmentation given the data is thus proportional to its prior probability under the generative model, and the best segmentation is that with the highest prior probability.There are two important points to note about the MBDP model.First, the distribution over L assigns higher probability to models with fewer lexical items.We have argued that this is necessary to avoid memorization, and indeed the unsegmented corpus is not the optimal solution under this model, as we will show in Section 3.Second, the factorization into four separate steps makes it theoretically possible to modify each step independently in order to investigate the effects of the various modeling assumptions.However, the mathematical statement of the model and the approximations necessary for the search procedure make it unclear how to modify the model in any interesting way.In particular, the fourth step uses a uniform distribution, which creates a unigram constraint that cannot easily be changed.Since our research aims to investigate the effects of different modeling assumptions on lexical acquisition, we develop in the following sections a far more flexible model that also incorporates a preference for sparse solutions.Our goal is a model of language that prefers sparse solutions, allows independent modification of components, and is amenable to standard search procedures.We achieve this goal by basing our model on the Dirichlet process (DP), a distribution used in nonparametric Bayesian statistics.Our unigram model of word frequencies is defined as where the concentration parameter α0 and the base distribution P0 are parameters of the model.Each word wi in the corpus is drawn from a distribution G, which consists of a set of possible words (the lexicon) and probabilities associated with those words.G is generated from a DP(α0, P0) distribution, with the items in the lexicon being sampled from P0 and their probabilities being determined by α0, which acts like the parameter of an infinite-dimensional symmetric Dirichlet distribution.We provide some intuition for the roles of α0 and P0 below.Although the DP model makes the distribution G explicit, we never deal with G directly.We take a Bayesian approach and integrate over all possible values of G. The conditional probability of choosing to generate a word from a particular lexical entry is then given by a simple stochastic process known as the Chinese restaurant process (CRP) (Aldous, 1985).Imagine a restaurant with an infinite number of tables, each with infinite seating capacity.Customers enter the restaurant and seat themselves.Let zi be the table chosen by the ith customer.Then where z−i = z1 ... zi−1, n(z−i) kis the number of customers already sitting at table k, and K(z−i) is the total number of occupied tables.In our model, the tables correspond to (possibly repeated) lexical entries, having labels generated from the distribution P0.The seating arrangement thus specifies a distribution over word tokens, with each customer representing one token.This model is an instance of the two-stage modeling framework described by Goldwater et al. (2006), with P0 as the generator and the CRP as the adaptor.Our model can be viewed intuitively as a cache model: each word in the corpus is either retrieved from a cache or generated anew.Summing over all the tables labeled with the same word yields the probability distribution for the ith word given previously observed words w−i: where n(w−i) w is the number of instances of w observed in w−i.The first term is the probability of generating w from the cache (i.e., sitting at an occupied table), and the second term is the probability of generating it anew (sitting at an unoccupied table).The actual table assignments z−i only become important later, in the bigram model.There are several important points to note about this model.First, the probability of generating a particular word from the cache increases as more instances of that word are observed.This richget-richer process creates a power-law distribution on word frequencies (Goldwater et al., 2006), the same sort of distribution found empirically in natural language.Second, the parameter α0 can be used to control how sparse the solutions found by the model are.This parameter determines the total probability of generating any novel word, a probability that decreases as more data is observed, but never disappears.Finally, the parameter P0 can be used to encode expectations about the nature of the lexicon, since it defines a probability distribution across different novel words.The fact that this distribution is defined separately from the distribution on word frequencies gives the model additional flexibility, since either distribution can be modified independently of the other.Since the goal of this paper is to investigate the role of context in word segmentation, we chose the simplest possible model for P0, i.e. a unigram phoneme distribution: where word w consists of the phonemes m1 ... mn, and p# is the probability of the word boundary #.For simplicity we used a uniform distribution over phonemes, and experimented with different fixed values of p#.1 A final detail of our model is the distribution on utterance lengths, which is geometric.That is, we assume a grammar similar to the one shown in Figure 1, with the addition of a symmetric Beta(τ2 ) prior over the probability of the U productions,2 and the substitution of the DP for the standard multinomial distribution over the W productions.Having defined our generative model, we are left with the problem of inference: we must determine the posterior distribution of hypotheses given our input corpus.To do so, we use Gibbs sampling, a standard Markov chain Monte Carlo method (Gilks et al., 1996).Gibbs sampling is an iterative procedure in which variables are repeatedly sampled from their conditional posterior distribution given the current values of all other variables in the model.The sampler defines a Markov chain whose stationary distribution is P(h|d), so after convergence samples are from this distribution.Our Gibbs sampler considers a single possible boundary point at a time, so each sample is from a set of two hypotheses, h1 and h2.These hypotheses contain all the same boundaries except at the one position under consideration, where h2 has a boundary and h1 does not.The structures are shown in Figure 2.In order to sample a hypothesis, we need only calculate the relative probabilities of h1 and h2.Since h1 and h2 are the same except for a few rules, this is straightforward.Let h− be all of the structure shared by the two hypotheses, including n− words, and let d be the observed data.Then where the second line follows from Equation 3 and the properties of the CRP (in particular, that it is exchangeable, with the probability of a seating configuration not depending on the order in which customers arrive (Aldous, 1985)).Also, where nr is the number of branching rules r = U —* W U in h−, and I(.) is an indicator function taking on the value 1 when its argument is true, and 0 otherwise.The nr term is derived by integrating over all possible values of pg, and noting that the total number of U productions in h− is n− + 1.Using these equations we can simply proceed through the data, sampling each potential boundary point in turn.Once the Gibbs sampler converges, these samples will be drawn from the posterior distribution P(h1d).In our experiments, we used the same corpus that NGS and MBDP were tested on.The corpus, supplied to us by Brent, consists of 9790 transcribed utterances (33399 words) of childdirected speech from the Bernstein-Ratner corpus (Bernstein-Ratner, 1987) in the CHILDES database (MacWhinney and Snow, 1985).The utterances have been converted to a phonemic representation using a phonemic dictionary, so that each occurrence of a word has the same phonemic transcription.Utterance boundaries are given in the input to the system; other word boundaries are not.Because our Gibbs sampler is slow to converge, we used annealing to speed inference.We began with a temperature of -y = 10 and decreased -y in 10 increments to a final value of 1.A temperature of -y corresponds to raising the probabilities of h1 and h2 to the power of γ 1 prior to sampling.We ran our Gibbs sampler for 20,000 iterations through the corpus (with -y = 1 for the final 2000) and evaluated our results on a single sample at that point.We calculated precision (P), recall (R), and F-score (F) on the word tokens in the corpus, where both boundaries of a word must be correct to count the word as correct.The induced lexicon was also scored for accuracy using these metrics (LP, LR, LF).Recall that our DP model has three parameters: T, p#, and α0.Given the large number of known utterance boundaries, we expect the value of T to have little effect on our results, so we simply fixed T = 2 for all experiments.Figure 3 shows the effects of varying of p# and α0.3 Lower values of p# cause longer words, which tends to improve recall (and thus F-score) in the lexicon, but decrease token accuracy.Higher values of α0 allow more novel words, which also improves lexicon recall, as a function of p#, with α0 = 20 and (b) as a function of α0, with p# = .5. but begins to degrade precision after a point.Due to the negative correlation between token accuracy and lexicon accuracy, there is no single best value for either p# or α0; further discussion refers to the solution for p# = .5, α0 = 20 (though others are qualitatively similar).In Table 1(a), we compare the results of our system to those of MBDP and NGS.4 Although our system has higher lexicon accuracy than the others, its token accuracy is much worse.This result occurs because our system often mis-analyzes frequently occurring words.In particular, many of these words occur in common collocations such as what’s that and do you, which the system interprets as a single words.It turns out that a full 31% of the proposed lexicon and nearly 30% of tokens consist of these kinds of errors.Upon reflection, it is not surprising that a unigram language model would segment words in this way.Collocations violate the unigram assumption in the model, since they exhibit strong word-toword dependencies.The only way the model can capture these dependencies is by assuming that these collocations are in fact words themselves.Why don’t the MBDP and NGS unigram models exhibit these problems?We have already shown that NGS’s results are due to its search procedure rather than its model.The same turns out to be true for MBDP.Table 2 shows the probabilider each model of the true solution, the solution with no utterance-internal boundaries, and the solutions found by each algorithm.Best solutions under each model are bold. ties under each model of various segmentations of the corpus.From these figures, we can see that the MBDP model assigns higher probability to the solution found by our Gibbs sampler than to the solution found by Brent’s own incremental search algorithm.In other words, Brent’s model does prefer the lower-accuracy collocation solution, but his search algorithm instead finds a higher-accuracy but lower-probability solution.We performed two experiments suggesting that our own inference procedure does not suffer from similar problems.First, we initialized our Gibbs sampler in three different ways: with no utteranceinternal boundaries, with a boundary after every character, and with random boundaries.Our results were virtually the same regardless of initialization.Second, we created an artificial corpus by randomly permuting the words in the true corpus, leaving the utterance lengths the same.The artificial corpus adheres to the unigram assumption of our model, so if our inference procedure works correctly, we should be able to correctly identify the words in the permuted corpus.This is exactly what we found, as shown in Table 1(b).While all three models perform better on the artificial corpus, the improvements of the DP model are by far the most striking.The results of our unigram experiments suggested that word segmentation could be improved by taking into account dependencies between words.To test this hypothesis, we extended our model to incorporate bigram dependencies using a hierarchical Dirichlet process (HDP) (Teh et al., 2005).Our approach is similar to previous n-gram models using hierarchical Pitman-Yor processes (Goldwater et al., 2006; Teh, 2006).The HDP is appropriate for situations in which there are multiple distributions over similar sets of outcomes, and the distributions are believed to be similar.In our case, we define a bigram model by assuming each word has a different distribution over the words that follow it, but all these distributions are linked.The definition of our bigram language model as an HDP is That is, P(wi|wi−1 = w) is distributed according to Hw, a DP specific to word w. Hw is linked to the DPs for all other words by the fact that they share a common base distribution G, which is generated from another DP.5 As in the unigram model, we never deal with Hw or G directly.By integrating over them, we get a distribution over bigram frequencies that can be understood in terms of the CRP.Now, each word type w is associated with its own restaurant, which represents the distribution over words that follow w. Different restaurants are not completely independent, however: the labels on the tables in the restaurants are all chosen from a common base distribution, which is another CRP.To understand the HDP model in terms of a grammar, we consider $ as a special word type, so that wi ranges over E∗ U J$J.After observing w−i, the HDP grammar is as shown in Figure 4, where h−i = (w−i, z−i); t$, tE∗, and twi are the total number of tables (across all words) labeled with $, non-$, and wi, respectively; t = t$ + tE∗ is the total number of tables; and n(wi−1,wi) is the number of occurrences of the bigram (wi−1, wi).We have suppressed the superscript (w−i) notation in all cases.The base distribution shared by all bigrams is given by P1, which can be viewed as a unigram backoff where the unigram probabilities are learned from the bigram table labels.We can perform inference on this HDP bigram model using a Gibbs sampler similar to our unigram sampler.Details appear in the Appendix.We used the same basic setup for our experiments with the HDP model as we used for the DP model.We experimented with different values of α0 and α1, keeping p# = .5 throughout.Some results of these experiments are plotted in Figure 5.With appropriate parameter settings, both lexicon and token accuracy are higher than in the unigram model (dramatically so, for tokens), and there is no longer a negative correlation between the two.Only a few collocations remain in the lexicon, and most lexicon errors are on low-frequency words.The best values of α0 are much larger than in the unigram model, presumably because all unique word types must be generated via P0, but in the bigram model there is an additional level of discounting (the unigram process) before reaching P0.Smaller values of α0 lead to fewer word types with fewer characters on average.Table 3 compares the optimal results of the HDP model to the only previous model incorporating bigram dependencies, NGS.Due to search, the performance of the bigram NGS model is not much different from that of the unigram model.In Figure 5: Word (F) and lexicon (LF) F-score (a) as a function of α0, with α1 = 10 and (b) as a function of α1, with α0 = 1000. in bold.HDP results are with p# = .5, α0 = 1000, and α1 = 10. contrast, our HDP model performs far better than our DP model, leading to the highest published accuracy for this corpus on both tokens and lexical items.Overall, these results strongly support our hypothesis that modeling bigram dependencies is important for accurate word segmentation.In this paper, we have introduced a new modelbased approach to word segmentation that draws on techniques from Bayesian statistics, and we have developed models incorporating unigram and bigram dependencies.The use of the Dirichlet process as the basis of our approach yields sparse solutions and allows us the flexibility to modify individual components of the models.We have presented a method of inference using Gibbs sampling, which is guaranteed to converge to the posterior distribution over possible segmentations of a corpus.Our approach to word segmentation allows us to investigate questions that could not be addressed satisfactorily in earlier work.We have shown that the search algorithms used with previous models of word segmentation do not achieve their objectives, which has led to misleading results.In particular, previous work suggested that the use of word-to-word dependencies has little effect on word segmentation.Our experiments indicate instead that bigram dependencies can be crucial for avoiding under-segmentation of frequent collocations.Incorporating these dependencies into our model greatly improved segmentation accuracy, and led to better performance than previous approaches on all measures.J. Saffran, E. Newport, and R. Aslin.1996.Word segmentation: The role of distributional cues.Journal of Memory and Language, 35:606–621.M. Sun, D. Shen, and B. Tsou.1998.Chinese word segmentation without using lexicon and hand-crafted training data.In Proceedings of COLING-ACL.Y. Teh, M. Jordan, M. Beal, and D. Blei.2005.Hierarchical Dirichlet processes.In Advances in Neural Information Processing Systems 17.MIT Press, Cambridge, MA.Y. Teh.2006.A Bayesian interpretation of interpolated kneser-ney.Technical Report TRA2/06, National University of Singapore, School of Computing.A. Venkataraman.2001.A statistical model for word discovery in transcribed speech.Computational Linguistics, 27(3):351–372.
The goal of capturing structured relational knowledge about lexical terms has been the motivating force underlying many projects in lexical acquisition, information extraction, and the construction of semantic taxonomies.Broad-coverage semantic taxonomies such as WordNet (Fellbaum, 1998) and CYC (Lenat, 1995) have been constructed by hand at great cost; while a crucial source of knowledge about the relations between words, these taxonomies still suffer from sparse coverage.Many algorithms with the potential for automatically extending lexical resources have been proposed, including work in lexical acquisition (Riloff and Shepherd, 1997; Roark and Charniak, 1998) and in discovering instances, named entities, and alternate glosses (Etzioni et al., 2005; Pasc¸a, 2005).Additionally, a wide variety of relationship-specific classifiers have been proposed, including pattern-based classifiers for hyponyms (Hearst, 1992), meronyms (Girju, 2003), synonyms (Lin et al., 2003), a variety of verb relations (Chklovski and Pantel, 2004), and general purpose analogy relations (Turney et al., 2003).Such classifiers use hand-written or automaticallyinduced patterns like Such NPy as NP,, or NPy like NP,, to determine, for example that NPy is a hyponym of NP,, (i.e., NPy IS-A NP,,).While such classifiers have achieved some degree of success, they frequently lack the global knowledge necessary to integrate their predictions into a complex taxonomy with multiple relations.Past work on semantic taxonomy induction includes the noun hypernym hierarchy created in (Caraballo, 2001), the part-whole taxonomies in (Girju, 2003), and a great deal of recent work described in (Buitelaar et al., 2005).Such work has typically either focused on only inferring small taxonomies over a single relation, or as in (Caraballo, 2001), has used evidence for multiple relations independently from one another, by for example first focusing strictly on inferring clusters of coordinate terms, and then by inferring hypernyms over those clusters.Another major shortfall in previous techniques for taxonomy induction has been the inability to handle lexical ambiguity.Previous approaches have typically sidestepped the issue of polysemy altogether by making the assumption of only a single sense per word, and inferring taxonomies explicitly over words and not senses.Enforcing a false monosemy has the downside of making potentially erroneous inferences; for example, collapsing the polysemous term Bush into a single sense might lead one to infer by transitivity that a rose bush is a kind of U.S. president.Our approach simultaneously provides a solution to the problems of jointly considering evidence about multiple relationships as well as lexical ambiguity within a single probabilistic framework.The key contribution of this work is to offer a solution to two crucial problems in taxonomy inProceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 801–808, Sydney, July 2006. c�2006 Association for Computational Linguistics duction and hyponym acquisition: the problem of combining heterogenous sources of evidence in a flexible way, and the problem of correctly identifying the appropriate word sense of each new word added to the taxonomy.1In section 2.1 we introduce our definitions for taxonomies, relations, and the taxonomic constraints that enforce dependencies between relations; in section 2.2 we give a probabilistic model for defining the conditional probability of a set of relational evidence given a taxonomy; in section 2.3 we formulate a local search algorithm to find the taxonomy maximizing this conditional probability; and in section 2.4 we extend our framework to deal with lexical ambiguity.We define a taxonomy T as a set of pairwise relations R over some domain of objects DT.For example, the relations in WordNet include hypernymy, holonymy, verb entailment, and many others; the objects of WordNet between which these relations hold are its word senses or synsets.We define that each relation R E R is a set of ordered or unordered pairs of objects (i, j) E DT; we define Rij E T if relationship R holds over objects For the case of hyponym acquisition, the objects in our taxonomy are WordNet synsets.In this paper we focus on two of the many possible relationships between senses: the hypernym relation and the coordinate term relation.We treat the hypernym or ISA relation as atomic; we use the notation Hinj if a sense j is the n-th ancestor of a sense i in the hypernym hierarchy.We will simply use Hij to indicate that j is an ancestor of i at some unspecified level.Two senses are typically considered to be “coordinate terms” or “taxonomic sisters” if they share an immediate parent in the hypernym hierarchy.We generalize this notion of siblinghood to state that two senses i and j are (m, n)-cousins if their closest least common ij to denote that i and j are (m, n)-cousins.Thus coordinate terms are (1,1)-cousins; technically the hypernym relation may also be seen as a specific case of this representation; an immediate parent in the hypernym hierarchy is a (1, 0)-cousin, and the k-th ancestor is a (k, 0)-cousin.A semantic taxonomy such as WordNet enforces certain taxonomic constraints which disallow particular taxonomies T. For example, the ISA transitivity constraint in WordNet requires that each synset inherits the hypernyms of its hypernym, and the part-inheritance constraint requires that each synset inherits the meronyms of its hypernyms.For the case of hyponym acquisition we enforce the following two taxonomic constraints on the hypernym and (m, n)-cousin relations: Constraint (1) requires that the each synset inherits the hypernyms of its direct hypernym; constraint (2) simply defines the (m, n)-cousin relation in terms of the atomic hypernym relation.The addition of any new hypernym relation to a preexisting taxonomy will usually necessitate the addition of a set of other novel relations as implied by the taxonomic constraints.We refer to the full set of novel relations implied by a new link Rij as I(Rij); we discuss the efficient computation of the set of implied links for the purpose of hyponym acquisition in Section 3.4.We propose that the event Rij E T has some prior probability P(Rij E T), and P(Rij E T) + P(Rij E� T) = 1.We define the probability of the taxonomy as a whole as the joint probability of its component relations; given a partition of all possible relations R = {A, B} where A E T and We assume that we have some set of observed evidence E consisting of observed features over pairs of objects in some domain DE; we’ll begin with the assumption that our features are over pairs of words, and that the objects in the taxonomy also correspond directly to words.4 Given a set of features ERij E E, we assume we have some model for inferring P(Rij E T|ERij), i.e., the posterior probability of the event Rij E T given the corresponding evidence ERij for that relation.For example, evidence for the hypernym relation EHij might be the set of all observed lexico-syntactic patterns containing i and j in all sentences in some corpus.For simplicity we make the following independence assumptions: first, we assume that each item of observed evidence ERij is independent of all other observed evidence given the taxonomy T, i.e., P(E|T) = 11ERijEE P(ERij|T).Further, we assume that each item of observed evidence ERij depends on the taxonomy T only by way of the corresponding relation Rij, i.e., For example, if our evidence EHij is a set of observed lexico-syntactic patterns indicative of hypernymy between two words i and j, we assume that whatever dependence the relations in T have on our observations may be explained entirely by dependence on the existence or non-existence of the single hypernym relation H(i, j).Applying these two independence assumptions we may express the conditional probability of our evidence given the taxonomy: Within our model we define the goal of taxonomy induction to be to find the taxonomy T� that maximizes the conditional probability of our observations E given the relationships of T, i.e., to find We propose a search algorithm for finding T� for the case of hyponym acquisition.We assume we begin with some initial (possibly empty) taxonomy T. We restrict our consideration of possible new taxonomies to those created by the single operation ADD-RELATION(Rij, T), which adds the single relation Rij to T. We define the multiplicative change OT(Rij) to the conditional probability P(E|T) given the addition of a single relation Rij: Here k is the inverse odds of the prior on the event Rij E T; we consider this to be a constant independent of i, j, and the taxonomy T. To enforce the taxonomic constraints in T, for each application of the ADD-RELATION operator we must add all new relations in the implied set I(Rij) not already in T.5 Thus we define the multiplicative change of the full set of implied relations as the product over all new relations: Rewriting the conditional probability in terms of our estimates of the posterior probabilities This definition leads to the following best-first search algorithm for hyponym acquisition, which at each iteration defines the new taxonomy as the union of the previous taxonomy T and the set of novel relations implied by the relation Rij that maximizes AT(I(Rij)) and thus maximizes the conditional probability of the evidence over all possible single relations: Since word senses are not directly observable, if the objects in the taxonomy are word senses (as in WordNet), we must extend our model to allow for a many-to-many mapping (e.g., a word-to-sense mapping) between DE and DT.For this setting we assume we know the function senses(i), mapping from the word i to all of is possible corresponding senses.We assume that each set of word-pair evidence ERij we possess is in fact sense-pair evidence ERkl for a specific pair of senses k0 E senses(i), l0 E senses(j).Further, we assume that a new relation between two words is probable only between the correct sense pair, i.e.: When computing the conditional probability of a specific new relation Rkl E I(Rab), we assume that the relevant sense pair k0, l0 is the one which maximizes the probability of the new relation, i.e. for k E senses(i), l E senses(j), Our independence assumptions for this extension need only to be changed slightly; we now assume that the evidence ERij depends on the taxonomy T via only a single relation between sensepairs Rkl.Using this revised independence assumption the derivation for best-first search over taxonomies for hyponym acquisition remains unchanged.One side effect of this revised independence assumption is that the addition of the single “sense-collapsed” relation Rkl in the taxonomy T will explain the evidence ERij for the relation over words i and j now that such evidence has been revealed to concern only the specific senses k and l.We demonstrate the ability of our model to use evidence from multiple relations to extend WordNet with novel noun hyponyms.While in principle we could use any number of relations, for simplicity we consider two primary sources of evidence: the probability of two words in WordNet being in a hypernym relation, and the probability of two words in WordNet being in a coordinate relation.In sections 3.1 and 3.2 we describe the construction of our hypernym and coordinate classifiers, respectively; in section 3.3 we outline the efficient algorithm we use to perform local search over hyponym-extended WordNets; and in section 3.4 we give an example of the implicit structure-based word sense disambiguation performed within our framework.Our classifier for the hypernym relation is derived from the “hypernym-only” classifier described in (Snow et al., 2005).The features used for predicting the hypernym relationship are obtained by parsing a large corpus of newswire and encyclopedia text with MINIPAR (Lin, 1998).From the resulting dependency trees the evidence EHij for each word pair (i, j) is constructed; the evidence takes the form of a vector of counts of occurrences that each labeled syntactic dependency path was found as the shortest path connecting i and j in some dependency tree.The labeled training set is constructed by labeling the collected feature vectors as positive “known hypernym” or negative “known non-hypernym” examples using WordNet 2.0; 49,922 feature vectors were labeled as positive training examples, and 800,828 noun pairs were labeled as negative training examples.The model for predicting P(Hij|EHij ) is then trained using logistic regression, predicting the noun-pair hypernymy label from WordNet from the feature vector of lexico-syntactic patterns.The hypernym classifier described above predicts the probability of the generalized hypernymancestor relation over words P(Hij|EHij ).For the purposes of taxonomy induction, we would prefer an ancestor-distance specific set of classifiers over senses, i.e., for k E senses(i), l E senses(j), the set of classifiers estimating {P(H� kl|EH ij ), P(Hkl|EHij ), ... }.One problem that arises from directly assigning the probability P(Hn ij|EH ij ) a P(Hij|EHij ) for all n is the possibility of adding a novel hyponym to an overly-specific hypernym, which might still satisfy P(Hn ij|EH ij ) for a very large n. In order to discourage unnecessary overspecification, we penalize each probability P(Hk ij|EH ij ) by a factor Ak−1 for some A < 1, and renormalize: P(Hkij|EHij ) a Ak−1P(Hij|EHij ).In our experiments we set A = 0.95.The classifier for learning coordinate terms relies on the notion of distributional similarity, i.e., the idea that two words with similar meanings will be used in similar contexts (Hindle, 1990).We extend this notion to suggest that words with similar meanings should be near each other in a semantic taxonomy, and in particular will likely share a hypernym as a near parent.Our classifier for (m, n)-cousins is derived from the algorithm and corpus given in (Ravichandran et al., 2005).In that work an efficient randomized algorithm is derived for computing clusters of similar nouns.We use a set of more than 1000 distinct clusters of English nouns collected by their algorithm over 70 million webpages6, with each noun i having a score representing its cosine similarity to the centroid c of the cluster to which it belongs, cos(B(i, c)).We use the cluster scores of noun pairs as input to our own algorithm for predicting the (m, n)cousin relationship between the senses of two words i and j.If two words i and j appear in a cluster together, with cluster centroid c, we set our single coordinate input feature to be the minimum cluster score min(cos(B(i, c)), cos(B(j, c))), and zero otherwise.For each such noun pair feature, we construct a labeled training set of (m, n)cousin relation labels from WordNet 2.1.We define a noun pair (i, j) to be a “known (m, n)cousin” if for some senses k E senses(i), l E senses(j), Cmn ij E WordNet; if more than one such relation exists, we assume the relation with smallest sum m + n, breaking ties by smallest absolute difference |m − n|.We consider all such labeled relationships from WordNet with 0 < m, n < 7; pairs of words that have no corresponding pair of synsets connected in the hypernym hierarchy, or with min(m, n) > 7, are assigned to a single class C'.Further, due to the symmetry of the similarity score, we merge each class Cmn = Cmn U Cnm; this implies that the resulting classifier will predict, as expected given a symmetric input, P(Cmn We find 333,473 noun synset pairs in our training set with similarity score greater than 0.15.We next apply softmax regression to learn a classifier that predicts P(Cmn ij |EC ij), predicting the WordNet class labels from the single similarity score derived from the noun pair’s cluster similarity.Hyponym acquisition is among the simplest and most straightforward of the possible applications of our model; here we show how we efficiently implement our algorithm for this problem.First, we identify the set of all the word pairs (i, j) over which we have hypernym and/or coordinate evidence, and which might represent additions of a novel hyponym to the WordNet 2.1 taxonomy (i.e., that has a known noun hypernym and an unknown hyponym, or has a known noun coordinate term and an unknown coordinate term).This yields a list of 95,000 single links over threshold P(Rij) > 0.12.For each unknown hyponym i we may have several pieces of evidence; for example, for the unknown term continental we have 21 relevant pieces of hypernym evidence, with links to possible hypernyms {carrier, airline, unit,... }; and we have 5 pieces of coordinate evidence, with links to possible coordinate terms {airline, american eagle, airbus, ... }.For each proposed hypernym or coordinate link involved with the novel hyponym i, we compute the set of candidate hypernyms for i; in practice we consider all senses of the immediate hypernym j for each potential novel hypernym, and all senses of the coordinate term k and its first two hypernym ancestors for each potential coordinate.In the continental example, from the 26 individual pieces of evidence over words we construct the set of 99 unique synsets that we will consider as possible hypernyms; these include the two senses of the word airline, the ten senses of the word carrier, and so forth.Next, we iterate through each of the possible hypernym synsets l under which we might add the new word i; for each synset l we compute the change in taxonomy score resulting from adding the implied relations I(H1il) required by the taxonomic constraints of T. Since typically our set of all evidence involving i will be much smaller than the set of possible relations in I(H1il), we may efficiently check whether, for each sense s E senses(w), for all words where we have some evidence ERiw, whether s participates in some relation with i in the set of implied relations I(H1il).7 If there is more than one sense s E senses(w), we add to I(H1il) the single relationship Ris that maximizes the taxonomy likelihood, i.e. arg maxsEsenses(w) AT(Ris).A major strength of our model is its ability to correctly choose the sense of a hypernym to which to add a novel hyponym, despite collecting evidence over untagged word pairs.In our algorithm word sense disambiguation is an implicit side-effect of our algorithm; since our algorithm chooses to add the single link which, with its implied links, yields the most likely taxonomy, and since each distinct synset in WordNet has a different immediate neighborhood of relations, our algorithm simply disambiguates each node based on its surrounding structural information.As an example of sense disambiguation in practice, consider our example of continental.Suppose we are iterating through each of the 99 possible synsets under which we might add continental as a hyponym, and we come to the synset airline#n#2 in WordNet 2.1, i.e.“a commercial organization serving as a common carrier.” In this case we will iterate through each piece of hypernym and coordinate evidence; we find that the relation H(continental, carrier) is satisfied with high probability for the specific synset carrier#n#S, the grandparent of airline#n#2; thus the factor AT(H3(continental, carrier#n#S)) is included in the factor of the set of implied relations AT (I(H1(continental, airline#n#2))).Suppose we instead evaluate the first synset of airline, i.e., airline#n#1, with the gloss “a hose that carries air under pressure.” For this synset none of the other 20 relationships directly implied by hypernym evidence or the 5 relationships implied by the coordinate evidence are implied by adding the single link H1(continental,airline#n#1); thus the resulting change in the set of implied links given by the correct “carrier” sense of airline is much higher than that of the “hose” sense.In fact it is the largest of all the 99 considered hypernym links for continental; H1(continental, airline#n#2) is link #18,736 added to the taxonomy by our algorithm.In order to evaluate our framework for taxonomy induction, we have applied hyponym acquisition to construct several distinct taxonomies, starting with the base of WordNet 2.1 and only adding novel noun hyponyms.Further, we have constructed taxonomies using a baseline algorithm, which uses the identical hypernym and coordinate classifiers used in our joint algorithm, but which does not combine the evidence of the classifiers.In section 4.1 we describe our evaluation methodology; in sections 4.2 and 4.3 we analyze the fine-grained precision and disambiguation precision of our algorithm compared to the baseline; in section 4.4 we compare the coarse-grained precision of our links (motivated by categories defined by the WordNet supersenses) against the baseline algorithm and against an “oracle” for named entity recognition.Finally, in section 4.5 we evaluate the taxonomies inferred by our algorithm directly against the WordNet 2.1 taxonomy; we perform this evaluation by testing each taxonomy on a set of human judgments of hypernym and non-hypernym noun pairs sampled from newswire text.We evaluate the quality of our acquired hyponyms by direct judgment.In four separate annotation sessions, two judges labeled {50,100,100,100} samples uniformly generated from the first {100,1000,10000,20000} single links added by our algorithm.For the direct measure of fine-grained precision, we simply ask for each link H(X, Y ) added by the system, is X a Y ?In addition to the fine-grained precision, we give a coarse-grained evaluation, inspired by the idea of supersense-tagging in (Ciaramita and Johnson, 2003).The 26 supersenses used in WordNet 2.1 are listed in Table 1; we label a hyponym link as correct in the coarse-grained evaluation if the novel hyponym is placed under the appropriate supersense.This evaluation task is similar to a fine-grained Named Entity Recognition (Fleischman and Hovy, 2002) task with 26 categories; for example, if our algorithm mistakenly inserts a novel non-capital city under the hyponym state capital, it will inherit the correct supersense location.Finally, we evaluate the ability of our algorithm to correctly choose the appropriate sense of the hypernym under which a novel hyponym is being added.Our labelers categorize each candidate sense-disambiguated hypernym synset suggested by our algorithm into the following categories: A single hyponym/hypernym pair is allowed to be simultaneously labeled 2 and 3.Table 2 displays the results of our evaluation of fine-grained precision for the baseline non-joint algorithm (Base) and our joint algorithm (Joint), as well as the relative error reduction (ER) of our algorithm over the baseline.We use the minimum of the two judges’ scores.Here we define fine-grained precision as c1/total.We see that our joint algorithm strongly outperforms the baseline, and has high precision for predicting novel hyponyms up to 10,000 links.Also in Table 2 we compare the sense disambiguation precision of our algorithm and the baseline.Here we measure the precision of sense-disambiguation among all examples where each algorithm found a correct hyponym word; our calculation for disambiguation precision is c1/ (c1 + c2).Again our joint algorithm outperforms the baseline algorithm at all levels of recall.Interestingly the baseline disambiguation precision improves with higher recall; this may be attributed to the observation that the highestconfidence hypernyms predicted by individual classifiers are likely to be polysemous, whereas hypernyms of lower confidence are more frequently monosemous (and thus trivially easy to disambiguate).We compute coarse-grained precision as (c1 + c3)/total.Inferring the correct coarse-grained supersense of a novel hyponym can be viewed as a fine-grained (26-category) Named Entity Recognition task; our algorithm for taxonomy induction can thus be viewed as performing high-accuracy fine-grained NER.Here we compare against both the baseline non-joint algorithm as well as an “oracle” algorithm for Named Entity Recognition, which perfectly classifies the supersense of all nouns that fall under the four supersenses {person, group, location, quantity}, but works only for those supersenses.Table 3 shows the results of this coarse-grained evaluation.We see that the baseline non-joint algorithm has higher precision than the NER oracle as 10,000 and 20,000 links; however, both are significantly outperformed by our joint algorithm, which maintains high coarse-grained precision (92%) even at 20,000 links.For our final evaluation we compare our learned taxonomies directly against the currently existing hypernym links in WordNet 2.1.In order to compare taxonomies we use a hand-labeled test set of over 5,000 noun pairs, randomly-sampled from newswire corpora (described in (Snow et al., 2005)).We measured the performance of both our inferred taxonomies and WordNet against this test set.8 The performance and comparison of the best WordNet classifier vs. our taxonomies is given in Table 4.Our best-performing inferred taxonomy on this test set is achieved after adding 30,000 novel hyponyms, achieving an 23% relative improvement in F-score over the WN2.1 classifier.We have presented an algorithm for inducing semantic taxonomies which attempts to globally optimize the entire structure of the taxonomy.Our probabilistic architecture also includes a new model for learning coordinate terms based on (m, n)-cousin classification.The model’s ability to integrate heterogeneous evidence from different classifiers offers a solution to the key problem of choosing the correct word sense to which to attach a new hypernym.Thanks to Christiane Fellbaum, Rajat Raina, Bill MacCartney, and Allison Buckley for useful discussions and assistance annotating data.Rion Snow is supported by an NDSEG Fellowship sponsored by the DOD and AFOSR.This work was supported in part by the Disruptive Technology Office (DTO)’s Advanced Question Answering for Intelligence (AQUAINT) Program.
Dependency grammar has proven to be a very useful syntactic formalism, due in no small part to the development of efficient parsing algorithms (Eisner, 2000; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007), which can be leveraged for a wide variety of learning methods, such as feature-rich discriminative models (Lafferty et al., 2001; Collins, 2002; Taskar et al., 2003).These parsing algorithms share an important characteristic: they factor dependency trees into sets of parts that have limited interactions.By exploiting the additional constraints arising from the factorization, maximizations or summations over the set of possible dependency trees can be performed efficiently and exactly.A crucial limitation of factored parsing algorithms is that the associated parts are typically quite small, losing much of the contextual information within the dependency tree.For the purposes of improving parsing performance, it is desirable to increase the size and variety of the parts used by the factorization.1 At the same time, the need for more expressive factorizations 1For examples of how performance varies with the degree of the parser’s factorization see, e.g., McDonald and Pereira (2006, Tables 1 and 2), Carreras (2007, Table 2), Koo et al. (2008, Tables 2 and 4), or Suzuki et al.(2009, Tables 3–6). must be balanced against any resulting increase in the computational cost of the parsing algorithm.Consequently, recent work in dependency parsing has been restricted to applications of secondorder parsers, the most powerful of which (Carreras, 2007) requires O(n4) time and O(n3) space, while being limited to second-order parts.In this paper, we present new third-order parsing algorithms that increase both the size and variety of the parts participating in the factorization, while simultaneously maintaining computational requirements of O(n4) time and O(n3) space.We evaluate our parsers on the Penn WSJ Treebank (Marcus et al., 1993) and Prague Dependency Treebank (Hajiˇc et al., 2001), achieving unlabeled attachment scores of 93.04% and 87.38%.In summary, we make three main contributions: The remainder of this paper is divided as follows: Sections 2 and 3 give background, Sections 4 and 5 describe our new parsing algorithms, Section 6 discusses related work, Section 7 presents our experimental results, and Section 8 concludes.In dependency grammar, syntactic relationships are represented as head-modifier dependencies: directed arcs between a head, which is the more “essential” word in the relationship, and a modifier, which supplements the meaning of the head.For example, Figure 1 contains a dependency between the verb “report” (the head) and its object “sales” (the modifier).A complete analysis of a sentence is given by a dependency tree: a set of dependencies that forms a rooted, directed tree spanning the words of the sentence.Every dependency tree is rooted at a special “*” token, allowing the selection of the sentential head to be modeled as if it were a dependency.For a sentence x, we define dependency parsing as a search for the highest-scoring analysis of x: Here, y(x) is the set of all trees compatible with x and SCORE(x, y) evaluates the event that tree y is the analysis of sentence x.Since the cardinality of y(x) grows exponentially with the length of the sentence, directly solving Eq.1 is impractical.A common strategy, and one which forms the focus of this paper, is to factor each dependency tree into small parts, which can be scored in isolation.Factored parsing can be formalized as follows: That is, we treat the dependency tree y as a set of parts p, each of which makes a separate contribution to the score of y.For certain factorizations, efficient parsing algorithms exist for solving Eq.1.We define the order of a part according to the number of dependencies it contains, with analogous terminology for factorizations and parsing algorithms.In the remainder of this paper, we focus on factorizations utilizing the following parts: Specifically, Sections 4.1, 4.2, and 4.3 describe parsers that, respectively, factor trees into grandchild parts, grand-sibling parts, and a mixture of grand-sibling and tri-sibling parts.Our new third-order dependency parsers build on ideas from existing parsing algorithms.In this section, we provide background on two relevant parsers from previous work.The first type of parser we describe uses a “firstorder” factorization, which decomposes a dependency tree into its individual dependencies.Eisner (2000) introduced a widely-used dynamicprogramming algorithm for first-order parsing; as it is the basis for many parsers, including our new algorithms, we summarize its design here.The Eisner (2000) algorithm is based on two interrelated types of dynamic-programming structures: complete spans, which consist of a headword and its descendents on one side, and incomplete spans, which consist of a dependency and the region between the head and modifier.Formally, we denote a complete span as Ch,e where h and a are the indices of the span’s headword and endpoint.An incomplete span is denoted as Ih,,,t where h and m are the index of the head and modifier of a dependency.Intuitively, a complete span represents a “half-constituent” headed by h, whereas an incomplete span is only a partial half-constituent, since the constituent can be extended by adding more modifiers to m. Each type of span is created by recursively combining two smaller, adjacent spans; the constructions are specified graphically in Figure 2.An incomplete span is constructed from a pair of complete spans, indicating the division of the range [h, m] into constituents headed by h and m. A complete span is created by “completing” an incomplete span with the other half of m’s constituent.The point of concatenation in each construction—m in Figure 2(a) or r in Figure 2(b)—is the split point, a free index that must be enumerated to find the optimal construction.In order to parse a sentence x, it suffices to find optimal constructions for all complete and incomplete spans defined on x.This can be accomplished by adapting standard chart-parsing techniques (Cocke and Schwartz, 1970; Younger, 1967; Kasami, 1965) to the recursive derivations defined in Figure 2.Since each derivation is defined by two fixed indices (the boundaries of the span) and a third free index (the split point), the parsing algorithm requires O(n3) time and O(n2) space (Eisner, 1996; McAllester, 1999).As remarked by Eisner (1996) and McDonald and Pereira (2006), it is possible to rearrange the dynamic-programming structures to conform to an improved factorization that decomposes each tree into sibling parts—pairs of dependencies with a shared head.Specifically, a sibling part consists of a triple of indices (h, m, s) where (h, m) and (h, s) are dependencies, and where s and m are successive modifiers to the same side of h. In order to parse this factorization, the secondorder parser introduces a third type of dynamicprogramming structure: sibling spans, which represent the region between successive modifiers of some head.Formally, we denote a sibling span as S,,,,, where s and m are a pair of modifiers involved in a sibling relationship.Modified versions of sibling spans will play an important role in the new parsing algorithms described in Section 4.Figure 3 provides a graphical specification of the second-order parsing algorithm.Note that incomplete spans are constructed in a new way: the second-order parser combines a smaller incomplete span, representing the next-innermost dependency, with a sibling span that covers the region between the two modifiers.Sibling parts (h, m, s) can thus be obtained from Figure 3(b).Despite the use of second-order parts, each derivation is still defined by a span and split point, so the parser requires O(n3) time and O(n2) space.In this section we describe our new third-order dependency parsing algorithms.Our overall method is characterized by the augmentation of each span with a “grandparent” index: an index external to the span whose role will be made clear below.This section presents three parsing algorithms based on this idea: Model 0, a second-order parser, and Models 1 and 2, which are third-order parsers.The first parser, Model 0, factors each dependency tree into a set of grandchild parts—pairs of dependencies connected head-to-tail.Specifically, a grandchild part is a triple of indices (g, h, m) where (g, h) and (h, m) are dependencies.3 In order to parse this factorization, we augment both complete and incomplete spans with grandparent indices; for brevity, we refer to these augmented structures as g-spans.Formally, we denote a complete g-span as Ch,e, where Ch,e is a normal complete span and g is an index lying outside the range [h, e], with the implication that (g, h) is a dependency.Incomplete g-spans are defined analogously and are denoted as Ih,..Figure 4 depicts complete and incomplete gspans and provides a graphical specification of the SCOREG is the scoring function for grandchild parts.We use the g-span identities as shorthand for their chart entries (e.g., Igi,j refers to the entry containing the maximum score of that g-span).Model 0 dynamic-programming algorithm.The algorithm resembles the first-order parser, except that every recursive construction must also set the grandparent indices of the smaller g-spans; fortunately, this can be done deterministically in all cases.For example, Figure 4(a) depicts the decomposition of Cgh,e into an incomplete half and a complete half.The grandparent of the incomplete half is copied from Cgh,e while the grandparent of the complete half is set to h, the head of m as defined by the construction.Clearly, grandchild parts (g, h, m) can be read off of the incomplete g-spans in Figure 4(b,d).Moreover, since each derivation copies the grandparent index g into successively smaller g-spans, grandchild parts will be produced for all grandchildren of g. Model 0 can be parsed by adapting standard top-down or bottom-up chart parsing techniques.For concreteness, Figure 5 provides a pseudocode sketch of a bottom-up chart parser for Model 0; although the sketch omits many details, it suffices for the purposes of illustration.The algorithm progresses from small widths to large in the usual manner, but after defining the endpoints (i, j) there is an additional loop that enumerates all possible grandparents.Since each derivation is defined by three fixed indices (the g-span) and one free index (the split point), the complexity of the algorithm is O(n4) time and O(n3) space.Note that the grandparent indices cause each gspan to have non-contiguous structure.For example, in Figure 4(a) the words between g and h will be controlled by some other g-span.Due to these discontinuities, the correctness of the Model 0 dynamic-programming algorithm may not be immediately obvious.While a full proof of correctness is beyond the scope of this paper, we note that each structure on the right-hand side of Figure 4 lies completely within the structure on the left-hand side.This nesting of structures implies, in turn, that the usual properties required to ensure the correctness of dynamic programming hold.We now describe our first third-order parsing algorithm.Model 1 decomposes each tree into a set of grand-sibling parts—combinations of sibling parts and grandchild parts.Specifically, a grand-sibling is a 4-tuple of indices (g, h, m, s) where (h, m, s) is a sibling part and (g, h, m) and (g, h, s) are grandchild parts.For example, in Figure 1, the words “must,” “report,” “sales,” and “immediately” form a grand-sibling part.In order to parse this factorization, we introduce sibling g-spans Shm,s, which are composed of a normal sibling span Sm,s and an external index h, with the implication that (h, m, s) forms a valid sibling part.Figure 6 provides a graphical specification of the dynamic-programming algorithm for Model 1.The overall structure of the algorithm resembles the second-order sibling parser, with the addition of grandparent indices; as in Model 0, the grandparent indices can be set deterministically in all cases.Note that the sibling g-spans are crucial: they allow grand-sibling parts (g, h, m, s) to be read off of Figure 6(b), while simultaneously propagating grandparent indices to smaller g-spans.Like Model 0, Model 1 can be parsed via adaptations of standard chart-parsing techniques; we omit the details for brevity.Despite the move to third-order parts, each derivation is still defined by a g-span and a split point, so that parsing requires only O(n4) time and O(n3) space.Higher-order parsing algorithms have been proposed which extend the second-order sibling factorization to parts containing multiple siblings (McDonald and Pereira, 2006, also see Section 6 for discussion).In this section, we show how our g-span-based techniques can be combined with a third-order sibling parser, resulting in a parser that captures both grand-sibling parts and tri-sibling parts—4-tuples of indices (h, m, s, t) such that both (h, m, s) and (h, s, t) are sibling parts.In order to parse this factorization, we introduce a new type of dynamic-programming structure: sibling-augmented spans, or s-spans.Formally, we denote an incomplete s-span as Ih,m,s where Ih,m is a normal incomplete span and s is an index lying in the strict interior of the range [h, m], such that (h, m, s) forms a valid sibling part.Figure 7 provides a graphical specification of the Model 2 parsing algorithm.An incomplete s-span is constructed by combining a smaller incomplete s-span, representing the next-innermost pair of modifiers, with a sibling g-span, covering the region between the outer two modifiers.As in Model 1, sibling g-spans are crucial for propagating grandparent indices, while allowing the recovery of tri-sibling parts (h, m, s, t).Figure 7(b) shows how an incomplete s-span can be converted into an incomplete g-span by exchanging the internal sibling index for an external grandparent index; in the process, grand-sibling parts (g, h, m, s) are enumerated.Since every derivation is defined by an augmented span and a split point, Model 2 can be parsed in O(n4) time and O(n3) space.It should be noted that unlike Model 1, Model 2 produces grand-sibling parts only for the outermost pair of grandchildren,4 similar to the behavior of the Carreras (2007) parser.In fact, the resemblance is more than passing, as Model 2 can emulate the Carreras (2007) algorithm by “demoting” each third-order part into a second-order part: SCOREGS(x, g, h, m, s) = SCOREG(x, g, h, m) SCORETS(x, h, m, s, t) = SCORES(x, h, m, s) where SCOREG, SCORES, SCOREGS and SCORETS are the scoring functions for grandchildren, siblings, grand-siblings and tri-siblings, respectively.The emulated version has the same computational complexity as the original, so there is no practical reason to prefer it over the original.Nevertheless, the relationship illustrated above highlights the efficiency of our approach: we are able to recover third-order parts in place of second-order parts, at no additional cost.The technique of grandparent-index augmentation has proven fruitful, as it allows us to parse expressive third-order factorizations while retaining an efficient O(n4) runtime.In fact, our thirdorder parsing algorithms are “optimally” efficient in an asymptotic sense.Since each third-order part is composed of four separate indices, there are 0(n4) distinct parts.Any third-order parsing algorithm must at least consider the score of each part, hence third-order parsing is Q(n4) and it follows that the asymptotic complexity of Models 1 and 2 cannot be improved.The key to the efficiency of our approach is a fundamental asymmetry in the structure of a directed tree: a head can have any number of modifiers, while a modifier always has exactly one head.Factorizations like that of Carreras (2007) obtain grandchild parts by augmenting spans with the indices of modifiers, leading to limitations on the grandchildren that can participate in the factorization.Our method, by “inverting” the modifier indices into grandparent indices, exploits the structural asymmetry.As a final note, the parsing algorithms described in this section fall into the category of projective dependency parsers, which forbid crossing dependencies.If crossing dependencies are allowed, it is possible to parse a first-order factorization by finding the maximum directed spanning tree (Chu and Liu, 1965; Edmonds, 1967; McDonald et al., 2005b).Unfortunately, designing efficient higherorder non-projective parsers is likely to be challenging, based on recent hardness results (McDonald and Pereira, 2006; McDonald and Satta, 2007).We briefly outline a few extensions to our algorithms; we hope to explore these in future work.Many statistical modeling techniques are based on partition functions and marginals—summations over the set of possible trees y(x).Straightforward adaptations of the inside-outside algorithm (Baker, 1979) to our dynamic-programming structures would suffice to compute these quantities.Our parsers are easily extended to labeled dependencies.Direct integration of labels into Models 1 and 2 would result in third-order parts composed of three labeled dependencies, at the cost of increasing the time and space complexities by factors of O(L3) and O(L2), respectively, where L bounds the number of labels per dependency.If each word in x has a set of possible “senses,” our parsers can be modified to recover the best joint assignment of syntax and senses for x, by adapting methods in Eisner (2000).Complexity would increase by factors of O(54) time and O(53) space, where 5 bounds the number of senses per word.If more vertical context is desired, the dynamicprogramming structures can be extended with additional ancestor indices, resulting in a “spine” of ancestors above each span.Each additional ancestor lengthens the vertical scope of the factorization (e.g., from grand-siblings to “great-grandsiblings”), while increasing complexity by a factor of O(n).Horizontal context can also be increased by adding internal sibling indices; each additional sibling widens the scope of the factorization (e.g., from grand-siblings to “grand-tri-siblings”), while increasing complexity by a factor of O(n).Our method augments each span with the index of the head that governs that span, in a manner superficially similar to parent annotation in CFGs (Johnson, 1998).However, parent annotation is a grammar transformation that is independent of any particular sentence, whereas our method annotates spans with indices into the current sentence.These indices allow the use of arbitrary features predicated on the position of the grandparent (e.g., word identity, POS tag, contextual POS tags) without affecting the asymptotic complexity of the parsing algorithm.Efficiently encoding this kind of information into a sentence-independent grammar transformation would be challenging at best.Eisner (2000) defines dependency parsing models where each word has a set of possible “senses” and the parser recovers the best joint assignment of syntax and senses.Our new parsing algorithms could be implemented by defining the “sense” of each word as the index of its head.However, when parsing with senses, the complexity of the Eisner (2000) parser increases by factors of O(53) time and O(52) space (ibid., Section 4.2).Since each word has n potential heads, a direct application of the word-sense parser leads to time and space complexities of O(n6) and O(n4), respectively, in contrast to our O(n4) and O(n3).5 Eisner (2000) also uses head automata to score or recognize the dependents of each head.An interesting question is whether these automata could be coerced into modeling the grandparent indices used in our parsing algorithms.However, note that the head automata are defined in a sentenceindependent manner, with two automata per word in the vocabulary (ibid., Section 2).The automata are thus analogous to the rules of a CFG and attempts to use them to model grandparent indices would face difficulties similar to those already described for grammar transformations in CFGs.It should be noted that third-order parsers have previously been proposed by McDonald and Pereira (2006), who remarked that their secondorder sibling parser (see Figure 3) could easily be extended to capture m > 1 successive modifiers in O(nm+1) time (ibid., Section 2.2).To our knowledge, however, Models 1 and 2 are the first third-order parsing algorithms capable of modeling grandchild parts.In our experiments, we find that grandchild interactions make important contributions to parsing performance (see Table 3).Carreras (2007) presents a second-order parser that can score both sibling and grandchild parts, with complexities of O(n4) time and O(n3) space.An important limitation of the parser’s factorization is that it only defines grandchild parts for outermost grandchildren: (g, h, m) is scored only when m is the outermost modifier of h in some direction.Note that Models 1 and 2 have the same complexity as Carreras (2007), but strictly greater expressiveness: for each sibling or grandchild part used in the Carreras (2007) factorization, Model 1 defines an enclosing grand-sibling, while Model 2 defines an enclosing tri-sibling or grand-sibling.The factored parsing approach we focus on is sometimes referred to as “graph-based” parsing; a popular alternative is “transition-based” parsing, in which trees are constructed by making a series of incremental decisions (Yamada and Matsumoto, 2003; Attardi, 2006; Nivre et al., 2006; McDonald and Nivre, 2007).Transition-based parsers do not impose factorizations, so they can define arbitrary features on the tree as it is being built.As a result, however, they rely on greedy or approximate search algorithms to solve Eq.1.In order to evaluate the effectiveness of our parsers in practice, we apply them to the Penn WSJ Treebank (Marcus et al., 1993) and the Prague Dependency Treebank (Hajiˇc et al., 2001; Hajiˇc, 1998).6 We use standard training, validation, and test splits7 to facilitate comparisons.Accuracy is measured with unlabeled attachment score (UAS): the percentage of words with the correct head.8Our parsing algorithms can be applied to scores originating from any source, but in our experiments we chose to use the framework of structured linear models, deriving our scores as: SCOREPART(x, p) = w · f(x, p) Here, f is a feature-vector mapping and w is a vector of associated parameters.Following standard practice for higher-order dependency parsing (McDonald and Pereira, 2006; Carreras, 2007), Models 1 and 2 evaluate not only the relevant third-order parts, but also the lower-order parts that are implicit in their third-order factorizations.For example, Model 1 defines feature mappings for dependencies, siblings, grandchildren, and grand-siblings, so that the score of a dependency parse is given by: Above, y is simultaneously decomposed into several different types of parts; trivial modifications to the Model 1 parser allow it to evaluate all of the necessary parts in an interleaved fashion.A similar treatment of Model 2 yields five feature mappings: the four above plus ftsib(x, h, m, s, t), which represents tri-sibling parts.The lower-order feature mappings fdep, fsib, and fgch are based on feature sets from previous work (McDonald et al., 2005a; McDonald and Pereira, 2006; Carreras, 2007), to which we added lexicalized versions of several features.For example, fdep contains lexicalized “in-between” features that depend on the head and modifier words as well as a word lying in between the two; in contrast, previous work has generally defined in-between features for POS tags only.As another example, our second-order mappings fsib and fgch define lexical trigram features, while previous work has generally used POS trigrams only.Our third-order feature mappings fgsib and ftsib consist of four types of features.First, we define 4-gram features that characterize the four relevant indices using words and POS tags; examples include POS 4-grams and mixed 4-grams with one word and three POS tags.Second, we define 4gram context features consisting of POS 4-grams augmented with adjacent POS tags: for example, fgsib(x, g, h, m, s) includes POS 7-grams for the tags at positions (g, h, m, s, g+1, h+1, m+1).Third, we define backed-offfeatures that track bigram and trigram interactions which are absent in the lower-order feature mappings: for example, ftsib(x, h, m, s, t) contains features predicated on the trigram (m, s, t) and the bigram (m, t), neither of which exist in any lower-order part.Fourth, noting that coordinations are typically annotated as grand-siblings (e.g., “report purchases and sales” in Figure 1), we define coordination features for certain grand-sibling parts.For example, fgsib(x, g, h, m, s) contains features examining the implicit head-modifier relationship (g, m) that are only activated when the POS tag of s is a coordinating conjunction.Finally, we make two brief remarks regarding the use of POS tags.First, we assume that input sentences have been automatically tagged in a preprocessing step.9 Second, for any feature that depends on POS tags, we include two copies of the feature: one using normal POS tags and another using coarsened versions10 of the POS tags.There are a wide variety of parameter estimation methods for structured linear models, such as log-linear models (Lafferty et al., 2001) and max-margin models (Taskar et al., 2003).We chose the averaged structured perceptron (Freund and Schapire, 1999; Collins, 2002) as it combines highly competitive performance with fast training times, typically converging in 5–10 iterations.We train each parser for 10 iterations and select paon English parsing.For each beam value, parsers were trained on the English training set and evaluated on the English validation set; the same beam value was applied to both training and validation data.Pass = %dependencies surviving the beam in training data, Orac = maximum achievable UAS on validation data, Acc1/Acc2 = UAS of Models 1/2 on validation data, and Time1/Time2 = minutes per perceptron training iteration for Models 1/2, averaged over all 10 iterations.For perspective, the English training set has a total of 39,832 sentences and 950,028 words.A beam of 0.0001 was used in all experiments outside this table. rameters from the iteration that achieves the best score on the validation set.In order to decrease training times, we follow Carreras et al. (2008) and eliminate unlikely dependencies using a form of coarse-to-fine pruning (Charniak and Johnson, 2005; Petrov and Klein, 2007).In brief, we train a log-linear first-order parser11 and for every sentence x in training, validation, and test data we compute the marginal probability P(h, m  |x) of each dependency.Our parsers are then modified to ignore any dependency (h, m) whose marginal probability is below 0.0001xmaxh' P(h', m  |x).Table 1 provides information on the behavior of the pruning method.Table 2 lists the accuracy of Models 1 and 2 on the English and Czech test sets, together with some relevant results from related work.12 The models marked “†” are not directly comparable to our work as they depend on additional sources of information that our models are trained without— unlabeled data in the case of Koo et al. (2008) and relevant results from related work.Note that Koo et al. (2008) is listed with standard features and semi-supervised features.†: see main text.Suzuki et al. (2009) and phrase-structure annotations in the case of Carreras et al.(2008).All three of the “†” models are based on versions of the Carreras (2007) parser, so modifying these methods to work with our new third-order parsing algorithms would be an interesting topic for future research.For example, Models 1 and 2 obtain results comparable to the semi-supervised parsers of Koo et al. (2008), and additive gains might be realized by applying their cluster-based feature sets to our enriched factorizations.In order to better understand the contributions of the various feature types, we ran additional ablation experiments; the results are listed in Table 3, in addition to the scores of Model 0 and the emulated Carreras (2007) parser (see Section 4.3).Interestingly, grandchild interactions appear to provide important information: for example, when Model 2 is used without grandchild-based features (“Model 2, no-G” in Table 3), its accuracy suffers noticeably.In addition, it seems that grandchild interactions are particularly useful in Czech, while sibling interactions are less important: consider that Model 0, a second-order grandchild parser with no sibling-based features, can easily outperform “Model 2, no-G,” a third-order sibling parser with no grandchild-based features.We have presented new parsing algorithms that are capable of efficiently parsing third-order factorizations, including both grandchild and sibling interactions.Due to space restrictions, we have been necessarily brief at some points in this paper; some additional details can be found in Koo (2010). on validation data.The term no-3rd indicates a parser that was trained and tested with the thirdorder feature mappings fgsib and ftsib deactivated, though lower-order features were retained; note that “Model 2, no-3rd” is not identical to the Carreras (2007) parser as it defines grandchild parts for the pair of grandchildren.The term no-G indicates a parser that was trained and tested with the grandchild-based feature mappings fgch and fgsib deactivated; note that “Model 2, no-G” emulates the third-order sibling parser proposed by McDonald and Pereira (2006).There are several possibilities for further research involving our third-order parsing algorithms.One idea would be to consider extensions and modifications of our parsers, some of which have been suggested in Sections 5 and 7.4.A second area for future work lies in applications of dependency parsing.While we have evaluated our new algorithms on standard parsing benchmarks, there are a wide variety of tasks that may benefit from the extended context offered by our thirdorder factorizations; for example, the 4-gram substructures enabled by our approach may be useful for dependency-based language modeling in machine translation (Shen et al., 2008).Finally, in the hopes that others in the NLP community may find our parsers useful, we provide a free distribution of our implementation.2We would like to thank the anonymous reviewers for their helpful comments and suggestions.We also thank Regina Barzilay and Alexander Rush for their much-appreciated input during the writing process.The authors gratefully acknowledge the following sources of support: Terry Koo and Michael Collins were both funded by a DARPA subcontract under SRI (#27-001343), and Michael Collins was additionally supported by NTT (Agmt. dtd.06/21/98).
Conditional Random Fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) constitute a widely-used and effective approach for supervised structure learning tasks involving the mapping between complex objects such as strings and trees.An important property of CRFs is their ability to handle large and redundant feature sets and to integrate structural dependency between output labels.However, even for simple linear chain CRFs, the complexity of learning and inference This work was partly supported by ANR projects CroTaL (ANR-07-MDCO-003) and MGA (ANR-07-BLAN-031102). grows quadratically with respect to the number of output labels and so does the number of structural features, ie. features testing adjacent pairs of labels.Most empirical studies on CRFs thus either consider tasks with a restricted output space (typically in the order of few dozens of output labels), heuristically reduce the use of features, especially of features that test pairs of adjacent labels1, and/or propose heuristics to simulate contextual dependencies, via extended tests on the observations (see discussions in, eg., (Punyakanok et al., 2005; Liang et al., 2008)).Limitating the feature set or the number of output labels is however frustrating for many NLP tasks, where the type and number of potentially relevant features are very large.A number of studies have tried to alleviate this problem.Pal et al. (2006) propose to use a “sparse” version of the forward-backward algorithm during training, where sparsity is enforced through beam pruning.Related ideas are discussed by Dietterich et al. (2004); by Cohn (2006), who considers “generalized” feature functions; and by Jeong et al.(2009), who use approximations to simplify the forward-backward recursions.In this paper, we show that the sparsity that is induced by il-penalized estimation of CRFs can be used to reduce the total training time, while yielding extremely compact models.The benefits of sparsity are even greater during inference: less features need to be extracted and included in the potential functions, speeding up decoding with a lesser memory footprint.We study and compare three different ways to implement il penalty for CRFs that have been introduced recently: orthantwise Quasi Newton (Andrew and Gao, 2007), stochastic gradient descent (Tsuruoka et al., 2009) and coordinate descent (Sokolovska et al., 2010), concluding that these methods have complementary strengths and weaknesses.Based on an efficient implementation of these algorithms, we were able to train very large CRFs containing more than a hundred of output labels and up to several billion features, yielding results that are as good or better than the best reported results for two NLP benchmarks, text phonetization and part-of-speech tagging.Our contribution is therefore twofold: firstly a detailed analysis of these three algorithms, discussing implementation, convergence and comparing the effect of various speed-ups.This comparison is made fair and reliable thanks to the reimplementation of these techniques in the same software package.Second, the experimental demonstration that using large output label sets is doable and that very large feature sets actually help improve prediction accuracy.In addition, we show how sparsity in structured feature sets can be used in incremental training regimes, where long-range features are progressively incorporated in the model insofar as the shorter range features have proven useful.The rest of the paper is organized as follows: we first recall the basics of CRFs in Section 2, and discuss three ways to train CRFs with a `1 penalty in Section 3.We then detail several implementation issues that need to be addressed when dealing with massive feature sets in Section 4.Our experiments are reported in Section 5.The main conclusions of this study are drawn in Section 6.In this section, we recall the basics of Conditional Random Fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) and introduce the notations that will be used throughout.CRFs are based on the following model where x = (x1, ...,xT) and y = (y1, ... , yT) are, respectively, the input and output sequences2, and Fk(x, y) is equal to PTt=1 fk(yt−1, yt, xt), where {fk}1≤k≤K is an arbitrary set of feature functions and {θk}1≤k≤K are the associated parameter values.We denote by Y and X, respectively, the sets in which yt and xt take their values.The normalization factor in (1) is defined by The most common choice of feature functions is to use binary tests.In the sequel, we distinguish between two types of feature functions: unigram features fy,x, associated with parameters µy,x, and bigram features fyl,y,x, associated with parameters λy/,y,x.These are defined as where 1(cond.) is equal to 1 when the condition is verified and to 0 otherwise.In this setting, the number of parameters K is equal to |Y |2x|X|train, where |· |denotes the cardinal and |X|train refers to the number of configurations of xt observed during training.Thus, even in moderate size applications, the number of parameters can be very large, mostly due to the introduction of sequential dependencies in the model.This also explains why it is hard to train CRFs with dependencies spanning more than two adjacent labels.Using only unigram features {fy,x}(y,x)∈Y ×X results in a model equivalent to a simple bag-of-tokens positionby-position logistic regression model.On the other hand, bigram features {fy/,y,x}(y,x)∈Y 2×X are helpful in modelling dependencies between successive labels.The motivations for using simultaneously both types of feature functions are evaluated experimentally in Section 5.Given N independent sequences {x(i), y(i)}Ni=1, where x(i) and y(i) contain T(i) symbols, conditional maximum likelihood estimation is based on the minimization, with respect to θ, of the negated conditional log-likelihood of the observations This term is usually complemented with an additional regularization term so as to avoid overfitting (see Section 3.1 below).The gradient of l(θ) is where Epθ(y|x) denotes the conditional expectation given the observation sequence, i.e.Although l(θ) is a smooth convex function, its optimum cannot be computed in closed form, and l(θ) has to be optimized numerically.The computation of its gradient implies to repeatedly compute the conditional expectation in (5) for all input sequences x(i) and all positions t. The standard approach for computing these expectations is inspired by the forward-backward algorithm for hidden Markov models: using the notations introduced above, the algorithm implies the computation of the forward and backward recursions These recursions require a number of operations that grows quadratically with |Y |.The standard approach for parameter estimation in CRFs consists in minimizing the logarithmic loss l(θ) defined by (3) with an additional `2 penalty term ρ22 kθk22, where ρ2 is a regularization parameter.The objective function is then a smooth convex function to be minimized over an unconstrained parameter space.Hence, any numerical optimization strategy may be used and practical solutions include limited memory BFGS (L-BFGS) (Liu and Nocedal, 1989), which is used in the popular CRF++ (Kudo, 2005) and CRFsuite (Okazaki, 2007) packages; conjugate gradient (Nocedal and Wright, 2006) and Stochastic Gradient Descent (SGD) (Bottou, 2004; Vishwanathan et al., 2006), used in CRFsgd (Bottou, 2007).The only caveat is to avoid numerical optimizers that require the full Hessian matrix (e.g., Newton’s algorithm) due to the size of the parameter vector in usual applications of CRFs.The most significant alternative to `2 regularization is to use a `1 penalty term ρ1kθk1: such regularizers are able to yield sparse parameter vectors in which many component have been zeroed (Tibshirani, 1996).Using a `1 penalty term thus implicitly performs feature selection, where ρ1 controls the amount of regularization and the number of extracted features.In the following, we will jointly use both penalty terms, yielding the socalled elastic net penalty (Zhou and Hastie, 2005) which corresponds to the objective function The use of both penalty terms makes it possible to control the number of non zero coefficients and to avoid the numerical problems that might occur in large dimensional parameter settings (see also (Chen, 2009)).However, the introduction of a `1 penalty term makes the optimization of (6) more problematic, as the objective function is no longer differentiable in 0.Various strategies have been proposed to handle this difficulty.We will only consider here exact approaches and will not discuss heuristic strategies such as grafting (Perkins et al., 2003; Riezler and Vasserman, 2004).To deal with `1 penalties, a simple idea is that of (Kazama and Tsujii, 2003), originally introduced for maxent models.It amounts to reparameterizing θk as θk = θ+k −θ−k , where θ+k and θ−k are positive.The `1 penalty thus becomes ρ1(θ+ − θ−).In this formulation, the objective function recovers its smoothness and can be optimized with conventional algorithms, subject to domain constraints.Optimization is straightforward, but the number of parameters is doubled and convergence is slow (Andrew and Gao, 2007): the procedure lacks a mechanism for zeroing out useless parameters.A more efficient strategy is the orthant-wise quasi-Newton (OWL-QN) algorithm introduced in (Andrew and Gao, 2007).The method is based on the observation that the `1 norm is differentiable when restricted to a set of points in which each coordinate never changes its sign (an “orthant”), and that its second derivative is then zero, meaning that the `1 penalty does not change the Hessian of the objective on each orthant.An OWL-QN update then simply consists in (i) computing the Newton update in a well-chosen orthant; (ii) performing the update, which might cause some component of the parameter vector to change sign; and (iii) projecting back the parameter value onto the initial orthant, thereby zeroing out those components.In (Gao et al., 2007), the authors show that OWL-QN is faster than the algorithm proposed by Kazama and Tsujii (2003) and can perform model selection even in very high-dimensional problems, with no loss of performance compared to the use of `2 penalty terms.Stochastic gradient (SGD) approaches update the parameter vector based on an crude approximation of the gradient (4), where the computation of expectations only includes a small batch of observations.SGD updates have the following form where η is the learning rate.In (Tsuruoka et al., 2009), various ways of adapting this update to `1penalized likelihood functions are discussed.Two effective ideas are proposed: (i) only update parameters that correspond to active features in the current observation, (ii) keep track of the cumulated penalty zk that θk should have received, had the gradient been computed exactly, and use this value to “clip” the parameter value.This is implemented by patching the update (7) as follows Based on a study of three NLP benchmarks, the authors of (Tsuruoka et al., 2009) claim this approach to be much faster than the orthant-wise approach and yet to yield very comparable performance, while selecting slightly larger feature sets.The coordinate descent approach of Dudik et al. (2004) and Friedman et al.(2008) uses the fact that optimizing a mono-dimensional quadratic function augmented with a `1 penalty can be performed analytically.For arbitrary functions, this idea can be adapted by considering quadratic approximations of the objective around the current value θ¯ Coordinate descent is ported to CRFs in (Sokolovska et al., 2010).Making this scheme practical requires a number of adaptations, including (i) approximating the second order term in (10), (ii) performing updates in block, where a block contains the |Y  |× |Y + 1 |features νy1,y,x and λy,x for a fixed test x on the observation sequence and (iii) approximating the Hessian for a block by its diagonal terms.(ii) is specially critical, as repeatedly cycling over individual features to perform the update (10) is only possible with restricted sets of features.The block update schemes uses the fact that all features within a block appear in the same set of sequences, which means that most of the computations needed to perform theses updates can be shared within the block.One advantage of the resulting algorithm, termed BCD in the following, is that the update of θk only involves carrying out the forward-backward recursions for the set of sequences that contain symbols x such that at least one {fk(y', y, x)}(y y1)EY 2 is non null, which can be much smaller than the whole training set.Efficiently processing very-large feature and observation sets requires to pay attention to many implementation details.In this section, we present several optimizations devised to speed up training.For all algorithms, the computation time is dominated by the evaluations of the gradient: our implementation takes advantage of the sparsity to accelerate these computations.Assume the set of bigram features {λy0,y,xt+1}(y0,y)∈Y 2 is sparse with only r(xt+1) « |Y |2 non null values and define al., 2010) explains how computational savings can be obtained using the fact that the vector/matrix products in the recursions above only involve the sparse matrix Mt+1(y0, y).They can thus be computed with exactly r(xt+1) multiplications instead of |Y |2.The same idea can be used when the set {µy,xt+1}y∈Y of unigram features is sparse.Using this implementation, the complexity of the forward-backward procedure for x(�) can be made proportional to the average number of active features per position, which can be much smaller than the number of potentially active features.For BCD, forward-backward can even be made slightly faster.When computing the gradient wrt. features λy,x and µy0,y,x (for all the values of y and y0) for sequence x(�), assuming that x only occurs once in x(�) at position t, all that is needed is α0t(y), bt0 < t and β0t(y), bt0 > t. ZB(x) is then recovered as Ey αt(y)βt(y).Forward-backward recursions can thus be truncated: in our experiments, this divided the computational cost by 1,8 on average.Note finally that forward-backward is performed on a per-observation basis and is easily parallelized (see also (Mann et al., 2009) for more powerful ways to distribute the computation when dealing with very large datasets).In our implementation, it is distributed on all available cores, resulting in significant speed-ups for OWL-QN and L-BFGS; for BCD the gain is less acute, as parallelization only helps when updating the parameters for a block of features that are occur in many sequences; for SGD, with batches of size one, this parallelization policy is useless.Most existing implementations of CRFs, eg.CRF++ and CRFsgd perform the forwardbackward recursions in the log-domain, which guarantees that numerical over/underflows are avoided no matter the length T(�) of the sequence.It is however very inefficient from an implementation point of view, due to the repeated calls to the exp() and log() functions.As an alternative way of avoiding numerical problems, our implementation, like crfSuite’s, resorts to “scaling”, a solution commonly used for HMMs.Scaling amounts to normalizing the values of αt and βt to one, making sure to keep track of the cumulated normalization factors so as to compute ZB(x) and the conditional expectations Epo(y|x).Also note that in our implementation, all the computations of exp(x) are vectorized, which provides an additional speed up of about 20%.Processing very large feature vectors, up to billions of components, is problematic in many ways.Sparsity has been used here to speed up forwardbackward, but we have made no attempt to accelerate the computation of the OWL-QN updates, which are linear in the size of the parameter vector.Of the three algorithms, BCD is the most affected by increases in the number of features, or more precisely, in the number of features blocks, where one block correspond to a specific test of the observation.In the worst case scenario, each block may require to visit all the training instances, yielding terrible computational wastes.In practice though, most blocks only require to process a small fraction of the training set, and the actual complexity depends on the average number of blocks per observations.Various strategies have been tried to further accelerate BCD, such as processing blocks that only visit one observation in parallel and updating simultaneously all the blocks that visit all the training instances, leading to a small speed-up on the POS-tagging task.Working with billions of features finally requires to worry also about memory usage.In this respect, BCD is the most efficient, as it only requires to store one K-dimensional vector for the parameter itself.SGD requires two such vectors, one for the parameter and one for storing the zk (see Eq.(8)).In comparison, OWL-QN requires much more memory, due to the internals of the update routines, which require several histories of the parameter vector and of its gradient.Typically, our implementation necessitates in the order of a dozen K-dimensional vectors.Parallelization only makes things worse, as each core will also need to maintain its own copy of the gradient.Our experiments use two standard NLP tasks, phonetization and part-of-speech tagging, chosen here to illustrate two very different situations, and to allow for comparison with results reported elsewhere in the literature.Unless otherwise mentioned, the experiments use the same protocol: 10 fold cross validation, where eight folds are used for training, one for development, and one for testing.Results are reported in terms of phoneme error rates or tag error rates on the test set.Comparing run-times can be a tricky matter, especially when different software packages are involved.As discussed above, the observed runtimes depend on many small implementation details.As the three algorithms share as much code as possible, we believe the comparison reported hereafter to be fair and reliable.All experiments were performed on a server with 64G of memory and two Xeon processors with 4 cores at 2.27 Ghz.For comparison, all measures of run-times include the cumulated activity of all cores and give very pessimistic estimates of the wall time, which can be up to 7 times smaller.For OWL-QN, we use 5 past values of the gradient to approximate the inverse of the Hessian matrix: increasing this value had no effect on accuracy or convergence and was detrimental to speed; for SGD, the learning rate parameter was tuned manually.Note that we have not spent much time optimizing the values of p1 and p2.Based on a pilot study on Nettalk, we found that taking p1 = .5 and p2 in the order of 10−5 to yield nearly optimal performance, and have used these values throughout.Our first benchmark is the word phonetization task, using the Nettalk dictionary (Sejnowski and Rosenberg, 1987).This dataset contains approximately 20,000 English word forms, their pronunciation, plus some prosodic information (stress markers for vowels, syllabic parsing for consonants).Grapheme and phoneme strings are aligned at the character level, thanks to the use of a “null sound” in the latter string when it is shorter than the former; likewise, each prosodic mark is aligned with the corresponding letter.We have derived two test conditions from this database.The first one is standard and aims at predicting the pronunciation information only.In this setting, the set of observations (X) contains 26 graphemes, and the output label set contains |Y  |= 51 phonemes.The second condition aims at jointly predicting phonemic and prosodic information3.The reasons for designing this new condition are twofold: firstly, it yields a large set of composite labels (|Y  |= 114) and makes the problem computationally challenging.Second, it allows to quantify how much the information provided by the prosodic marks help predict the phonemic labels.Both information are quite correlated, as the stress mark and the syllable openness, for instance, greatly influence the realization of some archi-phonemes.The features used in Nettalk experiments take the form fy,,,, (unigram) and fy0,y,,,, (bigram), where w is a n-gram of letters.The n-grm feature sets (n = 11, 3, 5, 7}) includes all features testing embedded windows of k letters, for all 0 < k < n; the n-grm- setting is similar, but only includes the window of length n; in the n-grm+ setting, we add features for odd-size windows; in the ngrm++ setting, we add all sequences of letters up to size n occurring in current window.For instance, the active bigram features at position t = 2 in the sequence x=’lemma’ are as follows: the 3grm feature set contains fy,y0, fy,y0,e and fy0,y,lem; only the latter appears in the 3-grm- setting.In the 3-grm+ feature set, we also have fy0,y,le and fy0,y,em.The 3-grm++ feature set additionally includes fy0,y,l and fy0,y,m.The number of features ranges from 360 thousands (1-grm setting) to 1.6 billion (7-grm).3Given the design of the Nettalk dictionary, this experiment required to modify the original database so as to reassign prosodic marks to phonemes, rather than to letters.Our second benchmark is a part-of-speech (POS) tagging task using the PennTreeBank corpus (Marcus et al., 1993), which provides us with a quite different condition.For this task, the number of labels is smaller (|Y  |= 45) than for Nettalk, and the set of observations is much larger (IXI = 43207).This benchmark, which has been used in many studies, allows for direct comparisons with other published work.We thus use a standard experimental set-up, where sections 0-18 of the Wall Street Journal are used for training, sections 19-21 for development, and sections 22-24 for testing.Features are also standard and follow the design of (Suzuki and Isozaki, 2008) and test the current words (as written and lowercased), prefixes and suffixes up to length 4, and typographical characteristics (case, etc.) of the words.Our baseline feature set also contains tests on individual and pairs of words in a window of 5 words.The first important issue is to assess the benefits of using large feature sets, notably including features testing both a bigram of labels and an observation.Table 1 compares the results obtained with and without these features for various setting (using OWL-QN to perform the optimization), suggesting that for the tasks at hand, these features are actually helping.The training speed depends of two main factors: the number of iterations needed to achieve convergence and the computational cost of one iteration.In this section, we analyze and compare the runtime efficiency of the three optimizers.As far as convergence is concerned, the two forms of regularization (E2 and E1) yield the same performance (see Table 3), and the three algorithms exhibit more or less the same behavior.They quickly reach an acceptable set of active parameters, which is often several orders of magnitude smaller than the whole parameter set (see results below in Table 4 and 5).Full convergence, reflected by a stabilization of the objective function, is however not so easily achieved.We have often observed a slow, yet steady, decrease of the log-loss, accompanied with a diminution of the number of active features as the number of iterations increases.Based on this observation, we have chosen to stop all algorithms based on their performance on an independent development set, allowing a fair comparison of the overall training time; for OWL-QN, it allowed to divide the total training time by almost 2.It has finally often been found useful to fine tune the non-zero parameters by running a final handful of L-BFGS iterations using only a small E2 penalty; at this stage, all the other features are removed from the model.This had a small impact BCD and SGD’s performance and allowed them to catch up with OWL-QN’s performance.As explained in section 4.1, the forward-backward algorithm can be written so as to use the sparsity of the matrix My�y,�,,.To evaluate the resulting speed-up, we ran a series of experiments using Nettalk (see Table 2).In this table, the 3-grm- setting corresponds to maximum sparsity for M, and training with the sparse algorithm is three times faster than with the non-sparse version.Throwing in more features has the effect of making M much more dense, mitigating the benefits of the sparse recursions.Nevertheless, even for very large feature sets, the percentage of zeros in M averages 20% to 30%, and the sparse version remains 10 to 20% faster than the non-sparse one.Note that the non-sparse version is faster with a E' penalty term than with only the E2 term: this is because exp(0) is faster to evaluate than exp(x) when x 7� 0.Table 4 displays the results achieved on the Nettalk task.The three algorithms yield very comparable accuracy results, and deliver compact models: for the 5-gram+ setting, only 50,000 out of 250 million features are selected.SGD is the fastest of the three, up to twice as fast as OWL-QN and BCD depending on the feature set.The performance it achieves are consistently slightly worst than the other optimizers, and only catch up when the parameters are fine-tuned (see above).There are not so many comparisons for Nettalk with CRFs, due to the size of the label set.Our results compare favorably with those reported in (Pal et al., 2006), where the accuracy attains 91.7% using 19075 examples for training and 934 for testing, and with those in (Jeong et al., 2009) (88.4% accuracy with 18,000 (2,000) training (test) instances).Table 5 gives the results obtained for the larger Nettalk+prosody task.Here, we only report the results obtained with SGD and BCD.For OWL-QN, the largest model we could handle was the 3-grm model, which contained 69 million features, and took 48min to train.Here again, performance steadily increase with the number of features, showing the benefits of large-scale models.We lack comparisons for this task, which seems considerably harder than the sole phonetization task, and all systems seem to plateau around 13.5% accuracy.Interestingly, simultaneously predicting the phoneme and its prosodic markers allows to improve the accuracy on the prediction of phonemes, which improves of almost a half point as compared to the best Nettalk system.For the POS tagging task, BCD appears to be unpractically slower to train than the others approaches (SGD takes about 40min to train, OWLQN about 1 hour) due the simultaneous increase in the sequence length and in the number of observations.As a result, one iteration of BCD typically requires to repeatedly process over and over the same sequences: on average, each sequence is visited 380 times when we use the baseline feature set.This technique should reserved for tasks where the number of blocks is small, or, as below, when memory usage is an issue.In many tasks, the ambiguity of tokens can be reduced by looking up increasingly large windows of local context.This strategy however quickly runs into a combinatorial increase of the number of features.A side note of the Nettalk experiments is that when using embedded features, the active feature set tends to reflect this hierarchical organization.This means that when a feature testing a n-gram is active, in most cases, the features for all embedded k-grams are also selected.Based on this observation, we have designed an incremental training strategy for the POS tagging task, where more specific features are progressively incorporated into the model if the corresponding less specific feature is active.This experiment used BCD, which is the most memory efficient algorithm.The first iteration only includes tests on the current word.During the second iteration, we add tests on bigram of words, on suffixes and prefixes up to length 4.After four iterations, we throw in features testing word trigrams, subject to the corresponding unigram block being active.After 6 iterations, we finally augment the model with windows of length 5, subject to the corresponding trigram being active.After 10 iterations, the model contains about 4 billion features, out of which 400,000 are active.It achieves an error rate of 2.63% (resp.2.78%) on the development (resp. test) data, which compares favorably with some of the best results for this task (for instance (Toutanova et al., 2003; Shen et al., 2007; Suzuki and Isozaki, 2008)).In this paper, we have discussed various ways to train extremely large CRFs with a ' penalty term and compared experimentally the results obtained, both in terms of training speed and of accuracy.The algorithms studied in this paper have complementary strength and weaknesses: OWL-QN is probably the method of choice in small or moderate size applications while BCD is most efficient when using very large feature sets combined with limited-size observation alphabets; SGD complemented with fine tuning appears to be the preferred choice in most large-scale applications.Our analysis demonstrate that training large-scale sparse models can be done efficiently and allows to improve over the performance of smaller models.The CRF package developed in the course of this study implements many algorithmic optimizations and allows to design innovative training strategies, such as the one presented in section 5.4.This package is released as open-source software and is available at http://wapiti.limsi.fr.In the future, we intend to study how sparsity can be used to speed-up training in the face of more complex dependency patterns (such as higher-order CRFs or hierarchical dependency structures (Rozenknop, 2002; Finkel et al., 2008).From a performance point of view, it might also be interesting to combine the use of large-scale feature sets with other recent improvements such as the use of semi-supervised learning techniques (Suzuki and Isozaki, 2008) or variable-length dependencies (Qian et al., 2009).
Noun phrase (NP) coreference resolution, the task of determining which NPs in a text or dialogue refer to the same real-world entity, has been at the core of natural language processing (NLP) since the 1960s.NP coreference is related to the task of anaphora resolution, whose goal is to identify an antecedent for an anaphoric NP (i.e., an NP that depends on another NP, specifically its antecedent, for its interpretation) [see van Deemter and Kibble (2000) for a detailed discussion of the difference between the two tasks].Despite its simple task definition, coreference is generally considered a difficult NLP task, typically involving the use of sophisticated knowledge sources and inference procedures (Charniak, 1972).Computational theories of discourse, in particular focusing (see Grosz (1977) and Sidner (1979)) and centering (Grosz et al. (1983; 1995)), have heavily influenced coreference research in the 1970s and 1980s, leading to the development of numerous centering algorithms (see Walker et al.(1998)).The focus of coreference research underwent a gradual shift from heuristic approaches to machine learning approaches in the 1990s.This shift can be attributed in part to the advent of the statistical NLP era, and in part to the public availability of annotated coreference corpora produced as part of the MUC-6 (1995) and MUC-7 (1998) conferences.Learning-based coreference research has remained vibrant since then, with results regularly published not only in general NLP conferences, but also in specialized conferences (e.g., the biennial Discourse Anaphora and Anaphor Resolution Colloquium (DAARC)) and workshops (e.g., the series of Bergen Workshop on Anaphora Resolution (WAR)).Being inherently a clustering task, coreference has also received a lot of attention in the machine learning community.Fifteen years have passed since the first paper on learning-based coreference resolution was published (Connolly et al., 1994).Our goal in this paper is to provide NLP researchers with a survey of the major milestones in supervised coreference research, focusing on the computational models, the linguistic features, the annotated corpora, and the evaluation metrics that were developed in the past fifteen years.Note that several leading coreference researchers have published books (e.g., Mitkov (2002)), written survey articles (e.g., Mitkov (1999), Strube (2009)), and delivered tutorials (e.g., Strube (2002), Ponzetto and Poesio (2009)) that provide a broad overview of coreference research.This survey paper aims to complement, rather than supersede, these previously published materials.In particular, while existing survey papers discuss learning-based coreference research primarily in the context of the influential mention-pair model, we additionally survey recently proposed learning-based coreference models, which attempt to address the weaknesses of the mention-pair model.Due to space limitations, however, we will restrict our discussion to the most commonly investigated kind of coreference relation: the identity relation for NPs, excluding coreference among clauses and bridging references (e.g., part/whole and set/subset relations).The widespread popularity of machine learning approaches to coreference resolution can be attributed in part to the public availability of annotated coreference corpora.The MUC-6 and MUC-7 corpora, though relatively small (60 documents each) and homogeneous w.r.t. document type (newswire articles only), have been extensively used for training and evaluating coreference models.Equally popular are the corpora produced by the Automatic Content Extraction (ACE1) evaluations in the past decade: while the earlier ACE corpora (e.g., ACE-2) consist of solely English newswire and broadcast news articles, the later ones (e.g., ACE 2005) have also included Chinese and Arabic documents taken from additional sources such as broadcast conversations, webblog, usenet, and conversational telephone speech.Coreference annotations are also publicly available in treebanks.These include (1) the English Penn Treebank (Marcus et al., 1993), which is labeled with coreference links as part of the OntoNotes project (Hovy et al., 2006); (2) the T¨ubingen Treebank (Telljohann et al., 2004), which is a collection of German news articles consisting of 27,125 sentences; (3) the Prague Dependency Treebank (Haji˘c et al., 2006), which consists of 3168 news articles taken from the Czech National Corpus; (4) the NAIST Text Corpus (Iida et al., 2007b), which consists of 287 Japanese news articles; (5) the AnCora Corpus (Recasens and Marti, 2009), which consists of Spanish and Catalan journalist texts; and (6) the GENIA corpus (Ohta et al., 2002), which contains 2000 MEDLINE abstracts.Other publicly available coreference corpora of interest include two annotated by Ruslan Mitkov’s research group: (1) a 55,000-word corpus in the domain of security/terrorism (Hasler et al., 2006); and (2) training data released as part of the 2007 Anaphora Resolution Exercise (Or˘asan et al., 2008), a coreference resolution shared task.There are also two that consist of spoken dialogues: the TRAINS93 corpus (Heeman and Allen, 1995) and the Switchboard data set (Calhoun et al., in press).Additional coreference data will be available in the near future.For instance, the SemEval-2010 shared task on Coreference Resolution in Multiple Languages (Recasens et al., 2009) has promised to release coreference data in six languages.In addition, Massimo Poesio and his colleagues are leading an annotation project that aims to collect large amounts of coreference data for English via a Web Collaboration game called Phrase Detectives2.In this section, we examine three important classes of coreference models that were developed in the past fifteen years, namely, the mention-pair model, the entity-mention model, and ranking models.The mention-pair model is a classifier that determines whether two NPs are coreferent.It was first proposed by Aone and Bennett (1995) and McCarthy and Lehnert (1995), and is one of the most influential learning-based coreference models.Despite its popularity, this binary classification approach to coreference is somewhat undesirable: the transitivity property inherent in the coreference relation cannot be enforced, as it is possible for the model to determine that A and B are coreferent, B and C are coreferent, but A and C are not coreferent.Hence, a separate clustering mechanism is needed to coordinate the pairwise classification decisions made by the model and construct a coreference partition.Another issue that surrounds the acquisition of the mention-pair model concerns the way training instances are created.Specifically, to determine whether a pair of NPs is coreferent or not, the mention-pair model needs to be trained on a data set where each instance represents two NPs and possesses a class value that indicates whether the two NPs are coreferent.Hence, a natural way to assemble a training set is to create one instance from each pair of NPs appearing in a training document.However, this instance creation method is rarely employed: as most NP pairs in a text are not coreferent, this method yields a training set with a skewed class distribution, where the negative instances significantly outnumber the positives.As a result, in practical implementations of the mention-pair model, one needs to specify not only the learning algorithm for training the model and the linguistic features for representing an instance, but also the training instance creation method for reducing class skewness and the clustering algorithm for constructing a coreference partition.As noted above, the primary purpose of training instance creation is to reduce class skewness.Many heuristic instance creation methods have been proposed, among which Soon et al.’s (1999; 2001) is arguably the most popular choice.Given an anaphoric noun phrase3, NPk, Soon et al.’s method creates a positive instance between NPk and its closest preceding antecedent, NPj, and a negative instance by pairing NPk with each of the intervening NPs, NPj+1, ..., NPk−1.With an eye towards improving the precision of a coreference resolver, Ng and Cardie (2002c) propose an instance creation method that involves a single modification to Soon et al.’s method: if NPk is non-pronominal, a positive instance should be formed between NPk and its closest preceding nonpronominal antecedent instead.This modification is motivated by the observation that it is not easy for a human, let alone a machine learner, to learn from a positive instance where the antecedent of a non-pronominal NP is a pronoun.To further reduce class skewness, some researchers employ a filtering mechanism on top of an instance creation method, thereby disallowing the creation of training instances from NP pairs that are unlikely to be coreferent, such as NP pairs that violate gender and number agreement (e.g., Strube et al. (2002), Yang et al.(2003)).While many instance creation methods are heuristic in nature (see Uryupina (2004) and Hoste and Daelemans (2005)), some are learning-based.For example, motivated by the fact that some coreference relations are harder to identify than the others (see Harabagiu et al. (2001)), Ng and Cardie (2002a) present a method for mining easy positive instances, in an attempt to avoid the inclusion of hard training instances that may complicate the acquisition of an accurate coreference model.Once a training set is created, we can train a coreference model using an off-the-shelf learning algorithm.Decision tree induction systems (e.g., C5 (Quinlan, 1993)) are the first and one of the most widely used learning algorithms by coreference researchers, although rule learners (e.g., RIPPER (Cohen, 1995)) and memory-based learners (e.g., TiMBL (Daelemans and Van den Bosch, 2005)) are also popular choices, especially in early applications of machine learning to coreference resolution.In recent years, statistical learners such as maximum entropy models (Berger et al., 1996), voted perceptrons (Freund and Schapire, 1999), and support vector machines (Joachims, 1999) have been increasingly used, in part due to their ability to provide a confidence value (e.g., in the form of a probability) associated with a classification, and in part due to the fact that they can be easily adapted to train recently proposed rankingbased coreference models (see Section 3.3).After training, we can apply the resulting model to a test text, using a clustering algorithm to coordinate the pairwise classification decisions and impose an NP partition.Below we describe some commonly used coreference clustering algorithms.Despite their simplicity, closest-first clustering (Soon et al., 2001) and best-first clustering (Ng and Cardie, 2002c) are arguably the most widely used coreference clustering algorithms.The closest-first clustering algorithm selects as the antecedent for an NP, NPk, the closest preceding noun phrase that is classified as coreferent with it.4 However, if no such preceding noun phrase exists, no antecedent is selected for NPk.The best-first clustering algorithm aims to improve the precision of closest-first clustering, specifically by selecting as the antecedent of NPk the most probable preceding NP that is classified as coreferent with it.One criticism of the closest-first and best-first clustering algorithms is that they are too greedy.In particular, clusters are formed based on a small subset of the pairwise decisions made by the model.Moreover, positive pairwise decisions are unjustifiably favored over their negative counterparts.For example, three NPs are likely to end up in the same cluster in the resulting partition even if there is strong evidence that A and C are not coreferent, as long as the other two pairs (i.e., (A,B) and (B,C)) are classified as positive.Several algorithms that address one or both of these problems have been used for coreference clustering.Correlation clustering (Bansal et al., 2002), which produces a partition that respects as many pairwise decisions as possible, is used by McCallum and Wellner (2004), Zelenko et al. (2004), and Finley and Joachims (2005).Graph partitioning algorithms are applied on a weighted, undirected graph where a vertex corresponds to an NP and an edge is weighted by the pairwise coreference scores between two NPs (e.g., McCallum and Wellner (2004), Nicolae and Nicolae (2006)).The Dempster-Shafer rule (Dempster, 1968), which combines the positive and negative pairwise decisions to score a partition, is used by Kehler (1997) and Bean and Riloff (2004) to identify the most probable NP partition.Some clustering algorithms bear a closer resemblance to the way a human creates coreference clusters.In these algorithms, not only are the NPs in a text processed in a left-to-right manner, the later coreference decisions are dependent on the earlier ones (Cardie and Wagstaff, 1999; Klenner and Ailloud, 2008).5 For example, to resolve an NP, NPk, Cardie and Wagstaff’s algorithm considers each preceding NP, NPj, as a candidate antecedent in a right-to-left order.If NPk and NPj are likely to be coreferent, the algorithm imposes an additional check that NPk does not violate any constraint on coreference (e.g., gender agreement) with any NP in the cluster containing NPj before positing that the two NPs are coreferent.Luo et al.’s (2004) Bell-tree-based algorithm is another clustering algorithm where the later coreference decisions are dependent on the earlier ones.A Bell tree provides an elegant way of organizing the space of NP partitions.Informally, a node in the ith level of a Bell tree corresponds to an ithorder partial partition (i.e., a partition of the first i NPs of the given document), and the ith level of the tree contains all possible ith-order partial partitions.Hence, a leaf node contains a complete partition of the NPs, and the goal is to search for the leaf node that contains the most probable partition.The search starts at the root, and a partitioning of the NPs is incrementally constructed as we move down the tree.Specifically, based on the coreference decisions it has made in the first i−1 levels of the tree, the algorithm determines at the ith level whether the ith NP should start a new cluster, or to which preceding cluster it should be assigned.While many coreference clustering algorithms have been developed, there have only been a few attempts to compare their effectiveness.For example, Ng and Cardie (2002c) report that bestfirst clustering is better than closest-first clustering.Nicolae and Nicolae (2006) show that bestfirst clustering performs similarly to Bell-treebased clustering, but neither of these algorithms performs as well as their proposed minimum-cutbased graph partitioning algorithm.While coreference clustering algorithms attempt to resolve each NP encountered in a document, only a subset of the NPs are anaphoric and therefore need to be resolved.Hence, knowledge of the anaphoricity of an NP can potentially improve the precision of a coreference resolver.Traditionally, the task of anaphoricity determination has been tackled independently of coreference resolution using a variety of techniques.For example, pleonastic it has been identified using heuristic approaches (e.g., Paice and Husk (1987), Lappin and Leass (1994), Kennedy and Boguraev (1996)), supervised approaches (e.g., Evans (2001), M¨uller (2006), Versley et al. (2008a)), and distributional methods (e.g., Bergsma et al.(2008)); and non-anaphoric definite descriptions have been identified using rule-based techniques (e.g., Vieira and Poesio (2000)) and unsupervised techniques (e.g., Bean and Riloff (1999)).Recently, anaphoricity determination has been evaluated in the context of coreference resolution, with results showing that training an anaphoricity classifier to identify and filter non-anaphoric NPs prior to coreference resolution can improve a learning-based resolver (e.g., Ng and Cardie (2002b), Uryupina (2003), Poesio et al. (2004b)).Compared to earlier work on anaphoricity determination, recently proposed approaches are more “global” in nature, taking into account the pairwise decisions made by the mention-pair model when making anaphoricity decisions.Examples of such approaches have exploited techniques including integer linear programming (ILP) (Denis and Baldridge, 2007a), label propagation (Zhou and Kong, 2009), and minimum cuts (Ng, 2009).From a learning perspective, a two-step approach to coreference — classification and clustering — is undesirable.Since the classification model is trained independently of the clustering algorithm, improvements in classification accuracy do not guarantee corresponding improvements in clustering-level accuracy.That is, overall performance on the coreference task might not improve.To address this problem, McCallum and Wellner (2004) and Finley and Joachims (2005) eliminate the classification step entirely, treating coreference as a supervised clustering task where a similarity metric is learned to directly maximize clustering accuracy.Klenner (2007) and Finkel and Manning (2008) use ILP to ensure that the pairwise classification decisions satisfy transitivity.6 While many of the aforementioned algorithms for clustering and anaphoricity determination have been shown to improve coreference performance, the underlying model with which they are used in combination — the mention-pair model — remains fundamentally weak.The model has two commonly-cited weaknesses.First, since each candidate antecedent for an anaphoric NP to be resolved is considered independently of the others, the model only determines how good a candidate antecedent is relative to the anaphoric NP, but not how good a candidate antecedent is relative to other candidates.In other words, it fails to answer the question of which candidate antecedent is most probable.Second, it has limitations in its expressiveness: the information extracted from the two NPs alone may not be sufficient for making an informed coreference decision, especially if the candidate antecedent is a pronoun (which is semantically empty) or a mention that lacks descriptive information such as gender (e.g., “Clinton”).Below we discuss how these weaknesses are addressed by the entity-mention model and ranking models.The entity-mention model addresses the expressiveness problem with the mention-pair model.To motivate the entity-mention model, consider an example taken from McCallum and Wellner (2003), where a document consists of three NPs: “Mr.Clinton,” “Clinton,” and “she.” The mentionpair model may determine that “Mr.Clinton” and “Clinton” are coreferent using string-matching features, and that “Clinton” and “she” are coreferent based on proximity and lack of evidence for gender and number disagreement.However, these two pairwise decisions together with transitivity imply that “Mr.Clinton” and “she” will end up in the same cluster, which is incorrect due to gender mismatch.This kind of error arises in part because the later coreference decisions are not dependent on the earlier ones.In particular, had the model taken into consideration that “Mr.Clinton” 6Recently, however, Klenner and Ailloud (2009) have become less optimistic about ILP approaches to coreference. and “Clinton” were in the same cluster, it probably would not have posited that “she” and “Clinton” are coreferent.The aforementioned Cardie and Wagstaff algorithm attempts to address this problem in a heuristic manner.It would be desirable to learn a model that can classify whether an NP to be resolved is coreferent with a preceding, possibly partially-formed, cluster.This model is commonly known as the entity-mention model.Since the entity-mention model aims to classify whether an NP is coreferent with a preceding cluster, each of its training instances (1) corresponds to an NP, NPk, and a preceding cluster, Cj, and (2) is labeled with either POSITIVE or NEGATIVE, depending on whether NPk should be assigned to Cj.Consequently, we can represent each instance by a set of cluster-level features (i.e., features that are defined over an arbitrary subset of the NPs in Cj).A cluster-level feature can be computed from a feature employed by the mention-pair model by applying a logical predicate.For example, given the NUMBER AGREEMENT feature, which determines whether two NPs agree in number, we can apply the ALL predicate to create a cluster-level feature, which has the value YES if NPk agrees in number with all of the NPs in Cj and NO otherwise.Other commonly-used logical predicates for creating cluster-level features include relaxed versions of the ALL predicate, such as MOST, which is true if NPk agrees in number with more than half of the NPs in Cj, and ANY, which is true as long as NPk agrees in number with just one of the NPs in Cj.The ability of the entity-mention model to employ cluster-level features makes it more expressive than its mention-pair counterpart.Despite its improved expressiveness, the entitymention model has not yielded particularly encouraging results.For example, Luo et al. (2004) apply the ANY predicate to generate cluster-level features for their entity-mention model, which does not perform as well as the mention-pair model.Yang et al. (2004b; 2008a) also investigate the entity-mention model, which produces results that are only marginally better than those of the mention-pair model.However, it appears that they are not fully exploiting the expressiveness of the entity-mention model, as cluster-level features only comprise a small fraction of their features.Variants of the entity-mention model have been investigated.For example, Culotta et al. (2007) present a first-order logic model that determines the probability that an arbitrary set of NPs are all co-referring.Their model resembles the entitymention model in that it enables the use of clusterlevel features.Daum´e III and Marcu (2005) propose an online learning model for constructing coreference chains in an incremental fashion, allowing later coreference decisions to be made by exploiting cluster-level features that are computed over the coreference chains created thus far.While the entity-mention model addresses the expressiveness problem with the mention-pair model, it does not address the other problem: failure to identify the most probable candidate antecedent.Ranking models, on the other hand, allow us to determine which candidate antecedent is most probable given an NP to be resolved.Ranking is arguably a more natural reformulation of coreference resolution than classification, as a ranker allows all candidate antecedents to be considered simultaneously and therefore directly captures the competition among them.Another desirable consequence is that there exists a natural resolution strategy for a ranking approach: an anaphoric NP is resolved to the candidate antecedent that has the highest rank.This contrasts with classification-based approaches, where many clustering algorithms have been employed to coordinate the pairwise classification decisions, and it is still not clear which of them is the best.The notion of ranking candidate antecedents can be traced back to centering algorithms, many of which use grammatical roles to rank forwardlooking centers (see Walker et al. (1998)).Ranking is first applied to learning-based coreference resolution by Connolly et al. (1994; 1997), where a model is trained to rank two candidate antecedents.Each training instance corresponds to the NP to be resolved, NP/,, as well as two candidate antecedents, NP� and NPj, one of which is an antecedent of NP,, and the other is not.Its class value indicates which of the two candidates is better.This model is referred to as the tournament model by Iida et al. (2003) and the twin-candidate model by Yang et al.(2003; 2008b).To resolve an NP during testing, one way is to apply the model to each pair of its candidate antecedents, and the candidate that is classified as better the largest number of times is selected as its antecedent.Advances in machine learning have made it possible to train a mention ranker that ranks all of the candidate antecedents simultaneously.While mention rankers have consistently outperformed the mention-pair model (Versley, 2006; Denis and Baldridge, 2007b), they are not more expressive than the mention-pair model, as they are unable to exploit cluster-level features, unlike the entitymention model.To enable rankers to employ cluster-level features, Rahman and Ng (2009) propose the cluster-ranking model, which ranks preceding clusters, rather than candidate antecedents, for an NP to be resolved.Cluster rankers therefore address both weaknesses of the mention-pair model, and have been shown to improve mention rankers.Cluster rankers are conceptually similar to Lappin and Leass’s (1994) heuristic pronoun resolver, which resolves an anaphoric pronoun to the most salient preceding cluster.An important issue with ranking models that we have eluded so far concerns the identification of non-anaphoric NPs.As a ranker simply imposes a ranking on candidate antecedents or preceding clusters, it cannot determine whether an NP is anaphoric (and hence should be resolved).To address this problem, Denis and Baldridge (2008) apply an independently trained anaphoricity classifier to identify non-anaphoric NPs prior to ranking, and Rahman and Ng (2009) propose a model that jointly learns coreference and anaphoricity.Another thread of supervised coreference research concerns the development of linguistic features.Below we give an overview of these features.String-matching features can be computed robustly and typically contribute a lot to the performance of a coreference system.Besides simple string-matching operations such as exact string match, substring match, and head noun match for different kinds of NPs (see Daum´e III and Marcu (2005)), slightly more sophisticated stringmatching facilities have been attempted, including minimum edit distance (Strube et al., 2002) and longest common subsequence (Casta˜no et al., 2002).Yang et al. (2004a) treat the two NPs involved as two bags of words, and compute their similarity using metrics commonly-used in information retrieval, such as the dot product, with each word weighted by their TF-IDF value.Syntactic features are computed based on a syntactic parse tree.Ge et al. (1998) implement a Hobbs distance feature, which encodes the rank assigned to a candidate antecedent for a pronoun by Hobbs’s (1978) seminal syntax-based pronoun resolution algorithm.Luo and Zitouni (2005) extract features from a parse tree for implementing Binding Constraints (Chomsky, 1988).Given an automatically parsed corpus, Bergsma and Lin (2006) extract from each parse tree a dependency path, which is represented as a sequence of nodes and dependency labels connecting a pronoun and a candidate antecedent, and collect statistical information from these paths to determine the likelihood that a pronoun and a candidate antecedent connected by a given path are coreferent.Rather than deriving features from parse trees, Iida et al. (2006) and Yang et al.(2006) employ these trees directly as structured features for pronoun resolution.Specifically, Yang et al. define tree kernels for efficiently computing the similarity between two parse trees, and Iida et al. use a boosting-based algorithm to compute the usefulness of a subtree.Grammatical features encode the grammatical properties of one or both NPs involved in an instance.For example, Ng and Cardie’s (2002c) resolver employs 34 grammatical features.Some features determine NP type (e.g., are both NPs definite or pronouns?).Some determine the grammatical role of one or both of the NPs.Some encode traditional linguistic (hard) constraints on coreference.For example, coreferent NPs have to agree in number and gender and cannot span one another (e.g., “Google” and “Google employees”).There are also features that encode general linguistic preferences either for or against coreference.For example, an indefinite NP (that is not in apposition to an anaphoric NP) is not likely to be coreferent with any NP that precedes it.There has been an increasing amount of work on investigating semantic features for coreference resolution.One of the earliest kinds of semantic knowledge employed for coreference resolution is perhaps selectional preference (Dagan and Itai, 1990; Kehler et al., 2004b; Yang et al., 2005; Haghighi and Klein, 2009): given a pronoun to be resolved, its governing verb, and its grammatical role, we prefer a candidate antecedent that can be governed by the same verb and be in the same role.Semantic knowledge has also been extracted from WordNet and unannotated corpora for computing the semantic compatibility/similarity between two common nouns (Harabagiu et al., 2001; Versley, 2007) as well as the semantic class of a noun (Ng, 2007a; Huang et al., 2009).One difficulty with deriving knowledge from WordNet is that one has to determine which sense of a given word to use.Some researchers simply use the first sense (Soon et al., 2001) or all possible senses (Ponzetto and Strube, 2006a), while others overcome this problem with word sense disambiguation (Nicolae and Nicolae, 2006).Knowledge has also been mined from Wikipedia for measuring the semantic relatedness of two NPs, NPj and NPk (Ponzetto and Strube (2006a; 2007)), such as: whether NPj/k appears in the first paragraph of the Wiki page that has NPk/j as the title or in the list of categories to which this page belongs, and the degree of overlap between the two pages that have the two NPs as their titles (see Poesio et al. (2007) for other uses of encyclopedic knowledge for coreference resolution).Contextual roles (Bean and Riloff, 2004), semantic relations (Ji et al., 2005), semantic roles (Ponzetto and Strube, 2006b; Kong et al., 2009), and animacy (Or˘asan and Evans, 2007) have also been exploited to improve coreference resolution.Lexico-syntactic patterns have been used to capture the semantic relatedness between two NPs and hence the likelihood that they are coreferent.For instance, given the pattern X is a Y (which is highly indicative that X and Y are coreferent), we can instantiate it with a pair of NPs and search for the instantiated pattern in a large corpus or the Web (Daum´e III and Marcu, 2005; Haghighi and Klein, 2009).The more frequently the pattern occurs, the more likely they are coreferent.This technique has been applied to resolve different kinds of anaphoric references, including other-anaphora (Modjeska et al., 2003; Markert and Nissim, 2005) and bridging references (Poesio et al., 2004a).While these patterns are typically hand-crafted (e.g., Garera and Yarowsky (2006)), they can also be learned from an annotated corpus (Yang and Su, 2007) or bootstrapped from an unannotated corpus (Bean and Riloff, 2004).Despite the large amount of work on discoursebased anaphora resolution in the 1970s and 1980s (see Hirst (1981)), learning-based resolvers have only exploited shallow discourse-based features, which primarily involve characterizing the salience of a candidate antecedent by measuring its distance from the anaphoric NP to be resolved or determining whether it is in a prominent grammatical role (e.g., subject).A notable exception is Iida et al. (2009), who train a ranker to rank the candidate antecedents for an anaphoric pronoun by their salience.It is worth noting that Tetreault (2005) has employed Grosz and Sidner’s (1986) discourse theory and Veins Theory (Ide and Cristea, 2000) to identify and remove candidate antecedents that are not referentially accessible to an anaphoric pronoun in his heuristic pronoun resolvers.It would be interesting to incorporate this idea into a learning-based resolver.There are also features that do not fall into any of the preceding categories.For example, a memorization feature is a word pair composed of the head nouns of the two NPs involved in an instance (Bengtson and Roth, 2008).Memorization features have been used as binary-valued features indicating the presence or absence of their words (Luo et al., 2004) or as probabilistic features indicating the probability that the two heads are coreferent according to the training data (Ng, 2007b).An anaphoricity feature indicates whether an NP to be resolved is anaphoric, and is typically computed using an anaphoricity classifier (Ng, 2004), hand-crafted patterns (Daum´e III and Marcu, 2005), and automatically acquired patterns (Bean and Riloff, 1999).Finally, the outputs of rule-based pronoun and coreference resolvers have also been used as features for learning-based coreference resolution (Ng and Cardie, 2002c).For an empirical evaluation of the contribution of a subset of these features to the mention-pair model, see Bengtson and Roth (2008).Two important issues surround the evaluation of a coreference resolver.First, how do we obtain the set of NPs that a resolver will partition?Second, how do we score the partition it produces?To obtain the set of NPs to be partitioned by a resolver, three methods are typically used.In the first method, the NPs are extracted automatically from a syntactic parser.The second method involves extracting the NPs directly from the gold standard.In the third method, a mention detector is first trained on the gold-standard NPs in the training texts, and is then applied to automatically extract system mentions in a test text.7 Note that these three extraction methods typically produce different numbers of NPs: the NPs extracted from a parser tend to significantly outnumber the system mentions, which in turn outnumber the gold NPs.The reasons are two-fold.First, in some coreference corpora (e.g., MUC-6 and MUC-7), the NPs that are not part of any coreference chain are not annotated.Second, in corpora such as those produced by the ACE evaluations, only the NPs that belong to one of the ACE entity types (e.g., PERSON, ORGANIZATION, LOCATION) are annotated.Owing in large part to the difference in the number of NPs extracted by these three methods, a coreference resolver can produce substantially different results when applied to the resulting three sets of NPs, with gold NPs yielding the best results and NPs extracted from a parser yielding the worst (Nicolae and Nicolae, 2006).While researchers who evaluate their resolvers on gold NPs point out that the results can more accurately reflect the performance of their coreference algorithm, Stoyanov et al. (2009) argue that such evaluations are unrealistic, as NP extraction is an integral part of an end-to-end fully-automatic resolver.Whichever NP extraction method is employed, it is clear that the use of gold NPs can considerably simplify the coreference task, and hence resolvers employing different extraction methods should not be compared against each other.The MUC scorer (Vilain et al., 1995) is the first program developed for scoring coreference partitions.It has two often-cited weaknesses.As a linkbased measure, it does not reward correctly identified singleton clusters since there is no coreference link in these clusters.Also, it tends to underpenalize partitions with overly large clusters.To address these problems, two coreference scoring programs have been developed: B3 (Bagga and Baldwin, 1998) and CEAF (Luo, 2005).Note that both scorers have only been defined for the case where the key partition has the same set of NPs as the response partition.To apply these scorers to automatically extracted NPs, different methods have been proposed (see Rahman and Ng (2009) and Stoyanov et al. (2009)).Since coreference is a clustering task, any general-purpose method for evaluating a response partition against a key partition (e.g., Kappa (Carletta, 1996)) can be used for coreference scoring (see Popescu-Belis et al. (2004)).In practice, these general-purpose methods are typically used to provide scores that complement those obtained via the three coreference scorers discussed above.It is worth mentioning that there is a trend towards evaluating a resolver against multiple scorers, which can indirectly help to counteract the bias inherent in a particular scorer.For further discussion on evaluation issues, see Byron (2001).While we have focused our discussion on supervised approaches, coreference researchers have also attempted to reduce a resolver’s reliance on annotated data by combining a small amount of labeled data and a large amount of unlabeled data using general-purpose semi-supervised learning algorithms such as co-training (M¨uller et al., 2002), self-training (Kehler et al., 2004a), and EM (Cherry and Bergsma, 2005; Ng, 2008).Interestingly, recent results indicate that unsupervised approaches to coreference resolution (e.g., Haghighi and Klein (2007; 2010), Poon and Domingos (2008)) rival their supervised counterparts, casting doubts on whether supervised resolvers are making effective use of the available labeled data.Another issue that we have not focused on but which is becoming increasingly important is multilinguality.While many of the techniques discussed in this paper were originally developed for English, they have been applied to learn coreference models for other languages, such as Chinese (e.g., Converse (2006)), Japanese (e.g., Iida (2007)), Arabic (e.g., Luo and Zitouni (2005)), Dutch (e.g., Hoste (2005)), German (e.g., Wunsch (2010)), Swedish (e.g., Nilsson (2010)), and Czech (e.g., Ngu.y et al. (2009)).In addition, researchers have developed approaches that are targeted at handling certain kinds of anaphora present in non-English languages, such as zero anaphora (e.g., Iida et al. (2007a), Zhao and Ng (2007)).As Mitkov (2001) puts it, coreference resolution is a “difficult, but not intractable problem,” and we have been making “slow, but steady progress” on improving machine learning approaches to the problem in the past fifteen years.To ensure further progress, researchers should compare their results against a baseline that is stronger than the commonly-used Soon et al. (2001) system, which relies on a weak model (i.e., the mention-pair model) and a small set of linguistic features.As recent systems are becoming more sophisticated, we suggest that researchers make their systems publicly available in order to facilitate performance comparisons.Publicly available coreference systems currently include JavaRAP (Qiu et al., 2004), GuiTaR (Poesio and Kabadjov, 2004), BART (Versley et al., 2008b), CoRTex (Denis and Baldridge, 2008), the Illinois Coreference Package (Bengtson and Roth, 2008), CherryPicker (Rahman and Ng, 2009), Reconcile (Stoyanov et al., 2010), and Charniak and Elsner’s (2009) pronoun resolver.We conclude with a discussion of two questions regarding supervised coreference research.First, what is the state of the art?This is not an easy question, as researchers have been evaluating their resolvers on different corpora using different evaluation metrics and preprocessing tools.In particular, preprocessing tools can have a large impact on the performance of a resolver (Barbu and Mitkov, 2001).Worse still, assumptions about whether gold or automatically extracted NPs are used are sometimes not explicitly stated, potentially causing results to be interpreted incorrectly.To our knowledge, however, the best results on the MUC-6 and MUC-7 data sets using automatically extracted NPs are reported by Yang et al. (2003) (71.3 MUC F-score) and Ng and Cardie (2002c) (63.4 MUC F-score), respectively;8 and the best results on the ACE data sets using gold NPs can be found in Luo (2007) (88.4 ACE-value).Second, what lessons can we learn from fifteen years of learning-based coreference research?The mention-pair model is weak because it makes coreference decisions based on local information (i.e., information extracted from two NPs).Expressive models (e.g., those that can exploit cluster-level features) generally offer better performance, and so are models that are “global” in nature.Global coreference models may refer to any kind of models that can exploit non-local information, including models that can consider multiple candidate antecedents simultaneously (e.g., ranking models), models that allow joint learning for coreference resolution and related tasks (e.g., anaphoricity determination), models that can directly optimize clustering-level (rather than classification) accuracy, and models that can coordinate with other components of a resolver, such as training instance creation and clustering.We thank the three anonymous reviewers for their invaluable comments on an earlier draft of the paper.This work was supported in part by NSF Grant IIS-0812261.Any opinions, findings, and conclusions or recommendations expressed are those of the author and do not necessarily reflect the views or official policies, either expressed or implied, of the NSF.
Recently, a number of automatic techniques for aligning sentences in parallel bilingual corpora have been proposed (Kay & ROscheisen 1988; Catizone et al. 1989; Gale & Church 1991; Brown et at.1991; Chen 1993), and coarser approaches when sentences are difficult to identify have also been advanced (Church 1993; Dagan el at.1993).Such corpora contain the same material that has been translated by human experts into two languages.The goal of alignment is to identify matching sentences between the languages.Alignment is the first stage in extracting structural information and statistical parameters from bilingual corpora.The problem is made more difficult because a sentence in one language may correspond to multiple sentences in the other; worse yet, sometimes several sentences' content is distributed across multiple translated sentences.Approaches to alignment fall into two main classes: lexical and statistical.Lexically-based techniques use extensive online bilingual lexicons to match sentences.In contrast, statistical techniques require almost no prior knowledge and are based solely on the lengths of sentences.The empirical results to date suggest that statistical methods yield performance superior to that of currently available lexical techniques.However, as far as we know, the literature on automatic alignment has been restricted to alphabetic Indo-European languages.This methodological flaw weakens the arguments in favor of either approach, since it is unclear to what extent a technique's superiority depends on the similarity between related languages.The work reported herein moves towards addressing this problem.'In this paper, we describe our experience with automatic alignment of sentences in parallel English-Chinese texts, which was performed as part of the SILC machine translation project.Our report concerns three related topics.In the first of the following sections, we describe the objectives of the HKUST English-Chinese Parallel Bilingual Corpus, and our progress.The subsequent sections report experiments addressing the applicability of a suitably modified version of Gale & Church's (1991) length-based statistical method to the task of aligning English with Chinese.In the final section, we describe an improved statistical method that also permits domain-specific lexical cues to be incorporated probabilistically.The dearth of work on non-Indo-European languages can partly be attributed to a lack of the prequisite bilingual corpora.As a step toward remedying this, we are in the process of constructing a suitable English-Chinese corpus.To be included, materials must contain primarily tight, literal sentence translations.This rules out most fiction and literary material.We have been concentrating on the Hong Kong Hansard, which are the parliamentary proceedings of the Legislative Council (LegCo).Analogously to the bilingual texts of the Canadian Hansard (Gale 8.6 Church 1991), LegCo transcripts are kept in full translation in both English and Cantonese.2 However, unlike the Canadian Hansard, the Hong Kong Hansard has not previously been available in machine-readable form.We have obtained and converted these materials by special arrangement.The materials contain high-quality literal translation.Statements in LegCo may be made using either English or Cantonese, and are transcribed in the original language.A translation to the other language is made later to yield complete parallel texts, with annotations specifying the source language used by each speaker.Most sentences are translated 1-for-1.A small proportion are 1-for-2 or 2-for-2, and on rare occasion 1-for-3, 3-for-3, or other configurations.Samples of the English and Chinese texts can be seen in figures 3 and 4.3 Because of the obscure format of the original data, it has been necessary to employ a substantial amount of automatic conversion and reformatting.Sentences are identified automatically using heuristics that depend on punctuation and spacing.Segmentation errors occur occasionally, due either to typographical errors in the original data, or to inadequacies of our automatic conversion heuristics.This simply results in incorrectly placed delimiters; it does not remove any text from the corpus.Although the emphasis is on clean text so that markup is minimal, paragraphs and sentences are marked following TEI-conformant SGML (Sperberg-McQueen & Burnard 1992).We use the term &quot;sentence&quot; in a generalized sense including lines in itemized lists, headings, and other nonsentential segments smaller than a paragraph.The corpus currently contains about 60Mb of raw data, of which we have been concentrating on approximately 3.2Mb.Of this, 2.1Mb is text comprised of approximately 0.35 million English words, with the corresponding Chinese translation occupying the remaining 1.1Mb.STATISTICALLY-BASEDThe statistical approach to alignment can be summarized as follows: choose the alignment that maximizes the probability over all possible alignments, given a pair of parallel texts.Formally, where A is an alignment, and Ti and 7-2 are the English and Chinese texts, respectively.An alignment A is a set consisting of L1 ,== L2 pairs where each L1 or L2 is an English or Chinese passage.This formulation is so extremely general that it is difficult to argue against its pure form.More controversial are the approximations that must be made to obtain a tractable version.The first commonly made approximation is that the probabilities of the individual aligned pairs within an alignment are independent, i.e., The other common approximation is that each Pr(Li L2171 , 7-2) depends not on the entire texts, but only on the contents of the specific passages within the alignment: Maximization of this approximation to the alignment probabilities is easily converted into a minimum-sum problem: The minimization can be implemented using a dynamic programming strategy.Further approximations vary according to the specific method being used.Below, we first discuss a pure length-based approximation, then a method with lexical extensions.Length-based alignment methods are based on the following approximation to equation (2): where l = length(Li) and /2 = length(L2), measured in number of characters.In other words, the only feature of L1 and L2 that affects their alignment probability is their length.Note that there are other length-based alignment methods that measure length in number of words instead of characters (Brown et al. 1991).However, since Chinese text consists of an unsegmented character stream without marked word boundaries, it would not be possible to count the number of words in a sentence without first parsing it.Although it has been suggested that lengthbased methods are language-independent (Gale Sz Church 1991; Brown et al. 1991), they may in fact rely to some extent on length correlations arising from the historical relationships of the languages being aligned.If translated sentences share cognates, then the character lengths of those cognates are of course correlated.Grammatical similarities between related languages may also produce correlations in sentence lengths.Moreover, the combinatorics of non-IndoEuropean languages can depart greatly from IndoEuropean languages.In Chinese, the majority of words are just one or two characters long (though collocations up to four characters are also common).At the same time, there are several thousand characters in daily use, as in conversation or newspaper text.Such lexical differences make it even less obvious whether pure sentence-length criteria are adequately discriminating for statistical alignment.Our first goal, therefore, is to test whether purely length-based alignment results can be replicated for English and Chinese, languages from unrelated families.However, before length-based methods can be applied to Chinese, it is first necessary to generalize the notion of &quot;number of characters&quot; to Chinese strings, because most Chinese text (including our corpus) includes occasional English proper names and abbreviations, as well as punctuation marks.Our approach is to count each Chinese character as having length 2, and each English or punctuation character as having length 1.This corresponds to the byte count for text stored in the hybrid English-Chinese encoding system known as Big 5.Gale & Church's (1991) length-based alignment method is based on the model that each English character in L1 is responsible for generating some number of characters in L2.This model leads to a further approximation which encapsulates the dependence to a single parameter .6 that is a function of /1 and /2: Pr(Li L2IL1, L2)',■+' Pr(Li L2 I6(11, 12)) However, it is much easier to estimate the distributions for the inverted form obtained by applying Bayes' Rule: where Pr(6) is a normalizing constant that can be ignored during minimization.The other two distributions are estimated as follows.First we choose a function for 6(4,12).To do this we look at the relation between /1 and 12 under the generative model.Figure 1 shows a plot of English versus Chinese sentence lengths for a hand-aligned sample of 142 sentences.If the sentence lengths were perfectly correlated, the points would lie on a diagonal through the origin.We estimate the slope of this idealized diagonal c = E(r) = E(12 1 11) by averaging over the training corpus of hand-aligned L1 L2 pairs, weighting by the length of Li.In fact this plot displays substantially greater scatter than the English-French data of Gale & Church (1991).4 The mean number of Chinese characters generated by each English character is c = 0.506, with a standard deviation o- = 0.166.We now assume that /2 — /1 c is normally distributed, following Gale & Church (1991), and transform it into a new gaussian variable of standard form (i.e., with mean 0 and variance 1) by appropriate normalization: This is the quantity that we choose to define as 6(/1,12).Consequently, for any two pairs in a proposed alignment, Pr(611,1 L2) can be estimated according to the gaussian assumption.To check how accurate the gaussian assumption is, we can use equation (4) to transform the same training points from figure 1 and produce a histogram.The result is shown in figure 2.Again, the distribution deviates from a gaussian distribution substantially more than Gale & Church (1991) report for French/German/English.Moreover, the distribution does not resemble any smooth distribution at all, including the logarithmic normal used by Brown et al. (1991), raising doubts about the potential performance of pure length-based alignment.Continuing nevertheless, to estimate the other term Pr(Li .= L2), a prior over six classes is constructed, where the classes are defined by the number of passages included within L1 and L2.Table 1 shows the probabilities used.These probabilities are taken directly from Gale & Church (1991); slightly improved performance might be obtained by estimating these probabilities from our corpus.The aligned results using this model were evaluated by hand for the entire contents of a ranpoint that the Honourable Member has made to say that, when at the outset of our discussions I said that I did not think that the Government would be regarded for long as having been extravagant yesterday, I did not realize that the criticisms would begin quite as rapidly as they have.J 12.The proposals that we make on public assistance, both the increase in scale rates, and the relaxation of the absence rule, are substantial steps forward in Hong Kong which will, I think, be very widely welcomed.J 13.But I know that there will always be those who, I am sure for very good reason, will say you should have gone further, you should have done more.J 14.Societies customarily make advances in social welfare because there are members of the community who develop that sort of case very often with eloquence and verve..1 domly selected pair of English and Chinese files corresponding to a complete session, comprising 506 English sentences and 505 Chinese sentences.Figure 3 shows an excerpt from this output.Most of the true 1-for-1 pairs are aligned correctly.In (4), two English sentences are correctly aligned with a single Chinese sentence.However, the English sentences in (6, 7) are incorrectly aligned 1for-1 instead of 2-for-1.Also, (11, 12) shows an example of a 3-for-1, 1-for-1 sequence that the model has no choice but to align as 2-for-2, 2-for-2.Judging relative to a manual alignment of the English and Chinese files, a total of 86.4% of the true L1 L2 pairs were correctly identified by the length-based method.However, many of the errors occurred within the introductory session header, whose format is domain-specific (discussed below).If the introduction is discarded, then the proportion of correctly aligned pairs rises to 95.2%, a respectable rate especially in view of the drastic inaccuracies in the distributions assumed.A detailed breakdown of the results is shown in Table 2.For reference, results reported for English/French generally fall between 96% and 98%.However, all of these numbers should be interpreted as highly domain dependent, with very small sample size.The above rates are for Type I errors.The alternative measure of accuracy on Type II errors is useful for machine translation applications, where the objective is to extract only 1-for-1 sentence pairs, and to discard all others.In this case, we are interested in the proportion of 1-for-1 output pairs that are true 1-for-1 pairs.(In information retrieval terminology, this measures precision whereas the above measures recall.)In the test session, 438 1-for-1 pairs were output, of which 377, or 86.1%, were true matches.Again, however, by discarding the introduction, the accuracy rises to a surprising 96.3%.The introductory session header exemplifies a weakness of the pure length-based strategy, namely, its susceptibility to long stretches of passages with roughly similar lengths.In our data this arises from the list of council members present and absent at each session (figure 4), but similar stretches can arise in many other domains.In such a situation, two slight perturbations may cause the entire stretch of passages between the perturbations to be misaligned.These perturbations can easily arise from a number of causes, including slight omissions or mismatches in the original parallel texts, a 1-for-2 translation pair preceding or following the stretch of passages, or errors in the heuristic segmentation preprocessing.Substantial penalties may occur at the beginning and ending boundaries of the misaligned region, where the perturbations lie, but the misalignment between those boundaries incurs little penalty, because the mismatched passages have apparently matching lengths.This problem is apparently exacerbated by the non-alphabetic nature of Chinese.Because Chinese text contains fewer characters, character length is a less discriminating feature, varying over a range of fewer possible discrete values than the corresponding English.The next section discusses a solution to this problem.In summary, we have found that the statistical correlation of sentence lengths has a far greater variance for our English-Chinese materials than with the Indo-European materials used by Gale & Church (1991).Despite this, the pure lengthbased method performs surprisingly well, except for its weakness in handling long stretches of sentences with close lengths.To obtain further improvement in alignment accuracy requires matching the passages' lexical content, rather than using pure length criteria.This is particularly relevant for the type of long mismatched stretches described above.Previous work on alignment has employed either solely lexical or solely statistical length criteria.In contrast, we wish to incorporate lexical criteria without giving up the statistical approach, which provides a high baseline performance.Our method replaces equation (3) with the following approximation: where vi = #occurrences(English cue, L1) and wi = #occurrences(Chinese cue, L2).Again, the dependence is encapsulated within difference parameters Si as follows: The prior Pr(Li =-• L2) is evaluated as before.We assume all Si values are approximately independent, giving (5) Pr(,30 , , on ILI L2) Pf.H Pr(di (Li L2) The same dynamic programming optimization can then be used.However, the computation and memory costs grow linearly with the number of lexical cues.This may not seem expensive until one considers that the pure length-based method only uses resources equivalent to that of a single lexical cue.It is in fact important to choose as few lexical cues as possible to achieve the desired accuracy.Given the need to minimize the number of lexical cues chosen, two factors become important.First, a lexical cue should be highly reliable, so that violations, which waste the additional computation, happen only rarely.Second, the chosen lexical cues should occur frequently, since computing the optimization over many zero counts is not useful.In general, these factors are quite domainspecific, so lexical cues must be chosen for the particular corpus at hand.Note further that when these conditions are met, the exact probability distribution for the lexical Si parameters does not have much influence on the preferred alignment.The bilingual correspondence lexicons we have employed are shown in figure 5.These lexical items are quite common in the LegCo domain.Items like &quot;C.B.E.&quot; stand for honorific titles such as &quot;Commander of the British Empire&quot;; the other cues are self-explanatory.The cues nearly always appear 1-to-1 and the differences Oi therefore have a mean of zero.Given the relative unimportance of the exact distributions, all were simply assumed to be normally distributed with a variance of 0.07 instead of sampling each parameter individually.This variance is fairly sharp, but nonetheless, conservatively reflects a lower reliability than most of the cues actually possess.Using the lexical cue extensions, the Type I results on the same test file rise to 92.1% of true L1 L2 pairs correctly identified, as compared to 86.4% for the pure length-based method.The improvement is entirely in the introductory session header.Without the header, the rate is 95.0% as compared to 95.2% earlier (the discrepancy is insignificant and is due to somewhat arbitrary decisions made on anomolous regions).Again, caution should be exercised in interpreting these percentages.By the alternative Type II measure, 96.1% of the output 1-for-1 pairs were true matches, compared to 86.1% using the pure length-based method.Again, there is an insignificant drop when the header is discarded, in this case from 96.3% down to 95.8%.Of our raw corpus data, we have currently aligned approximately 3.5Mb of combined English and Chinese texts.This has yielded 10,423 pairs classified as 1-for-1, which we are using to extract more refined information.This data represents over 0.217 million English words (about 1.269Mb) plus the corresponding Chinese text (0.659Mb).To our knowledge, this is the first large-scale empirical demonstration that a pure length-based method can yield high accuracy sentence alignments between parallel texts in Indo-European and entirely dissimilar non-alphabetic, non-IndoEuropean languages.We are encouraged by the results and plan to expand our program in this direction.We have also obtained highly promising improvements by hybridizing lexical and lengthbased alignment methods within a common statistical framework.Though they are particularly useful for non-alphabetic languages where character length is not as discriminating a feature, we believe improvements will result even when applied to alphabetic languages.I am indebted to Bill Gale for helpful clarifying discussions, Xuanyin Xia and Wing Hong Chan for assistance with conversion of corpus materials, as well as Graeme Hirst and Linda Peto.
A recent trend in natural language processing has been toward a greater emphasis on statistical approaches, beginning with the success of statistical part-of-speech tagging programs (Church 1988), and continuing with other work using statistical part-of-speech tagging programs, such as BBN PLUM (Weischedel et al. 1993) and NYU Proteus (Grishman and Sterling 1993).More recently, statistical methods have been applied to domain-specific semantic parsing (Miller et al. 1994), and to the more difficult problem of wide-coverage syntactic parsing (Magerman 1995).Nevertheless, most natural language systems remain primarily rule based, and even systems that do use statistical techniques, such as AT&T Chronus (Levin and Pieraccini 1995), continue to require a significant rule based component.Development of a complete end-to-end statistical understanding system has been the focus of several ongoing research efforts, including (Miller et al. 1995) and (Koppelman et al.1995).In this paper, we present such a system.The overall structure of our approach is conventional, consisting of a parser, a semantic interpreter, and a discourse module.The implementation and integration of these elements is far less conventional.Within each module, every processing step is assigned a probability value, and very large numbers of alternative theories are pursued in parallel.The individual modules are integrated through an n-best paradigm, in which many theories are passed from one stage to the next, together with their associated probability scores.The meaning of a sentence is determined by taking the highest scoring theory from among the n-best possibilities produced by the final stage in the model.Some key advantages to statistical modeling techniques are: sufficient to provide the system with examples specifying the correct parses for a set of training examples.There is no need to specify an exact set of rules or a detailed procedure for producing such parses. are principled techniques for estimating the gradations.The system is thus free to pursue unusual theories, while remaining aware of the fact that they are unlikely.In the event that a more likely theory exists, then the more likely theory is selected, but if no more likely interpretation can be found, the unlikely interpretation is accepted.The focus of this work is primarily to extract sufficient information from each utterance to give an appropriate response to a user's request.A variety of problems regarded as standard in computational linguistics, such as quantification, reference and the like, are thus ignored.To evaluate our approach, we trained an experimental system using data from the Air Travel Information (ATIS) domain (Bates et al. 1990; Price 1990).The selection of ATIS was motivated by three concerns.First, a large corpus of ATIS sentences already exists and is readily available.Second, ATIS provides an existing evaluation methodology, complete with independent training and test corpora, and scoring programs.Finally, evaluating on a common corpus makes it easy to compare the performance of the system with those based on different approaches.We have evaluated our system on the same blind test sets used in the ARPA evaluations (Pallett et al. 1995), and present a preliminary result at the conclusion of this paper.The remainder of the paper is divided into four sections, one describing the overall structure of our models, and one for each of the three major components of parsing, semantic interpretation and discourse.Given a string of input words W and a discourse history H, the task of a statistical language understanding system is to search among the many possible discourse-dependent meanings MD for the most likely meaning Mo: Mo = arg max P(MD I W, H).Directly modeling P(MD I W, H) is difficult because the gap that the model must span is large.A common approach in non-statistical natural language systems is to bridge this gap by introducing intermediate representations such as parse structure and pre-discourse sentence meaning.Introducing these intermediate levels into the statistical framework gives: where T denotes a semantic parse tree, and Ms denotes prediscourse sentence meaning.This expression can be simplified by introducing two independence assumptions: Now, since P(W) is constant for any given word string, the problem of finding meaning MD that maximizes We now introduce a third independence assumption: 3.The probability of words W does not depend on meaning Ms, given that parse T is known.This assumption is justified because the word tags in our parse representation specify both semantic and syntactic class information.Under this assumption: Finally, we assume that most of the probability mass for each discourse-dependent meaning is focused on a single parse tree and on a single pre-discourse meaning.Under this (Viterbi) assumption, the summation operator can be replaced by the maximization operator, yielding: This expression corresponds to the computation actually performed by our system which is shown in Figure 1.Processing proceeds in three stages: These parses, together with their probability scores, are passed to the semantic interpretation model.2.The constrained space of candidate parses T (received from the parsing model), combined with the full space of possible pre-discourse meanings Ms, is searched for n-best candidates according to the measure P(Ms,T) P(W IT).These pre-discourse meanings, together with their associated probability scores, are passed to the discourse model.3.The constrained space of candidate pre-discourse meanings Ms (received from the semantic interpretation model), combined with the full space of possible postdiscourse meanings MD, is searched for the single candidate that maximizes P( MD I H,M s) P(Ms,T) P(W IT) ,conditioned on the current history H. The discourse history is then updated and the post-discourse meaning is returned.We now proceed to a detailed discussion of each of these three stages, beginning with parsing.Our parse representation is essentially syntactic in form, patterned on a simplified head-centered theory of phrase structure.In content, however, the parse trees are as much semantic as syntactic.Specifically, each parse node indicates both a semantic and a syntactic class (excepting a few types that serve purely syntactic functions).Figure 2 shows a sample parse of a typical ATIS sentence.The semantic,/syntactic character of this representation offers several advantages: /top° /wh-question process: semantic labels identify the basic units of meaning, while syntactic structures help identify relationships between those units.The parsing model is a probabilistic recursive transition network similar to those described in (Miller et al. 1994) and (Seneff 1992).The probability of a parse tree T given a word string W is rewritten using Bayes rule as: Since P(W) is constant for any given word string, candidate parses can be ranked by considering only the product P(T) P(W I 7).The probability P(7) is modeled by state transition probabilities in the recursive transition network, and P(W I 7) is modeled by word transition probabilities.P(location/pp I arrivaL/vp-head, arrival/vp) is the probability of a location/pp following an arrivaMvphead within an arrivallvp constituent. probability along the path corresponding to T. Transition probabilities are estimated directly by observing occurrence and transition frequencies in a training corpus of annotated parse trees.These estimates are then smoothed to overcome sparse data limitations.The semantic/syntactic parse labels, described above, provide a further advantage in terms of smoothing: for cases of undertrained probability estimates, the model backs off to independent syntactic and semantic probabilities as follows: where A. is estimated as in (Placeway et al. 1993).Backing off to independent semantic and syntactic probabilities potentially provides more precise estimates than the usual strategy of backing off directly form bigram to unigram models.In order to explore the space of possible parses efficiently, the parsing model is searched using a decoder based on an adaptation of the Earley parsing algorithm (Earley 1970).This adaptation, related to that of (Stolcke 1995), involves reformulating the Earley algorithm to work with probabilistic recursive transition networks rather than with deterministic production rules.For details of the decoder, see (Miller 1996).Both pre-discourse and post-discourse meanings in our current system are represented using a simple frame representation.Figure 3 shows a sample semantic frame corresponding to the parse in Figure 2.Recall that the semantic interpreter is required to compute P(M s ,T) P(W IT) .The conditional word probability P(W IT) has already been computed during the parsing phase and need not be recomputed.The current problem, then, is to compute the prior probability of meaning Ms and parse T occurring together.Our strategy is to embed the instructions for constructing Ms directly into parse T , resulting in an augmented tree structure.For example, the instructions needed to create the frame shown in Figure 3 are: These instructions are attached to the parse tree at the points indicated by the circled numbers (see Meanings Ms are decomposed into two parts: the frame type FT, and the slot fillers S. The frame type is always attached to the topmost node in the augmented parse tree, while the slot filling instructions are attached to nodes lower down in the tree.Except for the topmost node, all parse nodes are required to have some slot filling operation.For nodes that do not directly trigger any slot fill operation, the special operation null is attached.The probability P(Ms, 7) is then: Obviously, the prior probabilities P(FT) can be obtained directly from the training data.To compute P(T I FT), each of the state transitions from the previous parsing model are simply rescored conditioned on the frame type.The new state transition probabilities are: To compute P(S I FT, 7) , we make the independence assumption that slot filling operations depend only on the frame type, the slot operations already performed, and on the local parse structure around the operation.This local neighborhood consists of the parse node itself, its two left siblings, its two right siblings, and its four immediate ancestors.Further, the syntactic and semantic components of these nodes are considered independently.Under these assumptions, the probability of a slot fill operation is: and the probability P(S I FT, 7) is simply the product of all such slot fill operations in the augmented tree.Transition probabilities are estimated from a training corpus of augmented trees.Unlike probabilities in the parsing model, there obviously is not sufficient training data to estimate slot fill probabilities directly.Instead, these probabilities are estimated by statistical decision trees similar to those used in the Spatter parser (Magerman 1995).Unlike more common decision tree classifiers, which simply classify sets of conditions, statistical decision trees give a probability distribution over all possible outcomes.Statistical decision trees are constructed in a two phase process.In the first phase, a decision tree is constructed in the standard fashion using entropy reduction to guide the construction process.This phase is the same as for classifier models, and the distributions at the leaves are often extremely sharp, sometimes consisting of one outcome with probability 1, and all others with probability 0.In the second phase, these distributions are smoothed by mixing together distributions of various nodes in the decision tree.As in (Magerman 1995), mixture weights are determined by deleted interpolation on a separate block of training data.Searching the interpretation model proceeds in two phases.In the first phase, every parse T received from the parsing model is rescored for every possible frame type, computing P(T I F7) (our current model includes only a half dozen different types, so this computation is tractable).Each of these theories is combined with the corresponding prior probability P(FT) yielding P(FT) P(T I PT).The n-best of these theories are then passed to the second phase of the interpretation process.This phase searches the space of slot filling operations using a simple beam search procedure.For each combination of FT and T, the beam search procedure considers all possible combinations of fill operations, while pruning partial theories that fall beneath the threshold imposed by the beam limit.The surviving theories are then combined with the conditional word probabilities P(W I 7), computed during the parsing model.The final result of these steps is the n-best set of candidate pre-discourse meanings, scored according to the measure P(Ms,T) P(W IT) .The discourse module computes the most probable postdiscourse meaning of an utterance from its pre-discourse meaning and the discourse history, according to the measure: Because pronouns can usually be ignored in the ATIS domain, our work does not treat the problem of pronominal reference.Our probability model is instead shaped by the key discourse problem of the ATIS domain, which is the inheritance of constraints from context.This inheritance phenomenon, similar in spirit to one-anaphora, is illustrated by the following dialog:: SYSTEM2: <displays Boston to Denver flights for Tuesday> In USER2, it is obvious from context that the user is asking about flights whose ORIGIN is BOSTON and whose DESTINATION is DENVER, and not all flights between any two cities.Constraints are not always inherited, however.For example, in the following continuation of this dialogue: USER3: Show me return flights from Denver to Boston, it is intuitively much less likely that the user means the &quot;on Tuesday&quot; constraint to continue to apply.The discourse history H simply consists of the list of all postdiscourse frame representations for all previous utterances in the current session with the system.These frames are the source of candidate constraints to be inherited.For most utterances, we make the simplifying assumption that we need only look at the last (i.e. most recent) frame in this list, which we call M. The statistical discourse model maps a 23 element input vector X onto a 23 element output vector Y.These vectors have the following interpretations: The 23 elements in vectors X and Y correspond to the 23 possible slots in the frame schema.Each element in X can have one of five values, specifying the relationship between the filler of the corresponding slot in Mp and Ms: INITIAL - slot filled in Ms but not in Mp TACIT - slot filled in Mp but not in Ms REITERATE - slot filled in both MA and Ms; value the same CHANGE - slot filled in both Mp and Ms; value different IRRELEVANT - slot not filled in either Mp or Ms Output vector Y is constructed by directly copying all fields from input vector X except those labeled TACIT.These direct copying operations are assigned probability 1.For fields labeled TACIT, the corresponding field in Y is filled with either INHERITED or NOT-INHERITED.The probability of each of these operations is determined by a statistical decision tree model.The discourse model contains 23 such statistical decision trees, one for each slot position.An ordering is imposed on the set of frame slots, such that inheritance decisions for slots higher in the order are conditioned on the decisions for slots lower in the order.The probability P(Y I X) is then the product of all 23 statistical models to additional linguistic phenomena such as quantification and anaphora resolution. decision probabilities: The discourse model is trained from a corpus annotated with both pre-discourse and post-discourse semantic frames.Corresponding pairs of input and output (X, Y) vectors are computed from these annotations, which are then used to train the 23 statistical decision trees.The training procedure for estimating these decision tree models is similar to that used for training the semantic interpretation model.Searching the discourse model begins by selecting a meaning frame Mp from the history stack H, and combining it with each pre-discourse meaning Ms received from the semantic interpretation model.This process yields a set of candidate input vectors X.Then, for each vector X, a search process exhaustively constructs and scores all possible output vectors Y according to the measure P(Y I X) (this computation is feasible because the number of TACIT fields is normally small).These scores are combined with the pre-discourse scores P(Ms,T) P(W IT) , already computed by the semantic interpretation process.This computation yields: The highest scoring theory is then selected, and a straightforward computation derives the final meaning frame MD from output vector Y.We have trained and evaluated the system on a common corpus of utterances collected from naive users in the ATIS domain.In this test, the system was trained on approximately 4000 ATIS 2 and ATIS 3 sentences, and then evaluated on the December 1994 test material (which was held aside as a blind test set).The combined system produced an error rate of 21.6%.Work on the system is ongoing, however, and interested parties are encouraged to contact the authors for more recent results.We have presented a fully trained statistical natural language interface system, with separate models corresponding to the classical processing steps of parsing, semantic interpretation and discourse.Much work remains to be done in order to refine the statistical modeling techniques, and to extend theWe wish to thank Robert Ingria for his effort in supervising the annotation of the training corpus, and for his helpful technical suggestions.This work was supported by the Advanced Research Projects Agency and monitored by the Office of Naval Research under Contract No.NO0014-91-C-0115, and by Ft. Huachuca under Contract Nos.DABT63-94-C-0061 and DABT63-94C-0063 .The content of the information does not necessarily reflect the position or the policy of the Government and no official endorsement should be inferred.
Given a word, its context and its possible meanings, the problem of word sense disambiguation (WSD) is to determine the meaning of the word in that context.WSD is useful in many natural language tasks, such as choosing the correct word in machine translation and coreference resolution.In several recent proposals (Hearst, 1991; Bruce and Wiebe, 1994; Leacock, Towwell, and Voorhees, 1996; Ng and Lee, 1996; Yarowsky, 1992; Yarowsky, 1994), statistical and machine learning techniques were used to extract classifiers from hand-tagged corpus.Yarowsky (Yarowsky, 1995) proposed an unsupervised method that used heuristics to obtain seed classifications and expanded the results to the other parts of the corpus, thus avoided the need to hand-annotate any examples.Most previous corpus-based WSD algorithms determine the meanings of polysemous words by exploiting their local contexts.A basic intuition that underlies those algorithms is the following: (1) Two occurrences of the same word have identical meanings if they have similar local contexts.In other words, most previous corpus-based WSD algorithms learn to disambiguate a polysemous word from previous usages of the same word.This has several undesirable consequences.Firstly, a word must occur thousands of times before a good classifier can be learned.In Yarowsky's experiment (Yarowsky, 1995), an average of 3936 examples were used to disambiguate between two senses.In Ng and Lee's experiment, 192,800 occurrences of 191 words were used as training examples.There are thousands of polysemous words, e.g., there are 11,562 polysemous nouns in WordNet.For every polysemous word to occur thousands of times each, the corpus must contain billions of words.Secondly, learning to disambiguate a word from the previous usages of the same word means that whatever was learned for one word is not used on other words, which obviously missed generality in natural languages.Thirdly, these algorithms cannot deal with words for which classifiers have not been learned.In this paper, we present a WSD algorithm that relies on a different intuition: (2) Two different words are likely to have similar meanings if they occur in identical local contexts.Consider the sentence: (3) The new facility will employ 500 of the existing 600 employees The word &quot;facility&quot; has 5 possible meanings in WordNet 1.5 (Miller, 1990): (a) installation, (b) proficiency/technique, (c) adeptness, (d) readiness, (e) toilet/bathroom.To disambiguate the word, we consider other words that appeared in an identical local context as &quot;facility&quot; in (3).Table 1 is a list of words that have also been used as the subject of &quot;employ&quot; in a 25-million-word Wall Street Journal corpus.The &quot;freq&quot; column are the number of times these words were used as the subject of &quot;employ&quot;.ORG includes all proper names recognized as organizations The logA column are their likelihood ratios (Dunning, 1993).The meaning of &quot;facility&quot; in (3) can be determined by choosing one of its 5 senses that is most similar' to the meanings of words in Table 1.This way, a polysemous word is disambiguated with past usages of other words.Whether or not it appears in the corpus is irrelevant.Our approach offers several advantages: The required resources of the algorithm include the following: (a) an untagged text corpus, (b) a broad-coverage parser, (c) a concept hierarchy, such as the WordNet (Miller, 1990) or Roget's Thesaurus, and (d) a similarity measure between concepts.In the next section, we introduce our definition of local contexts and the database of local contexts.A description of the disambiguation algorithm is presented in Section 3.Section 4 discusses the evaluation results.Psychological experiments show that humans are able to resolve word sense ambiguities given a narrow window of surrounding words (Choueka and Lusignan, 1985).Most WSD algorithms take as input ito be defined in Section 3.1 a polysemous word and its local context.Different systems have different definitions of local contexts.In (Leacock, Towwell, and Voorhees, 1996), the local context of a word is an unordered set of words in the sentence containing the word and the preceding sentence.In (Ng and Lee.1996), a local context of a word consists of an ordered sequence of 6 surrounding part-of-speech tags, its morphological features, and a set of collocations.In our approach, a local context of a word is defined in terms of the syntactic dependencies between the word and other words in the same sentence.A dependency relationship (Hudson, 1984; Mel'euk, 1987) is an asymmetric binary relationship between a word called head (or governor, parent), and another word called modifier (or dependent, daughter).Dependency grammars represent sentence structures as a set of dependency relationships.Normally the dependency relationships form a tree that connects all the words in a sentence.An example dependency structure is shown in (4).The local context of a word W is a triple that corresponds to a dependency relationship in which W is the head or the modifier: (type word position) where type is the type of the dependency relationship, such as subj (subject), adjn (adjunct), comp I (first complement), etc.; word is the word related to W via the dependency relationship; and posit ion can either be head or mod.The position indicates whether word is the head or the modifier in dependency relation.Since a word may be involved in several dependency relationships, each occurrence of a word may have multiple local contexts.The local contexts of the two nouns &quot;boy&quot; and &quot;dog&quot; in (4) are as follows (the dependency relations between nouns and their determiners are ignored): boy (subj chase head) dog (adjn brown mod) (compl chase head) Using a broad coverage parser to parse a corpus, we construct a Local Context Database.An entry in the database is a pair: where lc is a local context and C(lc) is a set of (word frequency likelihood)-triples.Each triple specifies how often word occurred in lc and the likelihood ratio of lc and word.The likelihood ratio is obtained by treating word and /c as a bigram and computed with the formula in (Dunning, 1993).The database entry corresponding to Table 1 is as follows:The polysemous words in the input text are disambiguated in the following steps: Step A. Parse the input text and extract local contexts of each word.Let LC. denote the set of local contexts of all occurrences of w in the input text.Step B.Search the local context database and find words that appeared in an identical local context as w. They are called selectors of w: Step C. Select a sense s of w that maximizes the similarity between w and Selectors.Step D. The sense s is assigned to all occurrences of w in the input text.This implements the &quot;one sense per discourse&quot; heuristic advocated in (Gale, Church, and Yarowsky, 1992).Step C. needs further explanation.In the next subsection, we define the similarity between two word senses (or concepts).We then explain how the similarity between a word and its selectors is maximized.There have been several proposed measures for similarity between two concepts (Lee, Kim, and Lee, 1989; Rada et al., 1989; Resnik, 1995b; Wu and Palmer, 1994).All of those similarity measures are defined directly by a formula.We use instead an information-theoretic definition of similarity that can be derived from the following assumptions: where cornmon(A, B) is a proposition that states the commonalities between A and B; /(s) is the amount of information contained in the proposition s. Assumption 2: The differences between A and B is measured by where describe(A, B) is a proposition that describes what A and B are.Assumption 3: The similarity between A and B, sim(A, B), is a function of their commonality and differences.That is, sim(A, B) = f (I (common(A, B)), I (describe(A, B))) The domain of f (x, y) is {(x , y)ix > 0, y > 0, y > x}.Assumption 4: Similarity is independent of the unit used in the information measure.According to Information Theory (Cover and Thomas, 1991), /(s) = —log bP(s), where P(s) is the probability of s and b is the unit.When b = 2, /(s) is the number of bits needed to encode s. Since log bx =12412 , Assumption 4 means that the function f must satisfy the following condition: Vc > 0, f(x, y) f (cx, Assumption 5: Similarity is additive with respect to commonality.If cornman(A, B) consists of two independent parts, then the sim(A, B) is the sum of the similarities computed when each part of the commonality is considered.In other words: f (xi + x2, y) = f(xi,y)+ f(x2,y).A corollary of Assumption 5 is that Vy, f(0, y) = f (x + 0, y) — f (x,y) = 0, which means that when there is no commonality between A and B, their similarity is 0, no matter how different they are.For example, the similarity between &quot;depth-first search&quot; and &quot;leather sofa&quot; is neither higher nor lower than the similarity between &quot;rectangle&quot; and &quot;interest rate&quot;.Assumption 6: The similarity between a pair of identical objects is 1.When A and B are identical. knowning their commonalities means knowing what they are, i.e., I (comman(A, B)) = I (describe(A.B)).Therefore, the function f must have the following property: Vx, f (x, x) = 1.Assumption 7: The function f (x. y) is continuous.Similarity Theorem: The similarity between A and B is measured by the ratio between the amount of information needed to state the commonality of A and B and the information needed to fully describe what A and B are: Proof: To prove the theorem. we need to show f(x,y) = s. Since f(x,y) = f(s.1) (due to Assumption 4), we only need to show that when LI, is a rational number, f (x,y) = .The result can be generalized to all real numbers because f is continuous and for any real number, there are rational numbers that are infinitely close to it.Suppose m and n are positive integers.(due to Assumption 5).Thus. f (x, y) = ;-; f (nx, y).Substituting fi for x in this equation: Q.E.D.For example.Figure 1 is a fragment of the WordNet.The nodes are concepts (or synsets as they are called in the WordNet).The links represent IS-A relationships.The number attached to a node C is the probability P(C) that a randomly selected noun refers to an instance of C. The probabilities are estimated by the frequency of concepts in SemCor (Miller et al., 1994), a sense-tagged subset of the Brown corpus.If x is a Hill and y is a Coast, the commonality between x and p is that &quot;x is a GeoForm and y is a GeoForm&quot;.The information contained in this statement is —2 x logP(GeoF arm).The similarity between the concepts Hill and Coast is: where p(ni co is the probability of that an object belongs to all the maximally specific super classes (Cts) of both C and C'.We now provide the details of Step C in our algorithm.The input to this step consists of a polysemous word V110 and its selectors {WI, W2 WO.The word Wi has ni senses: {sii, • • • , sin.}* Step C.1: Construct a similarity matrix (8).The rows and columns represent word senses.The matrix is divided into (k 1) x (k + 1) blocks.The blocks on the diagonal are all Os.The elements in block Sii are the similarity measures between the senses of Wi and the senses of It:).Similarity measures lower than a threshold 0 are considered to be noise and are ignored.In our experiments, 9 = 0.2 was used.Step C.5: Modify the similarity matrix to remove the similarity values between other senses of W.i„ and senses of other words.For all 1, j, m, such that 1 E [1,ni,..] and 1 0 Imax and j imax and m E [1, nil' Let's consider again the word &quot;facility&quot; in (3).It has two local contexts: subject of &quot;employ&quot; (subj employ head) and modifiee of &quot;new&quot; (adjn new mod).Table 1 lists words that appeared in the first local context.Table 2 lists words that appeared in the second local context.Only words with top-20 likelihood ratio were used in our experiments.The two groups of words are merged and used as the selectors of &quot;facility&quot;.The words &quot;facility&quot; has 5 senses in the WordNet.Senses 1 and 5 are subclasses of artifact.Senses 2 and 3 are kinds of state.Sense 4 is a kind of abstraction.Many of the selectors in Tables 1 and Table 2 have artifact senses, such as &quot;post&quot;, &quot;product&quot;, &quot;system&quot;, &quot;unit&quot;, &quot;memory device&quot;, &quot;machine&quot;, &quot;plant&quot;, &quot;model&quot;, &quot;program&quot;, etc.Therefore, Senses 1 and 5 of &quot;facility&quot; received much more support, 5.37 and 2.42 respectively, than other senses.Sense 1 is selected.Consider another example that involves an unknown proper name: We treat unknown proper nouns as a polysemous word which could refer to a person, an organization, or a location.Since &quot;DreamLand&quot; is the subject of &quot;employed&quot;, its meaning is determined by maximizing the similarity between one of {person, organization, locaton} and the words in Table 1.Since Table 1 contains many &quot;organization&quot; words, the support for the &quot;organization&quot; sense is much higher than the others.We used a subset of the SemCor (Miller et al., 1994) to evaluate our algorithm.General-purpose lexical resources, such as WordNet, Longman Dictionary of Contemporary English (LDOCE), and Roget's Thesaurus, strive to achieve completeness.They often make subtle distinctions between word senses.As a result, when the WSD task is defined as choosing a sense out of a list of senses in a general-purpose lexical resource, even humans may frequently disagree with one another on what the correct sense should be.The subtle distinctions between different word senses are often unnecessary.Therefore, we relaxed the correctness criterion.A selected sense sanswer is correct if it is &quot;similar enough&quot; to the sense tag S key in SemCor.We experimented with three interpretations of &quot;similar enough&quot;.The strictest interpretation is SiM(Sanswer, skey)=1, which is true only when sanswer=skey.The most relaxed interpretation is SiM(Sanewer, S key) >0, which is true if sanswer and skey are the descendents of the same top-level concepts in WordNet (e.g., entity, group, location, etc.).A compromise between these two is SiM(Sanswer, Skey) > 0.27, where 0.27 is the average similarity of 50,000 randomly generated pairs (w, w') in which w and w' belong to the same Roget's category.We use three words &quot;duty&quot;, &quot;interest&quot; and &quot;line&quot; as examples to provide a rough idea about what sim(sanswer, skey) > 0.27 means.The word &quot;duty&quot; has three senses in WordNet 1.5.The similarity between the three senses are all below 0.27, although the similarity between Senses 1 (responsibility) and 2 (assignment, chore) is very close (0.26) to the threshold.The word &quot;interest&quot; has 8 senses.Senses 1 (sake, benefit) and 7 (interestingness) are merged.2 Senses 3 (fixed charge for borrowing money), 4 (a right or legal share of something), and 5 (financial interest in something) are merged.The word &quot;interest&quot; is reduced to a 5-way ambiguous word.The other three senses are 2 (curiosity), 6 (interest group) and 8 (pastime, hobby).The word &quot;line&quot; has 27 senses.The similarity threshold 0.27 reduces the number of senses to 14.The reduced senses are where each group is a reduced sense and the numbers are original WordNet sense numbers.We used a 25-million-word Wall Street Journal corpus (part of LDC/DCI3 CDROM) to construct the local context database.The text was parsed in 126 hours on a SPARC-Ultra 1/140 with 96MB of memory.We then extracted from the parse trees 8,665,362 dependency relationships in which the head or the modifier is a noun.We then filtered out (lc, word) pairs with a likelihood ratio lower than 5 (an arbitrary threshold).The resulting database contains 354,670 local contexts with a total of 1,067,451 words in them (Table 1 is counted as one local context with 20 words in it).Since the local context database is constructed from WSJ corpus which are mostly business news, we only used the &quot;press reportage&quot; part of SemCor which consists of 7 files with about 2000 words each.Furthermore, we only applied our algorithm to nouns.Table 3 shows the results on 2,832 polysemous nouns in SemCor.This number also includes proper nouns that do not contain simple markers (e.g., Mr., Inc.) to indicate its category.Such a proper noun is treated as a 3-way ambiguous word: person, organization, or location.We also showed as a baseline the performance of the simple strategy of always choosing the first sense of a word in the WordNet.Since the WordNet senses are ordered according to their frequency in SemCor, choosing the first sense is roughly the same as choosing the sense with highest prior probability, except that we are not using all the files in SemCor.It can be seen from Table 3 that our algorithm performed slightly worse than the baseline when the strictest correctness criterion is used.However, when the condition is relaxed, its performance gain is much lager than the baseline.This means that when the algorithm makes mistakes, the mistakes tend to be close to the correct answer.The Step C in Section 3.2 is similar to Resnik's noun group disambiguation (Resnik, 1995a), although he did not address the question of the creation of noun groups.The earlier work on WSD that is most similar to ours is (Li, Szpakowicz, and Matwin, 1995).They proposed a set of heuristic rules that are based on the idea that objects of the same or similar verbs are similar.Our algorithm treats all local contexts equally in its decision-making.However, some local contexts hardly provide any constraint on the meaning of a word.For example, the object of &quot;get&quot; can practically be anything.This type of contexts should be filtered out or discounted in decision-making.Our assumption that similar words appear in identical context does not always hold.For example, where PER refers to proper names recognized as persons.None of these is similar to the &quot;body part&quot; meaning of &quot;heart&quot;.In fact, &quot;heart&quot; is the only body part that beats.We have presented a new algorithm for word sense disambiguation.Unlike most previous corpusbased WSD algorithm where separate classifiers are trained for different words, we use the same local context database and a concept hierarchy as the knowledge sources for disambiguating all words.This allows our algorithm to deal with infrequent words or unknown proper nouns.Unnecessarily subtle distinction between word senses is a well-known problem for evaluating WSD algorithms with general-purpose lexical resources.Our use of similarity measure to relax the correctness criterion provides a possible solution to this problem.
Much of the recent research on statistical parsing has focused on English; languages other than English are likely to pose new problems for statistical methods.This paper considers statistical parsing of Czech, using the Prague Dependency Treebank (PDT) (Haji, 1998) as a source of training and test data (the PDT contains around 480,000 words of general news, business news, and science articles Other Slavic languages (such as Polish, Russian, Slovak, Slovene, Serbo-croatian, Ukrainian) also show these characteristics.Many European languages exhibit FWO and HI phenomena to a lesser extent.Thus the techniques and results found for Czech should be relevant to parsing several other languages.This paper first describes a baseline approach, based on the parsing model of (Collins 97), which recovers dependencies with 72% accuracy.We then describe a series of refinements to the model, giving an improvement to 80% accuracy, with around 82% accuracy on newspaper/business articles.(As a point of comparison, the parser achieves 91% dependency accuracy on English (Wall Street Journal) text.)The Prague Dependency Treebank PDT (Hap, 1998) has been modeled after the Penn Treebank (Marcus et al. 93), with one important exception: following the Praguian linguistic tradition, the syntactic annotation is based on dependencies rather than phrase structures.Thus instead of &quot;nonterminal&quot; symbols used at the non-leaves of the tree, the PDT uses so-called analytical functions capturing the type of relation between a dependent and its governing node.Thus the number of nodes is equal to the number of tokens (words + punctuation) plus one (an artificial root node with rather technical function is added to each sentence).The PDT contains also a traditional morpho-syntactic annotation (tags) at each word position (together with a lemma, uniquely representing the underlying lexical unit).As Czech is a HI language, the size of the set of possible tags is unusually high: more than 3,000 tags may be assigned by the Czech morphological analyzer.The PDT also contains machine-assigned tags and lemmas for each word (using a tagger described in (Haji 6 and Hladka, 1998)).For evaluation purposes, the PDT has been divided into a training set (19k sentences) and a development/evaluation test set pair (about 3,500 sentences each).Parsing accuracy is defined as the ratio of correct dependency links vs. the total number of dependency links in a sentence (which equals, with the one artificial root node added, to the number of tokens in a sentence).As usual, with the development test set being available during the development phase, all final results has been obtained on the evaluation test set, which nobody could see beforehand.The parsing model builds on Model 1 of (Collins 97); this section briefly describes the model.The parser uses a lexicalized grammar — each nonterminal has an associated head-word and part-ofspeech (POS).We write non-terminals as X (x): X is the non-terminal label, and x is a (w, t) pair where w is the associated head-word, and t as the POS tag.See figure 1 for an example lexicalized tree, and a list of the lexicalized rules that it contains.Each rule has the fonnl : 'With the exception of the top rule in the tree, which has the form TOP H (h) .H is the head-child of the phrase, which inherits the head-word h from its parent P. L1...Ln and RI are left and right modifiers of H. Either n or m may be zero, and n = The model can be considered to be a variant of Probabilistic Context-Free Grammar (PCFG).In PCFGs each rule a —> in the CFG underlying the PCFG has an associated probability P(/31a).In (Collins 97), P (01a) is defined as a product of terms, by assuming that the right-hand-side of the rule is generated in three steps: probability I P, h, H), where Ln+i (in+1) = STOP.The STOP symbol is added to the vocabulary of nonterminals, and the model stops generating left modifiers when it is generated.Other rules in the tree contribute similar sets of probabilities.The probability for the entire tree is calculated as the product of all these terms.(Collins 97) describes a series of refinements to this basic model: the addition of &quot;distance&quot; (a conditioning feature indicating whether or not a modifier is adjacent to the head); the addition of subcategorization parameters (Model 2), and parameters that model wh-movement (Model 3); estimation techniques that smooth various levels of back-off (in particular using POS tags as word-classes, allowing the model to learn generalizations about POS classes of words).Search for the highest probability tree for a sentence is achieved using a CKY-style parsing algorithm.Many statistical parsing methods developed for English use lexicalized trees as a representation (e.g., (Jelinek et al. 94; Magerman 95; Ratnaparkhi 97; Charniak 97; Collins 96; Collins 97)); several (e.g., (Eisner 96; Collins 96; Collins 97; Charniak 97)) emphasize the use of parameters associated with dependencies between pairs of words.The Czech PDT contains dependency annotations, but no tree structures.For parsing Czech we considered a strategy of converting dependency structures in training data to lexicalized trees, then running the parsing algorithms originally developed for English.A key point is that the mapping from lexicalized trees to dependency structures is many-to-one.As an example, figure 2 shows an input dependency structure, and three different lexicalized trees with this dependency structure.The choice of tree structure is crucial in determining the independence assumptions that the parsing model makes.There are at least 3 degrees of freedom when deciding on the tree structures: To provide a baseline result we implemented what is probably the simplest possible conversion scheme: The baseline approach gave a result of 71.9% accuracy on the development test set.While the baseline approach is reasonably successful, there are some linguistic phenomena that lead to clear problems.This section describes some tree transformations that are linguistically motivated, and lead to improvements in parsing accuracy.In the PDT the verb is taken to be the head of both sentences and relative clauses.Figure 4 illustrates how the baseline transformation method can lead to parsing errors in relative clause cases.Figure 4(c) shows the solution to the problem: the label of the relative clause is changed to SBAR, and an additional VP level is added to the right of the relative pronoun.Similar transformations were applied for relative clauses involving Wh-PPs (e.g., &quot;the man to whom I gave a book&quot;), Wh-NPs (e.g., &quot;the man whose book I read&quot;) and Wh-Adverbials (e.g., &quot;the place where I live&quot;).The PDT takes the conjunct to be the head of coordination structures (for example, and would be the head of the NP dogs and cats).In these cases the baseline approach gives tree structures such as that in figure 5(a).The non-terminal label for the phrase is JP (because the head of the phrase, the conjunct and, is tagged as J).This choice of non-terminal is problematic for two reasons: (1) the JP label is assigned to all coordinated phrases, for example hiding the fact that the constituent in figure 5(a) is an NP; (2) the model assumes that left and right modifiers are generated independently of each other, and as it stands will give unreasonably high probability to two unlike phrases being coordinated.To fix these problems, the non-terminal label in coordination cases was altered to be the same as that of the second conjunct (the phrase directly to the right of the head of the phrase).See figure 5.A similar transformation was made for cases where a comma was the head of a phrase.Figure 6 shows an additional change concerning commas.This change increases the sensitivity of the model to punctuation.This section describes some modifications to the parameterization of the model. guish main clauses from relative clauses: both have a verb as the head, so both are labeled VP.(b) A typical parsing error due to relative and main clauses not being distinguished.(note that two main clauses can be coordinated by a comma, as in John likes Mary, Mary likes Tim).(c) The solution to the problem: a modification to relative clause structures in training data.The model of (Collins 97) had conditioning variables that allowed the model to learn a preference for dependencies which do not cross verbs.From the results in table 3, adding this condition improved accuracy by about 0.9% on the development set.The parser of (Collins 96) used punctuation as an indication of phrasal boundaries.It was found that if a constituent Z (...XY...) has two children X and Y separated by a punctuation mark, then Y is generally followed by a punctuation mark or the end of sentence marker.The parsers of (Collins 96,97) encoded this as a hard constraint.In the Czech parser we added a cost of -2.5 (log probability)2 to structures that violated this constraint.The model of section 3 made the assumption that modifiers are generated independently of each other.This section describes a hi gram model, where the context is increased to consider the previously generated modifier ((Eisner 96) also describes use of bigram statistics).The right-hand-side of a rule is now assumed to be generated in the following three step process: where Lo is defined as a special NULL symbol.Thus the previous modifier, Li_1, is added to the conditioning context (in the previous model the left modifiers had probability Introducing bigram-dependencies into the parsing model improved parsing accuracy by about 0.9 % (as shown in Table 3).Part of speech (POS) tags serve an important role in statistical parsing by providing the model with a level of generalization as to how classes of words tend to behave, what roles they play in sentences, and what other classes they tend to combine with.Statistical parsers of English typically make use of the roughly 50 POS tags used in the Penn Treebank corpus, but the Czech PDT corpus provides a much richer set of POS tags, with over 3000 possible tags defined by the tagging system and over 1000 tags actually found in the corpus.Using that large a tagset with a training corpus of only 19,000 sentences would lead to serious sparse data problems.It is also clear that some of the distinctions being made by the tags are more important than others for parsing.We therefore explored different ways of extracting smaller but still maximally informative POS tagsets.The POS tags in the Czech PDT corpus (Haji 6 and Hladka, 1997) are encoded in 13-character strings.Table 1 shows the role of each character.For example, the tag NNMP1 A would be used for a word that had &quot;noun&quot; as both its main and detailed part of speech, that was masculine, plural, nominative (case 1), and whose negativeness value was &quot;affirmative&quot;.Within the corpus, each word was annotated with all of the POS tags that would be possible given its spelling, using the output of a morphological analysis program, and also with the single one of those tags that a statistical POS tagging program had predicted to be the correct tag (Hake and Hladka, 1998).Table 2 shows a phrase from the corpus, with the alternative possible tags and machine-selected tag for each word.In the training portion of the corpus, the correct tag as judged by human annotators was also provided.In the baseline approach, the first letter, or &quot;main part of speech&quot;, of the full POS strings was used as the tag.This resulted in a tagset with 13 possible values.A number of alternative, richer tagsets were explored, using various combinations of character positions from the tag string.The most successful alternative was a two-letter tag whose first letter was always the main POS, and whose second letter was the case field if the main POS was one that displays case, while otherwise the second letter was the detailed POS.(The detailed POS was used for the main POS values D, J, V, and X; the case field was used for the other possible main POS values.)This two-letter scheme resulted in 58 tags, and provided about a 1.1% parsing improvement over the baseline on the development set.Even richer tagsets that also included the person, gender, and number values were tested without yielding any further improvement, presumably because the damage from sparse data outweighed the value of the additional information present.An entirely different approach, rather than searching by hand for effective tagsets, would be to use clustering to derive them automatically.We explored two different methods, bottom-up and topdown, for automatically deriving POS tag sets based on counts of governing and dependent tags extracted from the parse trees that the parser constructs from the training data.Neither tested approach resulted in any improvement in parsing performance compared to the hand-designed &quot;two letter&quot; tagset, but the implementations of each were still only preliminary, and a clustered tagset more adroitly derived might do better.One final issue regarding POS tags was how to deal with the ambiguity between possible tags, both in training and test.In the training data, there was a choice between using the output of the POS tagger or the human annotator's judgment as to the correct tag.In test data, the correct answer was not available, but the POS tagger output could be used if desired.This turns out to matter only for unknown words, as the parser is designed to do its own tagging, for words that it has seen in training at least 5 times, ignoring any tag supplied with the input.For &quot;unknown&quot; words (seen less than 5 times), the parser can be set either to believe the tag supplied by the POS tagger or to allow equally any of the dictionary-derived possible tags for the word, effectively allowing the parse context to make the choice.(Note that the rich inflectional morphology of Czech leads to a higher rate of &quot;unknown&quot; word forms than would be true in English; in one test, 29.5% of the words in test data were &quot;unknown&quot;) Our tests indicated that if unknown words are treated by believing the POS tagger's suggestion, then scores are better if the parser is also trained on the POS tagger's suggestions, rather than on the human annotator's correct tags.Training on the correct tags results in 1% worse performance.Even though the POS tagger's tags are less accurate, they are more like what the parser will be using in the test data, and that turns out to be the key point.On the other hand, if the parser allows all possible dictionary tags for unknown words in test material, then it pays to train on the actual correct tags.In initial tests, this combination of training on the correct tags and allowing all dictionary tags for unknown test words somewhat outperformed the alternative of using the POS tagger's predictions both for training and for unknown test words.When tested with the final version of the parser on the full development set, those two strategies performed at the same level.We ran three versions of the parser over the final test set: the baseline version, the full model with all additions, and the full model with everything but the bigram model.The baseline system on the fithat although the Science section only contributes 25% of the sentences in test data, it contains much longer sentences than the other sections and therefore accounts for 38% of the dependencies in test data. nal test set achieved 72.3% accuracy.The final system achieved 80.0% accuracy3: a 7.7% absolute improvement and a 27.8% relative improvement.The development set showed very similar results: a baseline accuracy of 71.9% and a final accuracy of 79.3%.Table 3 shows the relative improvement of each component of the mode14.Table 4 shows the results on the development set by genre.It is interesting to see that the performance on newswire text is over 2% better than the averaged performance.The Science section of the development set is considerably harder to parse (presumably because of longer sentences and more open vocabulary).The main piece of previous work on parsing Czech that we are aware of is described in (Kuboli 99).This is a rule-based system which is based on a manually designed set of rules.The system's accuracy is not evaluated on a test corpus, so it is difficult to compare our results to theirs.We can, however, make some comparison of the results in this paper to those on parsing English.(Collins 99) describes results of 91% accuracy in recovering dependencies on section 0 of the Penn Wall Street Journal Treebank, using Model 2 of (Collins 97).This task is almost certainly easier for a number of reasons: there was more training data (40,000 sentences as opposed to 19,000); Wall Street Journal may be an easier domain than the PDT, as a reasonable proportion of sentences come from a sub-domain, financial news, which is relatively restricted.Unlike model 1, model 2 of the parser takes subcategorization information into account, which gives some improvement on English and might well also improve results on Czech.Given these differences, it is difficult to make a direct comparison, but the overall conclusion seems to be that the Czech accuracy is approaching results on English, although it is still somewhat behind.The 80% dependency accuracy of the parser represents good progress towards English parsing performance.A major area for future work is likely to be an improved treatment of morphology; a natural approach to this problem is to consider more carefully how POS tags are used as word classes by the model.We have begun to investigate this issue, through the automatic derivation of POS tags through clustering or &quot;splitting&quot; approaches.It might also be possible to exploit the internal structure of the POS tags, for example through incremental prediction of the POS tag being generated; or to exploit the use of word lemmas, effectively splitting word—word relations into syntactic dependencies (POS tag—POS tag relations) and more semantic (lemma—lemma) dependencies.
For most sequence-modeling tasks with word-level evaluation, including named-entity recognition and part-ofspeech tagging, it has seemed natural to use entire words as the basic input features.For example, the classic HMM view of these two tasks is one in which the observations are words and the hidden states encode class labels.However, because of data sparsity, sophisticated unknown word models are generally required for good performance.A common approach is to extract word-internal features from unknown words, for example suffix, capitalization, or punctuation features (Mikheev, 1997, Wacholder et al., 1997, Bikel et al., 1997).One then treats the unknown word as a collection of such features.Having such unknown-word models as an add-on is perhaps a misplaced focus: in these tasks, providing correct behavior on unknown words is typically the key challenge.Here, we examine the utility of taking character sequences as a primary representation.We present two models in which the basic units are characters and character -grams, instead of words and word phrases.Earlier papers have taken a character-level approach to named entity recognition (NER), notably Cucerzan and Yarowsky (1999), which used prefix and suffix tries, though to our knowledge incorporating all character grams is new.In section 2, we discuss a character-level HMM, while in section 3 we discuss a sequence-free maximum-entropy (maxent) classifier which uses -gram substring features.Finally, in section 4 we add additional features to the maxent model, and chain these models into a conditional markov model (CMM), as used for tagging (Ratnaparkhi, 1996) or earlier NER work (Borthwick, 1999).Figure 1 shows a graphical model representation of our character-level HMM.Characters are emitted one at a time, and there is one state per character.Each state’s identity depends only on the previous state.Each character’s identity depends on both the current state and on the previous characters.In addition to this HMM view, it may also be convenient to think of the local emission models as type-conditional -gram models.Indeed, the character emission model in this section is directly based on the -gram proper-name classification engine described in (Smarr and Manning, 2002).The primary addition is the state-transition chaining, which allows the model to do segmentation as well as classification.When using character-level models for word-evaluated tasks, one would not want multiple characters inside a single word to receive different labels.This can be avoided in two ways: by explicitly locking state transitions inside words, or by careful choice of transition topology.In our current implementation, we do the latter.Each state is a pair where is an entity type (such as PERSON, and including an other type) and indicates the length of time the system has been in state .Therefore, a state like (PERSON, 2) indicates the second letter inside a person phrase.The final letter of a phrase is a following space (we insert one if there is none) and the state is a special final state like (PERSON, F).Additionally, once reaches our -gram history order, it stays there.We then use empirical, unsmoothed estimates for statestate transitions.This annotation and estimation enforces consistent labellings in practice.For example, (PERSON, 2) can only transition to the next state (PERSON, 3) or the final state (PERSON, F).Final states can only transition to beginning states, like (other, 1).For emissions, we must estimate a quantity of the form , for example, .1 We use an -gram model of order .2 The -gram estimates are smoothed via deleted interpolation.Given this model, we can do Viterbi decoding in the standard way.To be clear on what this model does and does not capture, we consider a few examples ( indicates a space).First, we might be asked for .In this case, we know both that we are in the middle of a location that begins with Denv and also that the preceding context was to.In essence, encoding into the state lets us distinguish the beginnings of phrases, which lets us model trends like named entities (all the classes besides other) generally starting with capital letters in English.Second, we may be asked for quantities like , which allows us to model the ends of phrases.Here we have a slight complexity: by the notation, one would expect such emissions to have probability 1, since nothing else can be emitted from a final state.In practice, we have a special stop symbol in our n-gram counts, and the probability of emitting a space from a final state is the probability of the n-gram having chosen the stop character.3 models.The value was the empirically optimal order.3This can be cleaned up conceptually by considering the entire process to have been a hierarchical HMM (Fine et al., 1998), where the -gram model generates the entire phrase, followed by a tier pop up to the phrase transition tier.Using this model, we tested two variants, one in which preceding context was discarded (for example, was turned into ), and another where context was used as outlined above.For comparison, we also built a first-order word-level HMM; the results are shown in table 1.We give F both per-category and overall.The word-level model and the (context disabled) character-level model are intended as a rough minimal pair, in that the only information crossing phrase boundaries was the entity type, isolating the effects of character- vs word-level modeling (a more precise minimal pair is examined in section 3).Switching to the character model raised the overall score greatly, from 74.5% to 82.2%.On top of this, context helped, but substantially less, bringing the total to 83.2%.We did also try to incorporate gazetteer information by adding -gram counts from gazetteer entries to the training counts that back the above character emission model.However, this reduced performance (by 2.0% with context on).The supplied gazetteers appear to have been built from the training data and so do not increase coverage, and provide only a flat distribution of name phrases whose empirical distributions are very spiked.Given the amount of improvement from using a model backed by character -grams instead of word -grams, the immediate question is whether this benefit is complementary to the benefit from features which have traditionally been of use in word level systems, such as syntactic context features, topic features, and so on.To test this, we constructed a maxent classifier which locally classifies single words, without modeling the entity type sequences .4 These local classifiers map a feature representation of each word position to entity types, such as PERSON.5 We present a hill-climb over feature sets for the English development set data in table 2.First, we tried only the local word as a feature; the result was that each word was assigned its most common class in the training data.The overall F-score was 52.29%, well below the official CoNLL baseline of 71.18%.6 We next added -gram features; specifically, we framed each word with special start and end symbols, and then added every contiguous substring to the feature list.Note that this subsumes the entire-word features.Using the substring features alone scored 73.10%, already breaking the the phrase-based CoNLL baseline, though lower than the no-context HMM, which better models the context inside phrases.Adding a current tag feature gave a score of 74.17%.At this point, the bulk of outstanding errors were plausibly attributable to insufficient context information.Adding even just the previous and next words and tags as (atomic) features raised performance to 82.39%.More complex, joint context features which paired the current word and tag with the previous and next words and tags raised the score further to 83.09%, nearly to the level of the HMM, still without actually having any model of previous classification decisions.In order to include state sequence features, which allow the classifications at various positions to interact, we have to abandon classifying each position independently.Sequence-sensitive features can be included by chaining our local classifiers together and performing joint inference, i.e., by building a conditional markov model (CMM), also known as a maximum entropy markov model (McCallum et al., 2000).Previous classification decisions are clearly relevant: for example the sequence Grace Road is a single location, not a person’s name adjacent to a location (which is the erroneous output of the model in section 3).Adding features representing the previous classification decision ( ) raised the score 2.35% to 85.44%.We found knowing that the previous word was an other wasn’t particularly useful without also knowing its part-of-speech (e.g., a preceding preposition might indicate a location).Joint tag-sequence features, along with longer distance sequence and tag-sequence features, gave 87.21%.The remaining improvements involved a number of other features which directly targetted observed error types.These features included letter type pattern features (for example 20-month would become d-x for digitlowercase and Italy would become Xx for mixed case).This improved performance substantially, for example allowing the system to detect ALL CAPS regions.Table 3 shows an example of a local decision for Grace in the context at Grace Road, using all of the features defined to date.Note that the evidence against Grace as a name completely overwhelms the -gram and word preference for PERSON.Other features included secondprevious and second-next words (when the previous or next words were very short) and a marker for capitalized words whose lowercase forms had also been seen.The final system also contained some simple error-driven postprocessing.In particular, repeated sub-elements (usually last names) of multi-word person names were given type PERSON, and a crude heuristic restoration of B- prefixes was performed.In total, this final system had an F-score of 92.31% on the English development set.Table 4 gives a more detailed breakdown of this score, and also gives the results of this system on the English test set, and both German data sets.The primary argument of this paper is that character substrings are a valuable, and, we believe, underexploited source of model features.In an HMM with an admittedly very local sequence model, switching from a word model to a character model gave an error reduction of about 30%.In the final, much richer chained maxent setting, the reduction from the best model minus -gram features to the reported best model was about 25% – smaller, but still substantial.This paper also again demonstrates how the ease of incorporating features into a discriminative maxent model allows for productive feature engineering.
The scientific process involves making hypotheses, gathering evidence, using inductive reasoning to reach a conclusion based on the data, and then making new hypotheses.Scientist are often not completely certain of a conclusion.This lack of definite belief is often reflected in the way scientists discuss their work.In this paper, we focus on expressions of levels of belief: the expressions of hypotheses, tentative conclusions, hedges, and speculations.“Affect” is used in linguistics as a label for this topic.This is not a well-known topic in the field of text processing of bioscience literature.Thus, we present a large number of examples to elucidate the variety and nature of the phenomena.We then return to a discussion of the goals, importance, and possible uses of this research.The sentences in the following box contain fragments expressing a relatively high level of speculation.The level of belief expressed by an author is often difficult to ascertain from an isolated sentence and often the context of the abstract is needed.All examples in the paper are from abstracts available at the Nation Library of Medicine PubMed webpage (currently http://www.ncbi.nlm.nih.gov/PubMed/).The PubMed identifier is provided following each sentence.Pdcd4 may thus constitute a useful molecular target for cancer prevention.(1131400) As the GT box has also previously been shown to play a role in gene regulation of other genes, these newly isolated Sp2 and Sp3 proteins might regulate expression not only of the TCR gene but of other genes as well.(1341900) On the basis of these complementary results, it has been concluded that curcumin shows very high binding to BSA, probably at the hydrophobic cavities inside the protein.(12870844) Curcumin down-regulates Ki67, PCNA and mutant p53 mRNAs in breast cancer cells, these properties may underlie chemopreventive action.(14532610) The next examples contain fragments that are speculative but probably less so than those above.(As we will discuss later, it is difficult to agree on levels of speculation.)The containing sentence does sibility.The examples above are speculative and the sentence below expresses a definite statement about two possibilities. provide some context but the rest of the abstract if not the full text is often necessary along with enough knowledge of field to understand text.Removal of the carboxy terminus enables ERP to interact with a variety of ets-binding sites including the E74 site, the IgH enhancer pi site, and the lck promoter ets site, suggesting a carboxy-terminal negative regulatory domain.(7909357) In addition, we show that a component of the Ras-dependent mitogen-activated protein kinase pathway, nerve growth factor-inducible c-Jun, exerts its effects on receptor gene promoter activity most likely through protein-protein interactions with Sp1.(11262397) Results suggest that one of the mechanisms of curcumin inhibition of prostate cancer may be via inhibition ofAkt.(12682902) The previous examples contain phrases such as most likely and suggesting, which in these cases, explicitly mark a level of belief less than 100%.The next examples are not as explicitly marked: to date and such as can also be used in purely definite statements.To date, we find that the signaling pathway triggered by each type of insult is distinct.(10556169) However, the inability of IGF-1, insulin and PMA to stimulate 3beta-HSD type 1 expression by themselves in the absence of IL-4 indicates that the multiple pathways downstream ofIRS-1 and IRS-2 must act in cooperation with an IL4-specific signaling molecule, such as the transcription factor Stat6.(11384880) These findings highlight the feasibility of modulating HO-1 expression during hypothermic storage to confer tissues a better protection to counteract the damage characteristic of organ transplantation.(12927811) The words may and might were both used to express speculation in the examples above but are ambiguous between expressing speculation versus posThe level of LFB1 binding activity in adenoidcystic as well as trabecular tumours shows some variation and may either be lower or higher than in the non-tumorous tissue.(7834800) The sentence below involves the adjective putative in an apositive noun phrase modifier, a different syntactic form that in the previous examples.It also clearly shows that the speculative portion is often confined to only a part of the information provided in a sentence.We report here the isolation ofhuman zinc finger 2 (HZF2), a putative zinc-finger transcription factor, by motif-directed differential display of mRNA extracted from histamine-stimulated human vein endothelial cells.(11121585) Of course, definite sentences also come in a variety.The definite sentences below vary in topic and form.Affinity chromatography and coimmunoprecipitation assays demonstrated that c-Jun and T-Ag physically interact with each other.(12692226) However, NF-kappaB was increased at 3 h while AP-1 (Jun B and Jun D) and CREB were increased at 15 h. (10755711) We studied the transcript distribution of c-jun, junB and junD in the rat brain.(1719462) An inclusive model for all steps in the targeting ofproteins to subnuclear sites cannot yet be proposed.(11389536) We have been talking about speculative fragments and speculative sentences.For the rest of the paper, we define a speculative sentence to be one that contains at least one speculative fragment.A definite sentence contains no speculative fragments.In this study we only considered annotations at the sentence level.However, in future work, we plan to work on sub-sentential annotations.Our general goal is to investigate speculative speech in bioscience literature and explore how it might be used in HLT applications for bioscientists.A more specific goal is to investigate the use of speculative speech in MEDLINE abstracts because of their accessibility.There are a number of reasons supporting the importance of understanding speculative speech: In the following, we expand upon these points in the contexts of i) information retrieval, ii) information extraction, and iii) knowledge discovery.In the context of information retrieval, an example information need might be “I am looking for speculations about the X gene in liver tissue.” One of the authors spoke at a research department of a drug company and the biologists present expressed this sort of information need.On the other hand, one of the authors has also encountered the opposite need: “I am looking for definite statements about transcription factors that interact with NF Kappa B.” Both these information needs would be easier to fulfill if automated annotation of speculative passages was possible.In the context of information extraction, a similar situation exists.For example, extracting tables of protein-protein interactions would benefit from knowing which interactions were speculative and which were definite.In the context of knowledge discovery (KR), speculation might play a number of roles.One possibility would be to use current speculative statements about a topic of interest as a seed for the automated knowledge discovery process.For example, terms could be extracted from speculative fragments and used to guide the initial steps of the knowledge discovery process.A less direct but perhaps even more important use is in building test/train datasets for knowledge discovery systems.For example, let us assume that in a 1985 publication we find a speculation about two topics/concepts A and C being related and later in a 1995 document there is a definite statement declaring that A and C are connected via B.This pair of statements can then form the basis of a discovery problem.We may use it to test a KR system’s ability to predict B as the connecting aspect between A and C and to do this using data prior to the 1995 publication.The same example could also be used differently: KR systems could be assessed on their ability to make a speculation between A and C using data up to 1985 excluding the particular publication making the speculation.In this way such pairs of temporally ordered speculative-definite statements may be of value in KR research.Differentiating between speculative and definite statements is one part of finding such statement pairs.We know of no work specifically on speculative speech in the context of text processing of bioscience literature.However, some work on information extraction from bioscience literature has dealt with speculative speech.For example, (Friedman et al., 1994) discusses uncertainty and hedging in radiology reports and their system assigns one of five levels of certainty to extracted findings.Text processing systems in general have focused “factual” language.However, a growing number of researchers have started work on other aspects of language such as expressing opinions, style of writing, etc.For example a human language technology workshop will be held this Spring entitled “Exploring Attitude and Affect in Text: Theories and Applications.” (Qu et al., 2004).Previous work along these lines includes (Wilson and Wiebe, 2003).This research focuses on newswire texts and other texts on the topic of politics and current events.There has been recent work on classifying sentences from MEDLINE abstracts for the categories such as object, background, conclusions (McKnight and Srinivasan, 2003).In addition, early work, (Liddy, 1988) built text grammars for empirical research abstracts categorized and assigned structure concerning rhetorical roles of the sentences.However, none of this work addresses the speculative vs. definite distinction we are interested in.There has also been some work on constructing test sets for knowledge discovery.Several researchers have used the discoveries by Swanson and Smalheiser to test their own algorithms.The two problems most commonly used in replication studies (e.g., (Weeber et al., 2001)) are their discovery of a link between Raynauds disease and fish oils (Swanson, 1986) and their discovery of several links between migraine and magnesium (Swanson, 1988).The most comprehensive replication to date is (Srinivasan, 2004) which employs eight Swanson and Smalheiser discoveries as a test bed.In the remainder of the paper, we describe a manual annotation experiment we performed, give preliminary results on our attempts to automatically annotate sentences as containing speculative fragments, and conclude with comments on possible future work.In this experiment, four human annotators manually marked sentences as highly speculative, low speculative, or definite.Some of the questions we hoped to answer with this experiment were: can we characterize what a speculative sentence is (as demonstrated by good inter-annotator agreement), can a distinction between high and low speculation be made, how much speculative speech is there, where are speculative sentences located in the abstract, is there variation across topics?The annotators were instructed to follow written annotation guidelines which we provide in appendix of this paper.We wanted to explore how well the annotators agreed on relatively abstract classifications such as “requires extrapolation from actual findings” and thus we refrained from writing instructions such as “if the sentence contains a form of suggest, then mark it as speculative” into the guidelines.We chose three topics to work on and used the following Pubmed queries to gather abstracts: The first topic is gene regulation and is about molecular biology research on transcription factors, promoter regions, gene expression, etc.The second topic is Crohn’s disease which is a chronic relapsing intestinal inflammation and has a number of genes (CARD15) or chromosomal loci associated with it.The third topic is turmeric (aka curcumin), a spice widely used in Asia and highly regarded for its curative and analgesic properties.These include the treatment of burns, stomach ulcers and ailments, and various skin diseases.There has been a surge of interest in curcumin over the last decade.Each abstract set was prepared for annotation as follows: the order of the abstracts was randomized and the abstracts were broken into sentences using Mxterminator (Reynar and Ratnaparkhi, 1997).The following people performed the annotations: Padmini Srinivasan, who has analyzed crohns and turmeric documents for a separate knowledge discover research task, Xin Ying Qiu, who is completely new to all three topics, Marc Light, who has some experience with gene regulation texts (e.g., (Light et al., 2003)), Vladimir Leontiev, who is a research scientist in an anatomy and cell biology department.It certainly would have been preferable to have four experts on the topics do the annotation but this was not possible.The following manual annotations were performed: The 63 double annotated gene regulation abstracts (set a) contained 547 sentences.The additional abstracts (set b) marked by Light' contained 344 sentences summing to 891 sentences of gene regulation abstracts.Thus, there is an average of almost 9 sentences per gene regulation abstract.The 100 turmeric abstracts (set e) contained 738 sentences.The other sets contain twice as many sentences as abstracts since only the last two sentences where annotated.The annotation of each sentence was performed in the context of its abstract.This was true even when only the last two sentences where annotated.The annotation guidelines in the appendix were used by all annotators.In addition, at the start of the experiment general issues were discussed but none of the specific examples in the sets a-f. We worked with three categories Low Speculative, High Speculative, and Definite.All sentences were annotated with one of these.The general idea behind the low speculative level was that the authors expressed a statement in such a way that it is clear that it follows almost directly from results but not quite.There is a small leap of faith.A high speculative statement would contain a more dramatic leap from the results mentioned in the abstract.Our inter-annotator agreement results are expressed in the following four tables.The first table contains values for the kappa statistic of agreement (see (Siegel and Castellan, 1988)) for the gene regulation data (set a) and the crohns data (set c).Three values were computed: kappa for three-way agreement (High vs. Low vs. Definite), two-way (Speculative vs. Definite) and two-way (High vs. Low).Due to the lack of any sentences marked High in set c, a kappa value for High vs. low (HvsL) is not possible.Kappa scores between 0.6 and 0.8 are generally considered encouraging but not outstanding.HvsLvsD SvsD HvsL geneReg 0.53 0.68 0.03 crohns 0.63 0.63 na 'Pun intended.The following two tables are confusion matrices, the first for gene regulation data (set a) and the second for the crohns data (set c).If we consider one of the annotators as defining truth (gold standard), then we can compute precision and recall numbers for the other annotator on finding speculative sentences.If we choose Leontiev and Srinivasan as defining truth, then Light and Qiu receive the scores below. precision recall 0.68 0.78 0.70 0.64 As is evident from the confusion matrices, the amount of data that we redundantly annotated is small and thus the kappa numbers are at best to be taken as trends.However, it does seem that the speculative vs. definite distinction can be made with some reliability.In contrast, the high speculation vs. low speculation distinction cannot.The gene regulation annotations marked by Light (sets a & b using only Light’s annotations) can be used to answer questions about the position of speculative fragments in abstracts.Consider the histogram-like table below.The first row refers to speculative sentences and the second to definite.The columns refer to the last sentence of an abstract, the penultimate, elsewhere, and a row sum.The number in brackets is the raw count.Remember that the number of abstracts in sets a & b together is 100.It is clear that almost all of the speculations come towards the end of the abstract.In fact the final sentence contains a speculation more often than not.In addition, consider the data where all sentences in an abstract were annotated (sets a & b & e, using Light’s annotation of a), there were 1456 definitive sentences (89%) and 173 speculative sentence Light Qiu (11%).Finally, if we consider the last two sentences of all the data (sets a-f), we have 1712 definitive sentences (82%) and 381 speculative sentences (18.20%).definite, does not receive precision and recall values.The substring method was run on a subset of the datasets where the gene regulation data (sets a&b) was removed.(It performs extremely well on the gene regulation data due to the fact that it was developed on that data.)We decided to explore the ability of an SVMbased text classifier to select speculative sentences from the abstracts.For this the abstracts were first processed using the SMART retrieval system (Salton, 1971) in order to obtain representation vectors (term-based).Alternative representations were tried involving stemming and term weighting (no weights versus TF*IDF weights).Since results obtained were similar we present only results using stemming and no weights.The classifier experiments followed a 10-fold cross-validation design.We used SV Mlight packages with all settings at default values.We ran experiments in two modes.First, we considered only the last 2 sentences.For this we pooled all hand tagged sentences from the three topic areas (sets a-f).Second, we explored classification on all sentences in the document (sets a,b,e).If we assume a default strategy as a simple baseline, where the majority decision is always made, then we get an accuracy of 82% for the classification problem on the last two sentences data set and 89% for the all sentences data set.Another baseline option is to use a set of strings and look for them as substrings in the sentences.The following 14 strings were identified by Light while annotating the gene regulation abstracts (sets a&b): suggest, potential, likely, may, at least, in part, possibl, potential, further investigation, unlikely, putative, insights, point toward, promise, propose.The automated system then looks for these substrings in a sentence and if found, the sentence is marked as speculative and as definite if not.In the table below the scores for the three methods of annotation are listed as rows.We give accuracy on the categorization task and precision and recall numbers for finding speculative sentences.The format is precision/recall(accuracy), all as percentages.The Majority method, annotating every sentence as Again the results are preliminary since the amount of data is small and the feature set we explored was limited to words.However, it should be noted that both the substring and the SVM systems performs well suggesting that speculation in abstracts is lexically marked but in a somewhat ambiguous fashion.This conclusion is also supported by the fact that neither system used positional features and yet the precision and recall on the all sentence data set is similar to the last two sentences data set.The work presented here is preliminary but promising.It seems that the notion of speculative sentence can be characterized enabling manual annotation.However, we did not manage to characterize the distinction between high and low speculation.In addition, it seems likely that automated systems will be able to achieve useful accuracy.Finally, abstracts seem to include a fair amount of speculative information.Future work concerning manual annotation would include revising the guidelines, throwing out the High vs. Low distinction, annotating more data, annotating sub-sentential units, annotating the focus of the speculation (e.g., a gene), and annotating full text articles.We are also ignorant of work in linguistics that almost certainly exists and may be informative.We have started this process by considering (Hyland, 1998) and (Harris et al., 1989).Future work concerning automatic annotation includes expanding the substring system with more substrings and perhaps more complicated regular expressions, expanding the feature set of the SVM, trying out other classification methods such as decision trees.Finally, we plan on building some of the applications mentioned: a speculation search engine, transcription factor interaction tables with a speculation/definite column, and knowledge discovery test sets.We would like to thank Vladimir Leontiev for his time and effort annotating gene regulation abstracts.In addition, we would like to thank David Eichmann for his assistance with our database queries.We would also like to thank Lynette Hirschman for assistance with the title of this paper.Finally, we would like to thank the anonymous workshop reviewers for their comments.
At the core of a phrase-based statistical machine translation system is a phrase table containing pairs of source and target language phrases, each weighted by a conditional translation probability.Koehn et al. (2003a) showed that translation quality is very sensitive to how this table is extracted from the training data.One particularly surprising result is that a simple heuristic extraction algorithm based on surface statistics of a word-aligned training set outperformed the phrase-based generative model proposed by Marcu and Wong (2002).This result is surprising in light of the reverse situation for word-based statistical translation.Specifically, in the task of word alignment, heuristic approaches such as the Dice coefficient consistently underperform their re-estimated counterparts, such as the IBM word alignment models (Brown et al., 1993).This well-known result is unsurprising: reestimation introduces an element of competition into the learning process.The key virtue of competition in word alignment is that, to a first approximation, only one source word should generate each target word.If a good alignment for a word token is found, other plausible alignments are explained away and should be discounted as incorrect for that token.As we show in this paper, this effect does not prevail for phrase-level alignments.The central difference is that phrase-based models, such as the ones presented in section 2 or Marcu and Wong (2002), contain an element of segmentation.That is, they do not merely learn correspondences between phrases, but also segmentations of the source and target sentences.However, while it is reasonable to suppose that if one alignment is right, others must be wrong, the situation is more complex for segmentations.For example, if one segmentation subsumes another, they are not necessarily incompatible: both may be equally valid.While in some cases, such as idiomatic vs. literal translations, two segmentations may be in true competition, we show that the most common result is for different segmentations to be recruited for different examples, overfitting the training data and overly determinizing the phrase translation estimates.In this work, we first define a novel (but not radical) generative phrase-based model analogous to IBM Model 3.While its exact training is intractable, we describe a training regime which uses wordlevel alignments to constrain the space of feasible segmentations down to a manageable number.We demonstrate that the phrase analogue of the Dice coefficient is superior to our generative model (a result also echoing previous work).In the primary contribution of the paper, we present a series of experiments designed to elucidate what re-estimation learns in this context.We show that estimates are overly determinized because segmentations are used in unintuitive ways for the sake of data likelihood.We comment on both the beneficial instances of segment competition (idioms) as well as the harmful ones (most everything else).Finally, we demonstrate that interpolation of the two estimates can provide a modest increase in BLEU score over the heuristic baseline.The generative model defined below is evaluated based on the BLEU score it produces in an endto-end machine translation system from English to French.The top-performing diag-and extraction heuristic (Zens et al., 2002) serves as the baseline for evaluation.1 Each approach – the generative model and heuristic baseline – produces an estimated conditional distribution of English phrases given French phrases.We will refer to the distribution derived from the baseline heuristic as φH.The distribution learned via the generative model, denoted φEM, is described in detail below.While our model for computing φEM is novel, it is meant to exemplify a class of models that are not only clear extensions to generative word alignment models, but also compatible with the statistical framework assumed during phrase-based decoding.The generative process we modeled produces a phrase-aligned English sentence from a French sentence where the former is a translation of the latter.Note that this generative process is opposite to the translation direction of the larger system because of the standard noisy-channel decomposition.The learned parameters from this model will be used to translate sentences from English to French.The generative process modeled has four steps:2 The corresponding probabilistic model for this generative process is: where P(e, ¯fi , ¯ei, a|f) factors into a segmentation model σ, a translation model φ and a distortion model d. The parameters for each component of this model are estimated differently: ing function based on absolute sentence position akin to the one used in IBM model 3.While similar to the joint model in Marcu and Wong (2002), our model takes a conditional form compatible with the statistical assumptions used by the Pharaoh decoder.Thus, after training, the parameters of the phrase translation model φEM can be used directly for decoding.Significant approximation and pruning is required to train a generative phrase model and table – such as φEM – with hidden segmentation and alignment variables using the expectation maximization algorithm (EM).Computing the likelihood of the data for a set of parameters (the e-step) involves summing over exponentially many possible segmentations for each training sentence.Unlike previous attempts to train a similar model (Marcu and Wong, 2002), we allow information from a word-alignment model to inform our approximation.This approach allowed us to directly estimate translation probabilities even for rare phrase pairs, which were estimated heuristically in previous work.In each iteration of EM, we re-estimate each phrase translation probability by summing fractional phrase counts (soft counts) from the data given the current model parameters.This training loop necessitates approximation because summing over all possible segmentations and alignments for each sentence is intractable, requiring time exponential in the length of the sentences.Additionally, the set of possible phrase pairs grows too large to fit in memory.Using word alignments, we can address both problems.4 In particular, we can determine for any aligned segmentation ( 1I1, eI1, a) whether it is compatible with the word-level alignment for the sentence pair.We define a phrase pair to be compatible with a word-alignment if no word in either phrase is aligned with a word outside the other phrase (Zens et al., 2002).Then, ( 1I1, eI1, a) is compatible with the word-alignment if each of its aligned phrases is a compatible phrase pair.The training process is then constrained such that, when evaluating the above sum, only compatible aligned segmentations are considered.That is, we allow P(e, �fI1 , eI1, aIf) > 0 only for aligned segmentations ( 1I1, eI1, a) such that a provides a oneto-one mapping from �fI1 to eI1 where all phrase pairs (�faj, ej) are compatible with the word alignment.This constraint has two important effects.First, we force P(ej |li) = 0 for all phrase pairs not compatible with the word-level alignment for some sentence pair.This restriction successfully reduced the total legal phrase pair types from approximately 250 million to 17 million for 100,000 training sentences.However, some desirable phrases were eliminated because of errors in the word alignments.Second, the time to compute the e-step is reduced.While in principle it is still intractable, in practice we can compute most sentence pairs’ contributions in under a second each.However, some spurious word alignments can disallow all segmentations for a sentence pair, rendering it unusable for training.Several factors including errors in the word-level alignments, sparse word alignments and non-literal translations cause our constraint to rule out approximately 54% of the training set.Thus, the reduced size of the usable training set accounts for some of the degraded performance of OEM relative to OH.However, the results in figure 1 of the following section show that OEM trained on twice as much data as OH still underperforms the heuristic, indicating a larger issue than decreased training set size.To test the relative performance of OEM and OH, we evaluated each using an end-to-end translation system from English to French.We chose this nonstandard translation direction so that the examples in this paper would be more accessible to a primarily English-speaking audience.All training and test data were drawn from the French/English section of the Europarl sentence-aligned corpus.We tested on the first 1,000 unique sentences of length 5 to 15 in the corpus and trained on sentences of length 1 to 60 starting after the first 10,000.The system follows the structure proposed in the documentation for the Pharaoh decoder and uses many publicly available components (Koehn, 2003b).The language model was generated from the Europarl corpus using the SRI Language Modeling Toolkit (Stolcke, 2002).Pharaoh performed decoding using a set of default parameters for weighting the relative influence of the language, translation and distortion models (Koehn, 2003b).A maximum phrase length of three was used for all experiments.To properly compare OEM to OH, all aspects of the translation pipeline were held constant except for the parameters of the phrase translation table.In particular, we did not tune the decoding hyperparameters for the different phrase tables. peHaving generated OH heuristically and OEM with EM, we now0compare their performance.While the model and training regimen for OEM differ from the model fromMarcu and Wong (2002), we achieved tion maximization algorithm for training OEM was initialized with the heuristic parameters OH, so the heuristic curve can be equivalently labeled as iteration 0.Thus, the first iteration of EM increases the observed likelihood of the training sentences while simultaneously degrading translation performance on the test set.As training proceeds, performance on the test set levels off after three iterations of EM.The system never achieves the performance of its initialization parameters.The pruning of our training regimen accounts for part of this degradation, but not all; augmenting OEM by adding back in all phrase pairs that were dropped during training does not close the performance gap between OEM and OH.Learning OEM degrades translation quality in large part because EM learns overly determinized segmentations and translation parameters, overfitting the training data and failing to generalize.The primary increase in richness from generative wordlevel models to generative phrase-level models is due to the additional latent segmentation variable.Although we impose a uniform distribution over segmentations, it nonetheless plays a crucial role during training.We will characterize this phenomenon through aggregate statistics and translation examples shortly, but begin by demonstrating the model’s capacity to overfit the training data.Let us first return to the motivation behind introducing and learning phrases in machine translation.For any language pair, there are contiguous strings of words whose collocational translation is non-compositional; that is, they translate together differently than they would in isolation.For instance, chat in French generally translates to cat in English, but appeler un chat un chat is an idiom which translates to call a spade a spade.Introducing phrases allows us to translate chat un chat atomically to spade a spade and vice versa.While introducing phrases and parameterizing their translation probabilities with a surface heuristic allows for this possibility, statistical re-estimation would be required to learn that chat should never be translated to spade in isolation.Hence, translating I have a spade with OH could yield an error.But enforcing competition among segmentations introduces a new problem: true translation ambiguity can also be spuriously explained by the segmentation.Consider the french fragment carte sur la table, which could translate to map on the table or notice on the chart.Using these two sentence pairs as training, one would hope to capture the ambiguity in the parameter table as: Assuming we only allow non-degenerate segmentations and disallow non-monotonic alignments, this parameter table yields a marginal likelihood P(fle) = 0.25 for both sentence pairs – the intuitive result given two independent lexical ambiguHence, a higher likelihood can be achieved by allocating some phrases to certain translations while reserving overlapping phrases for others, thereby failing to model the real ambiguity that exists across the language pair.Also, notice that the phrase sur la can take on an arbitrary distribution over any english phrases without affecting the likelihood of either sentence pair.Not only does this counterintuitive parameterization give a high data likelihood, but it is also a fixed point of the EM algorithm.The phenomenon demonstrated above poses a problem for generative phrase models in general.The ambiguous process of translation can be modeled either by the latent segmentation variable or the phrase translation probabilities.In some cases, optimizing the likelihood of the training corpus adjusts for the former when we would prefer the latter.We next investigate how this problem manifests in φEM and its effect on translation quality.The parameters of φEM differ from the heuristically extracted parameters φH in that the conditional distributions over English translations for some French words are sharply peaked for φEM compared to flatter distributions generated by φH.This determinism – predicted by the previous section’s example – is not atypical of EM training for other tasks.To quantify the notion of peaked distributions over phrase translations, we compute the entropy of the distribution for each French phrase according to the standard definition.The average entropy, weighted by frequency, for the most common 10,000 phrases in the learned table was 1.55, comparable to 3.76 for the heuristic table.The difference between the tables becomes much more striking when we consider the histogram of entropies for phrases in figure 2.In particular, the learned table has many more phrases with entropy near zero.The most pronounced entropy differences often appear for common phrases.Ten of the most common phrases in the French corpus are shown in figure 3.As more probability mass is reserved for fewer translations, many of the alternative translations under φH are assigned prohibitively small probabilities.In translating 1,000 test sentences, for example, no phrase translation with φ(e |f) less than 10−5 was used by the decoder.Given this empirical threshold, nearly 60% of entries in φEM are unusable, compared with 1% in φH.While this determinism of φEM may be desirable in some circumstances, we found that the ambiguity in φH is often preferable at decoding time.Several learned distributions have very low entropy.30 In particular, the pattern of translation-ambiguous 0 phrases receiving spuriously peaked distributions (as 0 - 01 01 - .5 5 - 1 1 described in section 3.1) introduces new traslation Entropy errors relative to the baseline.We now investigate both positive and negative effects of the learning process.The issue that motivated training a generative model is sometimes resolved correctly: for a word that translates differently alone than in the context of an idiom, the translation probabilities can more accurately reflect this.Returning to the previous example, the phrase table for chat has been corrected through the learning process.The heuristic process gives the incorrect translation spade with 61% probability, while the statistical learning approach gives cat with 95% probability.While such examples of improvement are encouraging, the trend of spurious determinism overwhelms this benefit by introducing errors in four related ways, each of which will be explored in turn.The first effect follows from our observation in section 3.2 that many phrase pairs are unusable due to vanishingly small probabilities.Some of the entries that are made unusable by re-estimation are helpful at decoding time, evidenced by the fact that pruning the set of OEM’s low-scoring learned phrases from the original heuristic table reduces BLEU score by 0.02 for 25k training sentences (below the score for OEM).The second effect is more subtle.Consider the sentence in figure 4, which to a first approximation can be translated as a series of cognates, as demonstrated by the decoding that follows from the Heuristic heuristic parameterization OH.6 Notice also that the Learned translation probabilities from heuristic extraction are non-deterministic.On the other hand, the translation system makes a significant lexical error on this sim> 2 ple sentence when parameterized by OEM: the use of caract´erise in this context is incorrect.This error arises from a sharply peaked distribution over English phrases for caract´erise.This example illustrates a recurring problem: errors do not necessarily arise because a correct translation is not available.Notice that a preferable translation of degree as degr´e is available under both parameterizations.Degr´e is not used, however, because of the peaked distribution of a competing translation candidate.In this way, very high probability translations can effectively block the use of more appropriate translations at decoding time.What is furthermore surprising and noteworthy in this example is that the learned, near-deterministic translation for caract´erise is not a common translation for the word.Not only does the statistical learning process yield low-entropy translation distributions, but occasionally the translation with undesirably high conditional probability does not have a strong surface correlation with the source phrase.This example is not unique; during different initializations of the EM algorithm, we noticed such patterns even for common French phrases such as de and ne.The third source of errors is closely related: common phrases that translate in many ways depending on the context can introduce errors if they have a spuriously peaked distribution.For instance, consider the lone apostrophe, which is treated as a single token in our data set (figure 5).The shape of the heuristic translation distribution for the phrase is intuitively appealing, showing a relatively flat distribution among many possible translations.Such a distribution has very high entropy.On the other hand, the learned table translates the apostrophe to the with probability very near 1. phe, the most common french phrase.The learned table contains a highly peaked distribution.Such common phrases whose translation depends highly on the context are ripe for producing translation errors.The flatness of the distribution of OH ensures that the single apostrophe will rarely be used during decoding because no one phrase table entry has high enough probability to promote its use.On the other hand, using the peaked entry OEM(the|') incurs virtually no cost to the score of a translation.The final kind of errors stems from interactions between the language and translation models.The selection among translation choices via a language model – a key virtue of the noisy channel framework – is hindered by the determinism of the translation model.This effect appears to be less significant than the previous three.We should note, however, that adjusting the language and translation model weights during decoding does not close the performance gap between OH and OEM.In light of the low entropy of OEM, we could hope to improve translations by retaining entropy.There are several strategies we have considered to achieve this.Broadly, we have tried two approaches: combining OEM and OH via heuristic interpolation methods and modifying the training loop to limit determinism.The simplest strategy to increase entropy is to interpolate the heuristic and learned phrase tables.Varying the weight of interpolation showed an improvement over the heuristic of up to 0.01 for 100k sentences.A more modest improvement of 0.003 for 25k training sentences appears in table 1.In another experiment, we interpolated the output of each iteration of EM with its input, thereby maintaining some entropy from the initialization parameters.BLEU score increased to a maximum of 0.394 using this technique with 100k training sentences, outperforming the heuristic by a slim margin of 0.005.We might address the determinization in OEM without resorting to interpolation by modifying the training procedure to retain entropy.By imposing a non-uniform segmentation model that favors shorter phrases over longer ones, we hope to prevent the error-causing effects of EM training outlined above.In principle, this change will encourage EM to explain training sentences with shorter sentences.In practice, however, this approach has not led to an improvement in BLEU.Another approach to maintaining entropy during the training process is to smooth the probabilities generated by EM.In particular, we can use the following smoothed update equation during the training loop, which reserves a portion of probability mass for unseen translations.We would like to thank the anonymous reviewers for their valuable feedback on this paper.In the equation above, l is the length of the French phrase and k is a tuning parameter.This formulation not only serves to reduce very spiked probabilities in OEM, but also boosts the probability of short phrases to encourage their use.With k = 2.5, this smoothing approach improves BLEU by .007 using 25k training sentences, nearly equaling the heuristic (table 1).Re-estimating phrase translation probabilities using a generative model holds the promise of improving upon heuristic techniques.However, the combinatorial properties of a phrase-based generative model have unfortunate side effects.In cases of true ambiguity in the language pair to be translated, parameter estimates that explain the ambiguity using segmentation variables can in some cases yield higher data likelihoods by determinizing phrase translation estimates.However, this behavior in turn leads to errors at decoding time.We have also shown that some modest benefit can be obtained from re-estimation through the blunt instrument of interpolation.A remaining challenge is to design more appropriate statistical models which tie segmentations together unless sufficient evidence of true non-compositionality is present; perhaps such models could properly combine the benefits of both current approaches.
In recent evaluations, phrase-based statistical machine translation systems have achieved good performance.Still the fluency of the machine translation output leaves much to desire.One reason is that most phrase-based systems use a very simple reordering model.Usually, the costs for phrase movements are linear in the distance, e.g. see (Och et al., 1999; Koehn, 2004; Zens et al., 2005).Recently, in (Tillmann and Zhang, 2005) and in (Koehn et al., 2005), a reordering model has been described that tries to predict the orientation of a phrase, i.e. it answers the question ’should the next phrase be to the left or to the right of the current phrase?’ This phrase orientation probability is conditioned on the current source and target phrase and relative frequencies are used to estimate the probabilities.We adopt the idea of predicting the orientation, but we propose to use a maximum-entropy based model.The relative-frequency based approach may suffer from the data sparseness problem, because most of the phrases occur only once in the training corpus.Our approach circumvents this problem by using a combination of phrase-level and word-level features and by using word-classes or part-of-speech information.Maximum entropy is a suitable framework for combining these different features with a well-defined training criterion.In (Koehn et al., 2005) several variants of the orientation model have been tried.It turned out that for different tasks, different models show the best performance.Here, we let the maximum entropy training decide which features are important and which features can be neglected.We will see that additional features do not hurt performance and can be safely added to the model.The remaining part is structured as follows: first we will describe the related work in Section 2 and give a brief description of the baseline system in Section 3.Then, we will present the discriminative reordering model in Section 4.Afterwards, we will evaluate the performance of this new model in Section 5.This evaluation consists of two parts: first we will evaluate the prediction capabilities of the model on a word-aligned corpus and second we will show improved translation quality compared to the baseline system.Finally, we will conclude in Section 6.As already mentioned in Section 1, many current phrase-based statistical machine translation systems use a very simple reordering model: the costs for phrase movements are linear in the distance.This approach is also used in the publicly available Pharaoh decoder (Koehn, 2004).The idea of predicting the orientation is adopted from (Tillmann and Zhang, 2005) and (Koehn et al., 2005).Here, we use the maximum entropy principle to combine a variety of different features.A reordering model in the framework of weighted finite state transducers is described in (Kumar and Byrne, 2005).There, the movements are defined at the phrase level, but the window for reordering is very limited.The parameters are estimated using an EM-style method.None of these methods try to generalize from the words or phrases by using word classes or part-ofspeech information.The approach presented here has some resemblance to the bracketing transduction grammars (BTG) of (Wu, 1997), which have been applied to a phrase-based machine translation system in (Zens et al., 2004).The difference is that, here, we do not constrain the phrase reordering.Nevertheless the inverted/monotone concatenation of phrases in the BTG framework is similar to the left/right phrase orientation used here.In statistical machine translation, we are given a source language sentence fJ1 = f1 ... fj ... fJ, which is to be translated into a target language sentence eI1 = e1 ... ei ... eI.Among all possible target language sentences, we will choose the sentence with the highest probability: The posterior probability Pr(eI1|fJ1 ) is modeled directly using a log-linear combination of several models (Och and Ney, 2002): (2) The denominator represents a normalization factor that depends only on the source sentence fJ1 .Therefore, we can omit it during the search process.As a decision rule, we obtain: This approach is a generalization of the sourcechannel approach (Brown et al., 1990).It has the advantage that additional models h(·) can be easily integrated into the overall system.The model scaling factors λM1 are trained with respect to the final translation quality measured by an error criterion (Och, 2003).We use a state-of-the-art phrase-based translation system (Zens and Ney, 2004; Zens et al., 2005) including the following models: an n-gram language model, a phrase translation model and a word-based lexicon model.The latter two models are used for both directions: p(f|e) and p(e|f).Additionally, we use a word penalty and a phrase penalty.The reordering model of the baseline system is distancebased, i.e. it assigns costs based on the distance from the end position of a phrase to the start position of the next phrase.This very simple reordering model is widely used, for instance in (Och et al., 1999; Koehn, 2004; Zens et al., 2005).In this section, we will describe the proposed discriminative reordering model.To make use of word level information, we need the word alignment within the phrase pairs.This can be easily stored during the extraction of the phrase pairs from the bilingual training corpus.If there are multiple possible alignments for a phrase pair, we use the most frequent one.The notation is introduced using the illustration in Figure 1.There is an example of a left and a right phrase orientation.We assume that we have already produced the three-word phrase in the lower part.Now, the model has to predict if the start position of the next phrase j′ is to the left or to the right of the current phrase.The reordering model is applied only at the phrase boundaries.We assume that the reordering within the phrases is correct.In the remaining part of this section, we will describe the details of this reordering model.The classes our model predicts will be defined in Section 4.2.Then, the feature functions will be defined in Section 4.3.The training criterion and the training events of the maximum entropy model will be described in Section 4.4.Ideally, this model predicts the start position of the next phrase.But as predicting the exact position is rather difficult, we group the possible start positions into classes.In the simplest case, we use only two classes.One class for the positions to the left and one class for the positions to the right.As a refinement, we can use four classes instead of two: 1) one position to the left, 2) more than one positions to the left, 3) one position to the right, 4) more than one positions to the right.In general, we use a parameter D to specify 2 · D classes of the types: Let cj,j′ denote the orientation class for a movement from source position j to source position j′ as illustrated in Figure 1.In the case of two orientation classes, cj,j′ is defined as: r left, if j′ < j cj,j′ =right, if j′ > j Then, the reordering model has the form p(cj,j′|fJ1 , eI1, i, j) A well-founded framework for directly modeling the probability p(cj,j′|fJ1 , eI1, i, j) is maximum entropy (Berger et al., 1996).In this framework, we have a set of N feature functions hn(fJ1 , eI1, i, j, cj,j′), n = 1, ... , N. Each feature function hn is weighted with a factor λn.The resulting model is: The functional form is identical to Equation 2, but here we will use a large number of binary features, whereas in Equation 2 usually only a very small number of real-valued features is used.More precisely, the resulting reordering model pλN1 (cj,j′|fJ1 , eI1, i, j) is used as an additional component in the log-linear combination of Equation 2.The feature functions of the reordering model depend on the last alignment link (j, i) of a phrase.Note that the source position j is not necessarily the end position of the source phrase.We use the source position j which is aligned to the last word of the target phrase in target position i.The illustration in Figure 1 contains such an example.To introduce generalization capabilities, some of the features will depend on word classes or partof-speech information.Let F1J denote the word class sequence that corresponds to the source language sentence fJ1 and let EI1 denote the target word class sequence that corresponds to the target language sentence eI1.Then, the feature functions are of the form hn(fJ1 , eI1, F1J , EI1, i, j, j′).We consider the following binary features: Here, δ(', ') denotes the Kronecker-function.In the experiments, we will use d E {−1, 0, 11.Many other feature functions are imaginable, e.g. combinations of the described feature functions, n-gram or multi-word features, joint source and target language feature functions.As training criterion, we use the maximum class posterior probability.This corresponds to maximizing the likelihood of the maximum entropy model.Since the optimization criterion is convex, there is only a single optimum and no convergence problems occur.To train the model parameters λN1 , we use the Generalized Iterative Scaling (GIS) algorithm (Darroch and Ratcliff, 1972).In practice, the training procedure tends to result in an overfitted model.To avoid overfitting, (Chen and Rosenfeld, 1999) have suggested a smoothing method where a Gaussian prior distribution of the parameters is assumed.This method tried to avoid very large lambda values and prevents features that occur only once for a specific class from getting a value of infinity.We train IBM Model 4 with GIZA++ (Och and Ney, 2003) in both translation directions.Then the alignments are symmetrized using a refined heuristic as described in (Och and Ney, 2003).This wordaligned bilingual corpus is used to train the reordering model parameters, i.e. the feature weights λN1 .Each alignment link defines an event for the maximum entropy training.An exception are the oneto-many alignments, i.e. one source word is aligned to multiple target words.In this case, only the topmost alignment link is considered because the other ones cannot occur at a phrase boundary.Many-toone and many-to-many alignments are handled in a similar way.The experiments were carried out on the Basic Travel Expression Corpus (BTEC) task (Takezawa et al., 2002).This is a multilingual speech corpus which contains tourism-related sentences similar to those that are found in phrase books.We use the Arabic-English, the Chinese-English and the Japanese-English data.The corpus statistics are shown in Table 1.As the BTEC is a rather clean corpus, the preprocessing consisted mainly of tokenization, i.e., separating punctuation marks from words.Additionally, we replaced contractions such as it’s or I’m in the English corpus and we removed the case information.For Arabic, we removed the diacritics and we split common prefixes: Al, w, f, b, l. There was no special preprocessing for the Chinese and the Japanese training corpora.To train and evaluate the reordering model, we use the word aligned bilingual training corpus.For evaluating the classification power of the reordering model, we partition the corpus into a training part and a test part.In our experiments, we use about 10% of the corpus for testing and the remaining part for training the feature weights of the reordering model with the GIS algorithm using YASMET (Och, 2001).The statistics of the training and test alignment links is shown in Table 2.The number of training events ranges from 119K for JapaneseEnglish to 144K for Arabic-English.The word classes for the class-based features are trained using the mkcls tool (Och, 1999).In the experiments, we use 50 word classes.Alternatively, one could use part-of-speech information for this purpose.Additional experiments were carried out on the large data track of the Chinese-English NIST task.The corpus statistics of the bilingual training corpus are shown in Table 3.The language model was trained on the English part of the bilingual training corpus and additional monolingual English data from the GigaWord corpus.The total amount of language model training data was about 600M running words.We use a fourgram language model with modified Kneser-Ney smoothing as implemented in the SRILM toolkit (Stolcke, 2002).For the four English reference translations of the evaluation sets, the accumulated statistics are presented.In this section, we present the classification results for the three language pairs.In Table 4, we present the classification results for two orientation classes.As baseline we always choose the most frequent orientation class.For Arabic-English, the baseline is with 6.3% already very low.This means that the word order in Arabic is very similar to the word order in English.For Chinese-English, the baseline is with 12.7% about twice as large.The most differences in word order occur for Japanese-English.This seems to be reasonable as Japanese has usually a different sentence structure, subject-objectverb compared to subject-verb-object in English.For each language pair, we present results for several combination of features.The three columns per language pair indicate if the features are based on the words (column label ’Words’), on the word classes (column label ’Classes’) or on both (column label ’W+C’).We also distinguish if the features depend on the target sentence (’Tgt’), on the source sentence (’Src’) or on both (’Src+Tgt’).For Arabic-English, using features based only on words of the target sentence the classification error rate can be reduced to 4.5%.If the features are based only on the source sentence words, a classification error rate of 2.9% is reached.Combining the features based on source and target sentence words, a classification error rate of 2.8% can be achieved.Adding the features based on word classes, the classification error rate can be further improved to 2.1%.For the other language pairs, the results are similar except that the absolute values of the classification error rates are higher.We observe the following: These are desirable properties of an appropriate reordering model.The main point is that these are fulfilled not only on the training data, but on unseen test data.There seems to be no overfitting problem.In Table 5, we present the results for four orientation classes.The final error rates are a factor 2-4 larger than for two orientation classes.Despite that we observe the same tendencies as for two orientation classes.Again, using more features always helps to improve the performance.For the translation experiments on the BTEC task, we report the two accuracy measures BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) as well as the two error rates: word error rate (WER) and position-independent word error rate (PER).These criteria are computed with respect to 16 references.In Table 6, we show the translation results for the BTEC task.In these experiments, the reordering model uses two orientation classes, i.e. it predicts either a left or a right orientation.The features for the maximum-entropy based reordering model are based on the source and target language words within a window of one.The word-class based features are not used for the translation experiments.The maximum-entropy based reordering model achieves small but consistent improvement for all the evaluation criteria.Note that the baseline system, i.e. using the distance-based reordering, was among the best systems in the IWSLT 2005 evaluation campaign (Eck and Hori, 2005).Some translation examples are presented in Table 7.We observe that the system using the maximum-entropy based reordering model produces more fluent translations.Additional translation experiments were carried out on the large data track of the Chinese-English NIST task.For this task, we use only the BLEU and NIST scores.Both scores are computed caseinsensitive with respect to four reference translations using the mteval-v11b tool1.For the NIST task, we use the BLEU score as primary criterion which is optimized on the NIST 2002 evaluation set using the Downhill Simplex algorithm (Press et al., 2002).Note that only the eight or nine model scaling factors of Equation 2 are optimized using the Downhill Simplex algorithm.The feature weights of the reordering model are trained using the GIS algorithm as described in Section 4.4.We use a state-of-the-art baseline system which would have obtained a good rank in the last NIST evaluation (NIST, 2005).The translation results for the NIST task are presented in Table 8.We observe consistent improvements of the BLEU score on all evaluation sets.The overall improvement due to reordering ranges from 1.2% to 2.0% absolute.The contribution of the maximum-entropy based reordering model to this improvement is in the range of 25% to 58%, e.g. for the NIST 2003 evaluation set about 58% of the improvement using reordering can be attributed to the maximum-entropy based reordering model.We also measured the classification performance for the NIST task.The general tendencies are identical to the BTEC task.We have presented a novel discriminative reordering model for statistical machine translation.This model is trained on the word aligned bilingual corpus using the maximum entropy principle.Several types of features have been used: We have evaluated the performance of the reordering model on a held-out word-aligned corpus.We have shown that the model is able to predict the orientation very well, e.g. for Arabic-English the classification error rate is only 2.1%.We presented improved translation results for three language pairs on the BTEC task and for the large data track of the Chinese-English NIST task.In none of the cases additional features have hurt the classification performance on the held-out test corpus.This is a strong evidence that the maximum entropy framework is suitable for this task.Another advantage of our approach is the generalization capability via the use of word classes or part-of-speech information.Furthermore, additional features can be easily integrated into the maximum entropy framework.So far, the word classes were not used for the translation experiments.As the word classes help for the classification task, we might expect further improvements of the translation results.Using partof-speech information instead (or in addition) to the automatically computed word classes might also be beneficial.More fine-tuning of the reordering model toward translation quality might also result in improvements.As already mentioned in Section 4.3, a richer feature set could be helpful.This material is partly based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.HR0011-06-C-0023, and was partly funded by the European Union under the integrated project TC-STAR (Technology and Corpora for Speech to Speech Translation, IST2002-FP6-506738, http://www.tc-star.org).
A syntax-directed translator first parses the source-language input into a parsetree, and then recursively converts the tree into a string in the target-language.We model this conversion by an extended treeto-string transducer that have multi-level trees on the source-side, which gives our system more expressive power and flexibility.We also define a direct probability model and use a linear-time dynamic programming algorithm to search for the best derivation.The model is then extended to the general log-linear framework in order to rescore with other features like n-gram language models.We devise a simple-yet-effective algorithm to generate non-duplicate k-best translations for n-gram rescoring.Initial experimental results on English-to-Chinese translation are presented.The concept of syntax-directed (SD) translation was originally proposed in compiling (Irons, 1961; Lewis and Stearns, 1968), where the source program is parsed into a tree representation that guides the generation of the object code.Following Aho and Ullman (1972), a translation, as a set of string pairs, can be specified by a syntax-directed translation schema (SDTS), which is essentially a synchronous context-free grammar (SCFG) that generates two languages simultaneously.An SDTS also induces a translator, a device that performs the transformation from input string to output string.In this context, an SD translator consists of two components, a sourcelanguage parser and a recursive converter which is usually modeled as a top-down tree-to-string transducer (G´ecseg and Steinby, 1984).The relationship among these concepts is illustrated in Fig.1.This paper adapts the idea of syntax-directed translator to statistical machine translation (MT).We apply stochastic operations at each node of the source-language parse-tree and search for the best derivation (a sequence of translation steps) that converts the whole tree into some target-language string with the highest probability.However, the structural divergence across languages often results in nonisomorphic parse-trees that is beyond the power of SCFGs.For example, the S(VO) structure in English is translated into a VSO word-order in Arabic, an instance of complex reordering not captured by any SCFG (Fig.2).To alleviate the non-isomorphism problem, (synchronous) grammars with richer expressive power have been proposed whose rules apply to larger fragments of the tree.For example, Shieber and Schabes (1990) introduce synchronous tree-adjoining grammar (STAG) and Eisner (2003) uses a synchronous tree-substitution grammar (STSG), which is a restricted version of STAG with no adjunctions.STSGs and STAGs generate more tree relations than SCFGs, e.g. the non-isomorphic tree pair in Fig.2.This extra expressive power lies in the extended domain of locality (EDL) (Joshi and Schabes, 1997), i.e., elementary structures beyond the scope of onelevel context-free productions.Besides being linguistically motivated, the need for EDL is also supported by empirical findings in MT that one-level rules are often inadequate (Fox, 2002; Galley et al., 2004).Similarly, in the tree-transducer terminology, Graehl and Knight (2004) define extended tree transducers that have multi-level trees on the source-side.Since an SD translator separates the sourcelanguage analysis from the recursive transformation, the domains of locality in these two modules are orthogonal to each other: in this work, we use a CFGbased Treebank parser but focuses on the extended domain in the recursive converter.Following Galley et al. (2004), we use a special class of extended tree-to-string transducer (zRs for short) with multilevel left-hand-side (LHS) trees.1 Since the righthand-side (RHS) string can be viewed as a flat onelevel tree with the same nonterminal root from LHS (Fig.2), this framework is closely related to STSGs: they both have extended domain of locality on the source-side, while our framework remains as a CFG on the target-side.For instance, an equivalent zRs rule for the complex reordering in Fig.2 would be While Section 3 will define the model formally, we first proceed with an example translation from English to Chinese (note in particular that the inverted phrases between source and target): 1Throughout this paper, we will use LHS and source-side interchangeably (so are RHS and target-side).In accordance with our experiments, we also use English and Chinese as the source and target languages, opposite to the Foreign-to-English convention of Brown et al. (1993).Figure 3 shows how the translator works.The English sentence (a) is first parsed into the tree in (b), which is then recursively converted into the Chinese string in (e) through five steps.First, at the root node, we apply the rule r1 which preserves the toplevel word-order and translates the English period into its Chinese counterpart: Then, the rule r2 grabs the whole sub-tree for “the gunman” and translates it as a phrase: (r2) NP-C ( DT (the) NN (gunman) ) —* qiangshou Now we get a “partial Chinese, partial English” sentence “qiangshou VP o” as shown in Fig.3 (c).Our recursion goes on to translate the VP sub-tree.Here we use the rule r3 for the passive construction: which captures the fact that the agent (NP-C, “the police”) and the verb (VBN, “killed”) are always inverted between English and Chinese in a passive voice.Finally, we apply rules r� and r5 which perform phrasal translations for the two remaining subtrees in (d), respectively, and get the completed Chinese string in (e).It is helpful to compare this approach with recent efforts in statistical MT.Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) are good at learning local translations that are pairs of (consecutive) sub-strings, but often insufficient in modeling the reorderings of phrases themselves, especially between language pairs with very different word-order.This is because the generative capacity of these models lies within the realm of finite-state machinery (Kumar and Byrne, 2003), which is unable to process nested structures and long-distance dependencies in natural languages.Syntax-based models aim to alleviate this problem by exploiting the power of synchronous rewriting systems.Both Yamada and Knight (2001) and Chiang (2005) use SCFGs as the underlying model, so their translation schemata are syntax-directed as in Fig.1, but their translators are not: both systems do parsing and transformation in a joint search, essentially over a packed forest of parse-trees.To this end, their translators are not directed by a syntactic tree.Although their method potentially considers more than one single parse-tree as in our case, the packed representation of the forest restricts the scope of each transfer step to a one-level contextfree rule, while our approach decouples the sourcelanguage analyzer and the recursive converter, so that the latter can have an extended domain of locality.In addition, our translator also enjoys a speedup by this decoupling, with each of the two stages having a smaller search space.In fact, the recursive transfer step can be done by a a linear-time algorithm (see Section 5), and the parsing step is also fast with the modern Treebank parsers, for instance (Collins, 1999; Charniak, 2000).In contrast, their decodings are reported to be computationally expensive and Chiang (2005) uses aggressive pruning to make it tractable.There also exists a compromise between these two approaches, which uses a k-best list of parse trees (for a relatively small k) to approximate the full forest (see future work).Besides, our model, as being linguistically motivated, is also more expressive than the formally syntax-based models of Chiang (2005) and Wu (1997).Consider, again, the passive example in rule r3.In Chiang’s SCFG, there is only one nonterminal X, so a corresponding rule would be ( was X(1) by X(2), bei X(2) X(1) ) which can also pattern-match the English sentence: I was [asleep]1 by [sunset]2 . and translate it into Chinese as a passive voice.This produces very odd Chinese translation, because here “was A by B” in the English sentence is not a passive construction.By contrast, our model applies rule r3 only if A is a past participle (VBN) and B is a noun phrase (NP-C).This example also shows that, one-level SCFG rule, even if informed by the Treebank as in (Yamada and Knight, 2001), is not enough to capture a common construction like this which is five levels deep (from VP to “by”).There are also some variations of syntax-directed translators where dependency structures are used in place of constituent trees (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005).Although they share with this work the basic motivations and similar speed-up, it is difficult to specify re-ordering information within dependency elementary structures, so they either resort to heuristics (Lin) or a separate ordering model for linearization (the other two works).2 Our approach, in contrast, explicitly models the re-ordering of sub-trees within individual transfer rules.In this section, we define the formal machinery of our recursive transformation model as a special case of xRs transducers (Graehl and Knight, 2004) that has only one state, and each rule is linear (L) and non-deleting (N) with regarding to variables in the source and target sides (henth the name 1-xRLNs).We require each variable xi E X occurs exactly once in t and exactly once in s (linear and non-deleting).We denote ρ(t) to be the root symbol of tree t. When writing these rules, we avoid notational overhead by introducing a short-hand form from Galley et al. (2004) that integrates the mapping into the tree, which is used throughout Section 1.Following TSG terminology (see Figure 2), we call these “variable nodes” such as x2:NP-C substitution nodes, since when applying a rule to a tree, these nodes will be matched with a sub-tree with the same root symbol.We also define |X  |to be the rank of the rule, i.e., the number of variables in it.For example, rules r1 and r3 in Section 1 are both of rank 2.If a rule has no variable, i.e., it is of rank zero, then it is called a purely lexical rule, which performs a phrasal translation as in phrase-based models.Rule r2, for instance, can be thought of as a phrase pair (the gunman, qiangshou).Informally speaking, a derivation in a transducer is a sequence of steps converting a source-language tree into a target-language string, with each step applying one tranduction rule.However, it can also be formalized as a tree, following the notion of derivation-tree in TAG (Joshi and Schabes, 1997): Definition 2.A derivation d, its source and target projections, noted £(d) and C(d) respectively, are recursively defined as follows: derivation with the root symbol of its source projection matches the corresponding substitution node in r, i.e., ρ(£(di)) = φ(xi), then d = r(d1, ... , dm) is also a derivation, where £(d) = [xi H £(di)]t and C(d) = [xi H C(di)]s. Note that we use a short-hand notation [xi H yi]t to denote the result of substituting each xi with yi in t, where xi ranges over all variables in t. For example, Figure 4 shows two derivations for the sentence pair in Example (1).In both cases, the source projection is the English tree in Figure 3 (b), and the target projection is the Chinese translation.Galley et al. (2004) presents a linear-time algorithm for automatic extraction of these xRs rules from a parallel corpora with word-alignment and parse-trees on the source-side, which will be used in our experiments in Section 6.Departing from the conventional noisy-channel approach of Brown et al. (1993), our basic model is a direct one: where e is the English input string and c* is the best Chinese translation according to the translation model Pr(c  |e).We now marginalize over all English parse trees T (e) that yield the sentence e: Rather than taking the sum, we pick the best tree T* and factors the search into two separate steps: parsing (4) (a well-studied problem) and tree-to-string translation (5) (Section 5): In this sense, our approach can be considered as a Viterbi approximation of the computationally expensive joint search using (3) directly.Similarly, we now marginalize over all derivations that translates English tree T into some Chinese string and apply the Viterbi approximation again to search for the best derivation d*: Assuming different rules in a derivation are applied independently, we approximate Pr(d) as where the probability Pr(r) of the rule r is estimated by conditioning on the root symbol p(t(r)):Following Och and Ney (2002), we extend the direct model into a general log-linear framework in order to incorporate other features: where Pr(c) is the language model and e−λ|c |is the length penalty term based on |c|, the length of the translation.Parameters a, Q, and A are the weights of relevant features.Note that positive A prefers longer translations.We use a standard trigram model for Pr(c).We first present a linear-time algorithm for searching the best derivation under the direct model, and then extend it to the log-linear case by a new variant of k-best parsing.Since our probability model is not based on the noisy channel, we do not call our search module a “decoder” as in most statistical MT work.Instead, readers who speak English but not Chinese can view it as an “encoder” (or encryptor), which corresponds exactly to our direct model.Given a fixed parse-tree T*, we are to search for the best derivation with the highest probability.This can be done by a simple top-down traversal (or depth-first search) from the root of T*: at each node q in T*, try each possible rule r whose Englishside pattern t(r) matches the subtree T*η rooted at q, and recursively visit each descendant node qi in T*η that corresponds to a variable in t(r).We then collect the resulting target-language strings and plug them into the Chinese-side s(r) of rule r, getting a translation for the subtree T*η.We finally take the best of all translations.With the extended LHS of our transducer, there may be many different rules applicable at one tree node.For example, consider the VP subtree in Fig.3 (c), where both r3 and r6 can apply.As a result, the number of derivations is exponential in the size of the tree, since there are exponentially many decompositions of the tree for a given set of rules.This problem can be solved by memoization (Cormen et al., 2001): we cache each subtree that has been visited before, so that every tree node is visited at most once.This results in a dynamic programming algorithm that is guaranteed to run in O(npq) time where n is the size of the parse tree, p is the maximum number of rules applicable to one tree node, and q is the maximum size of an applicable rule.For a given rule-set, this algorithm runs in time linear to the length of the input sentence, since p and q are considered grammar constants, and n is proportional to the input length.The full pseudocode is worked out in Algorithm 1.A restricted version of this algorithm first appears in compiling for optimal code generation from expression-trees (Aho and Johnson, 1976).In computational linguistics, the bottom-up version of this algorithm resembles the tree parsing algorithm for TSG by Eisner (2003).Similar algorithms have also been proposed for dependency-based translation (Lin, 2004; Ding and Palmer, 2005).Under the log-linear model, one still prefers to search for the globally best derivation d*: However, integrating the n-gram model with the translation model in the search is computationally very expensive.As a standard alternative, rather than aiming at the exact best derivation, we search for top-k derivations under the direct model using Algorithm 1, and then rerank the k-best list with the language model and length penalty.Like other instances of dynamic programming, Algorithm 1 can be viewed as a hypergraph search problem.To this end, we use an efficient algorithm by Huang and Chiang (2005, Algorithm 3) that solves the general k-best derivations problem in monotonic hypergraphs.It consists of a normal forward phase for the 1-best derivation and a recursive backward phase for the 2nd, 3rd, ..., kth derivations.Unfortunately, different derivations may have the same yield (a problem called spurious ambiguity), due to multi-level LHS of our rules.In practice, this results in a very small ratio of unique strings among top-k derivations.To alleviate this problem, determinization techniques have been proposed by Mohri and Riley (2002) for finite-state automata and extended to tree automata by May and Knight (2006).These methods eliminate spurious ambiguity by effectively transforming the grammar into an equivalent deterministic form.However, this transformation often leads to a blow-up in forest size, which is exponential to the original size in the worst-case.So instead of determinization, here we present a simple-yet-effective extension to the Algorithm 3 of Huang and Chiang (2005) that guarantees to output unique translated strings: This method should work in general for any equivalence relation (say, same derived tree) that can be defined on derivations.Our experiments are on English-to-Chinese translation, the opposite direction to most of the recent work in SMT.We are not doing the reverse direction at this time partly due to the lack of a sufficiently good parser for Chinese.Our training set is a Chinese-English parallel corpus with 1.95M aligned sentences (28.3M words on the English side).We first word-align them by GIZA++, then parse the English side by a variant of Collins (1999) parser, and finally apply the rule-extraction algorithm of Galley et al. (2004).The resulting rule set has 24.7M xRs rules.We also use the SRI Language Modeling Toolkit (Stolcke, 2002) to train a Chinese trigram model with Knesser-Ney smoothing on the Chinese side of the parallel corpus.Our evaluation data consists of 140 short sentences (< 25 Chinese words) of the Xinhua portion of the NIST 2003 Chinese-to-English evaluation set.Since we are translating in the other direction, we use the first English reference as the source input and the Chinese as the single reference.We implemented our system as follows: for each input sentence, we first run Algorithm 1, which returns the 1-best translation and also builds the derivation forest of all translations for this sentence.Then we extract the top 5000 non-duplicate translated strings from this forest and rescore them with the trigram model and the length penalty.We compared our system with a state-of-the-art phrase-based system Pharaoh (Koehn, 2004) on the evaluation data.Since the target language is Chinese, we report character-based BLEU score instead of word-based to ensure our results are independent of Chinese tokenizations (although our language models are word-based).The BLEU scores are based on single reference and up to 4-gram precisions (r1n4).Feature weights of both systems are tuned on the same data set.3 For Pharaoh, we use the standard minimum error-rate training (Och, 2003); and for our system, since there are only two independent features (as we always fix α = 1), we use a simple grid-based line-optimization along the language-model weight axis.For a given languagemodel weight Q, we use binary search to find the best length penalty A that leads to a length-ratio closest to 1 against the reference.The results are summarized in Table 1.The rescored translations are better than the 1-best results from the direct model, but still slightly worse than Pharaoh.This paper presents an adaptation of the classic syntax-directed translation with linguisticallymotivated formalisms for statistical MT.Currently we are doing larger-scale experiments.We are also investigating more principled algorithms for integrating n-gram language models during the search, rather than k-best rescoring.Besides, we will extend this work to translating the top k parse trees, instead of committing to the 1-best tree, as parsing errors certainly affect translation quality.

A Maximum-Entropy-Inspired Parser *We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal treebank.This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40, and 89.5% for sentences of length < 100, when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].Following [5,10], our parser is based upon a probabilistic generative model.That is, for all sentences s and all parses 7r, the parser assigns a probability p(s , 7r) = p(r), the equality holding when we restrict consideration to 7r whose yield * This research was supported in part by NSF grant LIS SBR 9720368.The author would like to thank Mark Johnson and all the rest of the Brown Laboratory for Linguistic Information Processing. is s. Then for any s the parser returns the parse ir that maximizes this probability.That is, the parser implements the function arg maxrp(7r s) = arg maxirp(7r, s) = arg maxrp(w).What fundamentally distinguishes probabilistic generative parsers is how they compute p(r), and it is to that topic we turn next.The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c, t(c) (t for &quot;tag&quot;), then the lexical head of c, h(c), and then the expansion of c into further constituents e(c).Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g., whether it is a noun phrase (np), verb-phrase, etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.Much of the interesting work is determining what goes into H (c).Whenever it is clear to which constituent we are referring we omit the (c) in, e.g., h(c).In this notation the above equation takes the following form: Next we describe how we assign a probability to the expansion e of a constituent.In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.The method that gives the best results, however, uses a Markov grammar — a method for assigning probabilities to any possible expansion using statistics gathered from the training corpus [6,10,15].The method we use follows that of [10].In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar, and a right-hand side that is a sequence of one or more such symbols.(We assume that all terminal symbols are generated by rules of the form &quot;preterm word&quot; and we treat these as a special case.)For us the non-terminal symbols are those of the tree-bank, augmented by the symbols aux and auxg, which have been assigned deterministically to certain auxiliary verbs such as &quot;have&quot; or &quot;having&quot;.For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).M(c) is the constituent from which the head lexical item h is obtained according to deterministic rules that pick the head of a constituent from among the heads of its children.To the left of M is a sequence of one or more left labels Li (c) including the special termination symbol A, which indicates that there are no more symbols to the left, and similarly for the labels to the right, Ri(c).Thus an expansion e(c) looks like: The expansion is generated by guessing first M, then in order L1 through L,„.+1 (= A), and similarly for RI through In a pure Markov PCFG we are given the left-hand side label 1 and then probabilistically generate the right-hand side conditioning on no information other than 1 and (possibly) previously generated pieces of the right-hand side itself.In the simplest of such models, a zeroorder Markov grammar, each label on the righthand side is generated conditioned only on / — that is, according to the distributions p(Li j1), p(M I 1), and p(Ri I 1).More generally, one can condition on the m previously generated labels, thereby obtaining an mth-order Markov grammar.So, for example, in a second-order Markov PCFG, L2 would be conditioned on L1 and M. In our complete model, of course, the probability of each label in the expansions is also conditioned on other material as specified in Equation 1, e.g., p(e t, h, H).Thus we would use p(L2 I L1, M, 1, t, h, H).Note that the As on both ends of the expansion in Expression 2 are conditioned just like any other label in the expansion.The major problem confronting the author of a generative parser is what information to use to condition the probabilities required in the model, and how to smooth the empirically obtained probabilities to take the sting out of the sparse data problems that are inevitable with even the most modest conditioning.For example, in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1, M,1, t, h, H).Also, remember that H is a pla,ceholder for any other information beyond the constituent c that may be useful in assigning c a probability.In the past few years the maximum entropy, or log-linear, approach has recommended itself to probabilistic model builders for its flexibility and its novel approach to smoothing [1,17].A complete review of log-linear models is beyond the scope of this paper.Rather, we concentrate on the aspects of these models that most directly influenced the model presented here.To compute a probability in a log-linear model one first defines a set of &quot;features&quot;, functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input.In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does.In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.For example, in computing the probability of the head's pre-terminal t we might want a feature schema f (t, 1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1, and zero otherwise.This feature is obviously composed of two sub-features, one recognizing t, the other 1.If both return 1, then the feature returns 1.Now consider computing a conditional probability p(a H) with a set of features h that connect a to the history H. In a log-linear model the probability function takes the following form: Here the Ai are weights between negative and positive infinity that indicate the relative importance of a feature: the more relevant the feature to the value of the probability, the higher the absolute value of the associated A.The function Z(H), called the partition function, is a normalizing constant (for fixed H), so the probabilities over all a sum to one.Now for our purposes it is useful to rewrite this as a sequence of multiplicative functions gi(a, H) for 0 < i < j: Here go(a, H) = 11Z (H) and gi(a, H) = eAi(a,H) fi(°,11).The intuitive idea is that each factor gi is larger than one if the feature in question makes the probability more likely, one if the feature has no effect, and smaller than one if it makes the probability less likely.Maximum-entropy models have two benefits for a parser builder.First, as already implicit in our discussion, factoring the probability computation into a sequence of values corresponding to various &quot;features&quot; suggests that the probability model should be easily changeable — just change the set of features used.This point is emphasized by Ratnaparkhi in discussing his parser [17).Second, and this is a point we have not yet mentioned, the features used in these models need have no particular independence of one another.This is useful if one is using a loglinear model for smoothing.That is, suppose we want to compute a conditional probability p(a b, c), but we are not sure that we have enough examples of the conditioning event b, c in the training corpus to ensure that the empirically obtained probability P (a I b, c) is accurate.The traditional way to handle this is also to compute P(a b), and perhaps P(a c) as well, and take some combination of these values as one's best estimate for p(a I b, c).This method is known as &quot;deleted interpolation&quot; smoothing.In max-entropy models one can simply include features for all three events f1 (a, b, c), f2 (a, b), and f3(a, c) and combine them in the model according to Equation 3, or equivalently, Equation 4.The fact that the features are very far from independent is not a concern.Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.As it stands, this last equation is pretty much content-free.But let us look at how it works for a particular case in our parsing scheme.Consider the probability distribution for choosing the pre-terminal for the head of a constituent.In Equation 1 we wrote this as p(t I 1, H).As we discuss in more detail in Section 5, several different features in the context surrounding c are useful to include in H: the label, head pre-terminal and head of the parent of c (denoted as lp, tp, hp), the label of c's left sibling (lb for &quot;before&quot;), and the label of the grandparent of c (la).That is, we wish to compute p(t 1, lp, tp, lb, lg, hp).We can now rewrite this in the form of Equation 5 as follows: Here we have sequentially conditioned on steadily increasing portions of c's history.In many cases this is clearly warranted.For example, it does not seem to make much sense to condition on, say, hp without first conditioning on ti,.In other cases, however, we seem to be conditioning on apples and oranges, so to speak.For example, one can well imagine that one might want to condition on the parent's lexical head without conditioning on the left sibling, or the grandparent label.One way to do this is to modify the simple version shown in Equation 6 to allow this: Note the changes to the last three terms in Equation 7.Rather than conditioning each term on the previous ones, they are now conditioned only on those aspects of the history that seem most relevant.The hope is that by doing this we will have less difficulty with the splitting of conditioning events, and thus somewhat less difficulty with sparse data.We make one more point on the connection of Equation 7 to a maximum entropy formulation.Suppose we were, in fact, going to compute a true maximum entropy model based upon the features used in Equation 7, Ii (t,1), f2(t,1,1p), f3(t,1,lp) ....This requires finding the appropriate Ais for Equation 3, which is accomplished using an algorithm such as iterative scaling [II] in which values for the Ai are initially &quot;guessed&quot; and then modified until they converge on stable values.With no prior knowledge of values for the Ai one traditionally starts with Ai = 0, this being a neutral assumption that the feature has neither a positive nor negative impact on the probability in question.With some prior knowledge, non-zero values can greatly speed up this process because fewer iterations are required for convergence.We comment on this because in our example we can substantially speed up the process by choosing values picked so that, when the maximum-entropy equation is expressed in the form of Equation 4, the gi have as their initial values the values of the corresponding terms in Equation 7.(Our experience is that rather than requiring 50 or so iterations, three suffice.)Now we observe that if we were to use a maximum-entropy approach but run iterative scaling zero times, we would, in fact, just have Equation 7.The major advantage of using Equation 7 is that one can generally get away without computing the partition function Z(H).In the simple (content-free) form (Equation 6), it is clear that Z(H) = 1.In the more interesting version, Equation 7, this is not true in general, but one would not expect it to differ much from one, and we assume that as long as we are not publishing the raw probabilities (as we would be doing, for example, in publishing perplexity results) the difference from one should be unimportant.As partition-function calculation is typically the major on-line computational problem for maximum-entropy models, this simplifies the model significantly.Naturally, the distributions required by Equation 7 cannot be used without smoothing.In a pure maximum-entropy model this is done by feature selection, as in Ratnaparkhi's maximum-entropy parser [17].While we could have smoothed in the same fashion, we choose instead to use standard deleted interpolation.(Actually, we use a minor variant described in [4].)We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.This allows the second pass to see expansions not present in the training corpus.We use the gathered statistics for all observed words, even those with very low counts, though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.We guess the preterminals of words that are not observed in the training data using statistics on capitalization, hyphenation, word endings (the last two letters), and the probability that a given pre-terminal is realized using a previously unobserved word.As noted above, the probability model uses five smoothed probability distributions, one each for Li, M,Ri,t, and h. The equation for the (unsmoothed) conditional probability distribution for t is given in Equation 7.The other four equations can be found in a longer version of this paper available on the author's website (www.cs.brown.eduhiec).L and R are conditioned on three previous labels so we are using a third-order Markov grammar.Also, the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events.This is due to the importance of this factor in parsing, as noted in, e.g., [14].In keeping with the standard methodology [5, 9,10,15,17], we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training, section 23 for testing, and section 24 for development (debugging and tuning).Performance on the test corpus is measured using the standard measures from [5,9,10,17].In particular, we measure labeled precision (LP) and recall (LR), average number of crossbrackets per sentence (CB), percentage of sentences with zero cross brackets (OCB), and percentage of sentences with < 2 cross brackets (2CB).Again as standard, we take separate measurements for all sentences of length < 40 and all sentences of length < 100.Note that the definitions of labeled precision and recall are those given in [9] and used in all of the previous work.As noted in [5], these definitions typically give results about 0.4% higher than the more obvious ones.The results for the new parser as well as for the previous top-three individual parsers on this corpus are given in Figure 1.As is typical, all of the standard measures tell pretty much the same story, with the new parser outperforming the other three parsers.Looking in particular at the precision and recall figures, the new parser's give us a 13% error reduction over the best of the previous work, Co1199 [9].In the previous sections we have concentrated on the relation of the parser to a maximumentropy approach, the aspect of the parser that is most novel.However, we do not think this aspect is the sole or even the most important reason for its comparative success.Here we list what we believe to be the most significant contributions and give some experimental results on how well the program behaves without them.We take as our starting point the parser labled Char97 in Figure 1 [5], as that is the program from which our current parser derives.That parser, as stated in Figure 1, achieves an average precision/recall of 87.5.As noted in [5], that system is based upon a &quot;tree-bank grammar&quot; - a grammar read directly off the training corpus.This is as opposed to the &quot;Markovgrammar&quot; approach used in the current parser.Also, the earlier parser uses two techniques not employed in the current parser.First, it uses a clustering scheme on words to give the system a &quot;soft&quot; clustering of heads and sub-heads.(It is &quot;soft&quot; clustering in that a word can belong to more than one cluster with different weights - the weights express the probability of producing the word given that one is going to produce a word from that cluster.)Second, Char97 uses unsupervised learning in that the original system was run on about thirty million words of unparsed text, the output was taken as &quot;correct&quot;, and statistics were collected on the resulting parses.Without these enhancements Char97 performs at the 86.6% level for sentences of length < 40.In this section we evaluate the effects of the various changes we have made by running various versions of our current program.To avoid repeated evaluations based upon the testing corpus, here our evaluation is based upon sentences of length < 40 from the development corpus.We note here that this corpus is somewhat more difficult than the &quot;official&quot; test corpus.For example, the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.This is indicated in Figure 2, where the model labeled &quot;Best&quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%, 0.4% lower than the results on the official test corpus.This is in accord with our experience that developmentcorpus results are from 0.3% to 0.5% lower than those obtained on the test corpus.The model labeled &quot;Old&quot; attempts to recreate the Char97 system using the current program.It makes no use of special maximum-entropyinspired features (though their presence made it much easier to perform these experiments), it does not guess the pre-terminal before guessing the lexical head, and it uses a tree-bank grammar rather than a Markov grammar.This parser achieves an average precision/recall of 86.2%.This is consistent with the average precision/recall of 86.6% for [5] mentioned above, as the latter was on the test corpus and the former on the development corpus.Between the Old model and the Best model, Figure 2 gives precision/recall measurements for several different versions of our parser.One of the first and without doubt the most significant change we made in the current parser is to move from two stages of probabilistic decisions at each node to three.As already noted, Char97 first guesses the lexical head of a constituent and then, given the head, guesses the PCFG rule used to expand the constituent in question.In contrast, the current parser first guesses the head's pre-terminal, then the head, and then the expansion.It turns out that usefulness of this process had already been discovered by Collins [10], who in turn notes (personal communication) that it was previously used by Eisner [12].However, Collins in [10] does not stress the decision to guess the head's pre-terminal first, and it might be lost on the casual reader.Indeed, it was lost on the present author until he went back after the fact and found it there.In Figure 2 we show that this one factor improves performance by nearly 2%.It may not be obvious why this should make so great a difference, since most words are effectively unambiguous.(For example, part-ofspeech tagging using the most probable preterminal for each word is 90% accurate [8].)We believe that two factors contribute to this performance gain.The first is simply that if we first guess the pre-terminal, when we go to guess the head the first thing we can condition upon is the pre-terminal, i.e., we compute p(h I t).This quantity is a relatively intuitive one (as, for example, it is the quantity used in a PCFG to relate words to their pre-terminals) and it seems particularly good to condition upon here since we use it, in effect, as the unsmoothed probability upon which all smoothing of p(h) is based.This one &quot;fix&quot; makes slightly over a percent difference in the results.The second major reason why first guessing the pre-terminal makes so much difference is that it can be used when backing off the lexical head in computing the probability of the rule expansion.For example, when we first guess the lexical head we can move from computing p(r I 1,1p, h) to p(r I /, t, /p, h).So, e.g., even if the word &quot;conflating&quot; does not appear in the training corpus (and it does not), the &quot;ng&quot; ending allows our program to guess with relative security that the word has the vbg pre-terminal, and thus the probability of various rule expansions can be considerable sharpened.For example, the tree-bank PCFG probability of the rule &quot;VP --+ vbg np&quot; is 0.0145, whereas once we condition on the fact that the lexical head is a vbg we get a probability of 0.214.The second modification is the explicit marking of noun and verb-phrase coordination.We have already noted the importance of conditioning on the parent label /p.So, for example, information about an np is conditioned on the parent — e.g., an s, vp, pp, etc.Note that when an np is part of an np coordinate structure the parent will itself be an np, and similarly for a vp.But nps and vps can occur with np and vp parents in non-coordinate structures as well.For example, in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3.Note that the subordinate vp has a vp parent.Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.A vp coordinate structure is defined here as a constituent with two or more vp children, one or more of the constituents comma, cc, conjp (conjunctive phrase), and nothing else; coordinate np phrases are defined similarly.Something very much like this is done in [15].As shown in Figure 2, conditioning on this information gives a 0.6% improvement.We believe that this is mostly due to improvements in guessing the sub-constituent's pre-terminal and head.Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worth while.Next we add the less obvious conditioning events noted in our previous discussion of the final model — grandparent label lg and left sibling label /b.When we do so using our maximum-entropy-inspired conditioning, we get another 0.45% improvement in average precision/recall, as indicated in Figure 2 on the line labeled &quot;MaxEnt-Inspired'.Note that we also tried including this information using a standard deleted-interpolation model.The results here are shown in the line &quot;Standard Interpolation&quot;.Including this information within a standard deleted-interpolation model causes a 0.6% decrease from the results using the less conventional model.Indeed, the resulting performance is worse than not using this information at all.Up to this point all the models considered in this section are tree-bank grammar models.That is, the PCFG grammar rules are read directly off the training corpus.As already noted, our best model uses a Markov-grammar approach.As one can see in Figure 2, a firstorder Markov grammar (with all the aforementioned improvements) performs slightly worse than the equivalent tree-bank-grammar parser.However, a second-order grammar does slightly better and a third-order grammar does significantly better than the tree-bank parser.We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].That the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it.The results reported here disprove this conjecture.The results of [13] achieved by combining the aforementioned three-best parsers also suggest that the limit on tree-bank trained parsers is much higher than previously thought.Indeed, it may be that adding this new parser to the mix may yield still higher results.From our perspective, perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head.Neither of these results were anticipated at the start of this research.As noted above, the main methodological innovation presented here is our &quot;maximumentropy-inspired&quot; model for conditioning and smoothing.Two aspects of this model deserve some comment.The first is the slight, but important, improvement achieved by using this model over conventional deleted interpolation, as indicated in Figure 2.We expect that as we experiment with other, more semantic conditioning information, the importance of this aspect of the model will increase.More important in our eyes, though, is the flexibility of the maximum-entropy-inspired model.Though in some respects not quite as flexible as true maximum entropy, it is much simpler and, in our estimation, has benefits when it comes to smoothing.Ultimately it is this flexibility that let us try the various conditioning events, to move on to a Markov grammar approach, and to try several Markov grammars of different orders, without significant programming.Indeed, we initiated this line of work in an attempt to create a parser that would be flexible enough to allow modifications for parsing down to more semantic levels of detail.It is to this project that our future parsing work will be devoted.
A Novel Use of Statistical Parsing to Extract Information from TextSince 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard.In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard.Yet, relatively few have embedded one of these algorithms in a task.Chiba, (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.The technique was benchmarked in the Seventh Message Understanding Conference (MUC-7) in 1998.Several technical challenges confronted us and were solved: TREEBANK on Wall Street Journal adequately train the algorithm for New York Times newswire, which includes dozens of newspapers?Manually creating sourcespecific training data for syntax was not required.Instead, our parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh, 1998).The Template Element (TE) task identifies organizations, persons, locations, and some artifacts (rocket and airplane-related artifacts).For each organization in an article, one must identify all of its names as used in the article, its type (corporation, government, or other), and any significant description of it.For each person, one must find all of the person's names within the document, his/her type (civilian or military), and any significant descriptions (e.g., titles).For each location, one must also give its type (city, province, county, body of water, etc.).For the following example, the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.For the following example, the template relation in Figure 2 was to be generated: &quot;Donald M. Goldstein, a historian at the University of Pittsburgh who helped write...&quot;Almost all approaches to information extraction — even at the sentence level — are based on the divide-and-conquer strategy of reducing a complex problem to a set of simpler ones.Currently, the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing, replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.However, pipelined architectures suffer from a serious disadvantage: errors accumulate as they propagate through the pipeline.For example, an error made during part-of-speech-tagging may cause a future error in syntactic analysis, which may in turn cause a semantic interpretation failure.There is no opportunity for a later stage, such as parsing, to influence or correct an earlier stage such as part-of-speech tagging.An integrated model can limit the propagation of errors by making all decisions jointly.For this reason, we focused on designing an integrated model in which tagging, namefinding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other.A second consideration influenced our decision toward an integrated model.We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993), and more recently, had begun using a generative statistical model for name finding (Bikel et al.1997).Finally, our newly constructed parser, like that of (Collins 1997), was based on a generative statistical model.Thus, each component of what would be the first three stages of our pipeline was based on the same general class of statistical model.Although each model differed in its detailed probability structure, we believed that the essential elements of all three models could be generalized in a single probability model.If the single generalized model could then be extended to semantic analysis, all necessary sentence level processing would be contained in that model.Because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties — especially their ability to learn from large amounts of data, and their robustness when presented with unexpected inputs — would also benefit semantic analysis.Our integrated model represents syntax and semantics jointly using augmented parse trees.In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.An example of an augmented parse tree is shown in Figure 3.The five key facts in this example are: Here, each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.For example, &quot;per-r&quot; identifies &quot;Nance&quot; as a named person, and &quot;per-desc-r&quot; identifies &quot;a paid consultant to ABC News&quot; as a person description.Other labels indicate relations among entities.For example, the coreference relation between &quot;Nance&quot; and &quot;a paid consultant to ABC News&quot; is indicated by &quot;per-desc-of.&quot; In this case, because the argument does not connect directly to the relation, the intervening nodes are labeled with semantics &quot;-ptr&quot; to indicate the connection.Further details are discussed in the section Tree Augmentation.To train our integrated model, we required a large corpus of augmented parse trees.Since it was known that the MUC-7 evaluation data would be drawn from a variety of newswire sources, and that the articles would focus on rocket launches, it was important that our training corpus be drawn from similar sources and that it cover similar events.Thus, we did not consider simply adding semantic labels to the existing Penn TREEBANK, which is drawn from a single source — the Wall Street Journal — and is impoverished in articles about rocket launches.Instead, we applied an information retrieval system to select a large number of articles from the desired sources, yielding a corpus rich in the desired types of events.The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.Initially, we tried to annotate the training corpus by hand marking, for each sentence, the entire augmented tree.It soon became painfully obvious that this task could not be performed in the available time.Our annotation staff found syntactic analysis particularly complex and slow going.By necessity, we adopted the strategy of hand marking only the semantics.Figure 4 shows an example of the semantic annotation, which was the only type of manual annotation we performed.To produce a corpus of augmented parse trees, we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus, now annotated with complete augmented trees like that in Figure 3.In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.For each sentence, combining these two sources involved five steps.These steps are given below:syntactic modifier of the other, the inserted node serves to indicate the relation as well as the argument.For example, in the phrase &quot;Lt. Cmdr.David Edwin Lewis,&quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.Whenever a relation involves an entity that is not a direct descendant of that relation in the parse tree, semantic pointer labels are attached to all of the intermediate nodes.These labels serve to form a continuous chain between the relation and its argument.In our statistical model, trees are generated according to a process similar to that described in (Collins 1996, 1997).The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.For each constituent, the head is generated first, followed by the modifiers, which are generated from the head outward.Head words, along with their part-of-speech tags and features, are generated for each modifier as soon as the modifier is created.Word features are introduced primarily to help with unknown words, as in (Weischedel et al. 1993).We illustrate the generation process by walking through a few of the steps of the parse shown in Figure 3.At each step in the process, a choice is made from a statistical distribution, with the probability of each possible selection dependent on particular features of previously generated elements.We pick up the derivation just after the topmost S and its head word, said, have been produced.The next steps are to generate in order: In this case, there are none.8.Post-modifier constituents for the PER/NP.First a comma, then an SBAR structure, and then a second comma are each generated in turn.This generation process is continued until the entire tree has been produced.We now briefly summarize the probability structure of the model.The categories for head constituents, cl„ are predicted based solely on the category of the parent node, cp: Modifier constituent categories, cm, are predicted based on their parent node, cp, the head constituent of their parent node, chp, the previously generated modifier, c„,_1, and the head word of their parent, wp.Separate probabilities are maintained for left (pre) and right (post) modifiers: Part-of-speech tags, t,,, for modifiers are predicted based on the modifier, cm, the partof-speech tag of the head word, th, and the head word itself, wh: Head words, w„„ for modifiers are predicted based on the modifier, cm, the part-of-speech tag of the modifier word , t„„ the part-ofspeech tag of the head word , th, and the head word itself, wh: lAwmicm,tm,th,wh), e.g.Finally, word features, fm, for modifiers are predicted based on the modifier, cm, the partof-speech tag of the modifier word , t„„ the part-of-speech tag of the head word th, the head word itself, wh, and whether or not the modifier head word, w„„ is known or unknown.The probability of a complete tree is the product of the probabilities of generating each element in the tree.If we generalize the tree components (constituent labels, words, tags, etc.) and treat them all as simply elements, e, and treat all the conditioning factors as the history, h, we can write:Maximum likelihood estimates for the model probabilities can be obtained by observing frequencies in the training corpus.However, because these estimates are too sparse to be relied upon, we use interpolated estimates consisting of mixtures of successively lowerorder estimates (as in Placeway et al. 1993).For modifier constituents, the mixture components are: For part-of-speech tags, the mixture components are: Finally, for word features, the mixture components are:Given a sentence to be analyzed, the search program must find the most likely semantic and syntactic interpretation.More precisely, it must find the most likely augmented parse tree.Although mathematically the model predicts tree elements in a top-down fashion, we search the space bottom-up using a chartbased search.The search is kept tractable through a combination of CKY-style dynamic programming and pruning of low probability elements.Whenever two or more constituents are equivalent relative to all possible later parsing decisions, we apply dynamic programming, keeping only the most likely constituent in the chart.Two constituents are considered equivalent if: threshold of the highest scoring constituent are maintained; all others are pruned.For purposes of pruning, and only for purposes of pruning, the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman, 1997).We can think of this prior probability as an estimate of the probability of generating a subtree with the constituent category, starting at the topmost node.Thus, the scores used in pruning can be considered as the product of: 1.The probability of generating a constituent of the specified category, starting at the topmost node.2.The probability of generating the structure beneath that constituent, having already generated a constituent of that category.Given a new sentence, the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence.The semantics — that is, the entities and relations — can then be directly extracted from these sentential trees.Our system for MUC-7 consisted of the sentential model described in this paper, coupled with a simple probability model for cross-sentence merging.The evaluation results are summarized in Table 1.In both Template Entity (TE) and Template Relation (TR), our system finished in second place among all entrants.Nearly all of the work was done by the sentential model; disabling the cross-sentence model entirely reduced our overall F-Score by only 2 points.Given multiple constituents that cover identical spans in the chart, only those constituents with probabilities within a While our focus throughout the project was on TE and TR, we became curious about how well the model did at part-of-speech tagging, syntactic parsing, and at name finding.We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97), and evaluated name finding accuracy on the MUC7 named entity test.The results are summarized in Table 2.While performance did not quite match the best previously reported results for any of these three tasks, we were pleased to observe that the scores were at or near state-of-the-art levels for all cases.We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.A single model proved capable of performing all necessary sentential processing, both syntactic and semantic.We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.The semantic training corpus was produced by students according to a simple set of guidelines.This simple semantic annotation was the only source of task knowledge used to configure the model.The work reported here was supported in part by the Defense Advanced Research Projects Agency.Technical agents for part of this work were Fort Huachucha and AFRL under contract numbers DABT63-94-C-0062, F30602-97-C-0096, and 4132-BBN-001.The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the United States Government.We thank Michael Collins of the University of Pennsylvania for his valuable suggestions.

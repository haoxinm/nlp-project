Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger.Contrary to claims found elsewhere in the literature, we argue that a tagger based on Markov models performs at least as well as other current approaches, including the Maximum Entropy framework.A recent comparison has even shown that TnT performs significantly better for the tested corpora.We describe the basic model of TnT, the techniques used for smoothing and for handling unknown words.Furthermore, we present evaluations on two corpora.
Figure 2: Sample sentence and parse tree we have an input sentence (ABCDEhas a parse tree shown in Figure 2. a human reduces the sentence to which can be translated to a series of decisions made along edges in the sentence parse tree as shown in Figure 3.The symbol &quot;y&quot; along an edge means the node it points to will be kept, and &quot;n&quot; means the node will be removed.Suppose the program reduces sentence to can be translated similarly to the annotated tree shown in Figure 4.Figure 4: Reduced form by the program We can see that along five edges (they are D—)T, D—*G, B-4A, B—>C), both the human and the program made decisions.Two out of the five decisions agree (they are D--÷B and D—>E), so the rate is 2/5 (40%).The rate defined as: man and the program have made the same decision success rate = the total # of edges along which both the human and the progam have made decisions 313 Note that the edges along which only the human or the program has made a decision (e.g., G--F and G—.>F in Figure 3 and Figure 4) are not considered in the computation of success rate, since there is no agreement issue in such cases.3.2 Evaluation result In the evaluation, we used 400 sentences in the corpus to compute the probabilities that a phrase is removed, reduced, or unchanged.We tested the program on the rest 100 sentences.Using five-fold validation (i.e., chose different 100 sentences for testing each time and repeating the experiment five times), The program achieved an average success rate of 81.3%.If we consider the baseline as removing all the prepositional phrases, clauses, to-infinitives and gerunds, the baseline performance is 43.2%.We also computed the success rate of program's decisions on particular types of phrases.For the decisions on removing or keeping a clause, the system has a success rate of 78.1%; for the decisions on removing or keeping a to-infinitive, the system has a success rate of 85.2%.We found out that the system has a low success rate on removing adjectives of noun phrases or removing adverbs of a sentence or a verb phrase.One reason for this is that our probability model can hardly capture the dependencies between a particular adjective and the head noun since the training corpus is not large enough, while the other sources of information, including grammar or context information, provide little evidence on whether an adjective or an adverb should be removed.Given that whether or not an adjective or an adverb is removed does not affect the conciseness of the sentence significantly and the system lacks of reliability in making such decisions, we decide not to remove adjectives and adverbs.On average, the system reduced the length of the 500 sentence by 32.7% (based on the number of words), while humans reduced it by 41.8%.The probabilities we computed from the training corpus covered 58% of instances in the test corpus.When the corpus probability is absent for a case, the system makes decisions based on the other two sources of knowledge.Some of the errors made by the system result from the errors by the syntactic parser.We randomly checked 50 sentences, and found that 8% of the errors made by the system are due to parsing errors.There are two main reasons responsible for this relative low percentage of errors resulted from mistakes in parsing.One reason is that we have taken some special measures to avoid errors introduced by mistakes in parsing.For example, PP attachment is a difficult problem in parsing and it is not rare that a PP is wrongly attached.Therefore, we take this into account when marking the obligatory components using subcategorization knowledge from the lexicon (step 2) — we not only look at the PPs that are attached to a verb phrase, but also PPs that are next to the verb phrase but not attached, in case it is part of the verb phrase.We also wrote a preprocessor to deal with particular structures that the parser often has problems with, such as appositions.The other reason is that parsing errors do not always result in reduction errors.For example, given a sentence &quot;The spokesperson of the University said that ...&quot;, although that-clause in the sentence may have a complicated structure and the parser gets it wrong, the reduction system is not necessarily affected since it may decide in this case to keep that-clause as it is, as humans often do, so the parsing errors will not matter in this example.4 Discussion and related work The reduction algorithm we present assumes generic summarization; that is, we want to generate a summary that includes the most important information in an article.We can tailor the reduction system to queries-based summarization.In that case, the task of the reduction is not to remove phrases that are extraneous in terms of the main topic of an article, but phrases that are not very relevant to users' queries.We extended our sentence reduction program to query-based summarization by adding another step in the algorithm to measure the relevance of users' queries to phrases in the sentence.In the last step of reduction when the system makes the final decision, the relevance of a phrase to the query is taken into account, together with syntactic, context, and corpus information.Ideally, the sentence reduction module should interact with other modules in a summarization system.It should be able to send feedback to the extraction module if it finds that a sentence selected by the extraction module may be inappropriate (for example, having a very low context importance score).It should also be able to interact with the modules that run after it, such as the sentence combination module, so that it can revise reduction decisions according to the feedback from these modules.Some researchers suggested removing phrases or clauses from sentences for certain applications.(Grefenstette, 1998) proposed to remove phrases in sentences to produce a telegraphic text that can be used to provide audio scanning service for the blind.(Corston-Oliver and Dolan, 1999) proposed to remove clauses in sentences before indexing documents for Information Retrieval.Both studies removed phrases based only on their syntactic categories, while the focus of our system is on deciding when it is appropriate to remove a phrase. researchers worked on the text simplifica- 314 tion problem, which usually involves in simplifying text but not removing any phrases.For example, (Carroll et al., 1998) discussed simplifying newspaper text by replacing uncommon words with common words, or replacing complicated syntactic structures with simpler structures to assist people with reading disabilities.(Chandrasekar et al., 1996) discussed text simplification in general.The difference between these studies on text simplification and our system is that a text simplification system usually not from an original sentence, although it may change its structure or words, but our system removes extraneous phrases from the extracted sentences.5 Conclusions and future work We present a novel sentence reduction system which removes extraneous phrases from sentences that are extracted from an article in text summarization.The deleted phrases can be prepositional phrases, clauses, to-infinitives, or gerunds, and multiple phrases can be removed form a single sentence.The focus of this work is on determining, for a sentence in a particular context, which phrases in the sentence are less important and can be removed.Our system makes intelligent reduction decisions based on multiple sources of knowledge, including syntactic knowledge, context, and probabilities computed from corpus analysis.We also created a corpus consisting of 500 sentences and their reduced forms produced by human professionals, and used this corpus for training and testing the system.The evaluation shows that 81.3% of reduction decisions made by the system agreed with those of humans.In the future, we would like to integrate our sentence reduction system with extraction-based summarization systems other than the one we have developed, improve the performance of the system further by introducing other sources of knowledge necessary for reduction, and explore other interesting applications of the reduction system.Acknowledgment This material is based upon work supported by the National Science Foundation under Grant No.IRI 96-19124 and IRI 96-18797.Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not
This paper describes a method for linear text segmentation which is twice as accurate and over seven times as fast as the state-of-the-art (Reynar, 1998).Inter-sentence similarity is replaced by rank in the local context.Boundary locations are discovered by divisive clustering.
This paper presents a corpus-based approach to word sense disambiguation that builds an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co—occurring words in varying sized windows of context.Despite the simplicity of this approach, empirical results disamthe widely studied nouns show that such an ensemble achieves accuracy rivaling the best previously published results.
We present a cut and paste based text summarizer, which uses operations derived from an analysis of human written abstracts.The summarizer edits extracted sentences, using reduction to remove inessential phrases and combination to merge resulting phrases together as coherent sentences.Our work includes a statistically based sentence decomposition program that identifies where the phrases of a summary originate in the original document, producing an aligned corpus of summaries and articles which we used to develop the summarizer.
We present three systems for surface natural language generation that are trainable from annotated corpora.The first two systems, called NLG1 and NLG2, require a corpus marked only with domainspecific semantic attributes, while the last system, called NLG3, requires a corpus marked with both semantic attributes and syntactic dependency information.All systems attempt to produce a grammatical natural language phrase from a domain-specific semantic representation.NLG1 serves a baseline system and uses phrase frequencies to generate a whole phrase in one step, while NLG2 and NLG3 use maximum entropy probability models to individually generate each word in the phrase.The systems NLG2 and NLG3 learn to determine both the word choice and the word order of the phrase.We present experiments in which we generate phrases to describe flights in the air travel domain.
and Vincent J. Della Pietra.1996.A maximum entropy approach to natural lanprocessing.Linguistics,
We propose a method for identifying diathesis alternations where a particular argument type is seen in slots which have different grammatical roles in the alternating forms.The method uses selectional preferences acquired as probability distributions over WordNet.Preferences for the target slots are compared using a measure of distributional similarity.The method is evaluated on the causative and conative alternations, but is generally applicable and does not require a priori knowledge specific to the alternation.
There are five missing brackets which are indicated as &quot;*[&quot; or &quot;1&quot;.Words with a second NP tag were identified as proper nouns in a prepass.[A/AT former/AP top/NN aide/NN] to/IN [At-torney/NP/NP General/NP/NP Edwin/NP/NP Meese/NP/NP] interceded/VBD to/TO extend/VB [an/AT aircraftNN company/NN 's/$ govern-ment/NN contract/NN] ,/, then/RB went/VBD into/IN [business/NM] with/IN [a/AT lobby-ist/NN1 [who/WPS] worked/VBD for/IN [the/AT defense/NN contractor/NN] ,/, according/IN to/IN [a/AT published/VBN report/NN] ./.[James/NP/NP E/NP./NP Jenkins/NP/NP] ,/, [a/AT one-time/JJ senior/JJ deputy/NN] to/IN [Meese/NP/NP] joined/VBD [the/AT board/NN] of/IN [directors/NNS] of/IN [Transworld/NP/NP Group/NP/NP Ltd/NP./NP] on/IN [April/NP/NP 28/CD] ,/, [1984/CD] ,/, [the/AT Chicago/NP/NP Tribune/NP/NP] reporteci/VBD in/IN [its/PP$ Tuesday/NR editions/NNS] ./.[The/AT principal/JJ figure/NN] in/IN [Trans-world/NP/NP] was/BEDZ [Richard/NP/NP Mill-man/NP/NP] ,/, [a/AT lobbyist/NN] for/IN [Fair-child/NP/NP Industries/NP/NP Inc/NP./NP] ,/, Virginia/NP/NP de fense/NN con- 142 tractor/NN] I, [the/AT Tribune/NP/NP] said/VBD .1.[A/AT federal/JJ grand/JJ jury/NN] is/BEZ in- [the/AT Fairchild/NP/NP transaction/NN] and/CC [other/AP actions/NNS] of/IN [Meese/NP/NP] and/CC [former/AP White/NP/NP House/NP/NP aide/NN Nofziger/NP/NP] in/IN [connection/NN] with/IN [Wedtech/NP/NP New/NP/NP York/NP/NP defense/NN company/NN] [that/WPS] received/VBD [$250/CD million/CD] in/IN [govemment/NN contracts/NNS] issued/VBN without/EN [competitive/JJ bidding/NN] during/IN [the/AT Reagan/NP/NP administration/NN] ./.[Jenkins/NP/NP] left/VBD [the/AT White/NP/NP House/NP/NP] in/IN [1984/CD] 1, and/CC joined/VBD [Wedtech/NP/NP] as/CS [its/PP$ director/NN] of/IN [marketing/NN *]*[ two/CD years/NNS] later/RBR .1.[Deborah/NP/NP Tucker/NP/NP] ,/, [a/AT spokeswoman/NN] for/IN [Fairchild/NP/NP] ,/, said/VBD [Friday/NR] that/CS [the/AT cornpany/NN] had/HVD been/BEN contacted/VBN by/IN [the/AT office/NN] of/IN [independent/JJ counsel/NN James/NP/NP McKay/NP/NP] and/CC [subpoenas/NNS] had/HVD been/BEN served/VBN on/IN [Fairchild/NP/NP] ./.[Tucker/NP/NP] said/VBD [the/AT investigation/NN] involving/IN [Fairchild/NP/NP] had/HVD been/BEN going/VBG on/IN [a/AT number/NN] of/IN [weeks/NNS] and/CC predates/VBZ [last/AP week/NN 's/$ exof/1N [McKay/NP/NP 's/$ investigation/NN] to/TO include/VB [Meese/NP/NP] ./.[The/AT company/NN] is/BEZ cooperating/VBG in/1N [the/AT investigation/NN] ,/, [Tucker/NP/NP] said/VBD ./.[A/AT source/NN *] close/NN***] to/IN [McKay/NP/NP] said/VBD [last/AP week/NN&quot; that/CS [Meese/NP/NP] isn't/BEZ* under/IN [cruninalaJ investigation/NN] in/IN [the/AT Fairchild/NP/NP matter/NN] ,/, but/RB is/BEZ [a/AT witness/NN] ./.[The/NP Tribune/NP/NP] said/VBD [Mill- ,/, acting/VBG as/CS [a/AT lobbyist/NN] for/1N [the/AT Chantilly/NP/NP] ,/, [Va/NP.-based/NP company/NN] ,/, went/VBD to/TO see/VB [Jenkins/NP/NP] in/IN [1982/CD] and/CC urged/VBD [him/PPO] and/CC [Meese/NP/NPI to/TO encourage/VB [the/AT Air/NP/NP Force/NP/NP] to/E0 extend/VB [the/AT production/NN] of/1/•1 [Fairchild/NP/NP 's/$ A-10/NP bomber/NN] for/IN [a/AT year/NN] ./.[Millman/NP/NP] said/VBD there/RB was/BEDZ [a/AT lucrative/JJ market/NN] in/IN [Third/NP/NP World/NP/NP countries/NNS] ,/, but/CC that/CS [Fairchild/NP/NP 's/$ chances/NNS] would/MD be/BE limited/VBN if/CS [the/AT Air/NP/NP Force/NP/NP] was/BEDZ not/* producing/VBG [the/AT plane/NN] ./.[The/AT Air/NP/NP Force/NP/NP] had/HVD decided/VBN to/TO discontinue/VB [production/NN] of/IN [the/AT A-10/NP] ,/, [a/AT 1960s-era/CD ground-support/NN attack/NN bomber/NN] at/IN [the/AT time/NN *]*[ Fairchild/NP/NP] was/BEDZ hoping/VBG to/TO sell/VB [A-10s/NP] abroad/RB j, [the/AT Tribune/NP/NP] said/VBD ./.[The/AT newspaper/NN] said/VBD [one/CD source/NN] reported/VBD that/CS after/CS [Millman/NP/NP] made/VBD [his/PPS pitch/NN] J, [Meese/NP/NP] ordered/VBD [Jenlcins/NP/NP] to/TO prepare/VB [a/AT memo/NN] on/IN [behalf/NN] of/IN [Fairchild/NP/NP] ./.[Memos/NP***] signed/VBD by/IN [Meese/NP/NP] ,/, stressing/VBG [the/AT importance/NN] of/IN [Fairchild/NP/NP 's/$ arranging/VBG sales/NNS] in/IN [Third/NP/NP World/NP/NP countries/NNS] j, were/BED sent/VBN to/IN [the/AT State/NP/NP Department/NP/NP] and/CC [the/AT Air/NP/NP Force/NP/NP] ./.[Millman/NP/NP] did/DOD not/* return/VB [telephone/NN calls/NNS] to/EN [his/PP$ office/NN1 and/CC [referral/NN numbers/NNS] [Monday/NR] ,I, [the/AT Tribune/NP/NP] said/VBD ./.
We present an implementation of a part-of-speech tagger based on a hidden Markov model.The methodology enables robust and accurate tagging with few resource requirements.Only a lexicon and some unlabeled training text are required.Accuracy exceeds 96%.We describe implementation strategies and optimizations which result in high-speed operation.Three applications for tagging are described: phrase recognition; word sense disambiguation; and grammatical function assignment.1 Desiderata Many words are ambiguous in their part of speech.For example, &quot;tag&quot; can be a noun or a verb.However, when a word appears in the context of other words, the ambiguity is often reduced: in &quot;a tag is a part-of-speech label,&quot; the &quot;tag&quot; can only be a noun.A tagger is a system that uses context to assign parts of speech to words.Automatic text tagging is an important first step in discovering the linguistic structure of large text corpora.Part-of-speech information facilitates higher-level analysis, such as recognizing noun phrases and other patterns in text.For a tagger to function as a practical component in a language processing system, we believe that a tagger must be: corpora contain ungrammatical constructions, isolated phrases (such as titles), and nonlinguistic data (such as tables).Corpora are also likely to contain words that are unknown to the tagger.It is desirable that a tagger deal gracefully with these situations. a tagger is to be used to analyze arbitrarily large corpora, it must be efficient—performing in time linear in the number of words tagged.Any training required should also be fast, enabling rapid turnaround with new corpora and new text genres.A should attempt to assign the correct part-of-speech tag to every word encountered.A should be able to take advantage of linguistic insights.One should be able to correct errors by supplying appropriate priori &quot;hints.&quot; It should be possible to give different hints for different corpora. effort required to retarget a tagger to new corpora, new tagsets, and new languages should be minimal.2 Methodology 2.1 Background Several different approaches have been used for building text taggers.Greene and Rubin used a rule-based approach in the TAGGIT program [Greene and Rubin, 1971], which was an aid in tagging the Brown corpus [Francis and KuEera, 1982].TAGGIT disambiguated 77% of the corpus; the rest was done manually over a period of several years.More recently, Koskenniemi also used a rule-based approach implemented with finite-state machines [Koskenniemi, 1990].Statistical methods have also been used (e.g., [DeRose, [Garside al., These provide the capability of resolving ambiguity on the basis of most likely interpretation.A form of Markov model has been widely used that assumes that a word depends probabilistically on just its part-of-speech category, which in turn depends solely on the categories of the preceding two words.Two types of training (i.e., parameter estimation) have been used with this model.The first makes use of a tagged training corpus.Derouault and Merialdo use a bootstrap method for training [Derouault and Merialdo, 1986].At first, a relatively small amount of text is manually tagged and used to train a partially accurate model.The model is then used to tag more text, and the tags are manually corrected and then used to retrain the model.Church uses the tagged Brown corpus for training [Church, 1988].These models involve probabilities for each word in the lexicon, so large tagged corpora are required for reliable estimation.The second method of training does not require a tagged training corpus.In this situation the Baum-Welch algorithm (also known as the forward-backward algorithm) can be used [Baum, 1972].Under this regime the model is a Markov model as state transitions (i.e., part-of-speech categories) are assumed to be unobservable.Jelinek has used this method for training a text tagger [Jelinek, 1985].Parameter smoothing can be conachieved using the method of interpolawhich weighted estimates are taken from secondand first-order models and a uniform probability distribution [Jelinek and Mercer, 1980].Kupiec used word equivclasses (referred to here as classes) on parts of speech, to pool data from individual words [Kupiec, 1989b].The most common words are still represented individually, as sufficient data exist for robust estimation.133 However all other words are represented according to the set of possible categories they can assume.In this manner, the vocabulary of 50,000 words in the Brown corpus can be reduced to approximately 400 distinct ambiguity classes [Kupiec, 1992].To further reduce the number of parameters, a first-order model can be employed (this assumes that a word's category depends only on the immediately preceding word's category).In [Kupiec, 1989a], networks are used to selectively augment the context in a basic firstorder model, rather than using uniformly second-order dependencies.2.2 Our approach We next describe how our choice of techniques satisfies the listed in section 1.The use of an complete flexibility in the choice of training corpora.Text from any desired domain can be used, and a tagger can be tailored for use with a particular text database by training on a portion of that database.Lexicons containing alternative tag sets can be easily accommodated without any need for re-labeling the training corpus, affording further flexibility in the use of specialized tags.As the resources required are simply a lexicon and a suitably large sample of ordinary text, taggers can be built with minimal effort, even for other languages, such as French (e.g., [Kupiec, 1992]).The use of ambiguity classes and a first-order model reduces the number of parameters to be estimated without significant reduction in accuracy (discussed in section 5).This also enables a tagger to be reliably trained using only moderate amounts of text.We have produced reasonable results training on as few as 3,000 sentences.Fewer parameters also reduce the time required for training.Relatively few ambiguity classes are sufficient for wide coverage, so it is unlikely that adding new words to the lexicon requires retraining, as their ambiguity classes are already accommodated.Vocabulary independence is achieved by predicting categories for words not in the lexicon, using both context and suffix information.Probabilities corresponding to category sequences that never occurred in the training data are assigned small, non-zero values, ensuring that the model will accept any sequence of tokens, while still providing the most likely tagging.By using the fact that words are typically associated with only a few part-ofspeech categories, and carefully ordering the computation, the algorithms have linear complexity (section 3.3).3 Hidden Markov Modeling The hidden Markov modeling component of our tagger is implemented as an independent module following the specgiven in [Levinson et with special attention to space and time efficiency issues.Only first-order modeling is addressed and will be presumed for the remainder of this discussion.3.1 Formalism brief, an a doubly stochastic process that generates sequence of symbols ={Sl,S2, , 1 <i< where W is some finite set of possible symbols, by composing an underlying Markov process with a state-dependent symbol generator (i.e., a Markov process with noise).'Th Markov process captures the notion of sequence depen and is described by a set of a matrix c probabilities A -= 1 < a, the probability of moving from state i to state of initial probabilities H = {70 1 < i < is the probability of starting in state i.The symbol ger erator is a state-dependent measure on V described by of symbol probabilities 1 < j < < < M = IW I is the probability ( symbol given that the Markov process is i In part-of-speech tagging, we will model word order di pendency through an underlying Markov process that 01 erates in terms of lexical tags,' yet we will only be ab to observe the sets of tags, or ambiguity classes, that ai possible for individual words.The ambiguity class of eac word is the set of its permitted parts of speech, only or of which is correct in context.Given the parameters .4, Markov modeling allows us to compute ti most probable sequence of state transitions, and hence a mostly likely sequence of lexical tags, corresponding to of ambiguity classes.In the following, identified with the number of possible .tags, and W wil the set of all ambiguity classes.Applying an HMM consists of two tasks: estimating ti parameters A, a training set; ar computing the most likely sequence of underlying sta transitions given new observations.Maximum likeliho( estimates (that is, estimates that maximize the probabili of the training set) can be found through application of z ternating expectation in a procedure known as the Baur Welch, or forward-backward, algorithm [Baum, 1972]. proceeds by recursively defining two sets of probabiliti( the forward probabilities eft+1.(i) = bi(St+i) < t < T — 1, ( = for all the backward prob bilities, i3(i) = — 1 < t < 1, ( 1.1 1 for all forward probabili the joint probability of the sequence up to tir t, S2, , the event that the Markov pr is in state i at time the backwa is the probability of seeing the sequen , ST} that the Markov process is state i at time t. It follows that the probability of t entire sequence is N N =
Automatic part of speech tagging is an area of natural language processing where statistical techniques have been more successful than rulebased methods.In this paper, we present a simple rule-based part of speech tagger which automatically acquires its rules and tags with accuracy comparable to stochastic taggers.The rule-based tagger has many advantages over these taggers, including: a vast reduction in stored information required, the perspicuity of a small set of meaningful rules, ease of finding and implementing improvements to the tagger, and better portability from one tag set, corpus genre or language to another.Perhaps the biggest contribution of this work is in demonstrating that the stochastic method is not the only viable method for part of speech tagging.The fact that a simple rule-based tagger that automatically learns its rules can perform so well should offer encouragement for researchers to further explore rule-based tagging, searching for a better and more expressive set of rule templates and other variations on the simple but effective theme described below.
1993).We that part of speech tagging and word alignment could have an important role in glossary construction for translation.Glossaries are extremely important for translation.How would Microsoft, or some other software vendor, want the term &quot;Character menu&quot; to be translated in their manuals?Technical terms are difficult for translators because they are generally not as familiar with the subject domain as either the author of the source text or the reader of the target text.In many cases, there may be a number of acceptable translations, but it is important for the sake of consistency to standardize on a single one.It would be unacceptable for a manual to use a variety of synonyms for a particular menu or button.Customarily, translation houses make extensive job-specific glossaries to ensure consistency and correctness of technical terminology for large jobs.A glossary is a list of terms and their translations.'We will subdivide the task of constructing a glossary into two subtasks: (1) generating a list of terms, and (2) finding the translation equivalents.The first task will be referred to as the monolingual task and the second as the bilingual task.How should a glossary be constructed?Translation schools teach their students to read as much background material as possible in both the source and target languages, an extremely time-consuming
In part of speech tagging by Hidden Markov Model, a statistical model is used to assign grammatical categories to words in a text.Early work in the field relied on a corpus which had been tagged by a human annotator to train the model.recently, Cutting al. suggest that training can be achieved with a minimal lexicon and a limited amount priori about probabilities, by using an Baum-Welch re-estimation to automatically refine the model.In this paper, I report two experiments designed to determine how much manual training information is needed.The first experiment suggests that initial biasing of either lexical or transition probabilities is essential to achieve a good accuracy.The second experiment reveals that there are three distinct patterns of Baum-Welch reestimation.In two of the patterns, the re-estimation ultimately reduces the accuracy of the tagging rather than improving it.The pattern which is applicable can be predicted from the quality of the initial model and the similarity between the tagged training corpus (if any) and the corpus to be tagged.Heuristics for deciding how to use re-estimation in an effective manner are given.The conclusions are broadly in agreement with those of Merialdo (1994), but give greater detail about the contributions of different parts of the model.1 Background Part-of-speech tagging is the process of assigning grammatical categories to individual words in a corpus.One widely used approach makes use of a statistical technique called a Hidden Markov Model (HMM).The model is defined by two collections of the probabilities, express the probability that a tag follows the preceding (or two for a second order model); and the the probability that a word has a given tag without regard to words on either side of it.To tag a text, the tags with non-zero probability are hypothesised for each word, and the most probable sequence of tags given the sequence of words is determined from the probabilities.Two algorithms are commonly used, known as the Forward-Backward (FB) and Viterbi algorithms.FB assigns a probability to every tag on every word, while Viterbi prunes tags which cannot be chosen because their probability is lower than the ones of competing hypotheses, with a corresponding gain in computational ef
We present a trainable model for identifying sentence boundaries in raw text.Given a corpus annotated with sentence boundaries, our model learns to classify each occurrence of., ?, and !as either a valid or invalid sentence boundary.The training procedure requires no hand-crafted rules, lexica, part-of-speech tags, or domain-specific information.The model can therefore be trained easily on any genre of English, and should be trainable on any other Romanalphabet language.Performance is comparable to or better than the performance of similar systems, but we emphasize the simplicity of retraining for new domains.
We describe a practical parser for unrestricted dependencies.The parser creates links between words and names the links according to their syntactic functions.We first describe the older Constraint Grammar parser where many of the ideas come from.Then we proceed to describe the central ideas of our new parser.Finally, the parser is evaluated.
This paper presents a statistical, learned approach to finding names and other nonrecursive entities in text (as per the MUC-6 definition of the NE task), using a variant of the standard hidden Markov model.We present our justification for the problem and our approach, a detailed discussion of the model itself and finally the successful results of this new approach.
trieving information from full text using linguisknowledge, In of the Fifteenth Online Meeting, New York, May.
of the system that are new: the extractor, classifier and evaluator.The grammar consists of 455 phrase structure rule schemata in the format accepted by the parser (a syntactic variant of a Definite Clause Grammar with iterative (Kleene) operators).It is 'shallow' in that no atof which thetempt is made to fully analyse unbounded dependencies.However, the distinction between arguments and adjuncts is expressed, following X-bar theory (e.g.Jackendoff, 1977), by Chomsky-adjunction to maximal projections of adjuncts (XP XP Adjunct) as opposed to 'government' of arguments (i.e. arguments are sisters within projections; X1 XO Argl... ArgN). more, all analyses are rooted (in S) so the grammar assigns global, shallow and often 'spurious' analyses to many sentences.There are 29 distinct values for VSUBCAT and 10 for PSUBCAT; these are analysed in patterns along with specific closed-class lemmas of arguments, such as suband so forth, to classify patterns as evidence for one of the 160 subcategorization classes.Each of these classes can be parameterized for specific predicates by, for example, different prepositions or particles.Currently, the coverage of this grammar—the proportion of sentences for which at least one analysis is found—is 79% when applied to the Susanne corpus (Sampson, 1995), a 138K word treebanked and balanced subset of the Brown corpus.Wide coverage is important since information is acquired only from successful parses.The combined throughput of the parsing components on a Sun UltraSparc 1/140 is around 50 words per CPU second.2.2 The Extractor, Classifier and Evaluator The extractor takes as input the ranked analyses from the probabilistic parser.It locates the subanalyses around the predicate, finding the constituents identified as complements inside each subanalysis, and the subject clause preceding it.Instances of passive constructions are recognized and treated specially.The extractor returns the predicate, the VSUBCAT value, and just the heads of the complements (except in the case of PPs, where it returns the PSUBCAT value, the preposition head, and the heads of the PP's complements).The subcategorization classes recognized by the classifier were obtained by manually merging the classes exemplified in the COMLEX Syntax and ANLT dictionaries and adding around 30 classes found by manual inspection of unclassifiable patterns for corpus examples during development of the system.These consisted of some extra patterns for phrasal verbs with complex complementation and with flexible ordering of the preposition/particle, some for non-passivizable patterns with a surface direct object, and some for rarer combinations of governed preposition and complementizer combinations.The classifier filters out as unclassifiable around 15% of patterns found by the extractor when run on all the patternsets extracted from the Susanne corpus.This demonstrates the value of the classifier as a filter of spurious analyses, as well as providing both translation between extracted patterns and two existing subcategorization dictionaries and a definition of the target subcategorization dictionary.The evaluator builds entries by taking the patterns for a given predicate built from successful parses and records the number of observations of each subcategorization class.Patterns provide several types of information which can be used to rank or select between patterns in the patternset for a given sentence exemplifying an instance of a predicate, such as the ranking of the parse from which it was extracted or the proportion of subanalyses supporting a specific pattern.Currently, we simply select the pattern supported by the highest ranked parse.However, we are experimenting with alternative approaches.The resulting set of putative classes for a predicate are filtered, following Brent (1993), 358 by hypothesis testing on binomial frequency data.Evaluating putative entries on binomial frequency data requires that we record the total number of patternsets n for a given predicate, and the number of these patternsets containing a pattern supporting an entry for given class m. These figures are straightforwardly computed from the output of the classifier; however, we also require an estimate of the probability that a pattern for class i will occur with a verb which is not a member of subcategorization class i. Brent proposes estimating these probabilities experimentally on the basis of the behaviour of the extractor.We estimate this probability more directly by first extracting the number of verbs which are members of each class in the ANLT dictionary (with intuitive estimates for the membership of the novel classes) and converting this to a probability of class membership by dividing by the total number of verbs in the dictionary; and secondly, by multiplying the complement of these probabilities by the probability of a pattern for class i, defined as the number of patterns for i extracted from the Susanne corpus divided by the total number of patterns.So, p(v -i), the probability of verb v not of class i occurring with a pattern for class i is: -i) = (1 Ipatterns_f or_i n! n, p) = mi(p im The probability of the event happening m or more times is: n,p) = i=m -i)) the probability that m or more occurrences of patterns for i will occur with a verb which is not a member of i, given n occurrences of that verb.Setting a threshold of less than or equal to 0.05 yields a 95% or better confidence that a high enough proportion of patterns for i have observed for the verb to be in class 2.3 Discussion Our approach to acquiring subcategorization classes is predicated on the following assumptions: • most sentences will not allow the application of all possible rules of English complementation; • some sentences will be unambiguous even given indeterminacy of the (1993:249-253) provides a detailed explanation and justification for the use of this measure. fact, 5% of sentences in Susanne are assigned only a single analysis by the grammar.• many incorrect analyses will yield patterns which are unclassifiable, and are thus filtered out; • arguments of a specific verb will occur with greater frequency than adjuncts (in potential argument positions); • the patternset generator will incorrectly output for certain classes more often than others; and even a highest ranked for i is only a probabilistic cue for membership of i, so membership should only be inferred if there are enough occurrences of patterns for i in the data to outweigh the error probability for i.This simple automated, hybrid linguistic/statistical approach contrasts with the manual linguistic analysis of the COMLEX Syntax lexicog- (Meyers at., who propose five criteria and five heuristics for argument-hood and six criteria and two heuristics for adjunct-hood, culled mostly from the linguistics literature.Many of these are not exploitable automatically because they rest on semantic judgements which cannot (yet) be made automatically: for example, optional arguments are often 'understood' or implied if missing.Others are syntactic tests involving diathesis alternation possibilities (e.g. passive, dative movement, Levin (1993)) which require recognition that the 'same' argument, defined usually by semantic class / thematic role, is occurring across argument positions.We hope to exploit this information where possible at a later stage in the development of our approach.However, recognizing same/similar arguments requires considerable quantities of lexical data or the ability to back-off to lexical semantic classes.At the moment, we exploit linguistic information about the syntactic type, obligatoriness and position of arguments, as well as the set of possible subcategorization classes, and combine this with statistical inference based on the probability of class membership and the frequency and reliability of patterns for classes.3 Experimental Evaluation 3.1 Lexicon Evaluation — Method In order to test the accuracy of our system (as developed so far) and to provide empirical feedback for further development, we took the Susanne, SEC (Taylor & Knowles, 1988) and LOB corpora (Garat., total of 1.2 million words—and extracted all sentences containing an occurrence of one of fourteen verbs, up to a maximum of 1000 citations of each.These verbs, listed in Figure 2, were chosen at random, subject to the constraint that they exhibited multiple complementation patterns.The sentences containing these verbs were tagged and parsed automatically, and the extractor, classifier and evaluator were applied to the resulting lanit_verbs1 'patterns' The binomial distribution gives the probability of an with probability exactly m times out of n attempts: 359 successful analyses.The citations from which entries were derived totaled approximately 70K words.The results were evaluated against a merged entry for these verbs from the ANLT and COMLEX Syntax dictionaries, and also against a manual analysis of the corpus data for seven of the verbs.The process of evaluating the performance of the system relative to the dictionaries could, in principle, be reduced to automated report of precision of correct subcategorization classes to all classes found) of correct classes found in the dictionary entry).However, since there are disagreements between the dictionaries and there are classes found in the corpus data that are not contained in either dictionary, we report results relative both to a manually merged entry from ANLT and COMLEX, and also, for seven of the verbs, to a manual analysis of the actual corpus data.The latter analysis is necessary because precision and recall measures against the merged entry will still tend to yield inaccurate results as the system cannot acquire classes not exemplified in the data, and may acquire classes incorrectly absent from the dictionaries.We illustrate these problems with reference to there is overlap, but not agreement between the COMLEX and ANLT entries.Thus, predict that occur with a sentential complement and dummy subject, but only ANLT predicts the possibility of a `wh' complement and only COMLEX predicts the (optional) presence of a PP[to] argument with the sentential complement.One ANLT entry covers two COMLEX entries given the different treatment of the relevant complements but the classifier keeps them distinct.The corpus for examples of further classes we judge valid, in which take a and infinitive complement, as in seems to to be insane, a passive participle, as in depressed. comparison illustrates the problem of errors of omission common to computational lexicons constructed manually and also from dictionaries.All classes for exemplified in the corpus data, but for example, eight classes (out of a possible 27 in the merged entry) are not present, so comparison only to the merged entry would give an unreasonably low estimate of recall.Lexicon Evaluation Figure 2 gives the raw results for the merged entries and corpus analysis on each verb.It shows the of positives correct classes proby our system, positives incorrect proposed by our system, and negatives classes not proposed by our system, as judged against the merged entry, and, for seven of the verbs, against the corpus analysis.It also shows, in the final column, the number of sentences from which classes were extracted.Dictionary (14 verbs) Corpus (7 verbs) Precision Recall 65.7% 76.6% 35.5% 43.4% Figure 3: Type precision and recall Ranking Accuracy ask 75.0% begin 100.0% believe 66.7% cause 100.0% give 70.0% seem 75.0% swing 83.3% Mean 81.4% Figure 4: Ranking accuracy of classes Figure 3 gives the type precision and recall of our system's recognition of subcategorization classes as evaluated against the merged dictionary entries (14 verbs) and against the manually analysed corpus data (7 verbs).The frequency distribution of classes is highly skewed: for example for there are 107 instances of the most common class in the corpus data, but only 6 instances in total of the least common four classes.More generally, for the manually analysed verbs, almost 60% of the false negatives have only one or two exemplars each in the corpus citations.None of them are returned by because the binomial filter always rejects classes hypothesised on the basis of such little evidence.In Figure 4 we estimate the accuracy with which our system ranks true positive classes against the correct ranking for the seven verbs whose corpus input was manually analysed.We compute this measure by calculating the percentage of pairs of classes at positions (n, m) s.t. n < m in the system ranking that are ordered the same in the correct ranking.This gives us an estimate of the accuracy of the relative frequencies of classes output by the system.For each of the seven verbs for which we undertook a corpus analysis, we calculate the token recall of our system as the percentage (over all exemplars) of true positives in the corpus.This gives us an estimate of the parsing performance that would result from providing a parser with entries built using the system, shown in Figure 5.Further evaluation of the results for these seven verbs reveals that the filtering phase is the weak in the systeni.There are only 13 negatives which the system failed to propose, each exemplified in the data by a mean of 4.5 examples.On the other there are 67 negatives by an mean of 7.1 examples which should, ide- 360 Merged TP FP Entry Corpus TP FP Data No. of FN FN Sentences ask 9 0 18 9 0 10 390 begin 4 1 7 4 1 7 311 believe 4 4 11 4 4 8 230 cause 2 3 6 2 3 5 95 expect 6 5 3 - - - 223 find 5 7 15 - - - 645 give 5 2 11 5 2 5 639 help 6 3 8 - - - 223 like 3 2 7 - - - 228 move 4 3 9 - - - 217 produce 2 1 3 - - 152 provide 3 2 6 - - - 217 seem 8 1 4 8 1 4 534 swing 4 0 10 4 0 8 45 Totals 65 34 118 36 11 47 4149 Figure 2: Raw results for test of 14 verbs Token Recall ask 78.5% begin 73.8% believe 34.5% cause 92.1% give 92.2% seem 84.7% swing 39.2% Mean 80.9% Figure 5: Token recall have been accepted by the filter, and 11 should have been rejected.The performance of the filter for classes with less than 10 exemplars is around chance, and a simple heuristic of accepting all classes with more than 10 exemplars would have produced broadly similar results for these verbs.The filter may well be performing poorly because the probability of generating a subcategorization class for a given verb is often lower than the error probability for that class.3.3 Parsing Evaluation In addition to evaluating the acquired subcategorization information against existing lexical resources, we have also evaluated the information in the context of an actual parsing system.In particular we wanted to establish whether the subcategorization frequency information for individual verbs could be used to improve the accuracy of a parser that uses statistical techniques to rank analyses.The experiment used the same probabilistic parser and tag sequence grammar as are present in the acquisition system (see references above)—although the experiment does not in any way rely on the Mean Recall Precision crossings 'Baseline' Lexicalised 1.00 70.7% 72.3% 0.93 71.4% 72.9% Figure 6: GEIG evaluation metrics for parser against Susanne bracketings parsers or grammars being the same.We randomly selected a test set of 250 in-coverage sentences (of lengths 3-56 tokens, mean 18.2) from the Susanne treebank, retagged with possibly multiple tags per word, and measured the 'baseline' accuracy of the unlexicalized parser on the sentences using the now standard PARSEVAL/GEIG evaluation metrics of mean crossing brackets per sentence and (unlabelled) bracket recall and precision (e.g.Gral., see figure Next, we colwords in the test corpus tagged as possibly being verbs (giving a total of 356 distinct lemmas) and retrieved all citations of them in the LOB corpus, plus Susanne with the 250 test sentences excluded.We acquired subcategorization and associated frequency information from the citations, in the process successfully parsing 380K words.We then parsed the test set, with each verb subcategorization possibility weighted by its raw frequency score, and using the naive add-one smoothing technique to allow for omitted possibilities.The GEIG measures for the lexicalized parser show a 7% improvement in the crossing bracket score (figure 6). the existing test corpus this is not statisti- 'Carroll & Briscoe (1996) use the same test set, although the baseline results reported here differ slightly due to differences in the mapping from parse trees to Susanne-compatible bracketings.361 significant at the 95% level p = if the pattern of differences were maintained over a larger test set of 470 sentences it would be significant.We expect that a more sophisticated smoothing technique, a larger acquisition corpus, and extensions to the system to deal with nominal and adjectival predicates would improve accuracy still further.Nevertheless, this experiment demonstrates that lexicalizing a grammar/parser with subcategorization frequencies can appreciably improve the accuracy of parse ranking.4 Related Work Brent's (1993) approach to acquiring subcategorization is based on a philosophy of only exploiting unambiguous and determinate information in unanalysed corpora.He defines a number of lexical patterns (mostly involving closed class items, such as pronouns) which reliably cue one of five subcategorization classes.Brent does not report comprehensive results, but for one class, sentential complement verbs, he achieves 96% precision and 76% recall at classifying individual tokens of 63 distinct verbs as exemplars or non-exemplars of this class.He does not attempt to rank different classes for a given verb. al. utilise a PoS tagged corpus and finite-state NP parser to recognize and calculate the relative frequency of six subcategorization classes.They report an accuracy rate of 83% (254 errors) at classifying 1565 classifiable tokens of 33 distinct verbs in running text and suggest that incorrect noun phrase boundary detection accounts for the majority of errors.They report that for 32 verbs their system correctly predicts the most frequent class, and for 30 verbs it correctly predicts the second most frequent class, if there was one.Our system rankings include all classes for each verb, from a total of 160 classes, and average 81.4% correct.Manning (1993) conducts a larger experiment, also using a PoS tagged corpus and a finite-state NP parser, attempting to recognize sixteen distinct complementation patterns.He reports that for a test sample of 200 tokens of 40 verbs in running text, the acquired subcategorization dictionary listed the appropriate entry for 163 cases, giving a token recall of 82% (as compared with 80.9% in our experiment).He also reports a comparison of acquired entries for verbs to the entries given in the Advanced Dictionary of Current English 1989) on which his system achieves a precision of 90% and a recall of 43%.His system averages 3.48 subentries (maximum 10)—less then half the number produced in our experiment.It is not clear what level of evidence the performance of Manning's system is based on, but the system was applied to 4.1 million words of text (c.f. our 1.2 million words) and the verbs are all common, so it is likely that considerably more exemplars of each verb were available.5 Conclusions and Further Work The experiment and comparison reported above suggests that our more comprehensive subcategorization class extractor is able both to assign classes to individual verbal predicates and also to rank them according to relative frequency with comparable accuracy to extant systems.We have also demonstrated that a subcategorization dictionary built with the system can improve the accuracy of a probabilistic parser by an appreciable amount.The system we have developed is straightforwardly extensible to nominal and adjectival predicates; the existing grammar distinguishes nominal and adjectival arguments from adjuncts structurally, so all that is required is extension of the classifier.Developing an analogous system for another language would be harder but not infeasible; similar taggers and parsers have been developed for a number of languages, but no extant subcategorization dictionaries exist to our knowledge, therefore the lexical statistics we utilize for statistical filtering would need to be estimated, perhaps using the technique described by Brent (1993).However, the entire approach to filtering needs improvement, as evaluation of our results demonstrates that it is the weakest link in our current system.Our system needs further refinement to narrow some subcategorization classes, for example, to choose between differing control options with predicative complements.It also needs supplementing with information about diathesis alternation possibilities (e.g.Levin, 1993) and semantic selection preferences on argument heads.Grishman & Sterling (1992), Poznanski & Sanfilippo (1993), Resnik (1993), Ribas (1994) and others have shown that it is possible to acquire selection preferences from (partially) parsed data.Our system already gathers head lemmas in patterns, so any of these approaches could be applied, in principle.In future work, we intend to extend the system in this direction.The ability to recognize that argument slots of different subcategorization classes for the same predicate share semantic restrictions/preferences would assist recognition that the predicate undergoes specific alternations, this in turn assisting inferences about control, equi and raising (e.g.Boguraev & Briscoe, 1987).
We consider here the problem of Base Noun Phrase translation.We propose a new method to perform the task.For a given Base NP, we first search its translation candidates from the web.We next determine the possible translation(s) from among the candidates using one of the two methods that we have developed.In one method, we employ an ensemble of Na?ve Bayesian Classifiers constructed with the EM Algorithm.In the other method, we use TF-IDF vectors also constructed with the EM Algorithm.Experimental results indicate that the coverage and accuracy of our method are significantly better than those of the baseline methods relying on existing technologies.
Named Entity (NE) recognition is a task in whichproper nouns and numerical information are extracted from documents and are classified into cat egories such as person, organization, and date.It is a key technology of Information Extraction and Open-Domain Question Answering.First, we showthat an NE recognizer based on Support Vector Ma chines (SVMs) gives better scores than conventional systems.However, off-the-shelf SVM classifiers are too inefficient for this task.Therefore, we present a method that makes the system substantially faster.This approach can also be applied to other similar tasks such as chunking and part-of-speech tagging.We also present an SVM-based feature selec tion method and an efficient training method.
This paper presents an unsupervised method forassembling semantic knowledge from a part-of speech tagged corpus using graph algorithms.The graph model is built by linking pairs of words which participate in particular syntacticrelationships.We focus on the symmetric relationship between pairs of nouns which occur to gether in lists.An incremental cluster-building algorithm using this part of the graph achieves82% accuracy at a lexical acquisition task, evaluated against WordNet classes.The model naturally realises domain and corpus specific am biguities as distinct components in the graph surrounding an ambiguous word.
Broad-coverage lexical resources such as WordNet are extremely useful.However, they often include many rare senses while missing domain-specific senses.We present a clustering algorithm called CBC (Cluster ing By Committee) that automatically discovers concepts from text.It initially discovers a set of tight clusters called committees that are well scattered in the similarity space.The centroid of the members of a committee is used as the feature vector of the cluster.We proceed by assigning elements to their most similar cluster.Evaluating cluster quality has always been a difficult task.We present a new evaluation methodology that is based on the editing distance between output clusters and classes extracted from WordNet (the answer key).Our experiments show that CBC outperforms several well-known clustering algorithms in cluster quality.
In this paper we address issues related to building a large-scale Chinese corpus.We try to answer four questions: (i) how to speed up annotation, (ii) how to maintain high annotation quality, (iii) for what purposes is the corpus applicable, and finally (iv) what future work we anticipate.
In order to respond correctly to a free form factual question given a large collection of texts, one needs to un derstand the question to a level that allows determiningsome of the constraints the question imposes on a pos sible answer.These constraints may include a semantic classification of the sought after answer and may even suggest using different strategies when looking for and verifying a candidate answer.This paper presents a machine learning approach toquestion classification.We learn a hierarchical classifier that is guided by a layered semantic hierarchy of answer types, and eventually classifies questions into finegrained classes.We show accurate results on a large col lection of free-form questions used in TREC 10.
This paper presents a deterministic dependency parser based on memory-based learning, which parses English text in linear time.When trainedand evaluated on the Wall Street Journal sec tion of the Penn Treebank, the parser achieves a maximum attachment score of 87.1%.Unlikemost previous systems, the parser produces la beled dependency graphs, using as arc labels a combination of bracket labels and grammaticalrole labels taken from the Penn Treebank II annotation scheme.The best overall accuracy ob tained for identifying both the correct head and the correct arc label is 86.0%, when restricted to grammatical role labels (7 labels), and 84.4% for the maximum set (50 labels).
An efficient bit-vector-based CKY-style parser for context-free parsing is presented.The parser computes a compact parse forest representation of the complete set of possible analyses forlarge treebank grammars and long input sen tences.The parser uses bit-vector operations to parallelise the basic parsing operations.The parser is particularly useful when all analyses are needed rather than just the most probable one.
This paper describes the role of supertagging in a wide-coverage CCG parser which uses a log-linear model to select an analysis.The supertagger reduces the derivation space over which model estimation is performed, reducing the space required for discriminative training.It also dramatically increases the speed of the parser.We show that large increases in speedcan be obtained by tightly integrating the su pertagger with the CCG grammar and parser.This is the first work we are aware of to success fully integrate a supertagger with a full parser which uses an automatically extracted grammar.We also further reduce the derivation space us ing constraints on category combination.The result is an accurate wide-coverage CCG parserwhich is an order of magnitude faster than comparable systems for other linguistically moti vated formalisms.
We investigate unsupervised techniques for acquiring monolingual sentence-level paraphrases from a corpus of temporally and topically clustered news articles collected from thousands of web-based news sources.Two techniques are employed: (1) simple string edit distance, and (2) a heuristic strategy that pairs initial (presumably summary) sentences from different news stories in the same cluster.We evaluate both datasets using a word alignment algorithm and a metric borrowed from machine translation.Results show that edit distance data is cleaner and more easily-aligned than the heuristic data, with an overall alignment error rate (AER) of 11.58% on a similarly-extracted test set.On test data extracted by the heuristic strategy, however, performance of the two training sets is similar, with AERs of 13.2% and 14.7% respectively.Analysis of 100 pairs of sentences from each set reveals that the edit distance data lacks many of the complex lexical and syntactic alternations that characterize monolingual paraphrase.The summary sentences, while less readily alignable, retain more of the non-trivial alternations that are of greatest interest learning paraphrase relationships.
We explore unsupervised language model adaptation techniques for Statistical Machine Translation.The hypotheses from the machine translation output are converted into queries at different levels of representation power and used to extract similar sentences from very large monolingual text collection.Specific language models are then build from the retrieved data and interpolated with a general background model.Experiments show significant improvements when translating with these adapted language models.
Comparisons of automatic evaluation metrics for machine translation are usually conducted on corpus level using correlation statistics such as Pearson?s product moment correlation coefficient or Spearman?s rank order correlation coefficient between human scores and automatic scores.However, such comparisons rely on human judgments of translation qualities such as adequacy and fluency.Unfortunately, these judgments are often inconsistent and very expensive to acquire.In this paper, we introduce a new evaluation method, ORANGE, for evaluating automatic machine translation evaluation metrics automatically without extra human involvement other than using a set of reference translations.We also show the results of comparing several existing automatic metrics and three new automatic metrics using ORANGE.
We present a new HMM tagger that exploits context on both sides of a word to be tagged, and evaluate it in both the unsupervised and supervised case.Along the way, we present the first comprehensive comparison of unsupervised methods for part-of-speech tagging, noting that published results to date have not been comparable across corpora or lexicons.Observing that the quality of the lexicon greatly impacts the accuracy that can be achieved by the algorithms, we present a method of HMM training that improves accuracy when training of lexical probabilities is unstable.Finally, we show how this new tagger achieves state-of-the-art results in a supervised, non-training intensive framework.
Chinese word segmentation is a difficult, im portant and widely-studied sequence modelingproblem.This paper demonstrates the abil ity of linear-chain conditional random fields(CRFs) to perform robust and accurate Chinese word segmentation by providing a principled framework that easily supports the in tegration of domain knowledge in the form of multiple lexicons of characters and words.We also present a probabilistic new word detection method, which further improves performance.Our system is evaluated on four datasets usedin a recent comprehensive Chinese word segmentation competition.State-of-the-art perfor mance is obtained.
Although vast amounts of textual data are freely available, many NLP algorithms exploit only a minute percentage of it.In this paper, we study the challenges of working at the terascale.We present an algorithm, designed for the terascale, for mining is-a relations that achieves similar performance to a state-of-the-art linguistically-rich method.We focus on the accuracy of these two systems as a func tion of processing time and corpus size.
This work investigates the variation in a word?s dis tributionally nearest neighbours with respect to the similarity measure used.We identify one type ofvariation as being the relative frequency of the neighbour words with respect to the frequency of the target word.We then demonstrate a three-way connec tion between relative frequency of similar words, aconcept of distributional gnerality and the seman tic relation of hyponymy.Finally, we consider theimpact that this has on one application of distributional similarity methods (judging the composition ality of collocations).
This paper shows how to construct semantic representations from the derivations producedby a wide-coverage CCG parser.Unlike the dependency structures returned by the parser itself, these can be used directly for semantic in terpretation.We demonstrate that well-formed semantic representations can be produced for over 97% of the sentences in unseen WSJ text.We believe this is a major step towards wide coverage semantic interpretation, one of the key objectives of the field of NLP.
We present a system for the semantic role la beling task.The system combines a machine learning technique with an inference procedurebased on integer linear programming that supports the incorporation of linguistic and struc tural constraints into the decision process.Thesystem is tested on the data provided in CoNLL 2004 shared task on semantic role labeling and achieves very competitive results.
Identifying sentiments (the affective parts of opinions) is a challenging problem.We present a system that, given a topic, automatically finds the people who hold opinions about that topic and the sentiment of each opinion.The system contains a module for determining word sentiment and another for combining sentiments within a sentence.We experiment with various models of classifying and combining sentiment at word and sentence levels, with promising results.
In this paper we generalise the sentence compression task.Rather than sim ply shorten a sentence by deleting words or constituents, as in previous work, we rewrite it using additional operations such as substitution, reordering, and insertion.We present a new corpus that is suitedto our task and a discriminative tree-to tree transduction model that can naturallyaccount for structural and lexical mis matches.The model incorporates a novelgrammar extraction method, uses a lan guage model for coherent output, and canbe easily tuned to a wide range of compres sion specific loss functions.
In this paper, we present an approach to the automatic identification and correction ofpreposition and determiner errors in non native (L2) English writing.We show that models of use for these parts of speech can be learned with an accuracy of 70.06% and 92.15% respectively on L1 text, and present first results in an error detection task for L2 writing.
Most work on unsupervised entailment rule acquisition focused on rules between templates with two variables, ignoring unary rules - entailment rules betweentemplates with a single variable.In this paper we investigate two approaches for unsupervised learning of such rules and com pare the proposed methods with a binary rule learning method.The results show that the learned unary rule-sets outperform the binary rule-set.In addition, a novel directional similarity measure for learning entailment, termed Balanced-Inclusion, is the best performing measure.
In this paper we describe a methodologyfor detecting preposition errors in the writ ing of non-native English speakers.Our system performs at 84% precision andclose to 19% recall on a large set of stu dent essays.In addition, we address the problem of annotation and evaluation inthis domain by showing how current ap proaches of using only one rater can skew system evaluation.We present a sampling approach to circumvent some of the issuesthat complicate evaluation of error detec tion systems.
Recognizing analogies, synonyms, anto nyms, and associations appear to be fourdistinct tasks, requiring distinct NLP al gorithms.In the past, the four tasks have been treated independently, using a widevariety of algorithms.These four seman tic classes, however, are a tiny sample of the full range of semantic phenomena, andwe cannot afford to create ad hoc algo rithms for each semantic phenomenon; weneed to seek a unified approach.We propose to subsume a broad range of phenom ena under analogies.To limit the scope of this paper, we restrict our attention to the subsumption of synonyms, antonyms, and associations.We introduce a supervised corpus-based machine learning algorithm for classifying analogous word pairs, and we show that it can solve multiple-choice SAT analogy questions, TOEFL synonymquestions, ESL synonym-antonym questions, and similar-associated-both ques tions from cognitive psychology.
In addition to a high accuracy, short parsing and training times are the most important properties of a parser.However, pars ing and training times are still relatively long.To determine why, we analyzed thetime usage of a dependency parser.We il lustrate that the mapping of the features onto their weights in the support vectormachine is the major factor in time complexity.To resolve this problem, we implemented the passive-aggressive percep tron algorithm as a Hash Kernel.The Hash Kernel substantially improves the parsing times and takes into account thefeatures of negative examples built dur ing the training.This has lead to a higher accuracy.We could further increase theparsing and training speed with a paral lel feature extraction and a parallel parsing algorithm.We are convinced that the HashKernel and the parallelization can be ap plied successful to other NLP applicationsas well such as transition based depen dency parsers, phrase structrue parsers, and machine translation.
In this paper, we consider sentence sim plification as a special form of translation with the complex sentence as the source and the simple sentence as the target.We propose a Tree-based Simplification Model (TSM), which, to our knowledge, is the first statistical simplification model covering splitting, dropping, reorderingand substitution integrally.We also de scribe an efficient method to train our model with a large-scale parallel dataset obtained from the Wikipedia and Simple Wikipedia.The evaluation shows that our model achieves better readability scores than a set of baseline systems.
In this paper, we propose an approach toautomatically detect sentiments on Twit ter messages (tweets) that explores some characteristics of how tweets are written and meta-information of the words that compose these messages.Moreover, we leverage sources of noisy labels as our training data.These noisy labels were provided by a few sentiment detectionwebsites over twitter data.In our experi ments, we show that since our features areable to capture a more abstract representation of tweets, our solution is more ef fective than previous ones and also more robust regarding biased and noisy data, which is the kind of data provided by these sources.
Automated identification of diverse sen timent types can be beneficial for manyNLP systems such as review summariza tion and public media analysis.In some ofthese systems there is an option of assign ing a sentiment value to a single sentence or a very short text.In this paper we propose a supervised sentiment classification framework whichis based on data from Twitter, a popu lar microblogging service.By utilizing50 Twitter tags and 15 smileys as sen timent labels, this framework avoids theneed for labor intensive manual annotation, allowing identification and classifi cation of diverse sentiment types of shorttexts.We evaluate the contribution of different feature types for sentiment classification and show that our framework successfully identifies sentiment types of untagged sentences.The quality of the senti ment identification was also confirmed byhuman judges.We also explore dependencies and overlap between different sen timent types represented by smileys and Twitter hashtags.
Categorial unification grammars (CUGs) embody the essential properties of both unification and categorial grammar formalisms.Their efficient and uniform way of encoding linguistic knowledge in well-understood and widely used representations makes them attractive for computational applications and for linguistic research.In this paper, the basic concepts of CUGs and simple examples of their application will be presented.It will be argued that the strategies and potentials of CUGs justify their further exploration i the wider context of research on unification grammars.Approaches to selected linguistic phenomena such as long-distance dependencies, adjuncts, word order, and extraposition are discussed.
In this paper, we describe a means for automatically building very large neural networks (VLNNs) from definition texts in machine-readable dictionaries, and demonslrate he use of these networks for word sense disambiguation.Our method brings together two earlier, independent approaches to word sense disambiguation: the use of machine-readable dictionaries and spreading and activation models.The automatic construction of VLNNs enables real-size xperiments with neural networks for natural language processing, which in turn provides insight into their behavior and design and can lead to possible improvements.
The unique properties of lree-adjoining grammars (TAG) present a challenge for the application of 'FAGs beyond the limited confines of syntax, for instance, to the task of semantic interpretation or automatic translation of nat- ural h'mguage.We present a variant of "FAGs, called synchronous TAGs, which chmacterize correspondences between languages."lq\]e formalism's intended usage is to relate expressions of natural anguages to their associ- ated semantics represented in a logical tbrm language, or to their translates in another natural anguage; in sum- mary, we intend it to allow TAGs to be used beyond their role in syntax proper.We discuss the application of synchronous TAGs to concrete examples, mention- ing primarily in passing some computational issues that tu:ise in its interpretation.
Although stochastic techniques applied to syntax mod- eling have recently regained popularity, current lazl- guage models uffer from obvious inherent inadequacies.Early proposals uch as Markov Models, N-gram mod- els (Pratt, 1942; Shannon, 1948; Shannon, 1951) and tlidden Markov Models were very quickly shown to be linguistically not appropriate for natural language (e.g. Chomsky (1964, pages 13-18)) since they are unable to capture long distance dependencies or to describe hier- archically the syntax of natural anguages.Stochastic context-free granunar (Booth, 1969) is a hierarchical model more appropriate for natural languages, however none of such proposals (Lari and Young, 1990; Jelinek, Lafferty, and Mercer, 1990) perform as well as the sim- pler Markov Models because of the difficulty of captur- ing lexical information.The parameters of a stochas- tic context-free grammar do not correspond irectly to a distribution over words since distributional phenom- ena over words that are embodied by the application of *This work was partially supported by DARPA Grant N0014- 90-31863, ARO Grant DAAL03-89-C-0031 and NSF Grant 1RI90- 16592.We thank Aravind Joshi for suggesting the use of TAGs for statistical nalysis during a private discussion that followed a presentation bybS'ed Jdinek during the June 1990 meeting of the DARPA Speech and Natural Language Workshop.We are also grateful to Peter Braun, FYed Jelinek, Mark Liberman, Mitch Marcus, Robert Mercer, Fernando Pereira said Stuart Shieber for providing vMu~ble comments.more than one context-free rule cannot be captured un- der the context-freeness a sumption.This leads to the difficulty of maintaining a standard hierarchical model while capturing lexieal dependencies.This fact prompted researchers in natural language processing to give up hierarchical language models in the favor of non-hierarchical statistical models over words (such as word N-grams models).Probably for lack of a better language model, it has also been ar- gued that the phenomena that such devices cannot cap- ture occur relatively infrequently.Such argumentation is linguistically not sound.Lexicalized tree-adjoining grammars (LTAG) t com- bine hierarchical structures while being hxieany sensi- tive and are therefore more appropriate for statistical analysis of language.In fact, LTAGs are the simplest hierarchical formalism which can serve as the basis for lexicalizing context-free grammar (Schabes, 1990; Joshi and Sehabes, 1991).LTAG is a tree-rewriting system that combines trees of large domain with adjoining and substitution.The trees found in a TAG take advantage of the available x- tended domain of locality by localizing syntactic depen- dencies (such as finer-gap, subject-verb, verb-objeet) and most semantic dependencies ( uch as predicate- argument relationship).For example, the following trees can be found in a LTAG lexicon: S /k NP,L VIP VP A V NPI NP NP VP* ADV L I I I uts J~n p~nutJ hungrily Since the elementary trees of a LTAG are minimal syntactic and semantic units, distributional analysis of the combination of these elementary trees based on a training corpus will inform us about relevant statistical aspects of the language such as the classes of words appearing as arguments of a predicative lement, the distribution of the adverbs licensed by a specific verb, or the adjectives licensed by a specific noun.This kind of statistical analysis as independently sug- gested in (Resnik, 1991) can be made with LTAGs be- cause of their extended omain of locality but also be- cause of their lexiealized property.lWe attallnle familiarity throughout the paper with TAGs and its lexicallzed variant, See, for instance, (Joehl, 1987), (Schabes, Abeill~, and Joehi, 1988), (Schabes, 1990) or (Joslfi and Schabes, 1~1).ACTES DE COLING-92.NANTES, 23-28 AOUT 1992 4 2 6 PROC.OF COLING-92, NANTES, AUG. 23-28, 1992 In this paper, this intuition is made formally precise by defining the notion of a stochastic lexicalized tree- adjoining rammar (SLTAG).We present an algorithm for computing the probability of a sentence generated by a SLTAG, and finally we introduce an iterative algo- r ithm for estimathlg the parameters of a SLTAG given a training corpus of text.This algorithm can either be used for refining the parameters of a SLTAG or for inferring a tree-adjoining grammar frmn a training cor- pus.We also report preliminary experiments with this algorithm.Due to the lack of space, in this paper tim algorithms are described succinctly without proofs of correctness and more attention is given to tile concepts and tech- niques used for SLTAG.hfformally speaking, SLTAGs are defined by assigning a probability to tile event that an elementary tree is combined (by adjunction or substitution) on a specific node of another elementary tree.These events of com- bination are the stochastic processes considered.Since SLTAG are defined on the basis of the deriva- tion and since TAG allows for a notion of derivation independent from the trees that are derived, a precise mathematical definition of the SLTAG derivation must be given.For this purpose, we use stochastic linear in- dexed grammars (SLIG) to formally express SLTAGs derivations.Linear Indexed grammar (LIG) (Alto, 1968; Gazdar, 1985) is a rewriting system in which the non-terminal symbols are augmented with a stack, in addition to rewriting non-terminals, the rules of the grammar can have the effect of pushing or popping symbols on top of tile stacks that are associated with each non-terminal symbol.A specific rule is triggered by the non-termlnal on the left hand side of the rule and the top element of its associated stack.The productions of a LIG are restricted to copy the stack corresponding to tile non-terminal being rewrit- ten to at most one stack associated with a non-terminal symbol on tile right hand side of the production?In tile following, \[..p\] refers to a possibly unbounded stack whose top element is p and whose remaining part is schematically written as '..'.\[$\] represents a stack whose only element is the bottom of the stack.While it is possible to define SLIGs in general, we define them for the particular case where the rules are binary branching and where tile left hand sides are always incomparable.A stochastic linear indexed grammar, G, is denoted by (VN, VT, VI, S, Prod), where VN is a finite set of non- terminal symbols; VT is a finite set of terminal symbols; VI is a finite set of stack symbols; S E VN is the start symbol; Prod is a finite set of productions of the form: Xo\[$po\] --* a Xo\[..po\] --.x~\[..m\] x~\[$p~\] x0\[..po\] -~ Xl\[$pd x~\[-.p~\] Xo\[$Po\] --~ Xl\[$pl\] X2\[$p2\] where Xk E Vjv, a E VT and po ~.VI, Pl,P2 E V\[; P, a probability distribution which assigns a probability, 0 < P(X\[..z\] ~ A) < 1, to a rule, X\[..x\] -* A ~.Prodsuch 2LIGs have been shown to be weakly eqtfivalent to "Ibee- Adjoining Graramars (V~jay-Shanker, 1987).that tbe sum of the probabilities of all the rules that can be applied to any non-terminal nnotated with a stack is equal to one.More precisely if, VX E VN,Vp E VI: ~ p(xt..pl -~ A) = 1 A P(X \[..p\] --* A) should be interpreted as the probability that X\[..p\] is rewritten as A. A derivation starts from S associated with the empty stack (S\[$\]) and each level of the derivation must be validated by a production rule.The language of a SLIG is defined as follows: L = {w E VT~ \[ S\[$\]~w}.The probability of a derivation is defined as the prod- uct of tile probabilities of all individual rules involved (counting repetition) in the derivation, the derivation being validated by a correct configuration of the stack at each level.The probability of a sentence is then com- puted as the sum of the probabilities of all derivations of tile sentence.Following tile construction described in (Vijay- Shanker and Weir, 1991), given a LTAG, Glaa, we con- struct an equivalent LIG, G,ua.Tile constructed LIG generates tile same language as Gtag and each deriva- tion of Gtaa corresponds to a unique LIG derivation corresponds to a unique derivation in G,ua (and con- versely).In addition, a probability is assigned to each production of the LIG.For simplicity of explanation and without loss of generality we assume that each node in an elementary tree in Gt,9 is either a leaf node (i.e. either a foot node or a non-empty terminal node) or binary branching, a The construction of the equivalent SLIG follows.The non-terminal symbols of Gstia are the two sym- bols 'top' (t) and 'bottom' (b), tile set of terminal sym- bols is the same as the one of Gta9, the set of stack symbols is the set of nodes (not node labels) found in the elementary trees of Gla~ augmented with the bot- tom of tile stack ($), and tile start symbol is ' top' (t).For "all root nodes ~10 of an initial tree whose root is labeled by S, the following starting rules are added: t\[$\] ~ t\[$,t0\] (1) These rules state that a derivation must start from the top of the root node of some initial tree.P is the prob- ability that a derivation starts from the initial tree as- sociated with a lexical item and rooted by %.Then, for all node '/ in an elementary tree, the fol- lowing rules are generated.If rhT/2 are ttle 2 children of a node r/sucb that r/2 is on the spine (i.e. subsumes tile foot node), include: b\[..~l ~&' tI$n, lt\[-.,~l (2) Since (2) encodes an immediate domination link de- fined by the tree-adjoining rammar, its associated probability is one.Similarly, if thT/~ are the 2 children of a node r/such that r h is on the spine (i.e. subsumes the foot node), include: b\[..rt\] P=-*~ t\["rl~\]t\[$~\] (3) Since (3) encodes a~t immediate domination link de- fined by the tree-adjoining rammar, its associated probability is one.aThe algorlthnm explained ill this paper cart be generalized to lexicadized tree-adjoining granunars that need not be in Chottmky Normal Form using techniqu?~ similar the one found in (Schabet, 1991).ACIES DE COLING-92, NANTES, 23-28 AO~rf 1992 4 2 7 P~oc.OF COLING-92, NANTES, AUG. 23-28, 1992 * If ~/tT/2 are the 2 children of a node q such that none of them is on the spine, include: b\[$~\] p~l \]~\[$I~1\]t\[$i~2 \] (4) Since (4) also encodes an immediate domination link defined by the tree-adjoining grammar, its associated probability is one.If 7?is a node labeled by a non-terminal symbol and if it does not have an obligatory adjoining constraint, then we need to consider the case that adjunetion might not take place.In this ease, include: t\[..~\] L b\[..~\] (5) The probabil ity of rule (5) corresponds to the proba- bility that no adjunetion takes place at node q. o If t/ is an node on which the auxiliary tree fl can be adjoined, the adjunetiou of fl can be predicted, therefore (assuming that ~tr is the root node of fl) include: t\["0\] L t\[..rl,,\] (6) The probability of rule (6) corresponds to the proba- bility of adjoining the auxiliary tree whose root node is ~/~, say/3, on the node 0 belonging to some elemen- tary tree, say a.4 ? If r)!is tim foot node of an auxiliary tree fl that has been adjoined, then the derivation of the node below q\] must resume.In this case, include: b\["0l\] ,~1 b\[..\] (7) The above stochastic production is included with probabil ity one since the decision of adjunction has already been made in rules of the form (6).Finally, if r h is the root node of an initial tree that can be substituted on a node marked for substitution r), include: t\[$~\] L t\[S~t\] (g) Here, p is the probability that the initial tree rooted by ~/~ is substituted at node q. It corresponds to the probability of substituting the lexicalized initial tree whose root node is 71, say 6, at the node q of a lexicalized elementary tree, say a. 5 The SLIG constructed as above is well defined if the following equalities hold for all nodes ~l: P(t\[..~/\] ---* b\[..~/\]) + E P(t\[..~/\] --* t\[..q0~\] ) = 1 (9) P(t\[$~/\] ---* t\[$Ol\]) ---- 1 (10) E P(t\[$\] -~ t\[$O0\]) = 1 (11) 4Since the granmmr is lexicalized, both trees a and /3 are a~ sociated with lexical iter~s, mad the site node for adjtmction ~ correuponds to some syntactic modification.Such llde encapsu- lates S modifiers (e.g. s~tential adverbs as in "apparently John left"), VP modifiers (e.g. verb phr~e adverbs as in "John left abruptly}", NP modifiers (e.g. relative clauses as in "The man who left was happy"), N modifiers (e.g. adtieetive~ asin "prelty woman"), or even sententiM complements (e.g. John think8 that Harry is sick).s Among other cases, the probability of thi~ rule corr~ponds to the probability of filling some argument p(~ition by a lexiealized tree.It will encapsulate he distribution for Belectional restriction since the position of substitution is taken into account.A gramular satisfying (12) is called consistent.6 E P ( t \ [$ \ ]~w)= 1 (12) wEZ* Beside the distributional phenomena that we mentioned earlier, SLTAG also captures the effect of adjoining con- straints (selective, obligatory or null adjoining) which are required for tree-adjoining rammar . 7 Probab i l i ty of a Sentence We now define an bottom-up algorithm for SLTAG which computes the probability of an input string.The algorithm is an extension of the CKY-type parser for tree-adjoining grammar (Vijay-Shanker, 1987).The ex- tended algorithm parses all spans of the input string and also computes tbelr probability in a bottom-up fashion.Since the string on the frontier of an auxiliary is bro- ken up into two substrings by the foot node, for the purpose of computing the probability of the sentence, we will consider the probability that a node derives two substrings of the input string.This entity will be called the inside probability.Its exact definition is given be- low.We will refer to the subsequenee of the input string w = ax "" aN from position i to j , w{'.It is defined as follows: w~/'~f { a i+t" .uj , i f i>_ j ' i f /< j Given a string w = at . . .a N and a SLTAG rewritten as in (1-8) the inside probability, F (pos , 71, i, j , k,l), is defined for all nodes 7/ contained in an elementary tree and for pos E {t,b}, and for all indices 0 < i < j < k < I < N as follows: (i) If the node 7/does not subsume the foot node of (~ (if there is one), then j and k are un- bound and: l~ (pos, ~, i , - , - , I) d~=l P(pos\[$@~ w~) (it) If the node y/subsumes the foot node 7/!of e, then: l~ (pos, rL i, j, k, l) a~l P ( pos \ [$@~ w{ b\[$o l lw~ ) In (ii), only the top element of the stack matters ince as a consequence of the eonstrnction of the SLIG, we have that if pos\[$tl\]~ w~b\[$rll\]w ~ then for all string 7 e V/~ we also have pos\[$Tr/\]~ w~b\[$7~l\]w~.S Initially, all inside probabilities are set to zero.Then, the computat ion goes bottom-up start ing from the pro- ductions introducing lexieal items: if r/ is a node such that b\[$7/\] --~ a, then: 1 i f l= i+ lAa=w~ +t (1~ IW(b 'T l ' i ' - ' - ' l ) = 0 otherwise.Then, the inside probabilities of larger substrings are computed bottom-up relying on the recurrence qua- ~We will not investigate im conditions under which (12) holds.We conjecture that the techniques used for dmcking the eolmis- tency of stochastic context-free grammars (Booth and Thomp6on, 1973) can be adapted to SLTAG.r For example, for a given node 0 setting to zero the probability o\[ all rules of the forts (6) ht~ the effect of blocking adjunction.8Thls can be seen by obae~.ing that for any node on the path from the root node to the foot node of an auxiliary tree, the stack remains unchanged.ACRES DE COLING-92, NANTES.23-28 AOt~T 1992 4 2 8 PROC.OF COLING-92, NANTES.AUG. 23-28, 1992 lions stated in Appendix A. This computation takes in the worst case O(IGl~N6)-time and O(IGINa)-space for a sentence of lengtb N. Once the inside probabilities cmnputed, we obtain the probability of the sentence flu follows: P(w)aJP(t\[$\]~,~) = Z~(t, $, 0 , - , - , Iwl) (14) Wc now consider the problem of re-estimating a SI,TAG.4 Ins ide -Ous ide A lgor i thm for.1%eest imat ing a SLTAG Given a set of positive example sentences, W = {wt ' "wK}, we would like to compute the probabil- ity of each rule of a given SLTAG in order to maximize thc probability that the corpus were generated by this SLTAG.An algorithm solving this problem can be used in two different ways.The first use is as a reestimation algorithm.In ttfis approach, the input SI,'1'A(~ derives structures that arc reasonable according to some criteria (such as a linguis- tic theory and some a priori kuowledge of the corpus) and the intended use of the algorithm is to refine the probability of each rule.The second use is as a learning algorithm.At the first iteration, a SLTAG which generates all possible struc- tures over a given set of nodes and terminal symbols is used.Initially the probability of each rule is randomly assigned and then tile algorithm will re-estimate tbese probabilities.Informally speaking, given a first estimate of the pa- rameters of a SLTAG, the algorithm re-estimates these parameters on the basis of the parses of each sentence in a training corpus obtained by a CKY-tyt)e parser.The algorithm is designed to derive a new estimate after each iteration such that the probability of the corpus is increased or equivalently such that tile cross entropy estimate (negative log probability) is decreased: log~(e(r0)) l t (W,G) - weW (15) wEW In order to derive a new estimate, the algorithm needs to compute for all seutences in W the in- side probabilities and the outside probabilities.Given a string w = al . . .aN, tbe outside probability, 0 ~ (pos, ~, i, j, k, It, is defined for all nodes r I contained in an elementary tree a and for pos E {t,b}, and for all indices 0 < i < j < k < l < N as follows: (it If the node r/does not subsume the foot node of a (if there is one), then j and k axe un- bound asld: ..de\] O'?(P os, O, i, - , - , t) - P(B"/ C V~ s.t. t\[$\]=~ Wio pos\[$Ttl\] w~) (ii) If the node ~/does ubsume the foot node ~/!of a then: 0 '~ (pos, O, i, j, k, l) aeJ- /'(37 ~ V~* s.t. t \ [$\]~ Wlo pos\[$Trl\] w~ and b\[$7~ll\]~w\]) Once the inside probabilities computed, the outside probabilities can be computed top-down by consider- ing smaller spans of the input string starting with O"( t ,$ ,O , - , - ,N ) = 1 (by definition).This is done by computing the recurrence quations tated in Ap- pendix B. In the following, we assume that r I subsumes the foot node r/l within a same elementary tree, and also that tll subsumes the foot node ~111 (within a same elementary tree).The other cases are handled similarly.Table 1 shows the reestimation formulae for the adjoining rules (16) and the null adjoining rules (17).(16) corresponds to the average number of time that tl . . .le L\[..T1\] .-* t\[..yqv\] is used, and (17) to th . . .age number of times no adjunction occnrred on T/.The denominators of (16) and of (17) estimate the average number of times that a derivation involves tlLe expan- sion oft\[-.~/\].The numerator of(16) estimates the aver- age number of times that a derivation involves the rule t\[.-7/\] -~ t\[..Tirfl\].Therefore, for example, (16) estimates the probability of using the rule/\['-~7\] ~ l\["rplt\].The algorittun reiterates until H(W, G) is unchanged (within some epsilon) between two iterations.Each it- eration of the algoritbm requires at most O(IGIN e) time for each sentence of length N. 5 Grammar In ference w i th.SLTAG The reestimation algorithm explained in Section 4 can be used botll to reestimate the paramcters for a SI,TAG derived by some other mean or to infer a grammar from scratch.Ill the following, we investigate grammar In- ference from scratch.The initial grammar for the reestimation algoritiim consists of all SLIG rules for the tress ill Lexical- ized Normal I~brm (ill short LNF) over a given set = {aill .< i _< T} of terminal symbols, with suit- ably assigned non zero probability: 9 S 0 $4 s h t~ a i The above normal form is capable not only to de- rive any lexicalized tree-adjoining language, but also to impose ally binary bracketing over the strings of the language.The latter property is important as we would like to be able to use bracketing information in the ilL- put corpus as in (Pereira and Schabes, 1992).The worst case complexity of tim reestimation algo- r ithm given iu Section 4 with respect o the length of the input string (O(NS)) makes this approach in gen- eral impractical for LNF grammars.However, if only trees of the form fit a' and a~" (or only of tile form /~' and a~) , the language generated is a context-free language and can be handled more efficiently by the reestimation algorithnL 9Adjoining constraints can be u~d in tiffs normal form, They will be reflected in the SLIG eq~vaient grammar.Indices have been added on S nodes in order to be able to uniquely refer to each node in the granunar.AcrEs OE COLING-92, NANTES.23-28 AOOT 1992 4 2 9 DROC.OF COLING-92, NANTES, AUG. 23-28, 1992 wwPW ) x QW(t\[..~/\] ~ t\[.-r/rp\]) P(t\[-.t/\] ---, t\[..~Tt/t\]) = 1 (16) ~wp--- ~ x \[R~0/) + ~_~O'~(t\[..O\] --, t\[..~/r/,\])\] 1 to~w /3(t\[..r/\] ---+ b\[..~/\]) = 1 (17) Ot?(t\["r/\] ~ t\["r/rY\]) = Z P(t\["O\]--*t\["O~Y\])?Iw(t'o/ ' i ' r 's ' l )xlW(b'o'r ' j 'k 's)xOW(t'~l' i ' j 'k ' l ) (18) i)r,j~k,t)l /~w(r/) = ~ P(t\[..r/\] ~ b\[..r/\]) x l~(t ,o, i , j ,k , l ) x O~?(b,)l,i,j,k,l) (19) i,j,k,I Table 1: Keestimation of adjoining rules (16) and null adjoining rules (17) It can be shown that if, only trees of the form ~a~ and ~a~ are considered, the reestimation algorithm requires in the worst case O(Na)-t ime) ? The system consisting of trees of the form ~' and c~ can be seen as a stochastic lexicalized conle~:t-free gram- mars since it generates exactly context-free languages while being lexically sensitive.In the following, due to the lack of space, we report only few experiments on grammar inference using these restricted forms of SLTAG and the reestimation algo- rithm given in Section 4.We compare the results of the TAG inside-outside algorithm with the results of the inside-outside algorithm for context-free grammars (Baker, 1979).These preliminary experiments suggest that SLTAG achieves faster convergence (and also to a better solu- tion) than stochastic ontext-free grmnmars.5.1 In ferr ing the Language {a"b"\]n > 0}.We consider first an artificial language.The train- ing corpus consists of 100 sentences in the language L = {a"b'~ln > 0} randomly generated by a stochastic context-free grammar.The initial grammar consists of the trees ~' , fl~, c~ a and ab with random probability of adjoining and null adjoining.The inferred grammar models correctly the language L. Its rules of the form (I), (5) or (fi) with high prob- ability follow (any excluded rule of the same form has probability at least l0 -a3 times lower than the rules given below).The structural rules of the form (2), (3), (4) or (7) are not shown since their probability always remain 1.Z?This can be Been by ol~ervin g that, for exaanple in l(posji, i,j,k,I), it i~ nece~y the ea~ that k = l, nnd also by noting that k is superfluous.t\[$,Tg\] s:~4 t\[S,lg,78\] t\[$og\] o_~ t\[$,lg,lg\] t\[.-t/~\] z_~,o b\[,.~7~\] t \ [~\ ] ,..~o b\[,~\] t\[..~\] ~,?b \ [~\ ] t\[..o~\] 1~0 b\[..o~\] In the above grammar, a node S'k in a tree c~ a or /~ associated with the symbol a is referred as t/~, and a node S~ in a tree associated with b as r/~.We also conducted a similar experiment with the inside-outside algorithm for context-free grammar (Baker, 1979), starting with all pc~sible Chomsky Nor- mal Form rules over 4 non-terminals and the set of ter- minal symbols {a,b} (72 rules).The inferred grammar does not quite correctly model the language L. Fur- thermore, the algorithm does not converge as fast as in the case of SLTAG (See Figure 1).1.8 1 .6 1,4 1.2 1 0 .8 0 .6 0.4 I I I I I I I I SLTAG - - SCFG . . ." \ 2 3 4 5 6 7 8 9 1 0 iteration Figure 1: Convergence for the Language {anb"ln > 0} 5.2 Exper iments on the ATIS Corpus.We consider the part-of-speech sequences of the spoken- language transcriptions in the Texas Instruments sub- ACT~ BE COIANG-92.NANTES, 23-28 AO~' 1992 4 3 0 PROC.OF COLING-92, NANTES, AUG. 23-28, 1992 set of the Air Travel hfformation System (ATIS) corpus (Hemphill, Godfrey, and Doddington, 1990).This cor- pus is of interest since it has been used for infcrring stochastic ontext-free grammars from partially brack- eted corpora (Pereira and Sehabes, 1992).We use the data given by Pereira and Schabes (1992) on raw text and compare with an inferred SLTAG.The initial grammar consists of all trees (96) of the form fl~, a ~ for all 48 terminal symbols for part-of- speech.As shown in Figure 2, the grannnar converges very rapidly to a lower value of the log probability than the stochastic ontext-free grammar eported by Pereira and Schabes (1992).16 14 12 i0 SCFG ....." i i i t 5 10 15 20 25 iteration Figure 2: Convergence for ATIS Corpus 6 Conc lus ion.A novel statistical language model and fundamental - gorithms for this model have been presented.SLTAGs provide a stochastic model both hierarchi- cal and sensitive to lexical information.They combiae the advantages of purely |exical models such ms N-gram distributions or Ilidden Markov Models and the one of ifierarchical modes as stochastic ontext-free gram- mars without their inhercnt limitations.The parame- ters of a SLTAG correspond to the probability of com- bining two structures each one associated with a word and therefore capture linguistically relevant distribu- tions over words.An algorithm for computing the probability of a sen- tence generated by a SLTAG was presented as well as an iterative algorithm for estimating the parameters of a SLTAG given a training corpus of raw text.Simi- larly to its context-free counterpart, he reestimation algorithm can be extended to handle partially parsed corpora (Pereira and Schabes, 1992).Preliminary experiments with a context-free subset of SLTAG confirms that SLTAG enables faster conver- gence than stochastic ontext-free grammars (SCFG).This is the case since SCFG are unable to represent lexieal influences on distribution except by a statisti- cally and eomputationally impractical proliferation of nonterminal symbols, whereas SLTAG allows for a lexi- eally sensitive distributional mmlysis while maintaining a hierarchical structure.Furthermore, the techniques explained in this paper apply to other grammatical formalisms uch as combi- natory categorial grammars and modified head gram- mars since they have been proven to be equivalent to tree-adjoining grammars and linear indexed grmnmars (Joshi, Vijay-Shanker, and Weir, 1991).Due to the lack of space, only few experiments with SLTAG were reported.A full version of tile paper will be available by tile time of the meeting and more exper- imental details will be reported uring the presentation of the paper.In collaboration with Aravind Joshi, Fernando Pereira and Stuart Slfieber, we are currently investigat- ing additional algorithnLs and applications for SLTAG, methods for lexical clustering and autonratic onstruc- tion of a SLTAG from a large training corpus.Aho, A. V. 1968.lndexed grammars - An extension to context free grammars.J ACM, 15:647-671.Baker, J.K. 1979.Trainable grammars tbr speech recognition.In Jared J. Wolf and Dennis H. Klatt, editors, Speech communication papers presentacd at the 97 ~h Meeting of the Acoustical Society of Amer- ica, MIT, Cambridge, MA, June.llooth, Taylor R. and Richard A. Thoml)son.1973.Applying probability measures to abstract languages.IEEE 7)'aasactions on Computers, C-22(5):442-450, May. Booth, T. 1969.Probabilistic representation f formal languages.In Tenth Annual IEEE Symposium on Switching and Automata Theory, October.Chomsky, N., 1964.Syntactic Structures, chapter 2-3, pages 13-18.Mouton.Gazdar, G. 1985.Applicability of indexed gr,'unmars to natural anguages.Technical Report CSLI-85-34, Center for Study of Language and Information.tlempttill, Charles T., John J. Godfrey, and George IL Doddington.1990.The ATIS spoken language sys- tems pilot corpus.In DARPA Speech and Natural Laaguage Workshop, Hidden Valley, Pennsylvania, June.Jelinek, F., J. D. Lafferty, and R. L. Mercer.1990.Ba- sic methods of probabilistic ontext free grammars.Technical Report RC 16374 (72684), IBM, Yorktown Heights, New York 10598.Joshi, Aravind K. and Yves Schabes.1991.Tree- adjoiuing grammars and lexiealized grammars.In Maurice Nivat and Andreas Podelski, editors, Defin- ability and Recognizability ofSets of Trees.Elsevier.Forthcoming.Joshi, Aravind K., K. Vijay-Simnker, and David Weir.1991.The convergence of mildly context-sensitive gramnmtical formalisms, in Peter Sells, Stuart Shieber, and Tom Wasow, editors, Foundational Is- sues in Natural Language Processing.MIT Press, Cambridge MA.Joshi, Aravind K. 1987.An Introduction to Tree Ad- joining Grammars.In A. Manaster-Ramer, editor, Mathematics of Language.John Beujamins, Amster- dana.Lari, K. and S. J. Young.1990.The estimation of stochastic ontext-free grmnmars using the Inside- Outside algorithm.Computer Speech and Language, 4:35-56.ACRES DE COL1NG-92, NANTES, 23-28 AO~r 1992 4 3 1 PROr'..OI: COLING-92, NANTES, AUG. 23-28, 1992 Pereira, Fernando and Yves Schabes.1992.Inside- outside reest imation from partial ly bracketed cor- pora.In 20 th Meeting of the Association for Compu- tational Linguistics (ACL '9~), Newark, Delaware.Prat t , Fletcher.1942.Secret and urgent, the story of codes and ciphers.Blue Ribbon Books.Resnik, Philip.1991.Lexicalized tree-adjoining ram- mar for distr ibutional analysis.In Penn Review of Linguistics, Spring.Schabes, Yves, Anne Abeill~, and Aravind K. Joshi.1988.Pars ing strategies with ' lexicalized' grarnmars: Application to tree adjoining gra~mnars.In Proceed- ings of the 1~ lh International Conference on Compu- tational Linguistics (COLING'88}, Budapest, Hun- gary, August . Sehabes, Yves.1990.Mathematical nd Computational Aspects of Lexicalized Grammars.Ph.D. thesis, Uni- versity of Pennsylvania, Philadelphia, PA, August.Available as technical report (MS-CIS-90-48, L INC LAB179) from the Department of Computer Science.Schabes, Yves.1991.An inside-outside algor i thm for est imat ing the parameters of a hidden stochastic context-free grammar based on Earley's algorithm.Manuscript.Shannon, C. E. 1948.A mathemat ica l theory of communicat ion.The Bell System Technical Journal, 27(3):379-423.Shannon, C. E. 1951.Predict ion and entropy of printed english.The Bell System Technical Journal, 30:50- 64.Vi jay-Shanker, K. and David J. Weir.1991.Pars ing constrained grammar formalisms.In preparation.Vi jay-Shanker, K. 1987.A Study of ?lbee Adjoining Grammars.Ph.D. thesis, Department of Computer and Information Science, University of Pennsylvmfia.A Comput ing the Ins ide P rob- ab i l i t i es In the following, the inside and outside probabilities are re\]ative to the input string w. 3 t" stands for the the set of foot nodes, S for the set of nodes on which substitution can occur, ~ for the set of root nodes of initial trees, and ,4 for the set of non-terminal nodes of auxiliary trees.The inside probability can be computed bottom-up with the following recurrence quations.For all node v/found in an elementary tree, it can be shown that: 1.If b\[$r/\] ~ a, I(b,7, i , - , - , I ) = dl if / = i+ 1 and if.a = w~ +1, 0 otherwise.2.\] f71 E3 c, l(b,7/,i,j,k,t)= l if i= j and if k = l, 0 otherwise.
We present an efI\]cient, broad-coverage, principle-based parser for English.The parser has been implemented in C++ and runs on SUN Sparcstations with X-windows.It con- rains a lexicon with over 90,000 entries, con- structed automatically b applying a set of ex- traction and conversion rules to entries from machine readable dictionaries.
Various methods have been proposed for aligning texts in two or more languages such as the Canadian Parliamentary Debates (Hansards).Some of these methods generate a bilingual lexicon as a by-product.We present an alternative alignment strategy which we call K-vec, that starts by estimating the lexicon.For example, it discovers that the English word fisheries is similar to the French p~ches by noting that the distribution of fisheries in the English text is similar to the distribution of p~ches in the French.K-vec does not depend on sentence boundaries.
I:n this paper, we describe a new corpus-based ap- proach to prepositional phrase attachment disam- biguation, and present results colnparing peffo> mange of this algorithm with other corpus-based approaches to this problem.
We present an algorithm for anaphora res- olutkm which is a modified and extended version of that developed by (Lappin and Leass,/994).In contrast to that work, our al- gorithm does not require in-depth, full, syn..tactic parsing of text.Instead, with minimal compromise in output quality, the modifica- tions enable the resolution process to work from tile output of a part of speech tag- ge~; enriched only with annotations of gram- matica\] functkm of lexical items in the in- put text stream.Evaluation of the results of our in-tplementation demonstrates that ac- curate anaphora resolution can be realized within natural anguage processing fl'ame- works which do not--~,)r cannot- employ ro- bust and rcqiable parsing components.
In this paper, we describe a new model for word alignment in statistical trans- lation and present experimental results.The idea of the model is to make the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions.To achieve this goal, the approach us- es a first-order Hidden Markov model (HMM) for the word alignment problem as they are used successfully in speech recognition for the time alignment prob- lem.The difference to the time align- ment HMM is that there is no monotony constraint for the possible word order- ings.We describe the details of the mod- el and test the model on several bilingual corpora.
Shallow semantic parsing, the automaticidentification and labeling of sentential constituents, has recently received much attention.Our work examines whether seman tic role information is beneficial to questionanswering.We introduce a general frame work for answer extraction which exploits semantic role annotations in the FrameNetparadigm.We view semantic role assignment as an optimization problem in a bipartite graph and answer extraction as an instance of graph matching.Experimental results on the TREC datasets demonstrate im provements over state-of-the-art models.
This paper presents a syntax-driven ap proach to question answering, specifically the answer-sentence selection problem forshort-answer questions.Rather than using syntactic features to augment exist ing statistical classifiers (as in previouswork), we build on the idea that ques tions and their (correct) answers relate toeach other via loose but predictable syntactic transformations.We propose a probabilistic quasi-synchronous grammar, inspired by one proposed for machine translation (D. Smith and Eisner, 2006), and parameterized by mixtures of a robust non lexical syntax/alignment model with a(n optional) lexical-semantics-driven log-linear model.Our model learns soft alignments as a hidden variable in discriminative training.Experimental results using the TREC dataset are shown to significantly outperform strong state-of-the-art baselines.
We show for the first time that incorporatingthe predictions of a word sense disambigua tion system within a typical phrase-based statistical machine translation (SMT) model consistently improves translation qualityacross all three different IWSLT ChineseEnglish test sets, as well as producing sta tistically significant improvements on the larger NIST Chinese-English MT task?and moreover never hurts performance on any test set, according not only to BLEUbut to all eight most commonly used au tomatic evaluation metrics.Recent work has challenged the assumption that word sense disambiguation (WSD) systems areuseful for SMT.Yet SMT translation qual ity still obviously suffers from inaccurate lexical choice.In this paper, we addressthis problem by investigating a new strategy for integrating WSD into an SMT sys tem, that performs fully phrasal multi-worddisambiguation.Instead of directly incor porating a Senseval-style WSD system, weredefine the WSD task to match the ex act same phrasal translation disambiguation task faced by phrase-based SMT systems.Our results provide the first known empirical evidence that lexical semantics are in deed useful for SMT, despite claims to the contrary.?This material is based upon work supported in part by the Defense Advanced Research Projects Agency (DARPA) under GALE Contract No.HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants
Many systems for tasks such as question answering, multi-document summarization, and infor mation retrieval need robust numerical measures of lexical relatedness.Standard thesaurus-based measures of word pair similarity are based on only a single path between those words in the thesaurus graph.By contrast, we propose a newmodel of lexical semantic relatedness that incorporates information from every explicit or implicit path connecting the two words in the en tire graph.Our model uses a random walk over nodes and edges derived from WordNet links and corpus statistics.We treat the graph as aMarkov chain and compute a word-specific sta tionary distribution via a generalized PageRank algorithm.Semantic relatedness of a word pair is scored by a novel divergence measure, ZKL, that outperforms existing measures on certain classes of distributions.In our experiments, the resultingrelatedness measure is the WordNet-based measure most highly correlated with human similar ity judgments by rank ordering at ? = .90.
We consider the problem of learning toparse sentences to lambda-calculus repre sentations of their underlying semantics and present an algorithm that learns a weighted combinatory categorial grammar (CCG).A key idea is to introduce non-standard CCG combinators that relax certain parts of the grammar?for example allowing flexible word order, or insertion of lexical items?with learned costs.We also present a new, online algorithm for inducing a weighted CCG.Results for the approach on ATIS data show 86% F-measure in recovering fully correct semantic analyses and 95.9% F-measure by a partial-match criterion, a more than 5% improvement over the 90.3% partial-match figure reported by He and Young (2006).
We present a nonparametric Bayesian model of tree structures based on the hierarchical Dirichlet process (HDP).Our HDP-PCFG model allows the complexity of the grammar to grow as more training data is available.In addition to presenting a fully Bayesianmodel for the PCFG, we also develop an ef ficient variational inference procedure.Onsynthetic data, we recover the correct grammar without having to specify its complexity in advance.We also show that our tech niques can be applied to full-scale parsingapplications by demonstrating its effective ness in learning state-split grammars.
This paper proposes a tree kernel with contextsensitive structured parse tree information for re lation extraction.It resolves two critical problems in previous tree kernels for relation extraction in two ways.First, it automatically determines a dynamic context-sensitive tree span for relation ex traction by extending the widely-used Shortest Path-enclosed Tree (SPT) to include necessary context information outside SPT.Second, it pro poses a context-sensitive convolution tree kernel, which enumerates both context-free and context sensitive sub-trees by considering their ancestor node paths as their contexts.Moreover, this paper evaluates the complementary nature between our tree kernel and a state-of-the-art linear kernel.Evaluation on the ACE RDC corpora shows that our dynamic context-sensitive tree span is much more suitable for relation extraction than SPT and our tree kernel outperforms the state-of-the-art Collins and Duffy?s convolution tree kernel.It also shows that our tree kernel achieves much bet ter performance than the state-of-the-art linear kernels . Finally, it shows that feature-based and tree kernel-based methods much complement each other and the composite kernel can well integrate both flat and structured features.
Syntactic reordering approaches are an effective method for handling word-order differences between source and target lan guages in statistical machine translation(SMT) systems.This paper introduces a reordering approach for translation from Chinese to English.We describe a set of syntac tic reordering rules that exploit systematic differences between Chinese and English word order.The resulting system is used as a preprocessor for both training and test sentences, transforming Chinese sentences to be much closer to English in terms of their word order.We evaluated the reordering approach within the MOSES phrase-based SMT system (Koehn et al, 2007).The reordering approach improved the BLEU score for the MOSES system from 28.52 to 30.86 on the NIST 2006 evaluation data.Wealso conducted a series of experiments to an alyze the accuracy and impact of different types of reordering rules.
We achieved a state of the art performance in statistical machine translation by using a large number of features with an onlinelarge-margin training algorithm.The mil lions of parameters were tuned only on a small development set consisting of less than1K sentences.Experiments on Arabic-to English translation indicated that a modeltrained with sparse binary features outper formed a conventional SMT system with a small number of features.
This paper reports on the benefits of largescale statistical language modeling in machine translation.A distributed infrastruc ture is proposed which we use to train on up to 2 trillion tokens, resulting in language models having up to 300 billion n-grams.Itis capable of providing smoothed probabilities for fast, single-pass decoding.We in troduce a new smoothing method, dubbed Stupid Backoff, that is inexpensive to train on large data sets and approaches the quality of Kneser-Ney Smoothing as the amount of training data increases.
We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level ?may it be linguistic markup or automatically generated word classes.In a num ber of experiments we show that factoredtranslation models lead to better translation performance, both in terms of auto matic scores, as well as more grammatical coherence.
The Conference on Computational Natural Language Learning features a shared task, inwhich participants train and test their learn ing systems on the same data sets.In 2007, as in 2006, the shared task has been devoted to dependency parsing, this year with both a multilingual track and a domain adaptation track.In this paper, we define the tasks of the different tracks and describe how the data sets were created from existing treebanks for ten languages.In addition, we characterize the different approaches of the participating systems, report the test results, and provide a first analysis of these results.
We describe a two-stage optimization of the MaltParser system for the ten languages in the multilingual track of the CoNLL 2007 shared task on dependency parsing.The first stage consists in tuning a single-parsersystem for each language by optimizing parameters of the parsing algorithm, the fea ture model, and the learning algorithm.Thesecond stage consists in building an ensemble system that combines six different parsing strategies, extrapolating from the opti mal parameters settings for each language.When evaluated on the official test sets, the ensemble system significantly outperforms the single-parser system and achieves the highest average labeled attachment score.
We present experiments with a dependency parsing model defined on rich factors.Ourmodel represents dependency trees with factors that include three types of relations be tween the tokens of a dependency and theirchildren.We extend the projective pars ing algorithm of Eisner (1996) for our case,and train models using the averaged perceptron.Our experiments show that considering higher-order information yields signifi cant improvements in parsing accuracy, but comes at a high cost in terms of both timeand memory consumption.In the multi lingual exercise of the CoNLL-2007 shared task (Nivre et al, 2007), our system obtains the best accuracy for English, and the second best accuracies for Basque and Czech.
It is possible to reduce the bulk of phrasetables for Statistical Machine Translation us ing a technique based on the significance testing of phrase pair co-occurrence in the parallel corpus.The savings can be quitesubstantial (up to 90%) and cause no reduction in BLEU score.In some cases, an im provement in BLEU is obtained at the same time although the effect is less pronounced if state-of-the-art phrasetable smoothing is employed.
A major engineering challenge in statistical machine translation systems is the efficient representation of extremely large translationrulesets.In phrase-based models, this prob lem can be addressed by storing the training data in memory and using a suffix array asan efficient index to quickly lookup and extract rules on the fly.Hierarchical phrasebased translation introduces the added wrin kle of source phrases with gaps.Lookup algorithms used for contiguous phrases nolonger apply and the best approximate pat tern matching algorithms are much too slow, taking several minutes per sentence.Wedescribe new lookup algorithms for hierar chical phrase-based translation that reduce the empirical computation time by nearly two orders of magnitude, making on-the-fly lookup feasible for source phrases with gaps.
We develop latent Dirichlet alocation with WORDNET (LDAWN), an unsupervised probabilistic topic model that includes word sense as a hidden variable.We develop a probabilistic posterior inference algorithm for simultaneously disambiguating a corpusand learning the domains in which to consider each word.Using the WORDNET hierarchy, we embed the construction of Ab ney and Light (1999) in the topic model and show that automatically learned domainsimprove WSD accuracy compared to alter native contexts.
We present a data-driven variant of the LR algorithm for dependency parsing, and extend it with a best-first search for probabil istic generalized LR dependency parsing.Parser actions are determined by a classifier, based on features that represent the current state of the parser.We apply this pars ing framework to both tracks of the CoNLL 2007 shared task, in each case taking ad vantage of multiple models trained with different learners.In the multilingual track, we train three LR models for each of the ten languages, and combine the analyses obtained with each individual model with a maximum spanning tree voting scheme.In the domain adaptation track, we use two models to parse unlabeled data in the target domain to supplement the labeled out-of domain training set, in a scheme similar to one iteration of co-training.
The technology of opinion extraction allowsusers to retrieve and analyze people?s opinions scattered over Web documents.We define an opinion unit as a quadruple consist ing of the opinion holder, the subject being evaluated, the part or the attribute in which the subject is evaluated, and the value of theevaluation that expresses a positive or neg ative assessment.We use this definition as the basis for our opinion extraction task.We focus on two important subtasks of opinion extraction: (a) extracting aspect-evaluationrelations, and (b) extracting aspect-of re lations, and we approach each task usingmethods which combine contextual and sta tistical clues.Our experiments on Japaneseweblog posts show that the use of contex tual clues improve the performance for both tasks.
This paper presents a new hypothesis alignment method for combining outputs of multiple machine translation (MT) systems.An indirect hidden Markov model (IHMM) is proposed to address the synonym matching and word ordering issues in hypothesis alignment.Unlike traditional HMMs whose parameters are trained via maximum likelihood estimation (MLE), the of the IHMM are estimated a variety of sources including word semantic similarity, word surface similarity, and a distance-based distortion penalty.The IHMM-based method significantly outperforms the state-of-the-art TER-based alignment model in our experiments on NIST benchmark datasets.Our combined SMT system using the proposed method achieved the best Chinese-to-English translation result in the constrained training track of the
Although research in other languages is increasing, much of the work in subjectivity analysis has been applied to English data, mainly due to the large body of electronic resources and tools that are available for this language.In this paper, we propose and evaluate methods that can be employed to transfer a repository of subjectivity resources across languages.Specifically, we attempt to leverage on the resources available for English and, by employing machine translation, generate resources for subjectivity analysis in other languages.Through comparative evaluations on two different languages (Romanian and Spanish), we show that automatic translation is a viable alternative for the construction of resources and tools for subjectivity analysis in a new target language.
We formulate dependency parsing as a graphical model with the novel ingredient of global constraints.We show how to apply loopy belief propagation (BP), a simple and tool for and inference.As a parsing algorithm, BP is both asymptotically and empirically efficient.Even with second-order features or latent variables, which would make exact parsing considerslower or NP-hard, BP needs only with a small constant factor.Furthermore, such features significantly improve parse accuracy over exact first-order methods.Incorporating additional features would increase the runtime additively rather than multiplicatively.
We combine lexical, syntactic, and discourse features to produce a highly predictive model of human readers’ judgments of text readability.This is the first study to take into account such a variety of linguistic factors and the first to empirically demonstrate that discourse relations are strongly associated with the perceived quality of text.We show that various surface metrics generally expected to be related to readability are not very good predictors of readability judgments in our Wall Street Journal corpus.We also establish that readability predictors behave differently depending on the task: predicting text readability or ranking the readability.Our experiments indicate that discourse relations are the one class of features that exhibits robustness across these two tasks.
We improve the quality of paraphrases extracted from parallel corpora by requiring that phrases and their paraphrases be the same syntactic type.This is achieved by parsing the English side of a parallel corpus and altering the phrase extraction algorithm to extract phrase labels alongside bilingual phrase pairs.In order to retain broad coverage of non-constituent phrases, complex syntactic labels are introduced.A manual evaluation indicates a 19% absolute improvement in paraphrase quality over the baseline method.
examples (partial) target tree-to-tree Ding and Palmer (2005) Translation rule extraction is a fundamental problem in machine translation, especially for syntax-based that need parse trees from either or both sides of the bitext.The current dominant practice only uses 1-best trees, which adversely affects the rule set quality due to parsing errors.So we propose a novel approach which extracts rules a forest compactly encodes exponentially many parses.Experiments show that this method improves translation quality by over 1 BLEU point on a state-of-the-art tree-to-string system, and is 0.5 points better than (and twice as fast as) extracting on 30best parses.When combined with our previous work on forest-based decoding, it achieves a 2.5 BLEU points improvement over the baseline, and even outperforms the hierarchical system of Hiero by 0.7 points.
Human linguistic annotation is crucial for many natural language processing tasks but can be expensive and time-consuming.We explore the use of Amazon’s Mechanical Turk system, a significantly cheaper and faster method for collecting annotations from a broad base of paid non-expert contributors over the Web.We investigate five tasks: affect recognition, word similarity, recognizing textual entailment, event temporal ordering, and word sense disambiguation.For all five, we show high agreement between Mechanical Turk non-expert annotations and existing gold standard labels provided by expert labelers.For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts.We propose a technique for bias correction that significantly improves annotation quality on two tasks.We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense.
In recent years there has been substantial work on the important problem of coreference resolution, most of which has concentrated on the development of new models and algorithmic techniques.These works often show that complex models improve over a weak pairwise baseline.However, less attention has been given to the importance of selecting strong features to support learning a coreference model.This paper describes a rather simple pairwise classification model for coreference resolution, developed with a well-designed set of features.We show that this produces a state-of-the-art system that outperforms systems built with complex models.We suggest that our system can be used as a baseline for the development of more complex models – which may have less impact when a more robust set of features is used.The paper also presents an ablation study and discusses the relative contributions of various features.
This paper describes a novel Bayesian approach to unsupervised topic segmentation.Unsupervised systems for this task are driven the tendency of wellformed segments to induce a compact and consistent lexical distribution.We show that lexical cohesion can be placed in a Bayesian context by modeling the words in each topic segment as draws from a multinomial language model associated with the segment; maximizing the observation likelihood in such a model yields a lexically-cohesive segmentation.This contrasts with previous approaches, which relied on hand-crafted cohesion metrics.The Bayesian framework provides a principled way to incorporate additional features such as cue phrases, a powerful indicator of discourse structure that has not been previously used in unsupervised segmentation systems.Our model yields consistent improvements over an array of state-of-the-art systems on both text and speech datasets.We also show that both an entropy-based analysis and a well-known previous technique can be de
There is growing interest in applying Bayesian techniques to NLP problems.There are a number of different estimators for Bayesian models, and it is useful to know what kinds of tasks each does well on.This paper compares a variety of different Bayesian estimators for Hidden Markov Model POS taggers with various numbers of hidden states on data sets of different sizes.Recent papers have given contradictory results when comparing Bayesian estimators to Expectation Maximization (EM) for unsupervised HMM POS tagging, and we show that the difference in reported results is largely due to differences in the size of the training data and the number of states in the HMM.We invesigate a variety of samplers for HMMs, including some that these earlier papers did not study.We find that all of Gibbs samplers do well with small data sets and few states, and that Variational Bayes does well on large data sets and is competitive with the Gibbs samplers.In terms of times of convergence, we find that Variational Bayes was the fastest of all the estimators, especially on large data sets, and that explicit Gibbs sampler (both pointwise and sentence-blocked) were generally faster than their collapsed counterparts on large data sets.
Graph-based and transition-based approaches to dependency parsing adopt very different views of the problem, each view having its own strengths and limitations.We study both approaches under the framework of beamsearch.By developing a graph-based and a transition-based dependency parser, we show that a beam-search decoder is a competitive choice for both methods.More importantly, we propose a beam-search-based parser that combines both graph-based and transitionbased parsing into a single system for training and decoding, showing that it outperforms both the pure graph-based and the pure transition-based parsers.Testing on the English and Chinese Penn Treebank data, the combined system gave state-of-the-art accuraof respectively.
We present Minimum Bayes-Risk (MBR) decoding over translation lattices that compactly encode a huge number of translation hypotheses.We describe conditions on the loss function that will enable efficient implementation of MBR decoders on lattices.We introduce an approximation to the BLEU score (Papineni et al., 2001) that satisfies these conditions.The MBR decoding under this approximate BLEU is realized using Weighted Finite State Automata.Our experiments show that the Lattice MBR decoder yields moderate, consistent gains in translation performance over N-best MBR decoding on Arabicto-English, Chinese-to-English and Englishto-Chinese translation tasks.We conduct a range of experiments to understand why Lattice MBR improves upon N-best MBR and study the impact of various parameters on MBR performance.
Machine learning approaches to coreference resolution are typically supervised, and require expensive labeled data.Some unsupervised approaches have been proposed (e.g., Haghighi and Klein (2007)), but they are less accurate.In this paper, we present the first unsupervised approach that is competitive with supervised ones.This is made possible by performing joint inference across mentions, in contrast to the pairwise classification typically used in supervised methods, and by using Markov logic as a representation language, which enables us to easily express relations like apposition and predicate nominals.On MUC and ACE datasets, our model outperforms Haghigi and Klein’s one using only a fraction of the training data, and often matches or exceeds the accuracy of state-of-the-art supervised models.
Minimum Error Rate Training (MERT) is an effective means to estimate the feature function weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training.To accomplish this, the training procedure determines for each feature function its exact error surface on a given set of candidate translations.The feature function weights are then adjusted by traversing the error surface combined over all sentences and picking those values for which the resulting error count reaches a minimum.Typically, in MERT are represented as lists which contain the probable translation hypotheses produced by a decoder.In this paper, we present a novel algorithm that allows for efficiently constructing and reprethe exact error surface of translations that are encoded in a phrase lattice. to MERT, the number of candidate translations thus taken into account increases by several orders of magnitudes.The proposed method is used to train the feature function weights of a phrase-based statistical machine translation system.Experiments conducted on the NIST 2008 translation tasks show significant runtime improvements moderate BLEU score gains over MERT.
In this paper, we present an algorithm for learning a generative model of natural language sentences together with their formal meaning representations with hierarchical structures.The model is applied to the task of mapping sentences to hierarchical representations of their underlying meaning.We introduce dynamic programming techniques for efficient training and decoding.In experiments, we demonstrate that the model, when coupled with a discriminative reranking technique, achieves state-of-the-art performance when tested on two publicly available corpora.The generative model degrades robustly when presented with instances that are different from those seen in training.This allows a notable improvement in recall compared to previous models.
Determining the polarity of a sentimentbearing expression requires more than a simple bag-of-words approach.In particular, words or constituents within the expression can interact with each other to yield a particular overall polarity.In this paper, we view such interactions in light of composiand present a novel learningbased approach that incorporates structural inference motivated by compositional semantics into the learning procedure.Our experiments show that (1) simple heuristics based on compositional semantics can perform better than learning-based methods that do not incorporate compositional semantics (accuracy of 89.7% vs. 89.1%), but (2) a method that integrates compositional semantics into learning performs better than all other alternatives (90.7%).We also find that “contentword negators”, not widely employed in previous work, play an important role in determining expression-level polarity.Finally, in contrast to conventional wisdom, we find that expression-level classification accuracy additional, potentially disambiguating, context is considered.
We show that jointly parsing a bitext can substantially improve parse quality on both sides.In a maximum entropy bitext parsing model, we define a distribution over source trees, target trees, and node-to-node alignments between them.Features include monolingual parse scores and various measures of syntactic divergence.Using the translated portion of the Chinese treebank, our model is trained iteratively to maximize the marginal likelihood of training tree pairs, with alignments treated as latent variables.The resulting bitext parser outperforms state-of-the-art monoparser baselines by 2.5 predicting side trees and 1.8 predicting Chinese side trees (the highest published numbers on these corpora).Moreover, these improved trees yield a 2.4 BLEU increase when used in a downstream MT evaluation.
We present the first unsupervised approach to the problem of learning a semantic parser, using Markov logic.Our USP system transforms dependency trees into quasi-logical forms, recursively induces lambda forms from these, and clusters them to abstract away syntactic variations of the same meaning.The MAP semantic parse of a sentence is obtained by recursively assigning its parts to lambda-form clusters and composing them.We evaluate our approach by using it to extract a knowledge base from biomedical abstracts and answer questions.USP substantially outperforms TextRunner, DIRT and an informed baseline on both precision and recall on this task.
Many statistical translation models can be regarded as weighted logical deduction.Under this paradigm, we use weights from the expectation semiring (Eisner, 2002), to compute first-order statistics (e.g., the expected hypothesis length or feature counts) over packed forests of translations (lattices or hypergraphs).We then introduce novel semiring, which computes second-order statistics (e.g., the variance of the hypothesis length or the gradient of entropy).This second-order semiring is essential for many interesting training paradigms such as minimum risk, deterministic annealing, active learning, and semi-supervised learning, where gradient descent optimization requires computing the gradient of entropy or risk.We use these semirings in an open-source machine translation toolkit, enabling minimum-risk training a benefit of up to 1.0
A significant portion of the world’s text is tagged by readers on social bookmarkwebsites. attribution an inherent problem in these corpora because most pages have multiple tags, but the tags do not always apply with equal specificity across the whole document.Solving the credit attribution problem requires associating each word in a document with the most appropriate tags and vice versa.This introduces a topic model that constrains Latent Dirichlet Allocation by defining a one-to-one correspondence between LDA’s latent topics and user tags.This allows Labeled LDA to directly learn word-tag correspondences.We demonstrate Labeled LDA’s improved expressiveness over traditional LDA with visualizations of a corpus of tagged web from Labeled LDA outperforms SVMs by more than 3 to 1 when extracting tag-specific document snippets.As a multi-label text classifier, our model is competitive with a discriminative baseline on a variety of datasets.
Manual evaluation of translation quality is generally thought to be excessively time consuming and expensive.We explore a fast and inexpensive way of doing it using Amazon’s Mechanical Turk to pay small sums to a large number of non-expert annotators.For $10 we redundantly recreate judgments from a WMT08 translation task.We find that when combined non-expert judgments have a high-level of agreement with the existing gold-standard judgments of machine translation quality, and correlate more strongly with expert judgments than Bleu does.We go on to show that Mechanical Turk can be used to calculate human-mediated translation edit rate (HTER), to conduct reading comprehension experiments with machine translation, and to create high quality reference translations.
This paper describes an empirical study of high-performance dependency parsers based on a semi-supervised learning approach.We describe an extension of semisupervised structured conditional models (SS-SCMs) to the dependency parsing problem, whose framework is originally proposed in (Suzuki and Isozaki, 2008).Moreover, we introduce two extensions related to dependency parsing: The first extension is to combine SS-SCMs with another semi-supervised approach, described in (Koo et al., 2008).The second extension is to apply the approach to secondorder parsing models, such as those described in (Carreras, 2007), using a twostage semi-supervised learning approach.We demonstrate the effectiveness of our proposed methods on dependency parsing experiments using two widely used test collections: the Penn Treebank for English, and the Prague Dependency Treebank for Czech.Our best results on test data in the above datasets achieve 93.79% parent-prediction accuracy for En
We connect two scenarios in structured parser trained on one corpus to another annotation style, and annotations from one to another.We propose quasigrammar features for these structured learning tasks.That is, we score a aligned pair of source and target trees based on local features of the trees and the alignment.Our quasi-synchronous model assigns positive probability to any alignment of any trees, in contrast to a synchronous grammar, which would insist on some form of structural parallelism.In monolingual dependency parser adaptation, we achieve high accuracy in translating among multiple annotation styles for the same sentence.On the more difficult problem of cross-lingual parser projection, we learn a dependency parser for a target language by using bilingual text, an English parser, and automatic word alignments.Our experiments show that unsupervised QG projection improves on parses trained using only highprecision projected annotations and far outperforms, by more than 35% absolute dependency accuracy, learning an unsupervised parser from raw target-language text alone.When a few target-language parse trees are available, projection gives a boost equivalent to doubling the number of target-language trees. first author would like to thank the Center for Intelligent Information Retrieval at UMass Amherst.We would also like to thank Noah Smith and Rebecca Hwa for helpful discussions and the anonymous reviewers for their suggestions for improving the paper.
Computing the pairwise semantic similarity between all words on the Web is a computationally challenging task.Parallelization and optimizations are necessary.We propose a highly scalable implementation based on distributional similarity, implemented in the MapReduce framework and deployed over a 200 billion word crawl of the Web.The pairwise similarity between 500 million terms is computed in 50 hours using 200 quad-core nodes.We apply the learned similarity matrix to the task of automatic set expansion and present a large empirical study to quantify the effect on expansion performance of corpus size, corpus quality, seed composition and seed size.We make public an experimental testbed for set expansion analysis that includes a large collection of diverse entity sets extracted from Wikipedia.
Traditional learning-based coreference reoperate by training a mentionfor determining whether two mentions are coreferent or not.Two independent lines of recent research have attempted to improve these mention-pair one by learning a mentionto rank preceding mentions for a given anaphor, and the other training an to determine whether a preceding cluster is coreferent with a given mention.We propose a cluster-ranking approach to coreference resolution that combines the strengths of mention rankers and entitymention models.We additionally show how our cluster-ranking framework naturally allows discourse-new entity detection to be learned jointly with coreference resolution.Experimental results on the ACE data sets demonstrate its superior performance to competing approaches.
Coreference systems are driven by syntactic, semantic, and discourse constraints.We present a simple approach which completely modularizes these three aspects.In contrast to much current work, which focuses on learning and on the discourse component, our system is deterministic and is driven entirely by syntactic and semantic compatibility as learned from a large, unlabeled corpus.Despite its simplicity and discourse naivete, our system substantially outperforms all unsupervised systems and most supervised ones.Primary contributions include (1) the presentation of a simpleto-reproduce, high-performing baseline and (2) the demonstration that most remaining errors can be attributed to syntactic and semantic factors external to the coreference phenomenon (and perhaps best addressed by non-coreference systems).
Jointly parsing two languages has been shown to improve accuracies on either or both sides.However, its search space is much bigger than the monolingual case, forcing existing approaches to employ complicated modeling and crude approximations.Here we propose a much simpler monowhere a source-language parser learns to exploit reorderings as adobservation, but to build the target-side tree as well.We show specifically how to enhance a shift-reduce dependency parser with alignment features to resolve shift-reduce conflicts.Experiments on the bilingual portion of Chinese Treebank show that, with just 3 bilingual features, we can improve parsing accuracies by 0.6% (absolute) for both English and Chinese over a state-of-the-art with negligible efficiency overhead, thus much faster than biparsing.
In this paper, we present a novel approach for mining opinions from product reviews, where it converts opinion mining task to identify product features, expressions of opinions and relations between them.By taking advantage of the observation that a lot of product features are phrases, a concept of phrase dependency parsing is introduced, which extends traditional dependency parsing to phrase level.This concept is then implemented for extracting relations between product features and expressions of opinions.Experimental evaluations show that the mining task can benefit from phrase dependency parsing.
paper introduces decomposition a framework for deriving inference algorithms for NLP problems.The approach relies on standard dynamic-programming algorithms as oracle solvers for sub-problems, together with a simple method for forcing agreement between the different oracles.The approach provably solves a linear programming (LP) relaxation of the global inference problem.It to algorithms that are in that they existing decoding algorithms; in that they avoid exact algorithms for the full and often in that empirically they often recover the correct solution in spite of using an LP relaxation.We give experimental results on two problems: 1) the combination of two lexicalized parsing models; and 2) the combination of a lexicalized parsing model and a trigram part-of-speech tagger.
Most coreference resolution models determine if two mentions are coreferent using a single function over a set of constraints or features.This approach can lead to incorrect decisions as lower precision features often overwhelm the smaller number of high precision ones.To overcome this problem, we propose a simple coreference architecture based on a sieve that applies tiers of deterministic coreference models one at a time from highest to lowest precision.Each tier builds on the previous tier’s entity cluster output.Further, our model propagates global information by sharing attributes (e.g., gender and number) across mentions in the same cluster.This cautious sieve guarantees that stronger features are given precedence over weaker ones and that each decision is made using all of the information available at the time.The framework is highly modular: new coreference modules can be plugged in without any change to the other modules.In spite of its simplicity, our approach outperforms many state-of-the-art supervised and unsupervised models on several standard corpora.This suggests that sievebased approaches could be applied to other NLP tasks.
We propose an approach to adjective-noun composition (AN) for corpus-based distributional semantics that, building on insights from theoretical linguistics, represents nouns as vectors and adjectives as data-induced (linear) functions (encoded as matrices) over nominal vectors.Our model significantly outperforms the rivals on the task of reconstructing AN vectors not seen in training.A small post-hoc analysis further suggests that, when the model-generated AN vector is not similar to the corpus-observed AN vector, this is due to anomalies in the latter.We show moreover that our approach provides two novel ways to represent adjective meanings, alternative to its representation via corpus-based co-occurrence vectors, both outperforming the latter in an adjective clustering task.
This paper addresses the problem of learning to map sentences to logical form, given training data consisting of natural language sentences paired with logical representations of their meaning.Previous approaches have been designed for particular natural languages or specific meaning representations; here we present a more general method.The approach induces a probabilistic CCG grammar that represents the meaning of individual words and defines how these meanings can be combined to analyze complete sentences.We use higher-order unification to define a hypothesis space containing all grammars consistent with the training data, and develop an online learning algorithm that efficiently searches this space while simultaneously estimating the parameters of a log-linear parsing model.Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations.
We present an approach to grammar induction that utilizes syntactic universals to improve dependency parsing across a range of languages.Our method uses a single set of manually-specified language-independent rules that identify syntactic dependencies between pairs of syntactic categories that commonly occur across languages.During inference of the probabilistic model, we use posterior expectation constraints to require that a minimum proportion of the dependencies we infer be instances of these rules.We also automatically refine the syntactic categories given in our coarsely tagged input.Across six languages our approach outperforms state-of-theart unsupervised methods by a significant mar
The rapid growth of geotagged social media raises new computational possibilities for investigating geographic linguistic variation.In this paper, we present a multi-level generative model that reasons jointly about latent topics and geographical regions.High-level topics such as “sports” or “entertainment” are rendered differently in each geographic region, revealing topic-specific regional distinctions.Applied to a new dataset of geotagged microblogs, our model recovers coherent topics and their regional variants, while identifying geographic areas of linguistic consistency.The model also enables prediction of an author’s geographic location from raw text, outperforming both text regression and supervised topic models.
This paper introduces algorithms for nonparsing based on decomposi- We focus on parsing algorithms for nonhead a generalization of head-automata models to non-projective structures.The dual decomposition algorithms are simple and efficient, relying on standard dynamic programming and minimum spanning tree algorithms.They provably solve an LP relaxation of the non-projective parsing problem.Empirically the LP relaxation is very often tight: for many languages, exact solutions are achieved on over 98% of test sentences.The accuracy of our models is higher than previous work on a broad range of datasets.
We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data.We first demonstrate that delexicalized parsers can be directly transferred between languages, producing significantly higher accuracies than unsupervised parsers.We then use a constraint driven learning algorithm where constraints are drawn from parallel corpora to project the final parser.Unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers.The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages.
We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions.Our method learns vector space representations for multi-word phrases.In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules.We also evaluate the model’s ability to predict sentiment distributions on a new dataset based on confessions from the experience project.The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions.Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines.
We explore efficient domain adaptation for the task of statistical machine translation based on extracting sentences from a large generaldomain parallel corpus that are most relevant to the target domain.These sentences may be selected with simple cross-entropy based methods, of which we present three.As these sentences are not themselves identical the in-domain data, we call them These subcorpora – 1% the size of the original – can then used to train small domain-adapted Statistical Machine Translation (SMT) systems which outperform systems trained on the entire corpus.Performance is further improved when we use these domain-adapted models in combination with a true in-domain model.The results show that more training data is not always better, and that best results are attained via proper domain-relevant data selection, as well as combining inand general-domain systems during decoding.
We offer a simple, effective, and scalable method for statistical machine translation parameter tuning based on the pairwise approach to ranking (Herbrich et al., 1999).Unlike the popular MERT algorithm (Och, 2003), our pairwise ranking optimization (PRO) method is not limited to a handful of parameters and can easily handle systems with thousands of features.Moreover, unlike recent approaches built upon the MIRA algorithm of Crammer and Singer (2003) (Watanabe et al., 2007; Chiang et al., 2008b), PRO is easy to implement.It uses off-the-shelf linear binary classifier software and can be built on top of an existing MERT framework in a matter of hours.We establish PRO’s scalability and effectiveness by comparing it to MERT and MIRA and demonstrate parity on both phrase-based and syntax-based systems in a variety of language pairs, using large scale data scenarios.
Modelling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists.We implement the abstract categorical model of Coecke et al. (2010) using data from the BNC and evaluate it.The implementation is based on unsupervised learning of matrices for relational words and applying them to the vectors of their arguments.The evaluation is based on the word disambiguation task developed by Mitchell and Lapata (2008) for intransitive sentences, and on a similar new experiment designed for transitive sentences.Our model matches the results of its competitors in the first experiment, and betters them in the second.The general improvement in results with increase in syntactic complexity showcases the compositional power of our model.
People tweet more than 100 Million times daily, yielding a noisy, informal, but sometimes informative corpus of 140-character messages that mirrors the zeitgeist in an unprecedented manner.The performance of standard NLP tools is severely degraded on tweets.This paper addresses this issue by re-building the NLP pipeline beginning with part-of-speech tagging, through chunking, to recognition.Our novel doubles compared with the NER system. the redundancy inherent in tweets to achieve this performance, using LabeledLDA to exploit Freebase dictionaries as a source of distant supervision.LabeledLDA outperforms coincreasing 25% over ten common entity types.NLP tools are available at:
Open Information Extraction (IE) is the task of extracting assertions from massive corpora without requiring a pre-specified vocabulary.This paper shows that the output of state-ofthe-art Open IE systems is rife with uninformative and incoherent extractions.To overcome these problems, we introduce two simple syntactic and lexical constraints on binary relations expressed by verbs.We imthe constraints in the Open IE system, which more than doubles the area under the precision-recall curve relative previous extractors such as More than are at precision higher— compared to virtually none for earlier systems.The paper concludes with a detailed analysis
In this paper we address the problem of modeling compositional meaning for phrases and sentences using distributional methods.We experiment with several possible combinations of representation and composition, exhibiting varying degrees of sophistication.Some are shallow while others operate over syntactic structure, rely on parameter learning, or require access to very large corpora.We find that shallow approaches are as good as more computationally intensive alternatives with regards to two particular tests: (1) phrase similarity and (2) paraphrase detection.The sizes of the involved training corpora and the generated vectors are not as important as the fit between the meaning representation and compositional method.
Most current dependency parsers presuppose that input words have been morphologically disambiguated using a part-of-speech tagger before parsing begins.We present a transitionbased system for joint part-of-speech tagging and labeled dependency parsing with nonprojective trees.Experimental evaluation on Chinese, Czech, English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of-theart results for all languages.
In this paper we discuss algorithms for clustering words into classes from unlabelled text using unsupervised algorithms, based on distributional and morphological information.We show how the use of morphological information can improve the performance on rare words, and that this is robust across a wide range of languages.
This paper investigates two elements of Maximum Entropy tagging: the use of a correction feature in the Generalised Iterative Scaling (Gis) estimation algorithm, and techniques for model smoothing.We show analytically and empirically that the correction feature, assumed to be required for the correctof unnecessary.We also explore the use of a Gaussian prior and a simple cutoff for smoothing.The experiments are performed with two tagsets: standard Penn Treebank and the larger set of lexical types from
Compounded words are a challenge for NLP applications such as machine translation (MT).We introduce methods to learn splitting rules from monolingual and parallel corpora.We evaluate them against a gold standard and measure their impact on performance of statistical MT systems.Results show accuracy of 99.1% and performance gains for MT of 0.039 BLEU on a German-English noun phrase translation task.Splitting options for the German word Aktionsplan Aktionsplan Aktion actionplan action plan Akt ion s plan act ion plan
We present a new method for detecting anddisambiguating named entities in open do main text.A disambiguation SVM kernel is trained to exploit the high coverage and rich structure of the knowledge encoded in an online encyclopedia.The resultingmodel significantly outperforms a less in formed baseline.
This paper describes a novel method for computing a consensus translation from the outputs of multiple machine translation (MT) systems.The outputs are combined and a possibly new translation hypothesis can be generated.Similarly to the well-established ROVER approach of (Fiscus, 1997) for combining speech recognition hypotheses, the consensus translation is computed by voting on a confusion network.To create the confusion network, we produce pairwise word alignments of the original machine translation hypotheses with an enhanced statistical alignment algorithm that explicitly models word reordering.The context of a whole document of translations rather than a single sentence is taken into account to produce the alignment.The proposed alignment and voting approach was evaluated on several machine translation tasks, including a large vocabulary task.The method was also tested in the framework of multi-source and speech translation.On all tasks and conditions, we achieved significant improvements in translation quality, increasing e. g. the BLEU score by as much as 15% relative.
In this paper we extend the maximum spanning tree (MST) dependency parsing framework of McDonald et al. (2005c) to incorporate higher-order feature representations and allow dependency structures with multiple parents per word.We show that those extensions can make the MST framework computationally intractable, but that the intractability can be circumvented with new approximate parsing algorithms.We conclude with experiments showing that discriminative online learning using those approximate algorithms achieves the best reported parsing accuracy for Czech and Danish.
In recent years tree kernels have been proposed for the automatic learning of natural language applications.Unfortunately, they show (a) an inherent super linear complexity and (b) a lower accuracy than traditional attribute/value methods.In this paper, we show that tree kernels are very helpful in the processing of natural language as (a) we provide a simple algorithm to compute tree kernels in linear average running time and (b) our study on the classification properties of diverse tree kernels show that kernel combinations always improve the traditional methods.Experiments with Support Vector Machines on the predicate argument classification task provide empirical support to our thesis.
mining a recent subdiscipline of computational linguistics which is concerned not with the topic a document is about, but with the opinion it expresses.To aid the extraction of opinions from text, recent work has tackled the issue determining the “subjective” terms contained in text, i.e. deciding whether a term that carries opinionated content has a positive or a negative connotation.This is believed to be of key importance for identifying the orientation of documents, i.e. determining whether a document expresses a positive or negative opinion about its subject matter.We contend that the plain determination of the orientation of terms is not a realistic problem, since it starts from the nonrealistic assumption that we already know whether a term is subjective or not; this would imply that a linguistic resource that marks terms as “subjective” or “objective” is available, which is usually not the case.In this paper we confront the task of deciding whether a given term has a positive or a negative connotation, no subjective connotation at this problem thus subsumes the problem of desubjectivity problem of determining orientation.We tackle this problem by testing three different variants of a semi-supervised method previously proposed for orientation detection.Our results show that determining subjectivity is a much harder problem than determining orientation alone.
Most state-of-the-art evaluation measures for machine translation assign high costs to movements of word blocks.In many cases though such movements still result in correct or almost correct sentences.In this paper, we will present a new evaluation measure which explicitly models block reordering as an edit operation.Our measure can be exactly calculated in quadratic time.Furthermore, we will show how some evaluation measures can be improved
We argue that the machine translation community is overly reliant on the Bleu machine translation evaluation metric.We show that an improved Bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality, and give two significant counterexamples to Bleu’s correlation with human judgments of quality.This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores.
We present a model for sentence compression that uses a discriminative largemargin learning framework coupled with a novel feature set defined on compressed bigrams as well as deep syntactic representations provided by auxiliary dependency and phrase-structure parsers.The parsers are trained out-of-domain and contain a significant amount of noise.We argue that the discriminative nature of the learning algorithm allows the model to learn weights relative to any noise in the feature set to optimize compression accuracy directly.This differs from current state-of-the-art models (Knight and Marcu, 2000) that treat noisy parse trees, for both compressed and uncompressed sentences, as gold standard when calculating model parameters.
We consider the evaluation problem in Language Generation and results for evaluating several systems with similar functionality, including a knowledge-based generator and several statistical systems.We compare evaluation results for these systems by human domain experts, human non-experts, and several automatic evaluation metrics, inand We that correlate best with human judgments, but that all automatic metrics we examined are biased in favour of generators that select on the basis of frequency alone.We conthat automatic evaluation of systems has considerable potential, in particular where high-quality reference texts and only a small number of human evaluators are available.However, in general it is probably best for automatic evaluations to be supported by human-based evaluations, or at least by studies that demonstrate that a particular metric correlates well with human judgments in a given domain.
In this paper we present TroFi (Trope Finder), a system for automatically classifying literal and nonliteral usages of verbs through nearly unsupervised word-sense disambiguation and clustering techniques.TroFi uses sentential context instead of selectional constraint violations or paths in semantic hierarchies.It also uses literal and nonliteral seed sets acquired and cleaned without human supervision in order to bootstrap learning.We adapt a word-sense disambiguation algorithm to our task and augment it with multiple seed set learners, a voting schema, and additional features like SuperTags and extrasentential context.Detailed experiments on hand-annotated data show that our enhanced algorithm outperforms the baseline by 24.4%.Using the TroFi algorithm, we also build the TroFi Example Base, an extensible resource of annotated literal/nonliteral examples which is freely available to the NLP research community.
We investigate the lexical and syntactic flexibility of a class of idiomatic expressions.We develop measures that draw on such linguistic properties, and demonstrate that these statistical, corpus-based measures can be successfully used for distinguishing idiomatic combinations from non-idiomatic ones.We also propose a means for automatically determining which syntactic forms a particular idiom can appear in, and hence should be included in its lexical representation.
We propose an approach for extracting relations between entities from biomedical literature based solely on shallow linguistic information.We use a combination of kernel functions to integrate two different information sources: (i) the whole sentence where the relation appears, and (ii) the local contexts around the interacting entities.We performed experiments on extracting gene and protein interactions from two different data sets.The results show that our approach outperforms most of the previous methods based on syntactic and semantic information.
In this paper we propose a new graphbased method that uses the knowledge in a LKB (based on WordNet) in order to perform unsupervised Word Sense Disambiguation.Our algorithm uses the full graph of the LKB efficiently, performing better than previous approaches in English all-words datasets.We also show that the algorithm can be easily ported to other languages with good results, with the only requirement of having a wordnet.In addition, we make an analysis of the performance of the algorithm, showing that it is efficient and that it could be tuned to be faster.
Sense induction seeks to automatically identify word senses directly from a corpus.A key assumption underlying previous work is that the context surrounding an ambiguous word is indicative of its meaning.Sense induction is thus typically viewed as an unsupervised clustering problem where the aim is to partition a word’s contexts into different classes, each representing a word sense.Our work places sense induction in a Bayesian context by modeling the contexts of the ambiguous word as samples from a multinomial distribution over senses which are in turn characterized as distributions over words.The Bayesian framework provides a principled way to incorporate a wide range of features beyond lexical cooccurrences and to systematically assess their utility on the sense induction task.The proposed approach yields improvements over state-of-the-art systems on a benchmark dataset.
a declarative language for representing a restricted class of inheritance networks, permitting both multiple and default inheritance.The principal intended area of application is the representation of lexical entries for natural language processing, and we use examples from this domain throughout.In this paper we present the syntax and inference mechanisms for language.The goal of the is the design of a simple language that (i) has the necessary expressive power to encode the lexical entries presupposed by contemporary work in the unification grammar tradition, (ii) can express all the evident generalizations about such entries, (iii) has an explicit theory of inference, (iv) is computationally tractable, and (v) has an explicit declarative semantics.The present paper is primarily concerned with (iii), though the examples used may hint at our strategy in respect of (i) and (ii).
We sketch and illustrate an approach to machine translation that exploits the potential of simultaneous correspondences between separate levels of linguistic representation, as in the of codescriptions.The approach is illustrated with examples from English, German and French where the source and the target language sentence show noteworthy differences in linguistic analysis.
It is often claimed that Named Entity recognition systems need extensive gazetteers—lists of names of people, organisations, locations, and other named entities.Indeed, the compilation of such gazetteers is sometimes mentioned as a bottleneck in the design of Named Entity recognition systems.We report on a Named Entity recognition system which combines rule-based grammars with statistical (maximum entropy) models.We report on the system's performance with gazetteers of different types and different sizes, using test material from the muc-7 competition.We show that, for the text type and task of this competition, it is sufficient to use relatively small gazetteers of well-known names, rather than large gazetteers of low-frequency names.We conclude with observations about the domain independence of the competition and of our experiments.
In statistical natural language processing we always face the problem of sparse data.One way to reduce this problem is to group words into equivalence classes which is a standard method in statistical language modeling.In this paper we describe a method to determine bilingual word classes suitable for statistical machine translation.We develop an optimization criterion based on a maximumlikelihood approach and describe a clustering algorithm.We will show that the usage of the bilingual word classes we get can improve statistical machine translation.
Dividing sentences in chunks of words is a useful preprocessing step for parsing, information extraction and information retrieval.(Ramshaw and Marcus, 1995) have introduced a &quot;convenient&quot; data representation for chunking by converting it to a tagging task.In this paper we will examine seven different data representations for the problem of recognizing noun phrase chunks.We will show that the the data representation choice has a minor influence on chunking performance.However, equipped with the most suitable data representation, our memory-based learning chunker was able to improve the best published chunking results for a standard data set.
We present a discriminative, large margin approach to feature-based matching for word alignment.In thisframework, pairs of word tokens re ceive a matching score, which is basedon features of that pair, including mea sures of association between the words,distortion between their positions, sim ilarity of the orthographic form, and soon.Even with only 100 labeled train ing examples and simple features whichincorporate counts from a large unlabeled corpus, we achieve AER perfor mance close to IBM Model 4, in muchless time.Including Model 4 predic tions as features, we achieve a relativeAER reduction of 22% in over inter sected Model 4 alignments.
Bilingual word alignment forms the foun dation of most approaches to statisticalmachine translation.Current word align ment methods are predominantly based on generative models.In this paper, we demonstrate a discriminative approachto training simple word alignment mod els that are comparable in accuracy tothe more complex generative models nor mally used.These models have the theadvantages that they are easy to add fea tures to and they allow fast optimization of model parameters using small amounts of annotated data.
This paper presents a maximum entropyword alignment algorithm for Arabic English based on supervised training data.We demonstrate that it is feasible to create training material for problems in machine translation and that a mixture of su pervised and unsupervised methods yields superior performance.The probabilisticmodel used in the alignment directly models the link decisions.Significant improvement over traditional word alignment tech niques is shown as well as improvement onseveral machine translation tests.Perfor mance of the algorithm is contrasted with human annotation performance.
We describe stochastic models of localphrase movement that can be incorporated into a Statistical Machine Translation (SMT) system.These models pro vide properly formulated, non-deficient, probability distributions over reorderedphrase sequences.They are implemented by Weighted Finite State Trans ducers.We describe EM-style parameter re-estimation procedures based on phrase alignment under the complete translationmodel incorporating reordering.Our ex periments show that the reordering modelyields substantial improvements in trans lation performance on Arabic-to-English and Chinese-to-English MT tasks.We also show that the procedure scales as the bitext size is increased.
This paper presents a new approach to phrase-level sentiment analysis that firstdetermines whether an expression is neu tral or polar and then disambiguates the polarity of the polar expressions.With thisapproach, the system is able to automat ically identify the contextual polarity for a large subset of sentiment expressions,achieving results that are significantly bet ter than baseline.
Recent systems have been developed forsentiment classification, opinion recognition, and opinion analysis (e.g., detecting polarity and strength).We pursue another aspect of opinion analysis: identi fying the sources of opinions, emotions, and sentiments.We view this problem as an information extraction task and adopta hybrid approach that combines Con ditional Random Fields (Lafferty et al, 2001) and a variation of AutoSlog (Riloff,1996a).While CRFs model source identification as a sequence tagging task, AutoSlog learns extraction patterns.Our re sults show that the combination of these two methods performs better than either one alone.The resulting system identifies opinion sources with 79.3% precision and 59.5% recall using a head noun matching measure, and 81.2% precision and 60.6% recall using an overlap measure.
Distributions of the senses of words are often highly skewed.This fact is exploitedby word sense disambiguation (WSD) sys tems which back off to the predominant sense of a word when contextual clues arenot strong enough.The domain of a doc ument has a strong influence on the sensedistribution of words, but it is not feasi ble to produce large manually annotated corpora for every domain of interest.In this paper we describe the construction of three sense annotated corpora in different domains for a sample of English words.We apply an existing method for acquiring predominant sense information automatically from raw text, and for our sam ple demonstrate that (1) acquiring suchinformation automatically from a mixeddomain corpus is more accurate than de riving it from SemCor, and (2) acquiringit automatically from text in the same do main as the target domain performs best by a large margin.We also show that for an all words WSD task this automatic method is best focussed on words that are salient to the domain, and on words with a different acquired predominant sense in that domain compared to that acquired from a balanced corpus.
This paper presents a bidirectional inference algorithm for sequence labeling problems such as part-of-speech tag ging, named entity recognition and text chunking.The algorithm can enumerate all possible decomposition structures andfind the highest probability sequence together with the corresponding decomposi tion structure in polynomial time.We also present an efficient decoding algorithm based on the easiest-first strategy, which gives comparably good performance tofull bidirectional inference with significantly lower computational cost.Exper imental results of part-of-speech tagging and text chunking show that the proposedbidirectional inference methods consis tently outperform unidirectional inference methods and bidirectional MEMMs give comparable performance to that achievedby state-of-the-art learning algorithms in cluding kernel support vector machines.
We formalize weighted dependency pars ing as searching for maximum spanning trees (MSTs) in directed graphs.Using this representation, the parsing algorithmof Eisner (1996) is sufficient for search ing over all projective trees in O(n3) time.More surprisingly, the representation isextended naturally to non-projective pars ing using Chu-Liu-Edmonds (Chu andLiu, 1965; Edmonds, 1967) MST algorithm, yielding an O(n2) parsing al gorithm.We evaluate these methodson the Prague Dependency Treebank using online large-margin learning tech niques (Crammer et al, 2003; McDonald et al, 2005) and show that MST parsingincreases efficiency and accuracy for lan guages with non-projective dependencies.
In addition to information, text con tains attitudinal, and more specifically, emotional content.This paper exploresthe text-based emotion prediction prob lem empirically, using supervised machinelearning with the SNoW learning architecture.The goal is to classify the emotional affinity of sentences in the narra tive domain of children?s fairy tales, forsubsequent usage in appropriate expressive rendering of text-to-speech synthe sis.Initial experiments on a preliminarydata set of 22 fairy tales show encourag ing results over a na??ve baseline and BOW approach for classification of emotional versus non-emotional contents, with some dependency on parameter tuning.We also discuss results for a tripartite model which covers emotional valence, as well as feature set alernations.In addition, we present plans for a more cognitively soundsequential model, taking into considera tion a larger set of basic emotions.
We use logical inference techniques for recognising textual entailment.As the performance of theorem proving turnsout to be highly dependent on not read ily available background knowledge, we incorporate model building, a technique borrowed from automated reasoning, and show that it is a useful robust method to approximate entailment.Finally, we use machine learning to combine these deep semantic analysis techniques with simpleshallow word overlap; the resulting hy brid model achieves high accuracy on the RTE testset, given the state of the art.Ourresults also show that the different techniques that we employ perform very dif ferently on some of the subsets of the RTE corpus and as a result, it is useful to use the nature of the dataset as a feature.
We present a novel approach to relation extraction, based on the observation thatthe information required to assert a rela tionship between two named entities in the same sentence is typically capturedby the shortest path between the two entities in the dependency graph.Exper iments on extracting top-level relationsfrom the ACE (Automated Content Ex traction) newspaper corpus show that thenew shortest path dependency kernel outperforms a recent approach based on de pendency tree kernels.
Linguistically annotated corpus based on texts in biomedical domain has been constructed to tune natural language processing (NLP) tools for bio textmining.As the focus of information extraction is shifting from "nominal" information such as named entity to "verbal" information such as function and interaction of substances, applica tion of parsers has become one of the key technologies and thus the corpus annotated for syntactic structure of sen tences is in demand.A subset of the GENIA corpus consisting of 500 MEDLINE abstracts has been annotated for syntactic structure in an XML based format based on Penn Treebank II (PTB) scheme.Inter-annotator agreement test indicated that the writ ing style rather than the contents of the research abstracts is the source of the difficulty in tree annotation, and that annotation can be stably done by linguists without much knowledge of bi ology with appropriate guidelines regarding to linguistic phenomena par ticular to scientific texts.
The second international Chinese word segmentation bakeoff was held in the summer of 2005 to evaluate the current state of the art in word segmentation.Twenty three groups submitted 130 result sets over two tracks and four different corpora.We found that the technol ogy has improved over the intervening two years, though the out-of-vocabularyproblem is still or paramount impor tance.
We present a Chinese word seg mentation system submitted to the closed track of Sighan bakeoff 2005.Our segmenter was built using a condi tional random field sequence model that provides a framework to use a large number of linguistic features such as character identity, morphological and character reduplication features.Because our morphological features were extracted from the training cor pora automatically, our system was not biased toward any particular variety of Mandarin.Thus, our system does not overfit the variety of Mandarin most familiar to the system's designers.Our final system achieved a F-score of 0.947 (AS), 0.943 (HK), 0.950 (PK) and 0.964 (MSR).
We present a modular system for detection and correction of errors made by non native (English as a Second Language = ESL) writers.We focus on two error types: the incorrect use of determiners and the choice of prepositions.We use a decision tree approach inspired by contextual spelling systems for detection and correction suggestions, and a large language model trained on the Gigaword corpus to provide additional information to filter out spurious suggestions.We show how this system performs on a corpus of non-native English text and discuss strategies for future enhancements.
The paper defines weighted head transducers, finite-state machines that perform middle-out string transduction.These transducers are strictly more expressive than the special case of standard leftto-right finite-state transducers.Dependency transduction models are then defined as collections of weighted head transducers that are applied hierarchically.A dynamic programming search algorithm is described for finding the optimal transduction of an input string with respect to a dependency transduction model.A method for automatically training a dependency transduction model from a set of input-output example strings is presented.The method first searches for hierarchical alignments of the training examples guided by correlation statistics, and then constructs the transitions of head transducers that are consistent with these alignments.Experimental results are given for applying the training method to translation from English to Spanish and Japanese.
Parallel texts (bitexts) have properties that distinguish them from other kinds of parallel data.First, most words translate to only one other word.Second, bitext correspondence is typically only partial—many words in each text have no clear equivalent in the other text.This article presents methods for biasing statistical translation models to reflect these properties.Evaluation with respect to independent human judgments has confirmed that translation models biased in this fashion are significantly more accurate than a baseline knowledge-free model.This article also shows how a statistical translation model can take advantage of preexisting knowledge that might be available about particular language pairs.Even the simplest kinds of languagespecific knowledge, such as the distinction between content words and function words, are shown to reliably boost translation model performance on some tasks.Statistical models that reflect knowledge about the model domain combine the best of both the rationalist and empiricist paradigms.The idea of a computer system for translating from one language to another is almost as old as the idea of computer systems.Warren Weaver wrote about mechanical translation as early as 1949.More recently, Brown et al. (1988) suggested that it may be possible to construct machine translation systems automatically.Instead of codifying the human translation process from introspection, Brown and his colleagues proposed machine learning techniques to induce models of the process from examples of its input and output.The proposal generated much excitement, because it held the promise of automating a task that forty years of research have proven very labor-intensive and error-prone.Yet very few other researchers have taken up the cause, partly because Brown et al. 's (1988) approach was quite a departure from the paradigm in vogue at the time.Brown et al. (1988) built statistical models of equivalence models', short).In the context of computational linguistics, translational equivalence is a relation that holds between two expressions with the same meaning, where the two expressions are in different languages.Empirical estimation statistical translation models is typically based on texts of texts that are translations of each other.As with all statistical models, the best translation models are those whose parameters correspond best with the sources of variance in the data.Probabilistic translation models whose parameters reflect universal properties of translational equivalence and/or existing knowledge about particular * D1-66F, 610 Opperman Drive, Eagan, MN 55123.E-mail: dan.melamed@twestgroup.com 1 The term translation model, which is standard in the literature, refers to a mathematical relationship two data sets. hi this context, the term implies nothing about the translation between natural languages, automated or otherwise.© 2000 Association for Computational Linguistics Computational Linguistics Volume 26, Number 2 languages and language pairs benefit from the best of both the empiricist and rationalist traditions.This article presents three such models, along with methods for efficiently estimating their parameters.Each new method is designed to account for an additional universal property of translational equivalence in bitexts: 1.Most word tokens translate to only one word token.I approximate this tendency with a one-to-one assumption.2.Most text segments are not translated word-for-word.I build an explicit noise model.3.Different linguistic objects have statistically different behavior in translation.I show a way to condition translation models on different word classes to help account for the variety Quantitative evaluation with respect to independent human judgments has shown that each of these three estimation biases significantly improves translation model accuracy over a baseline knowledge-free model.However, these biases will not produce the best possible translation models by themselves.Anyone attempting to build an optimal translation model should infuse it with all available knowledge sources, including syntactic, dictionary and cognate information.My goal here is only to demonstrate the value of some previously unused kinds of information that are always available for translation modeling, and to show how these information sources can be integrated with others.
Chinese is written without using spaces or other word delimiters.Although a text may be thought of as a corresponding sequence of words, there is considerable ambiguity in the placement of boundaries.Interpreting a text as a sequence of words is beneficial for some information retrieval and storage tasks: for example, full-text search, word-based compression, and key phrase extraction.We describe a scheme that infers appropriate positions for word boundaries using an adaptive language model that is standard in text compression.It is trained on a corpus of presegmented text, and when applied to new text, interpolates word boundaries so as to maximize the compression obtained.This simple and general method performs well with respect to specialized schemes for Chinese language segmentation.
We present an implemented system for processing definite descriptions in arbitrary domains.The design of the system is based on the results of a corpus analysis previously reported, which highlighted the prevalence of discourse-new descriptions in newspaper corpora.The annotated corpus was used to extensively evaluate the proposed techniques for matching definite descriptions with their antecedents, discourse segmentation, recognizing discourse-new descriptions, and suggesting anchors for bridging descriptions.
paper, it is argued that &quot;coreference&quot; annotations, as performed in the MUC community for example, go well beyond annotation of the relation of coreference proper.As a result, it is not always clear what semantic relation these annotations are encoding.The paper discusses a number of problems with these annotations and concludes that rethinking of the coreference task is needed before the task is expanded.In particular, it suggests a division of labor whereby annotation of the coreference relation proper is separated from other tasks such as annotation of bound anaphora and of the relation between a subject and a predicative NP.
We examine how differences in language models, learned by different data-driven systems performing the same NLP task, can be exploited to yield a higher accuracy than the best individual system.We do this by means of experiments involving the task of morphosyntactic word class tagging, on the basis of three different tagged corpora.Four well-known tagger generators (hidden Markov model, memory-based, transformation rules, and maximum entropy) are trained on the same corpus data.After comparison, their outputs are combined using several voting strategies and second-stage classifiers.All combination taggers outperform their best component.The reduction in error rate varies with the material in question, but can be as high as 24.3% with the LOB corpus.
Word sense disambiguation (WSD) is a computational linguistics task likely to benefit from the tradition of combining different knowledge sources in artificial intelligence research.An important step in the exploration of this hypothesis is to determine which linguistic knowledge sources are most useful and whether their combination leads to improved results.We present a sense tagger which uses several knowledge sources.Tested accuracy exceeds 94% on our evaluation corpus.Our system attempts to disambiguate all content words in running text rather than limiting itself to treating a restricted vocabulary of words.It is argued that this approach is more likely to assist the creation of practical systems.
Automatic acquisition of lexical knowledge is critical to a wide range of natural language processing tasks.Especially important is knowledge about verbs, which are the primary source of relational information in a sentence—the predicate-argument structure that relates an action or state to its participants (i.e., who did what to whom).In this work, we report on supervised learning experiments to automatically classify three major types of English verbs, based on their argument structure—specifically, the thematic roles they assign to participants.We use linguistically-motivated statistical indicators extracted from large annotated corpora to train the classifier, achieving 69.8% accuracy for a task whose baseline is 34%, and whose expert-based upper bound we calculate at 86.5%.A detailed analysis of the performance of the algorithm and of its errors confirms that the proposed features capture properties related to the argument structure of the verbs.Our results validate our hypotheses that knowledge about thematic relations is crucial for verb classification, and that it can be gleaned from a corpus by automatic means.We thus demonstrate an effective combination of deeper linguistic knowledge with the robustness and scalability of statistical techniques.
In this paper, we present a learning approach to coreference resolution of noun phrases in unrestricted text.The approach learns from a small, annotated corpus and the task includes resolving not just a certain type of noun phrase (e.g., pronouns) but rather general noun phrases.It also does not restrict the entity types of the noun phrases; that is, coreference is assigned whether they are of &quot;organization,&quot; &quot;person,&quot; or other types.We evaluate our approach on common data sets (namely, the MUC-6 and MUC-7 coreference corpora) and obtain encouraging results, indicating that on the general noun phrase coreference task, the learning approach holds promise and achieves accuracy comparable to that of nonlearning approaches.Our system is the first learning-based system that offers performance comparable to that of state-of-the-art nonlearning systems on these data sets.
metric, initially proposed by Beeferman, Berger, and Lafferty (1997), is becoming the standard measure for assessing text segmentation algorithms.However, a theoretical analysis of the metric finds several problems: the metric penalizes false negatives more heavily than false positives, overpenalizes near misses, and is affected by variation in segment size dis- We propose a simple modification to the that remedies these problems.This new metric—called WindowDiff—moves a fixed-sized window across the text and penalizes the algorithm whenever the number of boundaries within the window does not match the true number of boundaries for that window of text.
This paper brings a logical perspective to the generation of referring expressions, addressing the incompleteness of existing algorithms in this area.After studying references to individual objects, we discuss references to sets, including Boolean descriptions that make use of negated and disjoined properties.To guarantee that a distinguishing description is generated whenever such descriptions exist, the paper proposes generalizations and extensions of the Incremental
This article concerns the estimation of a particular kind of probability, namely, the probability of a noun sense appearing as a particular argument of a predicate.In order to overcome the accompanying sparse-data problem, the proposal here is to define the probabilities in terms of senses from a semantic hierarchy and exploit the fact that the senses can be grouped into classes consisting of semantically similar senses.There is a particular focus on the problem of how to determine a suitable class for a given sense, or, alternatively, how to determine a suitable level of generalization in the hierarchy.A procedure is developed that uses a chi-square test to determine a suitable level of generalization.In order to test the performance of the estimation method, a pseudo-disambiguation task is used, together with two alternative estimation methods.Each method uses a different generalization procedure; the first alternative uses the minimum description length principle, and the second uses Resnik’s measure of selectional preference.In addition, the performance of our method is investigated using both the standard Pearson chisquare statistic and the log-likelihood chi-square statistic.
present a system for identifying the semantic relationships, or filled by constituents of a sentence within a semantic frame.Given an input sentence and a target word frame, the system labels constituents with either abstract semantic roles, such as or more domain-specific semantic roles, such as and The system is based on statistical classifiers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project.We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and its position in the sentence.These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles.We used various lexical clustering algorithms to generalize across possible fillers of roles.Test sentences were parsed, were annotated with these features, and were then passed through the classifiers.Our system achieves 82% accuracy in identifying the semantic role of presegmented constituents.At the more difficult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall.Our study also allowed us to compare the usefulness of different features and feature combination methods in the semantic role labeling task.We also explore the integration of role labeling with statistical syntactic parsing and attempt to generalize to predicates unseen in the training data.
Edinburgh In this article we propose a strategy for the summarization of scientific articles that concentrates on the rhetorical status of statements in an article: Material for summaries is selected in such a way that summaries can highlight the new contribution of the source article and situate it with respect to earlier work.We provide a gold standard for summaries of this kind consisting of a substantial corpus of conference articles in computational linguistics annotated with human judgments of the rhetorical status and relevance of each sentence in the articles.We present several experiments measuring our judges’ agreement on these annotations.We also present an algorithm that, on the basis of the annotated training material, selects content from unseen articles and classifies it into a fixed set of seven rhetorical categories.The output of this extraction and classification system can be viewed as a single-document summary in its own right; alternatively, it provides starting material for the generation of task-oriented and user-tailored summaries designed to give users an overview of a scientific field.
We present and compare various methods for computing word alignments using statistical or heuristic models.We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements.These statistical models are compared with two heuristic models based on the Dice coefficient.We present different methodsfor combining word alignments to perform a symmetrization of directed statistical alignment models.As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment.We evaluate the models on the German-English Verbmobil task and the French-English Hansards task.We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes.An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models.In the Appendix, we present an efficient training algorithm for the alignment models presented.
This article describes a new approach to the generation of referring expressions.We propose to formalize a scene (consisting of a set of objects with various properties and relations) as a labeled directed graph and describe content selection (which properties to include in a referring expression) as a subgraph construction problem.Cost functions are used to guide the search process and to give preference to some solutions over others.The current approach has four main advantages: (1) Graph structures have been studied extensively, and by moving to a graph perspective we get direct access to the many theories and algorithms for dealing with graphs; (2) many existing generation algorithms can be reformulated in terms of graphs, and this enhances comparison and integration of the various approaches; (3) the graph perspective allows us to solve a number of problems that have plagued earlier algorithms for the generation of referring expressions; and (4) the combined use of graphs and cost functions paves the way for an integration of rule-based generation techniques with more recent stochastic approaches.
In this article, we describe an efficient beam search algorithm for statistical machine translation based on dynamic programming (DP).The search algorithm uses the translation model presented in Brown et al. (1993).Starting from a DP-based solution to the traveling-salesman problem, we present a novel technique to restrict the possible word reorderings between source and target language in order to achieve an efficient search algorithm.Word reordering restrictions especially useful for the translation direction German to English are presented.The restrictions are generalized, and a set offour parameters to control the word reordering is introduced, which then can easily be adopted to new translation directions.The beam search procedure has been successfully tested on the Uerbmobil task (German to English, 8,000-word vocabulary) and on the Canadian Hansards task (French to English, 100,000-word vocabulary).For the medium-sized Uerbmobil task, a sentence can be translated in a few seconds, only a small number of search errors occur, and there is no performance degradation as measured by the word error criterion used in this article.
Parallel corpora have become an essential resource for work in multilingual natural language processing.In this article, we report on our work using the STRAND system for mining parallel text on the World Wide Web,first reviewing the original algorithm and results and then presenting a set of significant enhancements.These enhancements include the use of supervised learning based on structural features of documents to improve classification performance, a new contentbased measure of translational equivalence, and adaptation of the system to take advantage of the Internet Archive for mining parallel text from the Web on a large scale.Finally, the value of these techniques is demonstrated in the construction of a significant parallel corpus for a low-density language pair.
This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen in a given corpus.We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine.We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudodisambiguation task.
This article describes three statistical models for natural language parsing.The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree.Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram dependencies, and preferences for close attachment.All of these preferences are expressed by probabilities conditioned on lexical heads.The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature.To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies.We analyze various characteristics of the models through experiments on parsing accuracy, by collectingfrequencies ofvarious structures in the treebank, and through linguistically motivated examples.Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models.
Selectional preferences have been used by word sense disambiguation (WSD) systems as one source of disambiguating information.We evaluate WSD using selectional preferences acquired for English adjective–noun, subject, and direct object grammatical relationships with respect to a standard test corpus.The selectional preferences are specific to verb or adjective classes, rather than individual word forms, so they can be used to disambiguate the co-occurring adjectives and verbs, rather than just the nominal argument heads.We also investigate use of the one-senseper-discourse heuristic to propagate a sense tag for a word to other occurrences of the same word within the current document in order to increase coverage.Although the preferences perform well in comparison with other unsupervised WSD systems on the same corpus, the results show that for many applications, further knowledge sources would be required to achieve an adequate level of accuracy and coverage.In addition to quantifying performance, we analyze the results to investigate the situations in which the selectional preferences achieve the best precision and in which the one-sense-per-discourse heuristic increases performance.
CorMet is a corpus-based system for discovering metaphorical mappings between concepts.It does this by finding systematic variations in domain-specific selectional preferences, which are inferred from large, dynamically mined Internet corpora.Metaphors transfer structure from a source domain to a target domain, making some concepts in the target domain metaphorically equivalent to concepts in the source domain.The verbs that select for a concept in the source domain tend to select for its metaphorical equivalent in the target domain.This regularity, detectable with a shallow linguistic analysis, is used to find the metaphorical interconcept mappings, which can then be used to infer the existence of higher-level conventional metaphors.Most other computational metaphor systems use small, hand-coded semantic knowledge bases and work on a few examples.Although CorMet’s only knowledge base is WordNet (Fellbaum 1998) it can find the mappings constituting many conventional metaphors and in some cases recognize sentences instantiating those mappings.CorMet is tested on its ability to find a subset of the
dialogue structure coding scheme.23(1):13–31.Cicchetti, Domenic V. and Alvan R. Feinstein.1990.High agreement but low kappa: II.Resolving the paradoxes. of Clinical 43(6):551–558.Cohen, Jacob.1960.A coefficient of for nominal scales.
In statistical machine translation, correspondences between the words in the source and the target language are learned from parallel corpora, and often little or no linguistic knowledge is used to structure the underlying models.In particular, existing statistical systems for machine translation often treat different inflectedforms of the same lemma as if they were independent ofone another.The bilingual training data can be better exploited by explicitly taking into account the interdependencies of related inflected forms.We propose the construction of hierarchical lexicon models on the basis of equivalence classes of words.In addition, we introduce sentence-level restructuring transformations which aim at the assimilation of word order in related sentences.We have systematically investigated the amount of bilingual training data required to maintain an acceptable quality of machine translation.The combination of the suggested methods for improving translation quality in frameworks with scarce resources has been successfully tested: We were able to reduce the amount of bilingual training data to less than 10% of the original corpus, while losing only 1.6% in translation quality.The improvement of the translation results is demonstrated on two German-English corpora taken from the Uerbmobil task and the Nespole! task.
Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations.There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization.The goal of this work is learning subjective language from corpora.Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identified using distributional similarity.The features are also examined working together in concert.The features, generated from different data sets using different procedures, exhibit consistency in performance in that they all do better and worse on the same data sets.In addition, this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective, and it provides the results of an annotation study assessing the subjectivity of sentences with high-density features.Finally, the clues are used to perform opinion piece recognition (a type of text categorization and genre detection) to demonstrate the utility of the knowledge acquired in this article.
A phrase-based statistical machine translation approach — the alignment template approach — is described.This translation approach allows for general many-to-many relations between words.Thereby, the context of words is taken into account in the translation model, and local changes in word order from source to target language can be learned explicitly.The model is described using a log-linear modeling approach, which is a generalization of the often used source–channel approach.Thereby, the model is easier to extend than classical statistical machine translation systems.We describe in detail the process for learning phrasal translations, the feature functions used, and the search algorithm.The evaluation of this approach is performed on three different For the German–English speech we analyze the effect of various syscomponents.On the French–English Canadian the alignment template system obtains significantly better results than a single-word-based translation model.In the Chinese–English 2002 National Institute of Standards and Technology (NIST) machine translation evaluation it yields statistically significantly better NIST scores than all competing research and commercial translation systems.
A system that can produce informative summaries, highlighting common information found in many online documents, will help Web users to pinpoint information that they need without extensive reading.In this article, we introduce sentence fusion, a novel text-to-text generation technique for synthesizing common information across documents.Sentence fusion involves bottom-up local multisequence alignment to identify phrases conveying similar information and statistical generation to combine common phrases into a sentence.Sentence fusion moves the summarization field from the use of purely extractive methods to the generation of abstracts that contain sentences not found in any of the input documents and can synthesize information across sources.
We present a novel method for discovering parallel sentences in comparable, non-parallel corpora.We train a maximum entropy classifier that, given a pair of sentences, can reliably determine whether or not they are translations of each other.Using this approach, we extract parallel data from large Chinese, Arabic, and English non-parallel newspaper corpora.We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system.We also show that a good-quality MT system can be built from scratch by starting with a very small parallel corpus (100,000 words) and exploiting a large non-parallel corpus.Thus, our method can be applied with great benefit to language pairs for which only scarce resources are available.
are at least two kinds of similarity. similarity correspondence between rein contrast with which is correspondence between attributes. two words have a high degree of attributional similarity, we call them When two pairs of words have a high degree of relational similarity, we say that their relations are For example, the word pair mason:stone is analogous to the pair carpenter:wood.This article introduces Latent Relational Analysis (LRA), a method for measuring relational similarity.LRA has potential applications in many areas, including information extraction, word sense disambiguation, and information retrieval.Recently the Vector Space Model (VSM) of information retrieval has been adapted to measuring relational similarity, achieving a score of 47% on a collection of 374 college-level multiple-choice word analogy questions.In the VSM approach, the relation between a pair of words is characterized by a vector offrequencies of predefined patterns in a large corpus.LRA extends the VSM approach in three ways: (1) The patterns are derived automatically from the corpus, (2) the Singular Value Decomposition (SVD) is used to smooth the frequency data, and (3) automatically generated synonyms are used to explore variations of the word pairs.LRA achieves 56% on the 374 analogy questions, statistically equivalent to the average human score of 57%.On the related problem of classifying semantic relations, LRA achieves similar gains over the VSM.
present a statistical machine translation model that uses that contain subphrases.The model is formally a synchronous context-free grammar but is learned from a parallel text without any syntactic annotations.Thus it can be seen as combining fundamental ideas from both syntax-based translation and phrase-based translation.We describe our system’s training and decoding methods in detail, and evaluate it for translation speed and translation accuracy.Using BLEU as a metric of translation accuracy, we find that our system performs significantly better than the Alignment Template System, a state-of-the-art phrasebased system.
article presents an algorithm for translating the Penn Treebank into a corpus of Combinatory Categorial Grammar (CCG) derivations augmented with local and long-range word–word dependencies.The resulting corpus, CCGbank, includes 99.4% of the sentences in the Penn Treebank.It is available from the Linguistic Data Consortium, and has been used to train widecoverage statistical parsers that obtain state-of-the-art rates of dependency recovery.In order to obtain linguistically adequate CCG analyses, and to eliminate noise and inconsistencies in the original annotation, an extensive analysis of the constructions and annotations in the Penn Treebank was called for, and a substantial number of changes to the Treebank were necessary.We discuss the implications of our findings for the extraction of other linguistically expressive grammars from the Treebank, and for the design offuture treebanks.
This article describes a number of log-linear parsing models for an automatically extracted lexicalized grammar.The models are “full” parsing models in the sense that probabilities are defined for complete parses, rather than for independent events derived by decomposing the parse tree.Discriminative training is used to estimate the models, which requires incorrect parses for each sentence in the training data as well as the correct parse.The lexicalized grammar formalism used is Combinatory Categorial Grammar (CCG), and the grammar is automatically extracted from CCGbank, a CCG version of the Penn Treebank.The combination of discriminative training and an automatically extracted grammar leads to a significant memory requirement (up to 25 GB), which is satisfied using a parallel implementation of the BFGS optimization algorithm running on a Beowulf cluster.Dynamic programming over a packed chart, in combination with the parallel implementation, allows us to solve one of the largest-scale estimation problems in the statistical parsing literature in under three hours.A key component of the parsing system, for both training and testing, is a Maximum Entropy supertagger which assigns CCG lexical categories to words in a sentence.The supertagger makes the discriminative training feasible, and also leads to a highly efficient parser.Surprisingly, given CCG’s “spurious ambiguity,” the parsing speeds are significantly higher than those reported for comparable parsers in the literature.We also extend the existing parsing techniques for CCG by developing a new model and efficient parsing algorithm which exploits all derivations, including CCG’s nonstandard derivations.This model and parsing algorithm, when combined with normal-form constraints, give state-of-the-art accuracy for the recovery of predicate–argument dependencies from CCGbank.The parser is also evaluated on DepBank and compared against the RASP parser, outperforming RASP overall and on the majority of relation types.The evaluation on DepBank raises a number of issues regarding parser evaluation.This article provides a comprehensive blueprint for building a wide-coverage CCG parser.We demonstrate that both accurate and highly efficient parsing is possible with CCG.
This article proposes a novel framework for representing and measuring local coherence.Central to this approach is the entity-grid representation of discourse, which captures patterns of entity distribution in a text.The algorithm introduced in the article automatically abstracts a text into a set of entity transition sequences and records distributional, syntactic, and referential information about discourse entities.We re-conceptualize coherence assessment as a learning task and show that our entity-based representation is well-suited for ranking-based generation and text classification tasks.Using the proposed representation, we achieve good performance on text ordering, summary coherence evaluation, and readability assessment.
Probabilistic modeling of lexicalized grammars is difficult because these grammars exploit complicated data structures, such as typed feature structures.This prevents us from applying common methods of probabilistic modeling in which a complete structure is divided into substructures under the assumption of statistical independence among sub-structures.For example, part-of-speech tagging of a sentence is decomposed into tagging of each word, and CFG parsing is split into applications of CFG rules.These methods have relied on the structure of the target problem, namely lattices or trees, and cannot be applied to graph structures including typed feature structures.This article proposes the feature forest model as a solution to the problem of probabilistic modeling of complex data structures including typed feature structures.The feature forest model provides a method for probabilistic modeling without the independence assumption when probabilistic events are represented with feature forests.Feature forests are generic data structures that represent ambiguous trees in a packed forest structure.Feature forest models are maximum entropy models defined over feature forests.A dynamic programming algorithm is proposed for maximum entropy estimation without unpacking feature forests.Thus probabilistic modeling of any data structures is possible when they are represented by feature forests.This article also describes methods for representing HPSG syntactic structures and predicate–argument structures with feature forests.Hence, we describe a complete strategy for developing probabilistic models for HPSG parsing.The effectiveness of the proposed methods is empirically evaluated through parsing experiments on the Penn Treebank, and the promise of applicability to parsing of real-world sentences is discussed.
We present a model for semantic role labeling that effectively captures the linguistic intuition that a semantic argument frame is a joint structure, with strong dependencies among the arguments.We show how to incorporate these strong dependencies in a statistical joint model with a rich set of features over multiple argument phrases.The proposed model substantially outperforms a similar state-of-the-art local model that does not include dependencies among different arguments.We evaluate the gains from incorporating this joint information on the Propbank corpus, when using correct syntactic parse trees as input, and when using automatically derived parse The gains amount to reduction on all arguments and core arguments for gold-standard parse trees on Propbank.For automatic parse trees, the error reductions are all and core arguments, respectively.We also present results on the CoNLL 2005 shared task data set.Additionally, we explore considering multiple syntactic analyses to cope with parser noise and uncertainty.
Parsing algorithms that process the input from left to right and construct a single derivation have often been considered inadequate for natural language parsing because of the massive ambiguity typically found in natural language grammars.Nevertheless, it has been shown that such algorithms, combined with treebank-induced classifiers, can be used to build highly accurate disambiguating parsers, in particular for dependency-based syntactic representations.In this article, we first present a general framework for describing and analyzing algorithms for deterministic incremental dependency parsing, formalized as transition systems.We then describe and analyze two families of such algorithms: stack-based and list-based algorithms.In the former family, which is restricted to projective dependency structures, we describe an arc-eager and an arc-standard variant; in the latter family, we present a projective and a nonprojective variant.For each of the four algorithms, we give proofs of correctness and complexity.In addition, we perform an experimental evaluation of all algorithms in combination with SVM classifiers for predicting the next parsing action, using data from thirteen languages.We show that all four algorithms give competitive accuracy, although the non-projective list-based algorithm generally outperforms the projective algorithms for languages with a non-negligible proportion of non-projective constructions.However, the projective algorithms often produce comparable results when combined with the technique known as pseudo-projective parsing.The linear time complexity of the stack-based algorithms gives them an advantage with respect to efficiency both in learning and in parsing, but the projective list-based algorithm turns out to be equally efficient in practice.Moreover, when the projective algorithms are used to implement pseudo-projective parsing, they sometimes become less efficient in parsing (but not in learning) than the non-projective list-based algorithm.Although most of the algorithms have been partially described in the literature before, this is the first comprehensive analysis and evaluation of the algorithms within a unified framework.
This article is a survey of methods for measuring agreement among corpus annotators.It exposes the mathematics and underlying assumptions of agreement coefficients, covering Krippendorff’s alpha as well as Scott’s pi and Cohen’s kappa; discusses the use of coefficients in several annotation tasks; and argues that weighted, alpha-like coefficients, traditionally less used than kappalike measures in computational linguistics, may be more appropriate for many corpus annotation tasks—but that their use makes the interpretation of the value of the coefficient even harder.
Many approaches to automatic sentiment analysis begin with a large lexicon of words marked with their prior polarity (also called semantic orientation).However, the contextual polarity of the phrase in which a particular instance of a word appears may be quite different from the word’s prior polarity.Positive words are used in phrases expressing negative sentiments, or vice versa.Also, quite often words that are positive or negative out of context are neutral in context, meaning they are not even being used to express a sentiment.The goal of this work is to automatically distinguish between prior and contextual polarity, with a focus on understanding which features are important for this task.Because an important aspect of the problem is identifying when polar terms are being used in neutral contexts, features for distinguishing between neutral and polar instances are evaluated, as well as features for distinguishing between positive and negative contextual polarity.The evaluation includes assessing the performance of features across multiple machine learning algorithms.For all learning algorithms except one, the combination of all features together gives the best performance.Another facet of the evaluation considers how the presence of neutral instances affects the performance offeatures for distinguishing between positive and negative polarity.These experiments show that the presence of neutral instances greatly degrades the performance of these features, and that perhaps the best way to improve performance across all polarity classes is to improve the system’s ability to identify when an instance is neutral.
The task of paraphrasing is inherently familiar to speakers of all languages.Moreover, the task of automatically generating or extracting semantic equivalences for the various units of language— words, phrases, and sentences—is an important part of natural language processing (NLP) and is being increasingly employed to improve the performance of several NLP applications.In this article, we attempt to conduct a comprehensive and application-independent survey of data-driven phrasal and sentential paraphrase generation methods, while also conveying an appreciation for the importance and potential use of paraphrases in the field of NLP research.Recent work done in manual and automatic construction of paraphrase corpora is also examined.We also discuss the strategies used for evaluating paraphrase generation techniques and briefly explore some future trends in paraphrase generation.
Research into corpus-based semantics has focused on the development of ad hoc models that treat single tasks, or sets of closely related tasks, as unrelated challenges to be tackled by extracting different kinds of distributional information from the corpus.As an alternative to this “one task, one model” approach, the Distributional Memory framework extracts distributional information once and for all from the corpus, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor.Different matrices are then generated from the tensor, and their rows and columns constitute natural spaces to deal with different semantic problems.In this way, the same distributional information can be shared across tasks such as modeling word similarity judgments, discovering synonyms, concept categorization, predicting selectional preferences of verbs, solving analogy problems, classifying relations between word pairs, harvesting qualia structures with patterns or example pairs, predicting the typical properties of concepts, and classifying verbs into alternation classes.Extensive empirical testing in all these domains shows that a Distributional Memory implementation performs competitively against task-specific algorithms recently reported in the literature for the same tasks, and against our implementations of several state-of-the-art methods.The Distributional Memory approach is thus shown to be tenable despite the constraints imposed by its multi-purpose nature.
We propose an account of indirect forms of speech acts to request and inform based on the hypothesis that language users can recognize actions being performed by others, infer goals being sought, and cooperate in their achievement.This cooperative behaviour is independently motivated and may or may not be intended by speakers.If the hearer believes it is intended, he or she can recognize the speech act as indirect; otherwise it is interpreted directly.Heuristics are suggested to decide among the interpretations.
Extraposition grammars are an extension of definite clause grammars, and are similarly defined in terms of logic clauses.The extended formalism makes it easy to describe left extraposition of constituents, an important feature of natural language syntax.
we construct a table so that the entry in the tells the parser how to parse i occurrences of 9.An Example Suppose for example that we were given the following grammar: (40a) S NP VP ADJS (40b) S V NP (PP) ADJS ADJS (40c) VP -0.V NP (PP) ADJS (40d) PP P NP (40e) NP NI NP PP ADJS adj ADJS I (In this example we will assume no lexical ambiguity V, P, inspection, we notice that NP are Catalan grammars and that ADJS is a Step grammar.PP = E i>0 NP = N ADJS = With these observations, the parser can process PPs, and by counting the number of occurrencof terminal symbols and looking up numbers in the appropriate tables.We now substitute (41a-c) into (40c).(42) VP = V NP (1 + PP)ADJS V (N E N)')(E (E and simplify the convolution of the two Catalan functions VP = V (N E adj') so that the parser can also find VPs by just counting coccurrences of terminal symbols.Now we simplify so that can also be parsed by just counting occurrences of terminal symbols. translate (40a-b) into the equation: (44) S = NP VP ADJS + V NP (1+PP) ADJS ADJS and then expand VP using (42) (45) S = NP (V NP (1+PP) ADJS) ADJS + V NP (1+PP) ADJS ADJS and factor S = (NP + 1) V NP (1+PP) That can be simplified considerably because NP (1 + PP) = N E E N E and (48) = E adj' E adj' = (i + so that S = (N E + 1) N E Cat.14- (i + has the following Jump Jump • • • • • (1, (i + 1+ (50) The entire example grammar has now been compiled into a form that is easier for parsing.This formula says that sentences are all of the form: (51) S (N (P N)*) V N (P N)* adj* which could be recognized by the following finite state machine: (52) ›c) Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982 Kenneth Church and Ramesh Patil Coping with Syntactic Ambiguity Furthermore, the number of parse trees for a given input sentence can be found by multiplying three numbers: (a) the Catalan of the number of P N's before the verb, (b) the Catalan of one more than the number of P N's after the verb, and (c) the ramp of the number of adj's.For example, the sentence (53) The man on the hill saw the boy with a telescope yesterday in the morning.Cat * * 3 = 6 parses.That is, there is one way to parse &quot;the man on the hill,&quot; two ways to parse &quot;saw the boy with a telescope&quot; (&quot;telescope&quot; is either a complement of &quot;see&quot; as in (54a-c) or is attached to &quot;boy&quot; as in (54d-f)), and three ways to parse the adjuncts (they could both attach to the S (54a,d), or they could both attach to the VP (54b,e), or they could split (54c,f)).(54a) [The man on the hill [saw the boy with a telescope] [yesterday in the morning.]](54b) The man on the hill [[saw the boy with a telescope] [yesterday in the morning.]](54c) The man on the hill [[saw the boy with a telescope] yesterday] in the morning.(54d) [The man on the hill saw [the boy with a telescope] [yesterday in the morning.]](54e) The man on the hill [saw [the boy with a telescope] [yesterday in the morning.]](54f) The man on the hill [saw [the boy with a telescope] yesterday] in the morning.All and only these possibilities are permitted by the grammar.10.Conclusion We began our discussion with the observation that certain grammars are &quot;every way ambiguous&quot; and suggested that this observation could lead to improved parsing performance.Catalan grammars were then introduced to remedy the situation so that the processor can delay attachment decisions until it discovers some more useful constraints.Until such time, the processor can do little more than note that the input sentence is &quot;every way ambiguous.&quot; We suggested that a table lookup scheme might be an effective method to implement such a processor.We then introduced rules for combining primitive grammars, such as Catalan grammars, into composite grammars.This linear systems view &quot;bundles up&quot; all the parse trees into a single concise description capable of telling us everything we might want to know about the parses (including how much it might cost to ask a particular question).This abstract view of ambiguity enables us to ask questions in the most convenient order, and to delay asking until it is clear that the pay-off will exceed the cost.This abstraction was strongly influenced by the notion of binding.We have presented combination rules in three different representation systems: power series, ATNs, and context-free grammars, each of which contributed its own insights.Power series are convenient for defining the algebraic operations, ATNs are most suited for discussing implementation issues, and context-free grammars enable the shortest derivations.Perhaps the following quotation best summarizes our motivation for alternating among these three representation systems: thing or idea seems meaningful only when we have different ways to represent it — different perspectives and different associations.Then you can turn it around in your mind, so to speak; however, it seems at the moment you can see it another way; you never come to a full stop.(Minsky 1981, p. 19) In each of these representation schemes, we have introduced five primitive grammars: Catalan, Unit Step, 1, and 0, and terminals; and four composition rules: addition, subtraction, multiplication, and division.We have seen that it is often possible to employ these analytic tools in order to re-organize (compile) the grammar into a form more suitable for processing efficiently.We have identified certain where the ambiguity is combinatoric, and have sketched a few modifications to the grammar that enable processing to proceed in a more efficient manner.In particular, we have observed it to be important for the grammar to avoid referencing quantities that are not easily determined, such as the dividing point between a noun phrase and a prepositional phrase as in (55) Put the block in the box on the table in the kitchen ... We have seen that the desired re-organization can be achieved by taking advantage of the fact that the autoconvolution of a Catalan series produces another Caseries.This reduced processing time from to almost linear time.Similar analyses have been discussed for a number of lexically and structurally ambiguous constructions, culminating with the example in section 9, where we transformed a grammar into a form that could be parsed by a single left-to-right pass over the terminal elements.Currently, these grammar reformulations have to be performed by hand.It ought to be possible to automate this process so that the reformulations could be performed by a grammar compiler.We leave this project open for future research.11.Acknowledgments We would like to thank Jon Allen, Sarah Ferguson, Lowell Hawkinson, Kris Halvorsen, Bill Long, Mitch Marcus, Rohit Parikh, and Peter Szolovits for their very useful comments on earlier drafts.We would Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982 especially like to thank Bill Martin for initiating the project.
In this paper we explore a new theory of discourse structure that stresses the role of purpose and processing in discourse.In this theory, discourse structure is composed of three separate but interrelated components: the structure of the sequence of utterances (called the linguistic structure), a structure of purposes (called the intentional structure), and the state of focus of attention (called the attentional state).The linguistic structure consists of segments of the discourse into which the utterances naturally aggregate.The intentional structure captures the discourse-relevant purposes, expressed in each of the linguistic segments as well as relationships among them.The attentional state is an abstraction of the focus of attention of the participants as the discourse unfolds.The attentional state, being dynamic, records the objects, properties, and relations that are salient at each point of the discourse.The distinction among these components is essential to provide an adequate explanation of such discourse phenomena as cue phrases, referring expressions, and interruptions.The theory of attention, intention, and aggregation of utterances is illustrated in the paper with a number of example discourses.Various properties of discourse are described, and explanations for the behavior of cue phrases, referring expressions, and interruptions are explored.This theory provides a framework for describing the processing of utterances in a discourse.Discourse processing requires recognizing how the utterances of the discourse aggregate into segments, recognizing the intentions expressed in the discourse and the relationships among intentions, and tracking the discourse through the operation of the mechanisms associated with attentional state.This processing description specifies in these recognition tasks the role of information from the discourse and from the participants' knowledge of the domain.
An efficient parsing algorithm for augmented context-free grammars is introduced, and its application to on-line natural language interfaces discussed.The algorithm is a generalized LR parsing algorithm, which precomputes an LR shift-reduce parsing table (possibly with multiple entries) from a given augmented context-free grammar.Unlike the standard LR parsing algorithm, it can handle arbitrary context-free grammars, including ambiguous grammars, while most of the LR efficiency is preserved by introducing the concept of a &quot;graph-structured stack&quot;.The graph-structured stack allows an LR shift-reduce parser to maintain multiple parses without parsing any part of the input twice in the same way.We can also view our parsing algorithm as an extended chart parsing algorithm efficiently guided by LR parsing tables.The algorithm is fast, due to the LR table precomputation.In several experiments with different English grammars and sentences, timings indicate a fiveto tenfold speed advantage over Earley's context-free parsing algorithm. algorithm parses a sentence strictly from left to right that is, starts parsing as soon as the user types in the first word of a sentence, without waiting for completion of the sentence.A practical on-line parser based on the algorithm has been implemented in Common Lisp, and running on Symbolics and HP Al workstations.The parser is used in the multi-lingual machine translation project at CMU.Also, a commercial on-line parser for Japanese language is being built by Intelligent Technology Incorporation, on the technique developed at
The syntactic structure of a sentence often manifests quite clearly the predicate-argument structure and relations of grammatical subordination.But scope dependencies are not so transparent.As a result, many systems for representing the semantics of sentences have ignored scoping or generated scopings with mechanisms that have often been inexplicit as to the range of scopings they choose among or profligate in the scopings they allow.This paper presents, along with proofs of some of its important properties, an algorithm that generates scoped semantic forms from unscoped expressions encoding predicate-argument structure.The algorithm is not profligate as are those based on permutation of quantifiers, and it can provide a solid foundation for computational solutions where completeness is sacrificed for efficiency and heuristic efficacy.
three previous efforts directed specifically to this problem.The first published effort is that of Klein and Simmons (1963), a simple system using suffix lists and limited frame rules.The second approach to lexical disambiguation is and Rubin (1971)), a system of several thousand context-frame rules.This algorithm was used to assign initial tags to the Brown Corpus.Third is the CLAWS system develto tag the (or LOB) Coris a corpus of British written English, parallel to the Brown Corpus.Parsing systems always encounter the problem of category ambiguity; but usually the focus of such systems is at other levels, making their responses less relevant for our purposes here.1.1 KLEIN AND SIMMONS Klein and Simmons (1963) describe a method directed primarily towards the task of initial categorial tagging rather than disambiguation.Its primary goal is avoiding &quot;the labor of constructing a very large dictionary&quot; (p. 335); a consideration of greater import then than now.The Klein and Simmons algorithm uses a palette of 30 categories, and claims an accuracy of 90% in tagging.The algorithm first seeks each word in dictionaries of about 400 function words, and of about 1500 words which &quot;are exceptions to the computational rules used&quot; (p. 339).The program then checks for suffixes and special characters as clues. of all, frame tests applied.These work on scopes bounded by unambiguous words, as do later algorithms.However, Klein and Simmons impose an explicit limit of three ambiguous words in a row.For such ambiguous words, the pair of unambiguous categories bounding it is mapped into a list.The list includes all known sequences of tags occurring between the particular bounding tags; all such sequences of the correct length become candidates.The program then matches the candidate sequences against the ambiguities remaining from earlier steps of the algorithm.When only one sequence is possible, disambiguation is successful.The samples used for calibration and testing were limited.First, Klein and Simmons (1963) performed &quot;hand analysis of a sample [size unspecified] of Golden Grammatical Category Disambiguation by Statistical Optimization Book Encyclopedia text&quot; (p. 342).Later, &quot;[w]hen it was run on several pages from that encyclopedia, it correctly and unambiguously tagged slightly over 90% of the words&quot; (p. 344).Further tests were run on small from the Americana from Scientific American.Klein and Simmons (1963) assert that &quot;[o]riginal fears that sequences of four or more unidentified parts of speech would occur with great frequency were not substantiated in fact&quot; (p. 3).This felicity, however, is an artifact.First, the relatively small set of categories reduces ambiguity.Second, a larger sample would reveal both (a) low-frequency ambiguities and (b) many long spans, as discussed below.1.2 GREENE AND RUBIN (TAGGIT) Greene and Rubin (1971) developed TAGGIT for tagging the Brown Corpus.The palette of 86 tags that TAGGIT uses has, with some modifications, also been used in both CLAWS and VOLSUNGA.The rationale underlying the choice of tags is described on pages 3-21 of Greene and Rubin (1971).Francis and Kucera (1982) report that this algorithm correctly tagged approxithe million words in the Brown Corpus (the tagging was then completed by human post-editors).Although this accuracy is substantially lower than that reported by Klein and Simmons, it should be remembered that Greene and Rubin were the first to attempt so large and varied a sample.TAGGIT divides the task of category assignment into initial (potentially ambiguous) tagging, and disambiguation.Tagging is carried out as follows: first, the program consults an exception dictionary of about 3,000 words.Among other items, this contains all known closed-class words.It then handles various special cases, such as words with initial &quot;$&quot;, contractions, special symbols, and capitalized words.The word's ending is then checked against a suffix list of about 450 strings.The lists were derived from lexicostatistics of the Brown Corpus.If TAGGIT has not assigned some tag(s) after these several steps, &quot;the word is tagged NN, VB, or JJ [that is, as being three-ways ambiguous], in order that the disambiguation routine may have something to work with&quot; (Greene and Rubin (1971), p. 25).After tagging, TAGGIT applies a set of 3300 context frame rules.Each rule, when its context is satisfied, has the effect of deleting one or more candidates from the list of possible tags for one word.If the number of candidates is reduced to one, disambiguation is considered successful subject to human post-editing.Each rule can include a scope of up to two unambiguous words on each side of the ambiguous word to which the rule is being applied.This constraint was determined as follows: In order to create the original inventory of Context Frame Tests, a 900-sentence subset of the Brown University Corpus was tagged... and its ambiguities were resolved manually; then a program was run 32 Computational Linguistics, Volume 14, Number 1, Winter 1988 Steven J. DeRose Grammatical Category Disambiguation by Statistical Optimization which produced and sorted all possible Context Frame Rules which would have been necessary to perform this disambiguation automatically.The rules generated were able to handle up to three consecutive ambiguous words preceded and followed by two non-ambiguous words [a constraint similar to Klein and Simmons'].However, upon examination of these rules, it was found that a sequence of two or three ambiguities rarely occurred more than once in a given context.Consequently, a decision was made to examine only one ambiguity at a time with up to two unambiguously tagged words on either side.The first rules created were the results of informed intuition (Greene and Rubin (1972), p. 32).1.3 CLAWS Marshall (1983, p. 139) describes the LOB Corpus tagging algorithm, later named CLAWS (Booth (1985)), as &quot;similar to those employed in the TAGGIT program&quot;.The tag set used is very similar, but somewhat larger, at about 130 tags.The dictionary used is derived from the tagged Brown Corpus, rather than from the untagged.It contains 7000 rather than 3000 entries, and 700 rather than 450 suffixes.CLAWS treats plural, possessive, and hyphenated words as special cases for purposes of initial tagging.The LOB researchers began by using TAGGIT on parts of the LOB Corpus.They noticed that While less than 25% of TAGGIT's context frame rules are concerned with only the immediately preceding or succeeding word... these rules were applied in about 80% of all attempts to apply rules.This relative overuse of minimally specified contexts indicated that exploitation of the relationship between successive tags, coupled with a mechanism that would be applied throughout a sequence of ambiguous words, would produce a more accurate and effective method of word disambiguation (Marshall (1983), p. 141).The main innovation of CLAWS is the use of a matrix probabilities, the relative likelihood of co-occurrence of all ordered pairs of tags.This matrix can be mechanically derived from any pre-tagged corpus.CLAWS used &quot;[a] large proportion of the Brown Corpus&quot;, 200,000 words (Marshall (1983), pp.141, 150).The ambiguities contained within a span of ambiguous words define a precise number of complete sets of mappings from words to individual tags.Each such of tags is called a path is composed of a number of tag collocations, and each such collocation has a probability which may be obtained from the collocation matrix.One may thus approximate each path's probability by the product of the probabilities of all its collocations.Each path corresponds to a unique assignment of tags to all words within a span. paths constitute a network, the path of maximal probability may be taken to contain the &quot;best&quot; tags.(1983) states that CLAWS the most probable sequence of tags, and in the majority of cases the correct tag for each individual word corresponds to the associated tag in the most probable sequence of tags&quot; (p. 142).But a more detailed examination of the Pascal code for CLAWS revealed that CLAWS has a more complex definition of &quot;most probable sequence&quot; than one might expect.A probability called &quot;SUMSUCCPROBS&quot; is predicated of each word.SUMSUCCPROBS is calculated by looping through all tags for the words immediately preceding, at, and following a word; for each tag triple, an increment is added, defined by: DownGrade(GetSucc(Tag2, Tag3), TagMark) * Get3SeqFactor(Tag1, Tag2, Tag3) the collocational probability of a tag either 1, or a special value the tag-triple list described below. the value of accordance with RTPs as described below.The CLAWS documentation describes SUMSUCC- PROBS as &quot;the total value of all relationships between the tags associated with this word and the tags associated with the next word...[found by] simulating all accesses to SUCCESSORS and ORDER2VALS which will be made....&quot; The probability of each node of the span network (or rather, tree) is then calculated in the following way as a tree representing all paths through which the span network is built: = currenttag), TagMark) * Get3SeqFactor(...))= PROB * (predecessor's It appears that the goal is to make each tag's probabe the summed probability of passing through it.At the final word of a span, pointers are followed back up the chosen path, and tags are chosen en route.We will see below that a simpler definition of optimal path is possible; nevertheless, there are several advantages of this general approach over previous ones.First, spans of unlimited length can be handled (subject to machine resources).Although earlier researchers (Klein and Simmons, Greene and Rubin) have suggested that spans of length over 5 are rare enough to be of little concern, this is not the case.The number of spans of a given length is a function of that length and the corpus size; so long spans may be obtained merely by examining more text.The total numbers of spans in the Brown Corpus, for each length from 3 to 19, are: 397111, 143447, 60224, 26515, 11409,5128, 2161, 903, 382, 161, 58, 29, 14, 6, 1, 0, 1.Graphing the logarithms Computational Linguistics, Volume 14, Number 1, Winter 1988 33 Steven J. DeRose Grammatical Category Disambiguation by Statistical Optimization of these quantities versus the span length for each, produces a near-perfect straight line.Second, a precise mathematical definition is possible for the fundamental idea of CLAWS.Whereas earlier efforts were based primarily on ad hoc or subjectively determined sets of rules and descriptions, and employed substantial exception dictionaries, this algorithm requires no human intervention for set-up; it is a systematic process.Third, the algorithm is quantitative and analog, rather than artificially discrete.The various tests and employed by earlier algorithms enforced absolute constraints on particular tags or collocations of tags.Here relative probabilities are weighed, and a series of very likely assignments can make possible a particular, a priori unlikely assignment with which they are associated.In addition to collocational probabilities, CLAWS also takes into account one other empirical quantity: Tags associated with words... can be with a marker @ or %; @ indicates that the tag is infrequently the correct tag for the associated word(s) (less than 1 in 10 occasions), % indicates is highly improbable...(less than 1 in 100 oc- ...The word disambiguation program currently uses these markers top devalue values when retrieving a value from the matrix, @ results in the value being halved, % in the value being divided by eight (Marshall (1983), p. 149).Thus, the independent probability of each possible tag for a given word influences the choice of an optimal Such probabilities will be referred to as Probabilities, Other features have been added to the basic algorithm.For example, a good deal of suffix analysis is used in initial tagging.Also, the program filters its output, considering itself to have failed if the optimal tag assignment for a span is not &quot;more than 90% probable&quot;. cases it reorders tags rather than actually disambiguating.On long spans this criterion is effectively more stringent than on short spans.A more significant addition to the algorithm is that a number of tag triples associated with a have been introduced which may either upgrade or downgrade values in the tree computed from the one-step matrix.For example, the triple [1] [2] adverb [3] past-tense-verb has been assigned a factor which downgrades a sequence containing this triple compared with a competing of [1] 'be' [2] adverb [3]-past-participle/adjective, on the basis that after a form of 'be', past participles and adjectives are more likely than a past tense verb (Marshall (1983), p. 146).A similar move was used near conjunctions, for which the words on either side, though separated, are more closely correlated to each other than either is to the conjunction itself (Marshall (1983), pp.146-147).For example, a verb/noun ambiguity conjoined to a verb should probably be taken as a verb.Leech, Garside, and Atwell (1983, p. 23) describe &quot;IDIOMTAG&quot;, which is applied after initial tag assignment and before disambiguation.It was developed as a means of dealing with sequences which would otherwise cause diffifor the automatic tagging.... for example, that tagged as a single conjunction....Tagging Program... can look at any combination of words and tags, with or without intervening words.It can delete tags, add tags, or change the probability of tags.Although this program might to be an hoc it is worth bearing in that any fully automatic language analysis syshas to come to with problems of lexical idiosyncrasy.IDIOMTAG also accounts for the fact that the probability of a verb being a past participle, and not simply past, is greater when the following word is &quot;by&quot;, as opposed to other prepositions.Certain cases of this sort may be soluble by making the collocational matrix distinguish classes of ambiguities—this question is being pursued.Approximately 1% of running text is tagged by IDIOMTAG (letter, G. N. Leech to Henry Kucera, June 7, 1985; letter, E. S. Atwell to Henry Kucera, June 20, 1985).Marshall notes the possibility of consulting a complete three-dimensional matrix of collocational probabilities.Such a matrix would map ordered triples of tags into the relative probability of occurrence of each such triple.Marshall points out that such a table would be too large for its probable usefulness.The author has proa table based upon more 85% of the Brown Corpus; it occupies about 2 megabytes (uncompressed).Also, the mean number of examples per triple is very low, thus decreasing accuracy.CLAWS has been applied to the entire LOB Corpus with an accuracy of &quot;between 96% and 97%&quot; (Booth (1985), p. 29).Without the idiom list, the algorithm was 94% accurate on a sample of 15,000 words (Marshall (1983)).Thus, the pre-processor tagging of 1% of all tokens resulted in a 3% change in accuracy; those particular assignments must therefore have had a substantial effect upon their context, resulting in changes of two other words for every one explicitly tagged.But CLAWS is timeand storage-inefficient in the extreme, and in some cases a fallback algorithm is employed to prevent running out of memory, as was discovered by examining the Pascal program code.How often the fallback is employed is not known, nor is it known what effect its use has on overall accuracy.Since CLAWS calculates the probability of every path, it operates in time and space proportional to the product of all the degrees of ambiguity of the words in the span.Thus, the time is exponential (and hence Non-Polynomial) in the span length.For the longest span in the Brown Corpus, of length 18, the number of paths examined would be 1,492,992.34 Computational Linguistics, Volume 14, Number 1, Winter 1988 Steven J. DeRose Grammatical Category Disambiguation by Statistical Optimization LINEAR-TIME ALGORITHM The algorithm described here depends on a similar empirically-derived transitional probability matrix to that of CLAWS, and has a similar definition of &quot;optimal path&quot;.The tagset is larger than TAGGIT's, though smaller than CLAWS', containing 97 tags.The ultimate assignments of tags are much like those of CLAWS.However, it embodies several substantive changes.Those features that can be algorithmically defined have been used to the fullest extent.Other add-ons have been minimized.The major differences are outlined below.First, the optimal path is defined to be the one whose component collocations multiply out to the highest probability.The more complex definition applied by using the sum of all paths at of the network, is not used.Second, VOLSUNGA overcomes the Non-Polynomial complexity of CLAWS.Because of this change, it is never necessary to resort to a fallback algorithm, and the program is far smaller.Furthermore, testing the algorithm on extensive texts is not prohibitively costly.Third, VOLSUNGA implements Relative Tag Probabilities (RTPs) in a more quantitative manner, based upon counts from the Brown Corpus.Where CLAWS scales probabilities by 1/2 for RTP < 0.1 (i.e., where less than 10% of the tokens for an ambiguous word are in the category in question), and by 1/8 for p < 0.01, VOLSUNGA uses the RTP value itself as a factor in the equation which defines probability.Fourth, VOLSUNGA uses no tag triples and no idioms.Because of this, manually constructing specialcase lists is not necessary.These methods are useful in certain cases, as the accuracy figures for CLAWS show; but the goal here was to measure the accuracy of a wholly algorithmic tagger on a standard corpus.
amp;quot;Two weeks later, Bonadea had already been his lover for a fortnight.&quot; Musil, Mann ohne Eigenschaften.A semantics of temporal categories in language and a theory of their use in defining the temporal relations between events both require a more complex structure on the domain underlying the meaning representations than is commonly assumed.This paper proposes an ontology based on such notions as causation and consequence, rather than on purely temporal primitives.A central notion in the ontology is that of an elementary event-complex called a &quot;nucleus.&quot; A nucleus can be thought of as an association of a goal event, or &quot;culmination,&quot; with a &quot;preparatory process&quot; by which it is accomplished, and a &quot;consequent state,&quot; which ensues.Natural-language categories like aspects, futurates, adverbials, and when-clauses are argued to change the temporal/aspectual category of propositions under the control of such a nucleic knowledge representation structure.The same concept of a nucleus plays a central role in a theory of temporal reference, and of the semantics of tense, which we follow McCawley, Partee, and Isard in regarding as an anaphoric category.We claim that any manageable formalism for naturallanguage temporal descriptions will have to embody such an ontology, as will any usable temporal database for knowledge about events which is to be interrogated using natural language.
In this paper, I consider a range of English expressions and show that their context-dependency can be characterized in terms of two properties: 1.They specify entities in an evolving model of the discourse that the listener is constructing; 2.The particular entity specified depends on another entity in that part of the evolving &quot;discourse model&quot; that the listener is currently attending to. expressions have been called show how tensed clauses share these characteristics, usually just attributed to anaphoric noun phrases.This not only allows us to capture in a simple way the but difficult-to-prove intuition that is anaphoric, also contributes to our knowledge of what is needed for understanding narrative text.
1982) for constructing language models for applications in speech recognition.2.Smadja (in press) discusses the separation between collocates in a very similar way.This definition y) a rectangular window.It might be interesting to consider alternatives (e.g. a triangular window or a decaying exponential) that would weight words less and less as they are separated by more and more words.Other windows are also possible.For example, Hindle (Church et al. 1989) has used a syntactic parser to select words in certain constructions of interest.4.Although the Good-Turing Method (Good 1953) is more than 35 years old, it is still heavily cited.For example, Katz (1987) uses the in order to estimate trigram probabilities in the recognizer.The Good-Turing Method is helpful for trigrams that have not been seen very often in the training corpus.The last unclassified line, ... shoppers anywhere from $50 ... raises interesting problems.Syntactic &quot;chunking&quot; shows that, in spite its co-occurrence of line does not belong here.An intriguing exercise, given the lookup table we are trying construct, is how to guard against false inferences such as that since tagged [PERSON], here count as either a LOCATION.Accidental coincidences of this kind do not have a significant effect on the measure, however, although they do serve as a reminder of the probabilistic nature of the findings.The word also occurs significantly in the table, but on closer it is clear that this use of to time) as something like a commodity or resource, not as part of a time adjunct.Such are the pitfalls of lexicography (obvious when they are pointed out).
present algorithm for generating strings from logical form encodings that improves upon previous algorithms in that it places fewer restrictions on the class of grammars to which it is applicable.In particular, unlike a previous bottom-up generator, it allows use of semantically nonmonotonic grammars, yet unlike top-down methods, it also permits left-recursion.The enabling design feature of the algorithm is its implicit traversal of the analysis tree for the string being generated in a semantic-head-driven fashion.
this paper, we present a statistical to machine translation.We describe the application of our approach to translation from French to English and give preliminary results.
1. virgin 31 2. pine 31 3. bush 31 (3, 1)8 4. trees 32 (4, 1)8 (4, 3)8 5. trunks 32 6. trees 33 (6, 4)8 (6, 1-3)8 Chain 6 Word Sentence Lexical Chain 1. hand-in-hand 34 2. matching 34 3. whispering 35 4. laughing 35 5. warm 38 (5, 1)8 (5, 4)8 Chain 7 Word Sentence Lexical Chain 1. first 1 2. initial 1 (2, 1)8 3. final 2 (3, 2-1)8 47 Computational Linguistics Volume 17, Number 1 Chain 8 Word Sentence Lexical Chain 1. night 2 2. dusk 3 3. darkness 3 Chain 9 Word Sentence Lexical Chain 1. environment 7 2. setting 7 3. surrounding 8
The met* method distinguishes selected examples of metonymy from metaphor and from literalness and anomaly in short English sentences.In the met* method, literalness is distinguished because it satisfies contextual constraints that the nonliteral others all violate.Metonymy is discriminated from metaphor and anomaly in a way that [1] supports Lakoff and Johnson's (1980) view that in metonymy one entity stands for another whereas in metaphor one entity is viewed as another, [2] permits chains of metonymies (Reddy 1979), and [3] allows metonymies to co-occur with instances of either literalness, metaphor, or anomaly.Metaphor is distinguished from anomaly because the former contains a relevant analogy, unlike the latter.The met* method is part of Collative Semantics, a semantics for natural language processing, and has been implemented in a computer program called meta5.Some examples of meta5's analysis of metaphor and metonymy are given.The met* method is compared with approaches from artificial intelligence, linguistics, philosophy, and psychology.
In this paper, I will discuss four major topics relating to current research in lexical semantics: methodology, descriptive coverage, adequacy of the representation, and the computational usefulness of representations.In addressing these issues, I will discuss what I think are some of the central problems facing the lexical semantics community, and suggest ways of best approaching these issues.Then, I will provide a method for the decomposition of lexical categories outline a theory of lexical semantics embodying a notion of well as several levels of semantic description, where the semantic load is spread more evenly throughout the lexicon.I argue that lexical decomposition is possible if it is perthan assuming a fixed set of primitives, I will assume a fixed number of generative devices that can be seen as constructing semantic expressions.I develop theory of Structure, representation language for lexical items, which renders much lexical ambiguity in the lexicon unnecessary, while still explaining the systematic polysemy that words carry.Finally, I discuss how individual lexical structures can be integrated into the lexical knowledge base through a theory of inheritance. provides us with the necessary principles of global organization for the lexicon, enabling us to fully integrate our natural language lexicon into a conceptual whole.
This paper addresses the problem of how to identify the intended meaning of individual words in unrestricted texts, without necessarily having access to complete representations of sentences.To discriminate senses, an understander can consider a diversity of information, including syntactic tags, word frequencies, collocations, semantic context, role-related expectations, and syntactic restrictions.However, current approaches make use of only small subsets of this information.Here we will describe how to use the whole range of information.Our discussion will include how the preference cues relate to general lexical and conceptual knowledge and to more specialized knowledge of collocations and contexts.We will describe a method of combining cues on the basis their individual than a fixed ranking among cue-types.We will also discuss an application of the approach in a system that computes sense tags for arbitrary texts, even when it is unable to determine a single syntactic or semantic representation for some sentences.
new natural language system, been developed for applications involving spoken tasks. key ideas from context free grammars, Augmented Transition (ATN's), and the unification concept. a seamless interface between syntactic and semantic analysis, and also produces a highly constraining probabilistic language model to improve recognition performance.An initial set of context-free rewrite rules provided by hand is first converted to a network structure.Probability assignments on all arcs in the network are obtained automatically from a set of example sentences.The parser uses a stack decoding search strategy, with a top-down control flow, and includes a feature-passing mechanism to deal long-distance movement, agreement, and semantic constraints. an automatic sentence generation capability that has been effective for identifying overgeneralization problems as well as in producing a word-pair language model for a recognizer.The parser is currently with MIT's for use in two application domains, with the parser screening recognizer outputs either at the sentential level or to filter partial theories during the active search process.
We address the problem of predicting a word from previous words in a sample of text.In particular, we discuss n-gram models based on classes of words.We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words.We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics.
(1992).&quot;Planning text for advisory dialogues: Capturing intentional, rhetorical and attentional information.&quot;
Much work has been done on the statistical analysis of text.In some cases reported in the literature, inappropriate statistical methods have been used, and statistical significance of results have not been addressed.In particular, asymptotic normality assumptions have often been used unjustifiably, leading to flawed results.This assumption of normal distribution limits the ability to analyze rare events.Unfortunately rare events do make up a large fraction of real text.However, more applicable methods based on likelihood ratio tests are available that yield good results with relatively small samples.These tests can be implemented efficiently, and have been used for the detection of composite terms and for the determination of domain-specific terms.In some cases, these measures perform much better than the methods previously used.In cases where traditional contingency table methods work well, the likelihood ratio tests described here are nearly identical.
Researchers in both machine translation (e.g., Brown et al. 1990) and bilingual lexicography (e.g., Klavans and Tzoukermann 1990) have recently become interested in studying bilingual corpora, bodies of text such as the Canadian Hansards (parliamentary proceedings), which are available in multiple languages (such as French and English).One useful step is to align the sentences, that is, to identify correspondences between sentences in one language and sentences in the other language.This paper will describe a method and a program (align) for aligning sentences based on a simple statistical model of character lengths.The program uses the fact that longer sentences in one language tend to be translated into longer sentences in the other language, and that shorter sentences tend to be translated into shorter sentences.A probabilistic score is assigned to each proposed correspondence of sentences, based on the scaled difference of lengths of the two sentences (in characters) and the variance of this difference.This probabilistic score is used in a dynamic programming framework to find the maximum likelihood alignment of sentences.It is remarkable that such a simple approach works as well as it does.An evaluation was performed based on a trilingual corpus of economic reports issued by the Union Bank of Switzerland (UBS) in English, French, and German.The method correctly aligned all but 4% of the sentences.Moreover, it is possible to extract a large subcorpus that has a much smaller error rate.By selecting the best-scoring 80% of the alignments, the error rate is reduced from 4% to 0.7%.There were more errors on the English—French subcorpus than on the English—German subcorpus, showing that error rates will depend on the corpus considered; however, both were small enough to hope that the method will be useful for many language pairs.To further research on bilingual corpora, a much larger sample of Canadian Hansards (approximately 90 million words, half in English and and half in French) has been aligned with the and will be available through the Data Collection Initiative of the Association for Computational Linguistics (ACL/DCI).In addition, in order to facilitate replication of the an appendix is provided with detailed c-code of the more difficult core of the program.
We propose that many ambiguous prepositional phrase attachments can be resolved on the basis of the relative strength of association of the preposition with verbal and nominal heads, estimated on the basis of distribution in an automatically parsed corpus.This suggests that a distributional approach can provide an approximate solution to parsing problems that, in the worst case, call for complex reasoning.
No Figure 4 SAT after where 254 is a translation of the latter part of 218 and the early part of 219: When a proton strikes a gas nucleus, it produces three kinds of pion, of which one kind decays into two gamma rays.The gamma rays travel close to the original trajectory of the proton, and the model predicts they will be beamed toward the earth at just two points on the pulsars orbit around the companion star.Trifft em Proton auf einen Atomkern in dieser Gashfille, werden drei Arten von Pionen erzeugt.Die neutralen Pionen zerfallen in jeweils zwei Gammaquanten, die sich beinahe in dieselbe Richtung wie das urspriingliche Proton bewegen.Nach der Modellvorstellung gibt es gerade zwei Positionen im Umlauf des Pulsars urn semen Begleitstern, bei denen die Strahlung in Richtung zum Beobachter auf der Erde ausgesandt wird.Another example is provided by English sentences 19 and 20, which appear in German as sentences 21 and 22.However the latter part of English sentence 19 is in fact transferred to sentence 22 in the German.This is also unmistakable in the final results.Notice also, in this example, that the definition of &quot;photon&quot; has become a parenthetical expression at the beginning of the second German sentence, a fact which is not reflected.The other end of the cosmic-ray energy spectrum is defined somearbitrarily: any quantum greater than electron volts arriving from space is considered a cosmic ray.The definition encompasses not only particles but also gamma-ray photons, which are quanta of electromagnetic radiation.2 4 4 4-5 ; 136 Martin Kay and Martin Riischeisen Text-Translation Alignment Le Correctness of sentence alignment in the various passes of the algorithm.Pass Correctness Coverage Constraint in SAT of SAT by AST 1 100% 12% 4% 2 100% 47% 17% 3 100% 89% 38% 4 99.7% 96% 41% Das untere Ende des Spektrums der kosmischen Strahlen ist verhaltnismai3ig unscharf definiert.Jedes Photon (Quant der elektromagnetischen Strahlung) Teilchen mit einer Energie von mehr als Elektronenvolt, das aus dem Weltraum eintrifft, bezeichnet man als kosmischen Strahl. frequently occurred in our data that sentences that separated by colons or semicolons in the original appeared as completely distinct sentences in the German translation.Indeed, the common usage in the two languages would probably have been better represented if we had treated colons and semicolons as sentence separators, along with periods, question marks, and the like.There are, of course, situations in which these punctuation marks are used in other ways, but they are considerably less frequent and, in any case, it seems that our program would almost always make the right associations.An example involving the colon is to be found in sentence 142 of the original, translated as sentences 163 and 164: The absorption lines established a lower limit on the distance of Cygnus X-3: it must be more distant than the farthest hydrogen cloud, which is believed to lie about 37,000 light-years away, near the edge of the galaxy. dieser Absorptionslinie kann eine untere Grenze der Entvon Cygnus X bestimmen.Die Quelle mu13 jenseits am weitesten entfernten Wasserstoff-Wolke sein, also weiter als ungefahr 37000 Lichtjahre entfernt, am Rande der Milchstrai3e. sentence 197, containing semicolon, is translated by German sentences 228 and 229: The estimate is conservative; because it is based on the gamma rays observed arriving at the earth, it does not take into account the likelihood that Cygnus X emits cosmic rays in all directions.Dies ist eine vorsichtige Absch5tzung.Sie ist nur aus den Gammastrahlen-Daten abgeleitet, die auf der Erde gemessen werden; daI3 Cygnus X-3 wahrscheirtlich kosmische Strahlung in alle Richtungen aussendet, ist dabei noch nicht beriicksichtigt.137 Computational Linguistics Volume 19, Number 1 German Sentence No CA e• tc•' x • 10 20 30 40 50 English Sentence No Figure 5 alignment of the first of the test texts: true alignment (dots) and hypothesis of the SAT after the first pass (circles) and after the second pass (crosses).Table 3 summarizes the accuracy of the algorithm as a function of the number of passes.The (thresholded) SAT is evaluated by two criteria: the number of correct by the total number alignments, and—since the SAT does not necessarily give an alignment for every sentence—the coverage, i.e., the number of sentences with at least one entry relative to the total number of sentences.An alignment is said to be correct if the SAT contains exactly the numbers of the sentences that are complete or partial translations of the original sentence.The coverage of 96% of the SAT in pass 4 is as much as one would expect, since the remaining nonaligned sentences are one-zero alignments, most of them due to the German subheadings that are not part of the English version.The table also shows that the AST always provides a significant number of candidates for alignment with each sentence before a pass: the fourth column gives the number of true sentence alignments relative to the total number of candidates in the AST.Recall that the final alignment is always a subset of the hypotheses in the AST in every preceding pass.Figure 5 shows the true sentence alignment for the first 50 sentences (dots), and how the algorithm discovered them: in the first pass, only a few sentences are set into correspondence (circles); after the second pass (crosses) already almost half of the correspondences are found.Note that there are no wrong alignments in the first two passes.In the third pass, almost all of the remaining alignments are found (for the first 50 sentences in the figure: all), and a final pass usually completes the alignment.Our algorithm produces very favorable results when allowed to converge gradually.Processing time in the original LISP implementation was high, typically several hours for each pass.By trading CPU time for memory massively, the time needed by a C++ implementation on a Sun 4/75 was reduced to 1.7 mm for the first pass, 0.8 mm for the second, and 0.5 min for the third pass in an application to this pair of articles.(Initialization, i.e., reading the files and building up the data structures, takes another 0.6 min in the beginning.)It should be noted that a naive implementation of 138 Martin Kay and Martin Roscheisen Text-Translation Alignment the algorithm without using the appropriate data structures can easily lead to times that are a factor of 30 higher and do not scale up to larger texts.The application of our method to a text that we put together from the Hansard corpus had essentially no problem in identifying the correct sentence alignment in a process of five passes.The alignments for the first 1000 sentences of the English text were checked by hand, and seven errors were found; five of them occurred in sentences where sentence boundaries were not correctly identified by the program of periods that did not mark a sentence boundary and were identified such by very simple preprocessing program.The other two errors involved two short sentences for which the SAT did not give an alignment.Processing time increased essentially linearly (per pass): the first pass took 8.3 min, the second 3.2 mm, and it further decreased until the last pass, which took 2.1 min.(Initialization took 4.2 min.)Note that the error rate depends crucially on the kind of &quot;annealing schedule&quot; used: if the thresholds that allow a word pair in the WAT to influence the SAT are lowered too fast, only a few passes are needed, but accuracy deteriorates.For example, in an application where the process terminated after only three passes, the accuracy was only in the eighties (estimated on the basis of the first 120 sentences of the English Hansard text checked by hand).Since processing time after the first pass is usually already considerably lower, we have found that a high accuracy can safely be attained when more passes are allowed than are actually necessary.In order to evaluate the sensitivity of the algorithm to the lengths of the texts that are to be aligned, we applied it to text samples that ranged in length from 10 to 1000 sentences, and examined the accuracy of the WAT after the first pass; that is, more precisely, the number of word pairs in the WAT that are valid translations relative to the total number of word pairs with a similarity of not less than 0.7 (the measurements are cross-validated over different texts).The result is that this accuracy increases asymptotically to 1 with the text length, and is already higher than 80% for text length of 100 sentences (which sufficient to reach an almost perfect alignment in the end).Roughly speaking, the accuracy is almost 1 for texts longer than 150 sentences, and around 0.5 for text length in the lower range from 20 to 60.In other words, texts of a length of more than 150 sentences are suitable to be processed in this way; text fragments shorter than 80 sentences do not have a high proportion of correct word pairs in the first WAT, but further experiments showed that the final alignment for texts of this length is, on average, again almost perfect: the drawback of a less accurate initial WAT is apparently largely compensated for by the fact that the AST is also narrower for these texts; however, the variance in the alignment accuracies is significantly higher.5.Related Work Since we addressed the text translation alignment problem in 1988, a number of researchers, among them Gale and Church (1991) and Brown, Lai, and Mercer (1991), have worked on the problem.Both methods are based on the observation that the length of text unit is highly correlated to the length of the translation of this unit, no matter whether length is measured in number of words or in number of characters (see Figure 6).Consequently, they are both easier to implement than ours, though not necessarily more efficient.The method of Brown, Lai, and Mercer (1991) is based on a hidden Markov model for the generation of aligned pairs of corpora, whose parameters are estimated from a large text.For an application of this method to the Canadian Hansard, good results are reported.However, the problem was also considerably facilitated by the way the implementation made use of Hansard-specific comments 139 Linguistics Volume 19, Number German: Length in wards CO German: Length in chars 40 60 80 120 200 400 600 800 English: Length in words English: Length in chars Figure 6 Lengths of Aligned Paragraphs are Correlated: Robust regression between lengths of aligned paragraphs.Left: length measured in words.Right: length measured in characters. and annotations: these are used in a preprocessing step to find anchors for sentence alignment such that, on average, there are only ten sentences in between.Moreover, corpus is for the near literalness of its translations, and it is therefore unclear to what extent the good results are due to the relative ease of the problem.This would be an important consideration when comparing various algorithms; when the algorithms are actually applied, it is clearly very desirable to incorporate as much prior knowledge (say, on potential anchors) as possible.Moreover, long texts can almost always be expected to contain natural anchors, such as chapter section headings, at which to make an priori Gale and Church (1991) note that their method performed considerably better when lengths of sentences were measured in number of characters instead of in number of words.Their method is based on a probabilistic model of the distance between and a dynamic programming algorithm is used to minimize the total aligned Their implementation assumes that character in one language gives rise to, on average, one character in the other language.'In our texts, one character in English on average gives rise to somewhat more than 1.2 characters in German, and the correlation between the lengths (in characters) of aligned paragraphs in the two languages was with 0.952 lower than the 0.991 that are menin Gale and Church (1991), which supports our impression that the we used are hard texts to align, but it is not clear to what extent this would deteriorate the results.In applications to economic reports from the Union Bank of Switzerland, the method performs very well on simple alignments (one-to-one, oneto-two), but has at the moment problems with complex matches.The method has the 8 Recall that, in a similar way, we assumed in our implementation that one sentence in one language gives rise to, on average, n/rn sentences in the other language (see first footnote in Section 2.3).140 Martin Kay and Martin Riischeisen Text-Translation Alignment advantage of associating a score with pairs of sentences so that it is easy to extract a subset for which there is a high likelihood that the alignments are correct.Given the simplicity of the methods proposed by Brown, Lai, and Mercer and Gale and Church, either of them could be used as a heuristic in the construction of the initial AST in our algorithm.In the current version, the number of candidate sentence pairs that are considered in the first pass near the middle of a text contributes disproportionally to the cost of the computation.In fact, as we remarked earlier, the of this step is proposed modification would effectively make it linear.6.Future Work For most practical purposes, the alignment algorithm we have described produces very satisfactory results, even when applied to relatively free translations.There are doubtless many places in which the algorithm itself could be improved.For example, it is clear that the present method of building the SAT favors associations between long sentences, and this is not surprising, because there is more information in long sentences.But we have not investigated the extent of this bias and we do not therefore know it as appropriate.The present algorithm rests on being able to identify one-to-one associations between certain words, notably technical terms and proper names.It is clear from a brief inspection of Table 2 that very few correspondences are noticed among everyday words and, when they are, it is usually because those words also have precise technical uses.The very few exceptions include &quot;only&quot;/&quot;nur&quot; and 'the&quot;/&quot;die-.&quot; The pair &quot;per&quot;/&quot;pro&quot; might also qualify but if the languages afford any example of a scientific preposition, this is surely it.The most interesting further developments would be in the direction of loosening up this dependence on one-to-one associations both because this would present a very significant challenge and also because we are convinced that our present method identifies essentially all the significant one-to-one associations.There are two obvious kinds of looser associations that could be investigated.One would consist of connections between a single vocabulary item in one language and two or more in the other, or even between several items in one language and several in the other.The other would involve connections—one–one, one–many, or many–many—between phrases or recurring sequences.We have investigated the first of these enough to satisfy ourselves that there is latent information on one-to-many associations in the text, and that it can be revealed by suitable extensions of our methods.However, it is clear that the combinatorial problems associated with this approach are severe, and pursuing it would require much fine tuning of the program and designing much more effective ways of indexing the most important data structures.The key to reducing the combinatorial explosion probably lies in using tables of similarities such as those the present algorithm uses to suggest combinations of items that would be worth considering.If such an approach could be made efficient enough, it is even possible that it would provide a superior way of solving the problem for which our heuristic methods of morphological analysis were introduced.Its superiority would come from the fact that it would not depend on words being formed by concatenation, but would also accommodate such phenomena as umlaut, ablaut, vowel harmony, and the nonconcatenative process of Semitic morphology.The problems of treating recurring sequences are less severe.Data structures, such as the Patricia tree (Knuth 1973; pp.490-493) provide efficient means of identifying all such sequences and, once identified, the data they provide could be added to 141 Computational Linguistics Volume 19, Number 1 the WAT much as we now add the results of morphological analysis.Needless to say, this would only allow for uninterrupted sequences.Any attempt to deal with discontinuous sequences would doubtless also involve great combinatorial problems.These avenues for further development are intriguing and would surely lead to interesting results.But it is unlikely that they would lead to much better sets of associations among sentences than are to be found in the SATs that our present program produces, and it was mainly these results that we were interested in from the outset.The other avenues we have mentioned concern improvements in the WAT which, for us, was always a secondary interest.
Natural languages are full of collocations, recurrent combinations of words that co-occur more often than expected by chance and that correspond to arbitrary word usages.Recent work in lexicography indicates that collocations are pervasive in English; apparently, they are common in all types of writing, including both technical and nontechnical genres.Several approaches have been proposed to retrieve various types of collocations from the analysis of large samples of textual data.These techniques automatically produce large numbers of collocations along with statistical figures intended to reflect the relevance of the associations.However, none of these techniques provides functional information along with the collocation.Also, the results produced often contained improper word associations reflecting some spurious aspect of the training corpus that did not stand for true collocations.In this paper, we describe a set of techniques based on statistical methods for retrieving and identifying collocations from large textual corpora.These techniques produce a wide range of collocations and are based on some original filtering methods that allow the production of richer and higher-precision output.These techniques have been implemented and resulted in a tool, techniques are described and some results are presented on a 10 corpus of stock market news reports.A lexicographic evaluation of a retrieval tool has been made, and the estimated precision of 80%.
Imagine a language that is completely unfamiliar; the only means of studying it are an ordinary grammar book and a very large corpus of text.No dictionary is available.How can easily recognized, surface grammatical facts be used to extract from a corpus as much syntactic information as possible about individual words?This paper describes an approach based on two principles.First, rely on local morpho-syntactic cues to structure rather than trying to parse entire sentences.Second, treat these cues as probabilistic rather than absolute indicators of syntactic structure.Apply inferential statistics to the data collected using the cues, rather than drawing a categorical conclusion from a single occurrence of a cue.The effectiveness of this approach for inferring the syntactic frames of verbs is supported by experiments on an English corpus using a program called Lerner.Lerner starts out with no knowledge of content words—it bootstraps from determiners, auxiliaries, modals, prepositions, pronouns, complementizers, coordinating conjunctions, and punctuation.
We describe a series of five statistical models of the translation process and give algorithms for estimating the parameters of these models given a set of pairs of sentences that are translations of one another.We define a concept of word-by-word alignment between such pairs of sentences.For any given pair of such sentences each of our models assigns a probability to each of the possible word-by-word alignments.We give an algorithm for seeking the most probable of these alignments.Although the algorithm is suboptimal, the alignment thus obtained accounts well for the word-by-word relationships in the pair of sentences.We have a great deal of data in French and English from the proceedings of the Canadian Parliament.Accordingly, we have restricted our work to these two languages; but we feel that because our algorithms have minimal linguistic content they would work well on other pairs of languages.We also feel, again because of the minimal linguistic content of our algorithms, that it is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus.
In this paper we outline a research program for computational linguistics, making extensive use of text corpora.We demonstrate how a semantic framework for lexical knowledge can suggest richer relationships among words in text beyond that of simple co-occurrence.The work suggests how linguistic phenomena such as metonymy and polysemy might be exploitable for semantic tagging of lexical items.Unlike with purely statistical collocational analyses, the framework of a semantic theory allows the automatic construction of predictions about deeper semantic among words appearing in systems. illustrate the approach for the acquisition of lexical information for several classes of nominals, and how such techniques can fine-tune the lexical structures acquired from an initial seeding of a machine-readable dictionary.In addition to conventional lexical semantic relations, we show how information concerning lexical presuppositions and preference relations can also be acquired from corpora, when analyzed with the appropriate semantic tools.Finally, we discuss the potential that corpus studies have for enriching the data set for theoretical linguistic research, as well as helping to confirm or disconfirm linguistic hypotheses.
Heights, NY.Katz, S. M. (1987).&quot;Estimation of probabilities from sparse data for the language model component of a speech In Transactions on Acoustics, Speech, and Signal Processing, Vol.ASSP-35 No.3.Kuhn, R., and De Mori, R. (1990).&quot;A cache-based natural language model for recognition.&quot; In Transactions on Pattern Analysis and Machine Intelligence, 12,570-583.Kupiec, J.(1989).&quot;Augmenting a hidden Markov model for phrase-dependent tagging.&quot; In Speech and Language Workshop.
phrases are linguistic expressions such as now and function as explicit indicators of structure of a discourse.For example, signal the beginning of a subtopic or a return a previous topic, while mark subsequent material as a response to prior material, or as an explanatory comment.However, while cue phrases may convey discourse structure, each also one or more alternate uses.While be used sententially as an adverbial, for example, the discourse use initiates a digression.Although distinguishing discourse and sentential uses of cue phrases is critical to the interpretation and generation of discourse, the question of how speakers and hearers accomplish this disambiguation is rarely addressed.This paper reports results of empirical studies on discourse and sentential uses of cue phrases, in which both text-based and prosodic features were examined for disambiguating power.Based on these studies, it is proposed that discourse versus sentential usage may be distinguished by intonational features, specifically, pitch accent and prosodic phrasing.A prosodic model that characterizes these distinctions is identified.This model is associated with features identifiable from text analysis, including orthography and part of speech, to permit the application of the results of the prosodic analysis to the generation of appropriate intonational features for discourse and sentential uses of cue phrases in synthetic speech. phrases, and phrases that directly signal the structure of a discourse, been variously termed words, discourse markers, discourse connectives, particles the computational linguistic and conversational analysis
This paper presents a set of mathematical and computational tools for manipulating and reasoning about regular languages and regular relations and argues that they provide a solid basis for computational phonology.It shows in detail how this framework applies to ordered sets of context-sensitive rewriting rules and also to grammars in Koskenniemi's two-level formalism.This analysis provides a common representation of phonological constraints that supports efficient generation and recognition by a single simple interpreter.
This paper presents a syntactic analysis method that first detects conjunctive structures in a sentence by checking parallelism of two series of words and then analyzes the dependency structure of the sentence with the help of the information about the conjunctive structures.Analysis of long sentences is one of the most difficult problems in natural language processing.The main reason for this difficulty is the structural ambiguity that is common for conjunctive structures that appear in long sentences.Human beings can recognize conjunctive structures because of a certain, but sometimes subtle, similarity that exists between conjuncts.Therefore, we have developed an algorithm for calculating a similarity measure between two arbitrary series of words from the left and the right of a conjunction and selecting the two most similar series of words that can reasonably be considered as composing a conjunctive structure.This is realized using a dynamic programming technique.A long sentence can be reduced into a shorter form by recognizing conjunctive structures.Consequently, the total dependency structure of a sentence can be obtained by relatively simple head-dependent rules.A serious problem concerning conjunctive structures, besides the ambiguity of their scopes, is the ellipsis of some of their components.Through our dependency analysis process, we can find the ellipses and recover the omitted components.We report the results of analyzing 150 Japanese sentences to illustrate the effectiveness of this method.
This paper presents an algorithm for identifying the noun phrase antecedents of third person pronouns and lexical anaphors (reflexives and reciprocals).The algorithm applies to the syntactic representations generated by McCord's Slot Grammar parser and relies on salience measures derived from syntactic structure and a simple dynamic model of attentional state.Like the parser, the algorithm is implemented in Prolog.The authors have tested it extensively on computer manual texts and conducted a blind test on manual text containing 360 pronoun occurrences.The algorithm successfully identifies the antecedent of the pronoun for 86% of these pronoun occurrences.The relative contributions of the algorithm's components to its overall success rate in this blind test are examined.Experiments were conducted with an enhancement of the algorithm that contributes statistically modelled information concerning semantic and real-world relations to the algorithm's decision procedure.Interestingly, this enhancement only marginally improves the algorithm's performance (by 2%).The algorithm is compared with other approaches to anaphora resolution that have been proposed in the literature.In particular, the search procedure of Hobbs' algorithm was implemented in the Slot Grammar framework and applied to the sentences in the blind test set.The authors' algorithm achieves a higher rate of success (4%) than Hobbs' algorithm.The relation of the algorithm to the centering approach is discussed, as well as to models of anaphora resolution that invoke a variety of informational factors in ranking antecedent candidates.
This paper presents a new approach for resolving lexical ambiguities in one language using statistical data from a monolingual corpus of another language.This approach exploits the differences between mappings of words to senses in different languages.The paper concentrates on the problem of target word selection in machine translation, for which the approach is directly applicable.The presented algorithm identifies syntactic relations between words, using a source language parser, and maps the alternative interpretations of these relations to the target language, using a bilingual lexicon.The preferred senses are then selected according to statistics on lexical relations in the target language.The selection is based on a statistical model and on a constraint propagation algorithm, which simultaneously handles all ambiguities in the sentence.The method was evaluated using three sets of Hebrew and German examples and was found to be very useful for disambiguation.The paper includes a detailed comparative analysis of statistical sense disambiguation methods.
We describe an extension of Earley's parser for stochastic context-free grammars that computes the following quantities given a stochastic context-free grammar and an input string: a) probabilities of successive prefixes being generated by the grammar; b) probabilities of substrings being generated by the nonterminals, including the entire string being generated by the grammar; c) most likely (Viterbi) parse of the string; d) posterior expected number of applications of each grammar production, as required for reestimating rule probabilities.Probabilities (a) and (b) are computed incrementally in a single left-to-right pass over the input.Our algorithm compares favorably to standard bottom-up parsing methods for SCFGs in that it works efficiently on sparse grammars by making use of Earley's top-down control structure.It can process any context-free rule format without conversion to some normal form, and combines computations for (a) through (d) in a single algorithm.Finally, the algorithm has simple extensions for processing partially bracketed inputs, and for finding partial parses and their likelihoods on ungrammatical inputs.
the original motivations for centering, the basic definitions underlying the centering framework, and the original theoretical claims.This paper attempts to meet that need.To accomplish this goal, we have chosen to remove descriptions of many open research questions posed in Grosz, Joshi, and Weinstein (1986) as well as solutions that were only partially developed.We have also greatly shortened the discussion of criteria for and constraints on a possible semantic theory as a foundation for this work.
Recently, there has been a rebirth of empiricism in the field of natural language processing.Manual encoding of linguistic information is being challenged by automated corpus-based learning as a method of providing a natural language processing system with linguistic knowledge.Although corpus-based approaches have been successful in many different areas of natural language processing, it is often the case that these methods capture the linguistic information they are modelling indirectly in large opaque tables of statistics.This can make it difficult to analyze, understand and improve the ability of these approaches to model underlying linguistic behavior.In this paper, we will describe a simple rule-based approach to automated learning of linguistic knowledge.This approach has been shown for a number of tasks to capture information in a clearer and more direct fashion without a compromise in performance.We present a detailed case study of this learning method applied to part-of-speech tagging.
Collocations are notoriously difficult for non-native speakers to translate, primarily because they are opaque and cannot be translated on a word-by-word basis.We describe a program named given a pair of parallel corpora in two different languages and a list of collocations in one of them, automatically produces their translations.Our goal is to provide a tool for compiling bilingual lexical information above the word level in multiple languages, for different domains.The algorithm we use is based on statistical methods and produces p-word translations of collocations in which not be the same.For example, ... decision, employment equity, market ... decision, equite matiere d'emploi, Testing three years' worth of the Hansards corpus yielded the French translations of 300 collocations for each year, evaluated at 73% accuracy on average.In this paper, we describe the statistical measures used, the algorithm, the implementation of our results and evaluation.
The concept of maximum entropy can be traced back along multiple threads to Biblical times.Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition.In this paper, we describe a method for statistical modeling based on maximum entropy.We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.
Currently, computational linguists and cognitive scientists working in the area of discourse and dialogue argue that their subjective judgments are reliable using several different statistics, none of which are easily interpretable or comparable to each other.Meanwhile, researchers in content analysis have already experienced the same difficulties and come up with a solution in the kappa statistic.We discuss what is wrong with reliability measures as they are currently used for discourse and dialogue work in computational linguistics and cognitive science, and argue that we would be better off as afield adopting techniques from content analysis.
This paper describes the reliability of a dialogue structure coding scheme based on utterance function, game structure, and higher-level transaction structure that has been applied to a corpus of spontaneous task-oriented spoken dialogues.
Text Tiling is a technique for subdividing texts into multi-paragraph units that represent passages, or subtopics.The discourse cues for identifying major subtopic shifts are patterns of lexical co-occurrence and distribution.The algorithm is fully implemented and is shown to produce segmentation that corresponds well to human judgments of the subtopic boundaries of 12 texts.Multi-paragraph subtopic segmentation should be useful for many text analysis tasks, including information retrieval and summarization.
The need to model the relation between discourse structure and linguistic features of utterances is almost universally acknowledged in the literature on discourse.However, there is only weak consensus on what the units of discourse structure are, or the criteria for recognizing and generating them.We present quantitative results of a two-part study using a corpus of spontaneous, narrative monologues.The first part of our paper presents a method for empirically validating multiutterance units referred to as discourse segments.We report highly significant results of segmentations performed by naive subjects, where a commonsense notion of speaker intention is the segmentation criterion.In the second part of our study, data abstracted from the subjects' segmentations serve as a target for evaluating two sets of algorithms that use utterance features to perform segmentation.On the first algorithm set, we evaluate and compare the correlation of discourse segmentation with three types of linguistic cues (referential noun phrases, cue words, and pauses).We then develop a second set using two methods: error analysis and machine learning.Testing the new algorithms on a new data set shows that when multiple sources of linguistic knowledge are used concurrently, algorithm performance improves.
Finite-state machines have been used in various domains of natural language processing.We consider here the use of a type of transducer that supports very efficient programs: sequential transducers.We recall classical theorems and give new ones characterizing sequential string-tostring transducers.Transducers that output weights also play an important role in language and speech processing.We give a specific study of string-to-weight transducers, including algorithms for determinizing and minimizing these transducers very efficiently, and characterizations of the transducers admitting determinization and the corresponding algorithms.Some applications of these algorithms in speech recognition are described and illustrated.
Technology introduce (1) a novel inversion transduction formalism bilingual modeling of sentence-pairs, and (2) the concept of parsing a variety of parallel corpus analysis applications.Aside from the bilingual orientation, three major features distinguish the formalism from the finite-state transducers more traditionally found in computational linguistics: it skips directly to a context-free rather than finite-state base, it permits a minimal extra degree of ordering flexibility, and its probabilistic formulation admits an efficient maximum-likelihood bilingual parsing algorithm.A convenient normal form is shown to exist.Analysis of the formalism's expressiveness suggests that it is particularly well suited to modeling ordering shifts between languages, balancing needed flexibility against complexity constraints.We discuss a number of examples of how stochastic inversion transduction grammars bring bilingual constraints to bear upon problematic corpus analysis tasks such as segmentation, bracketing, phrasal alignment, and parsing.
Words unknown to the lexicon present a substantial problem to NLP modules that rely on morphosyntactic information, such as part-of-speech taggers or syntactic parsers.In this paper we present a technique for fully automatic acquisition of rules that guess possible part-of-speech tags for unknown words using their starting and ending segments.The learning is performed from a general-purpose lexicon and word frequencies collected from a raw corpus.Three complimentary sets of word-guessing rules are statistically induced: prefix morphological rules, suffix morphological rules and ending-guessing rules.Using the proposed technique, unknown-word-guessing rule sets were induced and integrated into a stochastic tagger and a rule-based tagger, which were then applied to texts with unknown words.
Probabilistic analogues of regular and context-free grammars are well known in computational linguistics, and currently the subject of intensive research.To date, however, no satisfactory probabilistic analogue of attribute-value grammars has been proposed: previous attempts have failed to define an adequate parameter-estimation algorithm.In the present paper, I define stochastic attribute-value grammars and give an algorithm for computing the maximum-likelihood estimate of their parameters.The estimation algorithm is adapted from Della Pietra, Della Pietra, and Lafferty (1995).To estimate model parameters, it is necessary to compute the expectations of certain functions under random fields.In the application discussed by Della Pietra, Della Pietra, and Lafferty (representing English orthographic constraints), Gibbs sampling can be used to estimate the needed expectations.The fact that attribute-value grammars generate constrained languages makes Gibbs sampling inapplicable, but I show that sampling can be done using the more general Metropolis-Hastings algorithm.
Corpus-based approaches to word sense identification have flexibility and generality but suffer from a knowledge acquisition bottleneck.We show how knowledge-based techniques can be used to open the bottleneck by automatically locating training corpora.We describe a statistical classifier that combines topical context with local cues to identify a word sense.The classifier is used to disambiguate a noun, a verb, and an adjective.A knowledge base in the form of WordNet's lexical relations is used to automatically locate training examples in a general text corpus.Test results are compared with those from manually tagged training examples.
We present the results of a study of the use of definite descriptions in written texts aimed at assessing the feasibility of annotating corpora with information about definite description interpretation.We ran two experiments, in which subjects were asked to classify the uses of definite descriptions in a corpus of 33 newspaper articles, containing a total of 1,412 definite descriptions.We measured the agreement among annotators about the classes assigned to definite descriptions, as well as the agreement about the antecedent assigned to those definites that the annotators classified as being related to an antecedent in the text.The most interesting result of this study a corpus annotation perspective was the rather low agreement = 0.63) we obtained versions of Hawkins's and Prince's classification schemes; better results = 0.76) obtained using the simplified scheme proposed by Fraurud that includes only two classes, firstmention and subsequent-mention.The agreement about antecedents was also not complete.These findings raise questions concerning the strategy of evaluating systems for definite description inby comparing their results with a standardized annotation. a linguistic of view, the most interesting observations were the great number of discourse-new definites in our corpus (in one of our experiments, about 50% of the definites in the collection were classified as discourse-new, 30% as anaphoric, and 18% as associative/bridging) and the presence of definites that did not seem to require a complete disambiguation.
A new method for automatically acquiring case frame patterns from large corpora is proposed.In particular, the problem of generalizing values of a case frame slot for a verb is viewed as that of estimating a conditional probability distribution over a partition of words, and a new generalization method based on the Minimum Description Length (MDL) principle is proposed.In order to assist with efficiency, the proposed method makes use of an existing thesaurus and restricts its attention to those partitions that are present as &quot;cuts&quot; in the thesaurus tree, thus reducing the generalization problem to that of estimating a &quot;tree cut model&quot; of the thesaurus tree.An efficient algorithm is given, which provably obtains the optimal tree cut model for the given frequency data of a case slot, in the sense of MDL.Case frame patterns obtained by the method were used to resolve PP-attachment ambiguity.Experimental results indicate that the proposed method improves upon or is at least comparable with existing methods.
Best-first parsing methods for natural language try to parse efficiently by considering the most likely constituents first.Some figure of merit is needed by which to compare the likelihood of constituents, and the choice of this figure has a substantial impact on the efficiency of the parser.While several parsers described in the literature have used such techniques, there is little published data on their efficacy, much less attempts to judge their relative merits.We propose and evaluate several figures of merit for best-first parsing, and we identify an easily computable figure of merit that provides excellent performance on various measures and two different grammars.
We present a methodology for summarization of news about current events in the form of briefings that include appropriate background (historical) information.The system that we developed, SUMMONS, uses the output of systems developed for the DARPA Message Understanding Conferences to generate summaries of multiple documents on the same or related events, presenting similarities and differences, contradictions, and generalizations among sources of information.We describe the various components of the system, showing how information from multiple articles is combined, organized into a paragraph, and finally, realized as English sentences.A feature of our work is the extraction of descriptions of entities such as people and places for reuse to enhance a briefing.
The kinds of tree representations used in a treebank corpus can have a dramatic effect on performance of a parser based on the PCFG estimated from that corpus, causing the estimated likelihood of a tree to differ substantially from its frequency in the training corpus.This paper points out that the Penn II treebank representations are of the kind predicted to have such an effect, and describes a simple node relabeling transformation that improves a treebank PCFG-based parser's average precision and recall by around 8%, or approximately half of the performance difference between a simple PCFG model and the best broad-coverage parsers available today.This performance variation comes about because any PCFG, and hence the corpus of trees from which the PCFG is induced, embodies independence assumptions about the distribution of words and phrases.The particular independence assumptions implicit in a tree representation can be studied theoretically and investigated empirically by means of a tree transformation / detransformation process.
that are available in two languages becoming more and more plentiful, both in private data warehouses and on publicly accessible sites on the World Wide Web.As with other kinds of data, the value of bitexts largely depends on the efficacy of the available data mining tools.The first step in extracting useful information from bitexts is to find corresponding words and/or segment boundaries in their two halves maps).This article advances the state of the art of bitext mapping by formulating the problem in terms of pattern recognition.From this point of view, the success of a bitext mapping algorithm hinges on how well it performs three tasks: signal generation, noise filtering, and search.The Smooth Injective Map Recognizer (SIMR) algorithm presented here integrates innovative approaches to each of these tasks.Objective evaluation has shown that SIMR's accuracy is consistently high for language pairs as diverse as French/English and Korean/English.If necessary, SIMR's bitext maps can be efficiently converted into segment alignments using the Geometric Segment Alignment (GSA) algorithm, which is also presented here.SIMR has produced bitext maps for over 200 megabytes of French-English bitexts.GSA has converted these maps into alignments.Both the maps and the alignments are available from the
In this paper, we have proposed novel methods for robust parsing that integrate the flexibility of linguistically motivated lexical descriptions with the robustness of statistical techniques.Our thesis is that the computation of linguistic structure can be localized if lexical items are associated with rich descriptions (supertags) that impose complex constraints in a local context.The supertags are designed such that only those elements on which the lexical item imposes constraints appear within a given supertag.Further, each lexical item is associated with as many supertags as the number of different syntactic contexts in which the lexical item can appear.This makes the number of different descriptions for each lexical item much larger than when the descriptions are less complex, thus increasing the local ambiguity for a parser.But this local ambiguity can be resolved by using statistical distributions of supertag co-occurrences collected from a corpus of parses.We have explored these ideas in the context of the Lexicalized Tree-Adjoining Grammar (LTAG) framework.The supertags in LTAG combine both phrase structure information and dependency information in a single representation.Supertag disambiguation results in a representation that is effectively a parse (an almost parse), and the parser need &quot;only&quot; combine the individual supertags.This method of parsing can also be used to parse sentence fragments such as in spoken utterances where the disambiguated supertag sequence may not combine into a single structure.
Considering empirical evidence from a free-word-order language (German) we propose a revision of the principles guiding the ordering of discourse entities in the forward-looking center list within the centering model.We claim that grammatical role criteria should be replaced by criteria that reflect the functional information structure of the utterances.These new criteria are based on the distinction between hearer-old and hearer-new discourse entities.We demonstrate that such a functional model of centering can be successfully applied to the analysis of several forms of referential text phenomena, viz. pronominal, nominal, and functional anaphora.Our methodological and empirical claims are substantiated by two evaluation studies.In the first one, we compare success rates for the resolution of pronominal anaphora that result from a grammaticalrole-driven centering algorithm and from a functional centering algorithm.The second study deals with a new cost-based evaluation methodology for the assessment of centering data, one which can be directly derived from and justified by the cognitive load premises of the centering model.
We synthesize work on parsing algorithms, deductive parsing, and the theory of algebra applied to formal languages into a general system for describing parsers.Each parser performs abstract computations using the operations of a semiring.The system allows a single, simple representation to be used for describing parsers that compute recognition, derivation forests, Viterbi, n-best, inside values, and other values, simply by substituting the operations of different semirings.We also show how to use the same representation, interpreted differently, to compute outside values.The system can be used to describe a wide variety of parsers, including Earley's algorithm, tree adjoining grammar parsing, Graham Harrison Ruzzo parsing, and prefix value computation.
Statistical machine translation is a relatively new approach to the long-standing problem of translating human languages by computer.Current statistical techniques uncover translation rules from bilingual training texts and use those rules to translate new texts.The general architecture is the source-channel model: an English string is statistically generated (source), then statistically transformed into French (channel).In order to translate (or &quot;decode&quot;) a French string, we look for the most likely English source.We show that for the simplest form of statistical models, this problem is NP-complete, i.e., probably exponential in the length of the observed sentence.We trace this complexity to factors not present in other decoding problems.
This paper deals with the relationship between weblog content and time.With the proposed temporal mutual information, we analyze the collocations in time dimension, and the interesting collocations related to special events.The temporal mutual information is employed to observe the strength of term-to-term associations over time.An event detection algorithm identifies the collocations that may cause an event in a specific timestamp.An event summarization algorithm retrieves a set of collocations which describe an event.We compare our approach with the approach without considering the time interval.The experimental results demonstrate that the temporal collocations capture the real world semantics and real world events over time.1.2.
Transformation-based learning has been successfully employed to solve many natural language processing problems.It achieves state-of-the-art performance on many natural language processing tasks and does not overtrain easily.However, it does have a serious drawback: the training time is often intorelably long, especially on the large corpora which are often used in NLP.In this paper, we present a novel and realistic method for speeding up the training time of a transformation-based learner without sacrificing performance.The paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems: a standard transformation-based learner, and the ICA system (Hepple, 2000).The results of these experiments show that our system is able to achieve a significant improvement in training time while still achieving the same performance as a standard transformation-based learner.This is a valuable contribution to systems and algorithms which utilize transformation-based learning at any part of the execution.
AND (y is one of the hypernyms of x) AND AND (z is SYNONYM of y) AND AND (z is SYNONYM of anaphor) then Cast_in_Chain(Anaphor,antecedent) (Riloff and Jones 1999) note that the performance of the mutual bootstrapping algorithm can deteriorate rapidly if erroneous rules are entered.To make the algorithm more robust we use the same solution by introducing a second level of bootrapping. outer level, called most reliable based on semantic consistency and discard all the others before restarting the mutual bootstrapping loop again.In our experiments we have retained only those rules for which the new performance, given by the F-measure was larger than the median of the past four loops.The formula for the van Rijsbergen's F-measure combines precision the recall = 6 Evaluation To measure the performance of COCKTAIL we have trained the system on 30 MUC-6 and MUC-7 texts and tested it on the remaining 30 documents.computed the the Fperformance measures have been obtained automatically using the MUC-6 coreference scoring program (Vilain et al. 1995).Table 4 lists the results.Precision Recall F-measure rules 87.1% 61.7% 72.3% rules combined 91.3% 58.6% 71.8% +bootstrapping 92.0% 73.9% 81.9% Table 4: Bootstrapping effect on COCKTAIL Table 4 shows that the seed set of rules had good precision but poor recall.By combining the rules with the entropy-based measure, we obtained further enhancement in precision, but the recall dropped.The application of the bootstrapping methodology determined an enhancement of recall, and thus of the F-measure.In the future we intend to compare the overall effect of rules that recognize referential expressions on the overall performance of the system.7 Conclusion We have introduced a new data-driven method for corefresolution, implemented in the system.Unlike other knowledge-poor methods for corefresolution (Baldwin 1997) (Mitkov 1998), COCKits most performant rules through massive data, generated by its component.Furthermore, by using an entropy-based method we determine the best partition of corefering expressions chains. rules are learned by applying a bootstrapping methodology that uncovers additional semantic consistency data.References Breck Baldwin.1997.CogNIAC: high precision coreference with limited knowledge and linguistic resources.
We present a simple architecture for parsing transcribed speech in which an edited-word detector first removes such words from the sentence string, and then a standard statistical parser trained on transcribed speech parses the remaining words.The edit detector achieves a misclassification rate on edited words of 2.2%. which marks everything as not edited, has an error rate of 5.9%.)To evaluate our parsing results we introduce a new evaluation metric, the purpose of which is to make evaluation of a parse tree relatively indifferent the exact tree position of By this metric the parser achieves 85.3% precision and 86.5% recall.
6: Multipath Translation Induction language word), so the system's performance is lower than the Section 3 results.Since all available dictionaries are incomplete, it is difficult to decide which set of English words to compare against.Table 6 presents results for different choices of word coverage: the subset of existing pairs for English-Spanish, the union over all languages, and the intersection of all languages.Trends across subsets are relatively consistent.As an illustration, Table 7 shows consensus formation on English-Norweigian and English-Portuguese translation mappings via multiple bridge languages.Note that the English-French dictionary used here has no entry for &quot;bait&quot;, preventing its use as a bridge language for this word.As can be seen in Table 6, the distance-based combination methods are more successful at combining the different proposals than the rank-N combinations.One possible explanation for this is that rankbased classifiers pick the candidate with the best allaround distance, while distance-based combinations choose the single best candidate.Choosing the best all-around performer is detrimental when cognates exist for some languages but not for others.English Bridge language Bridge Word Target Word Score Rank bay (NORWEGIAN) Danish German Dutch bugt bucht baai bukt bukt baug (bow) bukt 1 1 1 1 25 2 1.5 2.5 distance-based method: bukt 1 1 rank-based method: bukt 27 1 (PORTUGUESE) bait Italian esca isca .5 1 nada (nothing) 3 54 Spanish carnada corneta (trumpet) 2 1 nada 3 12 isca 3.5 153 Romanian nada, nada (nothing) 0.5 1 isca 3.5 153 French N/A N/A N/A N/A distance-based method: isca 0.5 1 nada 0.5 2 rank-based method: nada 67 1 isca 307 20 Table 7: End-to-End Multipath Translation Induction The performance of an oracle, if allowed to choose the correct translation if it appears within the top-N in any language, would provide an upper bound for the performance of the combination methods.Results for such oracles are also reported in Table 6.The methods corresponding to &quot;oracle-1&quot; and &quot;distance&quot; are choosing from the same set of proposed targets, and the &quot;distance&quot; method achieves performance close to that of the oracle (77 vs. 82.8).6 Path Differences This section investigates the effect of different pathway configurations on the performance of the final multi-path system by examining the following situations: • English to Portuguese, using the other Romance languages as bridges.• English to Norwegian, using the Germanic languages as bridges.• English to Ukrainian, using the Slavic languages as bridges.• Portuguese to English, using the Germanic languages and French as bridges.The results of these experiments are shown in Taen=English, pt=Portuguese, fr=French, it=Italian, es=Spanish, ro=Romanian, du=Dutch, no=Norwegian, de=German, da=Danish, cz=Czech, uk=Ukrainian, po=Polish, sr=Serbian, ru=Russian The data sets used in these experiments were apthe same size as those used in the previous experiment 1100-1300 translation word Dictionaries for Russian and Ukrainian were converted into romanized pronunciation dictionaries.There are three observations which can be made from the multipath results.1.Adding more pathways usually results in an accuracy improvement.When there is a drop in accuracy on the cognate vocabulary by adding an additional bridge language there tends to be an improvement in accuracy on the full vocabulary due to significantly more cognate pathways (yielding greater coverage).2.It is difficult to substantially improve upon the performance of the single closest bridge language, especially when they are as close as enes-pt.Improvements on performance relative to the single best ranged from 2% to 20%.3.Several mediocre pathways can be combined to improve performance.Though it is always better to find one high-performing pathway, it is often possible to get good performance from the combination of several, less well-performing pathways (e.g. en-[sr po]-uk vs. en-ru-uk).In Table 8 &quot;Cvg&quot; or cognate coverage is the percentage words in the source language for which any of the bridge languages contains a cognate to the target translation.Italian and French bridges, for example, offer additional translation pathways to Portuguese which augment the Spanish pathways.Path Accuracy on Full Vocab Accuracy Cvg Cognate Vocab en-es-pt 58.7 86.7 65.5 en-it-pt 44.0 85.4 31.9 en-fr-pt 30.6 74.3 24.8 en-[fr it]-pt 41.2 79.4 42.2 en-[fr it es]-pt 60.2 84.2 70.3 en-da-no 71.9 92.4 75.4 en-du-no 36.1 76.7 39.8 en-de-no 36.1 74.7 38.9 en-[du de]-no 42.3 72.2 54.3 en-[da du de]-no 77.0 87.5 87.4 en-ru-uk 48.8 89.0 44.7 en-po-uk 38.1 87.8 31.9 en-sr-uk 31.9 86.7 30.8 en-[sr po]-uk 45.0 82.0 50.3 en-[ru sr po]-uk 58.4 74.6 71.0 pt-du-en 29.1 69.0 38.4 pt-fr-en 28.1 84.0 24.2 pt-de-en 25.3 68.4 32.1 pt-[de fr]-en 36.5 72.5 48.5 pt-[de fr du]-en 47.0 69.7 66.6 Table 8: Translation Accuracy via Different Bridge Language Paths (using L-A model) Using all languages together improves coverage, although this often does not improve performance over using the best single bridge language.As a final note, Table 9 shows the cross-language translation rates for some of the investigated languages.When translating from English to one of the Romance languages, using Spanish as the bridge language achieves the highest accuracy; and using Russian as the bridge language achieves the best performance when translating from English to the Slavic languages.However, note that using English alone without a bridge language when translating to the Romance languages still achieves reasonable performance, due to the substantial French and Latinate presence in English vocabulary.7 Related Work Probabilistic string edit distance learning techniques have been studied by Ristad and Yianilos (1998) for use in pronunciation modeling for speech recognition.Satta and Henderson (1997) propose a transformation learning method for generic string transduction.Brill and Moore (2000) propose an alternative string distance metric and learning algorithm.While early statistical machine translation models, such as Brown et al. (1993), did not use any cognate based information to seed their wordto-word translation probabilities, subsequent models (Chen, 1993 and Simard et al., 1992) incorporated some simple deterministic heuristics to increase the translation model probabilities for cognates.Other methods have been demonstrated for building bilingual dictionaries using simple heuristic rules includes Kirschner (1982) for English/Czech dictionaries and Chen (1998) for Chinese/English proper names.Tiedemann (1999) improves on these alignment seedings by learning all-or-nothing rules for detecting Swedish/English cognates.Hajie et al. (2000) has studied the exploitation of language similarity for use in machine translation in the case of the very closely related languages (Czech/Slovak).Covington (1998) uses an algorithm based on heuristic orthographic changes to find cognate words for purposes of historical comparison.Perhaps the most comprehensive study of word alignment via string transduction methods was pioneered by Knight and Graehl (1998).While restricted to single language transliteration, it very effectively used intermediary phonological models to bridge direct lexical borrowing across distant languages.8 Conclusion The experiments reported in this paper extend prior research in a number of directions.The novel probabilistic paradigm for inducing translation lexicons for words from unaligned word lists is introduced.The set of languages on which we demonstrate these methods is broader than previously examined.Finally, the use of multiple bridge languages and of the high degree of intra-family language similarity for dictionary induction is new.There are a number of open questions.The first is whether there exists a better string transformation algorithm to use in the induction step.One possible area of investigation is to use larger dictionaries and assess how much better stochastic transducers, and distance metrics derived from them, perform with more training data.Another option is to investigate the use of multi-vowel or multi-consonant compounds which better reflect the underlying phonetic units, using an more sophisticated edit distance measure.In this paper, we explore ways of using cognate pairs to create translation lexicons.It is an interesting research question as to whether we can augment these methods with translation probabilities estimated from statistical frequency information gleaned from loosely aligned or unaligned bilingual corpora for non-cognate pairs.Various machine learning techniques, including co-training and mutual bootstrapping, could employ these additional measures in creating better estimates.The techniques presented here are useful for language pairs where an on-line translation lexicon does not already exist, including the large majority of the world's lower-density languages.For language pairs with existing translation lexicons, these methods can help improve coverage, especially for technical vocabulary and other more recent borrowings which are often cognate but frequently missing from existing dictionaries.In both cases, the great potential of English -x Romance Accuracy on Cognate Vocab (35-68%) TL Bridge Language pt it es fr ro 0 pt (100) 85.6 86.7 74.3 72.1 79.4 it 83.7 (100) 85.1 75.5 82.1 78.0 es 85.8 84.0 (100) 78.1 82.1 79.3 fr 73.9 75.5 76.7 (100) 75.2 78.7 ro 72.8 84.4 82.8 76.1 (100) 78.3 av 78.2 82.0 82.2 75.7 77.7 78.4 English -x Romance Accuracy on Full Vocab TL Bridge Language pt it es fr ro 0 pt (100) 42.6 58.7 29.8 28.4 23.1 it 42.0 (100) 45.6 33.8 34.8 21.3 es 57.5 44.3 (100) 31.8 29.7 22.5 fr 30.7 35.2 32.7 (100) 33.3 24.9 ro 28.5 35.7 30.5 35.0 (100) 23.9 av 39.2 39.0 41.2 32.0 31.0 22.6 English -x Slavic Accuracy on Cognate Vocab TL Bridge Language cz ru pl sr uk 0 cz (100) 70.3 81.4 81.0 81.4 75.0 ru 72.7 (100) 84.1 80.3 87.3 73.9 pl 81.2 85.7 (100) 84.5 88.2 78.2 sr 85.7 82.9 85.8 (100) 85.5 76.7 uk 83.6 89.1 87.9 86.0 (100) 73.9 av 80.2 81.5 84.2 82.7 85.2 75 English -x Slavic Accuracy on Full Vocab TL Bridge Language cz ru pl sr uk 0 cz (100) 20.5 25.5 27.3 25.4 12.0 ru 23.3 (100) 29.9 27.3 47.1 13.4 pl 27.6 30.3 (100) 27.8 36.8 15.0 sr 31.0 29.6 29.4 (100) 33.1 18.5 uk 27.0 48.7 38.0 31.4 (100) 15.7 av 27 31.7 30.2 28 35.2 14.6 Table 9: Accuracy of English to TL (Target Language) via One Bridge Language (using L-A model) (0 = direct mapping no bridge) this work is the ability to leverage a single bilingual dictionary into translation lexicons for its entire language family, without any additional resources beyond raw wordlists for the other languages in the family.9 Acknowledgements The authors would like to thank the following people for their insightful comments and feedback on drafts of this work: Radu Florian, Jan Hajie, Ellen Riloff, Charles Schafer, and Richard Wicentowski.Thanks also to the Johns Hopkins NLP lab in general for the productive and stimulating environment.References E. Brill and R. Moore.2000.An improved errorfor noisy channel spelling correction.ACL, 286-293.P.F.Brown, S.A. Della Pietra, V.J.Della Pietra, and R. Mercer.1993.The mathematics of statistical translation.Linguistics, 19(2):263-311.Buck.1949.A of Selected Synonyms in the Principal Indo-European Languages.Chicago:University of Chicago Press.H-H. Chen, S-J.Huang, Y-W. Ding, and S-C. Tsai.1998.Proper name translation in cross-language retrieval. of ACL/COLING, pages 232-236.Chen.1993.Aligning sentences in bilingual corusing lexical information. of ACL, pages 9-16.M. Covington.1998.Aligning multiple languages historical comparison. of COLING- 275-280.J. Hajie, J. Hric, and V. Kubori.2000.Cesilko : Machine translation between closely related lanof ANLP, 7-12.Jelinek.1997.Methods for Speech Press.Z. Kirshner.1982.A dependency based analysis of english for the purpose of machine translation.Explizite Beschreibung der Sprache und automa- Textbearbeitung, Knight and J. Graehl.1998.Machine transliter- Linguistics, E. Ristad and P. Yianilos.1998.Learning string distance.Trans.PAMI, G. Satta and J. Henderson.1997.String transforlearning. of ACL/EACL, 444- 451.M. Simard, G.F. Foster, and P. Isabelle.1992.Using cognates to align sentences in bilingual corpora.
In human sentence processing, cognitive load can be defined many ways.This report considers a definition of cognitive load in terms of the total probability of structural options that have been disconfirmed at point in a sentence: the surprisal of word its prefix on a phrase-structural language model.These loads can be efficiently calculated using a probabilistic Earley parser (Stolcke, 1995) which is interpreted as generating predictions about reading time on a word-by-word basis.Under grammatical assumptions supported by corpusfrequency data, the operation of Stolcke’s probabilistic Earley parser correctly predicts processing phenomena associated with garden path structural ambiguity and with the subject/object relative asymmetry.
We propose a novel Co-Training method for statistical parsing.The algorithm takes as input a small corpus (9695 sentences) annotated with parse trees, a dictionary of possible lexicalized structures for each word in the training set and a large pool of unlabeled text.The algorithm iteratively labels the entire data set with parse trees.Using empirical results based on parsing the Wall Street Journal corpus we show that training a statistical parser on the combined labeled and unlabeled data strongly outperforms training only on the labeled data.
We propose an algorithm to automatically induce the morphology of inflectional languages using only text corpora and no human input.Our algorithm combines cues from orthography, semantics, and syntactic distributions to induce morphological relationships in German, Dutch, and English.Using CELEX as a gold standard for evaluation, we show our algorithm to be an improvement over any knowledge-free algorithm yet proposed.
We apply Support Vector Machines (SVMs) to identify English base phrases (chunks).SVMs are known to achieve high generalization performance even with input data of high dimensional feature spaces.Furthermore, by the Kernel principle, SVMs can carry out training with smaller computational overhead independent of their dimensionality.We apply weighted voting of 8 SVMsbased systems trained with distinct chunk representations.Experimental results show that our approach achieves higher accuracy than previous approaches.
E. Brill.1995.Transformation-based error-driven learning and natural language processing: A case study in part of tagging.Linguistics,
We address the text-to-text generation problem of sentence-level paraphrasing — a phenomenon distinct from and more difficult than wordor phrase-level paraphrasing.Our apapplies alignment sentences gathered from unannotated comparable corpora: it learns a set of paraphrasing patrepresented by lattice and automatically determines how to apply these patterns to rewrite new sentences.The results of our evaluation experiments show that the system derives accurate paraphrases, outperforming baseline systems.
We present a neural network method for inducing representations of parse histories and using these history representations to estimate the probabilities needed by a statistical left-corner parser.The resulting statistical parser achieves performance (89.1% F-measure) on the Penn Treebank which is only 0.6% below the best current parser for this task, despite using a smaller vocabulary size and less prior linguistic knowledge.Crucial to this success is the use of structurally determined soft biases in inducing the representation of the parse history, and no use of hard independence assumptions.
We present an extension of the classic A* search procedure to tabular PCFG parsing.The use of A* search can dramatically reduce the time required to find a best parse by conservatively estimating the probabilities of parse completions.We discuss various estimates and give efficient algorithms for computing them.On average-length Penn treebank sentences, our most detailed estimate reduces the total number of edges processed to less than 3% of that required by exhaustive parsing, and a simpler estimate, which requires less than a minute of precomputation, reduces the work to less than 5%.Unlike best-first and finite-beam methods for achieving this kind of speed-up, an A* method is guaranteed to find the most likely parse, not just an approximation.Our parser, which is simpler to implement than an upward-propagating best-first parser, is correct for a wide range of parser control strategies and maintains worst-case cubic time.
We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models.Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models.Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations.Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance.Learning only syntactically motivated phrases degrades the performance of our systems.
Following the recent adoption by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, we conduct an in-depth study of a similar idea for evaluating summaries.The results show that automatic evaluation using unigram cooccurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct application of the BLEU evaluation procedure does not always give good results.
translational equivalence between their components. link generated by a D-MTG has components.Some (but not all) components of a link may be empty.An empty component indicates that an expression vanishes in translation.To express empty we add a special terminal to special nonterminal MTG applications, the different components of a link will typically come largely disjoint subsets of vocabularies or sets of grammatical categories from different languages.Each MTG also has a set of production rules (or just &quot;productions&quot; for short), which fall into one of two categories.'YIELD productions have the form t X is a link of and t is a link of is empty if and only if empty, < D. productions have the form X=NPM (2) where M is a non-empty vector of nonterminal links, is a non-empty vector of and is a rendering function, explained The rank of an MTG production is the number of nonterminal links on its RHS.The rank of an MTG is the maximum rank of its production rules. is the class of MTGs of rank Each row of P and M corresponds to a different component of multitext.Each permutation is written as a row in P, and each link is written as a column M, as in Equation 3 below.If empty, then the dth component of every link in M must be empty If not empty, then at least one of the links in M must have a non-empty dth component.The position of a non-empty terminal or nonterminal relative to other non-empty elements of its component is its role.If there are m non-empty nonterminals component (row) M then a permutation roles from 1 to Pd empty if and only if is empty.The D-MTG derivation process begins with the link $, which is a vector of of the start symbol $ derivation continues with nondeterministic application of production rules.The semantics of = are the usual semantics of rewriting systems, i.e., that the expression on the LHS can be rewritten as the expression on the RHS.Following convention, we let be the reflexive transitive closure of dichotomy imposes a convenient normal form, without loss of generality. rendering function is a notational convenience; MTGs can be defined without it.When no more productions can be applied, i.e., when all nonterminals have been rewritten into terminals, the rendering functions are evaluated in order.The rearranges the nonempty terminals in each row of a link vector according to that row's permutation.For example, c [1,3,2,4] =wxyz t Env vut By reordering the terminals independently in each component, the join operator hides information about which terminals were derived from the same link.Thus, the translational equivalence represented by links is not observable in MTG yields, just as it is not observable in raw multitext.To avoid spurious ambiguity, we stipulate a normal form for components of P: In each permutation, first appearance of role precede the first of role y for all y, except where the arrangement is incompatible with a preceding permutation in P. We could, for example, obtain the result above if we put put and switch their roles in the 2nd and 3rd permutations.However, the normal form requires the 2nd to be [1, 3, 2, 4], not [4, 3, 2, 1], so must be listed last.Let Q be an MTG derivation where no more production rules can be applied.Let Render(Q) be the result of evaluating all the N's in Q.The (formal) an MTG the set of multithat can be generated by applying zto the link of then evaluating all the joins.I.e., = : $ z Due to the importance of lexical information in disambiguating syntactic structure, we shall pay special attention to lexicalized MTGs (LMTGs) of the variety A bilexical MTG has a set A of &quot;delexicalized&quot; nonterminal labels.Intuitively, A corresponds to the nonterminal set of an CFG.Then, every nonterminal in form some terminal some label The terminal the lexical head of its constituent, or just the head.One link on the RHS each production serves as the heir of the link on the LHS.Each component of the heir link inherits the lexical head of its parent nonterminal.An of a derivation is in Figure 1. nonterminal always lexicalized with the ternonterminals may also be lexicalized represent empty categories.The special start nonterminal $ is lexicalized with the special start terminal S. Following Eisner ..4z Satta (1999), we can then that the language of interest is actually : Q'$ E (3) ix[1, 2] (S[fed] $[S] 0.1[1,2] [1, 2, 3] ( NP[cat] [1, 2] S[kormil] [1, 2] [1,3, 2] NP[kota]) S[S]) 0.1[1, 2] [1, 0.1[1,2] N[cat] 2] [1, 3, 2] Pro[ya] V[kormil] [1] ) S[S]) ix[1, 2] [1, 2, 3] ( I fed ) (the cat )) $) 0.1[1, 2] ( (I fed the cat) $) I fed the cat $ 2] [1, 3, 2] ya kormil [1] $ kota kormil) s) kota kormil Figure 1: A 2-L2MTG derivation in English and transliterated Russian: (4-5) DEPEND productions; (6) YIELD productions, followed by rendering. and superclasses of MTG have been studied before.The non-lexicalized 2-MTG(2) is equivalent to ITG (Wu, 1997).Alal.&quot;collections of finite-state head transducers&quot; can be viewed as a subclass of 2-LMTG where, among other restrictions, A contains only one (dummy) nonterminal label.&quot;Syntax-directed transof order 1969) are equivalent to k-MTG(2).On the other hand, MTG is a subof Multiple CFG (Seki al., where the functions that render the RHS of production rules may not mix symbols from different components.3 Synchronous Parsers Inference of synchronous structures requires a synparser.A parser an algorithm that can infer the syntactic structure of each component text in a multitext and simultaneously infer the correspondence relation between these struc- To facilitate complexity analysis (below), we specify our parsers using declarative inference rules.'&quot;X :- Y, Z&quot; means that X can be inferred from Y and Z. V means the same thing.An item that appears in an inference rule stands for the proposition that the item is in the parse chart.A production rule that appears in an inference rule stands for the proposition that the production is in the grammar.Such specifications are nondeterministic: they do not indicate the order in which a parser should attempt inferences.A deterministic parsing strategy can always be chosen later, to suit the application.Any reasonable parsing strategy will have the same asymptotic complexity (McAllester, 2002).3.1 Naive Synchronous Bilexical Parsers For expository purposes, we begin with Parser R2D2A, which is a naive CKY-style parser for The chart of suitable set of monolingual parsers can also infer the syntactic structure of each component, but cannot infer the correspondence relation between these structures. use both Horn clauses and sequents to save space.R2D2A can be compared to Wu (1997)'s procedure for parsing non-lexicalized ITGs, which runs in —1 22 j1 j1 j2 X7.[hl] X2 [h2] X2 [h2]/Z2 [h2] i2 —1 iz hook seeds hedge Figure 2: Items used by our parsers for 2-L2MTG(2).R2D2A is with &quot;seed&quot; items, illustrated in Figure 2.A one-dimensional seed is put in the chart for every word in every component of the input.After initialization, the parser can translational equivalence between seeds components by firing Y inference rules: —1 —1 hi i2 inference rules infer rules.Each two-dimensional instantiation expresses the equivalence of two word tokens, at positions and in their respective components.One-dimensional Y inferences assert that a word vanishes in translation.E.g.: i2 6[6] i2 i2 X2 [h2] ' 21_2 [(12] 122 „ L R2D2A most of its time compospairs of non-seed items into larger A bottom-up one-dimensional parser composes onedimensional items until it infers an item that covers the input text.A bottom-up synchronous parser composes multi-dimensional items until it infers an item that covers the multitext space spanned by the input multitext.The items composed by synchronous parsers are called short.The 2D hedges composed by Parser R2D2A are shown in Figure 2.The particular hedge in the figure represents a constituent beword boundaries ji of the first compo- As Eisner & Satta (1999) have shown, yields of bilexical grammars are generally more expensive to parse than their nonlexicalized counterparts. term to any partial parse.[pi.][p2] assert in different X1 [hl] X2 [h2] i2 —1 h2 X2 [h2] h2 7 22 1h Z h 2 2 Z h 1 1 Z h 2 2 1 Y g 1 Y g 1 1 2g 2g Y g 1 1 Z h 1 1 Z h 2 2 Z h 1 1 Z h 2 2 2g Y g 1 1 2g I go there quite often 3 2 4 1 I J’ often quite go there y vais 1 2 3 souvent y vais 2 1 souvent 3 4 Pat went home early Pat went home early ghar Pat−nay ghar Pat−nay juldee gayee juldee gayee a gift for you from France a gift from France for you un cadeau de France pour vous un cadeau de France pour vous rank with distinguished heir without 2 2 2 3 3 2 4 or 5 3 3 6 4 3 7 to 9 4 4 Table 1: Highest possible cardinality of minimizing decompositions over all 2D productions of the given rank.Figures 6 and 5 exemplify highest-cardinality productions of ranks 3 and 4, respectively.As we shall see in Section 5, however, bad binarization can worsen recognition complexity.The Binarization Rules apply deterministically,' but there are multiple ways to decompose the RHS of a non- DEPEND production into nested Some decompositions may give rise to more discontinuities than others.Let the cardinality of an RTV be the total number of partitions in all its components, and let the cardinality of a decomposithe cardinality of the RTVs that it contains.A minimizing decomposition for a given production is one of those with lowest cardi- Then, the cardinality of a production cardinality of its minimizing decomposition. cardinality of a production is bounded by its rank, as Table 1 shows for the 2D case.Finally, the caran MTG the maximum of the cardinalities of its productions.5 Inference of Discontinuous Constituents Parser A is a parser for arbitrary MTGs.It initializes its chart and fires Y inferences just like Parser R2A.It then composes pairs of items into larger items using inference rule A.0 (see below).Just like items in ordinary parsers, Parser A items need to know their positions in the input multitext, but not their internal structure.However, items with discontinuities need to remember all their boundaries, not just the outermost ones.Expanding on Johnson (1985), we define a discontinuous span (or dshort) as a of zero or more intervals = .; where the left boundaries and the are right boundaries between word positions in a text, so /, for 1 • < for 1 < < m which means that the intervals do not overlap. predefined equivalence classes for new nonterminals.&quot;For correct binarization of productions with a distinguished heir, the decomposition must put the heir in the most deeply nested DLV.This requirement tends to increase the cardinality of L2MTGs, as shown in Table 1.In addition, we say that a d-span is in normal form all the inequalities between and are strict, i.e. there is a gap between each pair of consecutive Now, a hedge item Parser A is d-link with a vector of d-spans normal form.The cardinality of an item is the total number of intervals in its d-span vector.Binarized MTG productions can be inferred under generalizations of the ID and LP constraints described in Section 3.We use two helper functions to express these constraints.+ is the concatenation operator for d-spans: Given two d-spans, it outthe union of their intervals in normal The 0 function computes the role template that describes the relative positions of the intervals in two E.g., if v = (1, 3; 8, 9) and = then + = (1, 3; 7, 9) and v 0 = [1], [2, 1].Both operators apply componentwise to vectors of d-spans.With their help, we state the composition inference rule of Parser A: Y(v),X =N (Y,Z) + The space complexity of Parser A is a function of the maximum number of boundaries stored in its signatures, and the number INIof nonterminals in the grammar.The maximum number of required boundaries is exactly twice the cardinality of the MTG, and each of the boundaries can range over positions.Thus, the space complexity Parser A for an MTG in 0(1Ni (G) is bilexical, then the number of possible nonterminals a factor of , the space complexity of A to +2C (G)) The time complexity of Parser A depends on how many boundaries are shared between antecedent items in A.0 rules.In the best case, all the boundaries are shared except the two outermost boundaries in each dimension, and the inferred item is contiguous.In the worst case, no boundaries are shared, and the inferred item stores all the boundaries of antecedent items.In any case, if cardinalities of the composed items, and the cardinality of the inferred item, then the number of boundaries in an A.0 inference is + z.Thus, in the worst case, the number of free boundinvolved in an A.0 inference is beeach boundary can range over values, where n is the length of the longest component of the input multitext.We still have 3 nonterminal labels per dimension per inference.Also, each inference now needs to compute an RTV at a cost inputs of ± must have no overlapping intervals, or else the output is undefined.A.C: (C (G)). the time complexity of Parser A in (C (G) For a binarized which also needs to keep track of two lexical heads per dimension per inference, this complexity rises to (G)13D Parser B is a generalization of Parser R2B for biof arbitrary rank.It decomposes inference rule A.0 into ID and LP subrules, using generalized hooks that carry an RTV.The decomposition can happen in one of two ways, depending on the heir's role (1 or 2) in the DLV.X[h] Y[g]) Z [h] 0 \Z [h] + X[h] Z [II]) 0 X[h](v)[v + The rules in Section 3.2 are simple examples of B.ID1 and B.LP1.Parser B is faster than Parser A, but takes more space.The hooks of Parser B must keep track of one more nonterminal label per dimension than hedges.The size of an RTV is bounded by the cardinality of the grammar.Thus, the space complexity of B is in (C (G)12D ) On the other hand, The B.ID rules involve only one d-span instead of two, reducing the number of free variables by (C (G)) .B.LP rules again involve only one lexical head instead of two, reducing the number of free by a factor of < turns out that the worst-case running time of Parser B is than that of Parser A by a factor of under of any rank and dimensionality.6 Conclusion We have proposed Multitext Grammars (MTGs) as a convenient and relatively expressive foundation for building practical models of translational equivalence.To encourage their use for this purpose, we have explored algorithms for parsing bilexical MTGs of arbitrary rank and dimensionality.Our exploration highlighted some little-known properties of synchronous parsing: (1) some optimizations of monolingual parsers generalize to the synchronous case, but others do not; (2) discontinuous constituents are essential for parsing bitexts even in similar Western languages; (3) different binarization schemes lead to different time and space complexity.There are many aspects of translational equivalence that MTG cannot express, such as some of those described by Dorr (1994).In future work, we hope to extend the formalism to cover some of the aspects that would not raise the computational complexity of its recognition, such as discontinuous and/or phrasal terminals.Concurrently, we shall explore the empirical properties of MTG, by inducing stochastic MTGs from real multitexts.Acknowledgments Thanks to Jason Eisner, Sanjeev Khudanpur, Owen Rambow, Giorgio Satta, and members of NYU's Proteus Project for helpful discussions.The idea of treating binarization as an optimization problem is due to Wei Wang.Dan Klein proposed the term &quot;hook.&quot; This research was supported by the DARPA TIDES program, by an NSF CAREER award, and by a gift from Sun Microsystems.
Recent TREC results have demonstrated the need for deeper text understanding methods.This paper introduces the idea of automated reasoning applied to question answering and shows the feasibility of integrating a logic prover into a Question Answering system.The approach is to transform questions and answer passages into logic representations.World knowledge axioms as well as linguistic axioms are supplied to the prover which renders a deep understanding of the relationship between question text and answer text.Moreover, the trace of the proofs provide answer justifications.The results show that the prover boosts the performance of the QA system on TREC questions by 30%.
We describe a syntax-based algorithm that automatically builds Finite State Automata (word lattices) from semantically equivalent translation sets.These FSAs are good representations of paraphrases.They can be used to extract lexical and syntactic paraphrase pairs and to generate new, unseen sentences that express the same meaning as the sentences in the input sets.Our FSAs can also predict the correctness of alternative semantic renderings, which may be used to evaluate the quality of translations.
We present an application of ambiguity packing and stochastic disambiguation techniques for Lexical-Functional Grammars (LFG) to the domain of sentence condensation.Our system incorporates a linguistic parser/generator for LFG, a transfer component for parse reduction operating on packed parse forests, and a maximum-entropy model for stochastic output selection.Furthermore, we propose the use of standard parser evaluation methods for automatically evaluating the summarization quality of sentence condensation systems.An experimental evaluation of summarization quality shows a close correlation between the automatic parse-based evaluation and a manual evaluation of generated strings.Overall summarization quality of the proposed system is state-of-the-art, with guaranteed grammaticality of the system output due to the use of a constraint-based parser/generator.
Conditional random fields for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position.Among sequence labeling tasks in language processing, shallow parsing has received much attention, with the development of standard evaluation datasets and extensive comparison among methods.We show here how to train a conditional random field to achieve performance as good as any reported base noun-phrase chunking method on the CoNLL task, and better than any reported single model.Improved training methods based on modern optimization algorithms were critical in achieving these results.We present extensive comparisons between models and training methods that confirm and strengthen previous results on shallow parsing and training methods for maximum-entropy models.
We introduce two probabilistic models that can be used to identify elementary discourse units and build sentence-level discourse parse trees.The models use syntactic and lexical features.A discourse parsing algorithm that implements these models derives discourse parse trees with an error reduction of 18.8% over a state-ofthe-art decision-based discourse parser.A set of empirical evaluations shows that our discourse parsing model is sophisticated enough to yield discourse trees at an accuracy level that matches near-human levels of performance.
We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) fine-grained modeling of unknown word features.Using these ideas together, the resulting tagger gives a 97.24% accuracy on the Penn Treebank WSJ, an error reduction of 4.4% on the best previous single automatically learned tagging result.
We introduce factored language models (FLMs) and generalized parallel backoff (GPB).An FLM represents words as bundles of features (e.g., morphological classes, stems, data-driven clusters, etc.), and induces a probability model covering sequences of bundles rather than just words.GPB extends standard backoff to general conditional probability tables where variables might be heterogeneous types, where no obvious natural (temporal) backoff order exists, and where multiple dynamic backoff strategies are allowed.These methodologies were implemented during the JHU 2002 workshop as extensions to the SRI language modeling toolkit.This paper provides initial perplexity results on both CallHome Arabic and on Penn Treebank Wall Street Journal articles.Significantly, FLMs with GPB can produce bigrams with significantly lower perplexity, sometimes lower than highly-optimized baseline trigrams.In a multi-pass speech recognition context, where bigrams are used to create first-pass bigram lattices or N-best lists, these results are highly relevant.
Entity detection and tracking is a relatively new addition to the repertoire of natural language tasks.In this paper, we present a statistical language-independent framework for identifying and tracking named, nominal and pronominal references to entities within unrestricted text documents, and chaining them into clusters corresponding to each logical entity present in the text.Both the mention detection model and the novel entity tracking model can use arbitrary feature types, being able to integrate a wide array of lexical, syntactic and semantic features.In addition, the mention detection model crucially uses feature streams derived from different named entity classifiers.The proposed framework is evaluated with several experiments run in Arabic, Chinese and English texts; a system based on the approach described here and submitted to the latest Automatic Content Extraction (ACE) evaluation achieved top-tier results in all three evaluation languages.
This paper reports some experiments that compare the accuracy and performance of two stochastic parsing systems.The currently popular Collins parser is a shallow parser whose output contains more detailed semanticallyrelevant information than other such parsers.The XLE parser is a deep-parsing system that couples a Lexical Functional Grammar to a loglinear disambiguation component and provides much richer representations theory.We measured the accuracy of both systems against a gold standard of the PARC 700 dependency bank, and also measured their processing times.We found the deep-parsing system to be more accurate than the Collins parser with only a reduction in parsing
Many probabilistic models for natural language are now written in terms of hierarchical tree structure.Tree-based modeling still lacks many of the standard tools taken for granted in (finitestate) string-based modeling.The theory of tree transducer automata provides a possible framework to draw on, as it has been worked out in an extensive literature.We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-totree and tree-to-string transducers.
We consider the problem of modeling the content structure of texts within a specific do main, in terms of the topics the texts address and the order in which these topics appear.We first present an effective knowledge-leanmethod for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models.We then apply our method to two complementary tasks: information ordering and ex tractive summarization.Our experiments showthat incorporating content models in these ap plications yields substantial improvement over previously-proposed methods.
Previous work demonstrated that web counts can be used to approximate bigram frequencies, and thus should be useful for a wide variety of NLP tasks.So far, only two generation tasks (candidate selection for machine translation and confusion-set disambiguation) have been tested using web-scale data sets.The present paper investigates if these results generalize to tasks covering both syntax and semantics, both generation and analysis, and a larger of For the majority of tasks, we find that simple, unsupervised models perform when frequencies are obtained from the web rather than from a large corpus.However, in most cases, web-based models fail to outperform more sophisticated state-of-theart models trained on small corpora.We argue that web-based models should therefore be used as a baseline for, rather than an alternative to, standard models.
We present an empirically grounded method for evaluating content selection in summarization.It incorporates the idea that no single best model summary for a collection of documents exists.Our method quantifies the relative importance of facts to be conveyed.We argue that it is reliable, predictive and diagnostic, thus improves considerably over the shortcomings of the human evaluation method currently used in the Document Understanding Conference.
We describe a methodology for rapid experimentation in statistical machine translation which we use to add a large number of features to a baseline system exploiting features from a wide range of levels of syntactic representation.Feature values were combined in a log-linear model to select the highest scoring candidate from an list.Feature weights were optimized directly against the BLEU evaluation metric on held-out data.We present results for a small selection of features at each level of syntactic representation.
We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation.This statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance.We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings, word-to-word alignments from an MT system, and syntactic structure from parse-trees of source and target language sentences.We report the performance of the MBR decoders on a Chinese-to-English translation task.Our results show that MBR decoding can be used to tune statistical MT performance for specific loss functions.
This paper describes the application of discriminative reranking techniques to the problem of machine translation.For each sentence in the source language, we obtain from a baseline statistical machine translation system, a ranked best list of candidate translations in the target language.We introduce two novel perceptroninspired reranking algorithms that improve on the quality of machine translation over the baseline system based on evaluation using the BLEU metric.We provide experimental results on the NIST 2003 Chinese-English large data track evaluation.We also provide theoretical analysis of our algorithms and experiments that verify that our algorithms provide state-of-theart performance in machine translation.
We demonstrate a new research approach to the problem of predicting the reading difficulty of a text passage, by recasting readability in terms of statistical language modeling.We derive a measure based on an extension of multinomial naïve Bayes classification that combines multiple language models to estimate the most likely grade level for a given passage.The resulting classifier is not specific to any particular subject and can be trained with relatively little labeled data.We perform predictions for individual Web pages in English and compare our performance to widely-used semantic variables from traditional readability measures.We show that with minimal changes, the classifier may be retrained for use with French Web documents.For both English and French, the classifier maintains consistently good correlation with labeled grade level (0.63 to 0.79) across all test sets.Some traditional semantic variables such as type-token ratio gave the best performance on commercial calibrated test passages, while our language modeling approach gave better accuracy for Web documents and very short passages (less than 10 words).
In this paper, we propose a machine learning algorithm for shallow semantic parsing, extend ing the work of Gildea and Jurafsky (2002),Surdeanu et al (2003) and others.Our al gorithm is based on Support Vector Machineswhich we show give an improvement in performance over earlier classifiers.We show perfor mance improvements through a number of newfeatures and measure their ability to general ize to a new test set drawn from the AQUAINT corpus.
In statistical machine translation, the currently best performing systems are based in some way on phrases or word groups.We describe the baseline phrase-based translation system and various refinements.We describe a highly efficient monotone search algorithm with a complexity linear in the input sentence length.We present translation results for three tasks: Verbmobil, Xerox and the Canadian Hansards.For the Xerox task, it takes less than 7 seconds to translate the whole test set consisting of more than 10K words.The translation results for the Xerox and Canadian Hansards task are very promising.The system even outperforms the alignment template system.
We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora.We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data.
Systems that automatically discover semantic classes have emerged in part to address the limitations of broad-coverage lexical resources such as WordNet and Cyc.The current state of the art discovers many semantic classes but fails to label their concepts.We propose an algorithm labeling semantic and for leveraging them to extract relationships using a top-down approach.
With the increasing use of research paper search engines, such as CiteSeer, for both literature search and hiring decisions, the accuracy of such systems is of paramount importance.This paper employs Conditional Random Fields (CRFs) for the task of extracting various common fields from the headers and citation of research papers.The basic theory of CRFs is becoming well-understood, but best-practices for applying them to real-world data requires additional exploration.This paper makes an empirical exploration of several factors, including variations on Gaussian, expoand priors for improved regularization, and several classes of features and Markov order.On a standard benchmark data set, we achieve new state-of-the-art performance, reducing error in average F1 by 36%, and word error rate by 78% in comparison with the previous best SVM results.Accuracy compares even more favorably against HMMs.
We present a technique for augmenting annotated training data with hierarchical word clusters that are automatically derived from a large unannotated corpus. membership is encoded in features that are incorporated in a discriminatively trained tagging model.Active learning is used to select training examples.We evaluate the technique for named-entity tagging.Compared with a state-of-the-art HMM-based name finder, the presented technique requires only 13% as much annotated data to achieve the same level of performance.Given a large annotated training set of 1,000,000 words, the technique achieves a 25% reduction in error over the state-of-the-art HMM trained on the same material.
WordNet::Similarity is a freely available software package that makes it possible to measure the semantic similarity and relatedness between a pair of concepts (or synsets).It provides six measures of similarity, and three measures of relatedness, all of which are based on the lexical database WordNet.These measures are implemented as Perl modules which take as input two concepts, and return a numeric value that represents the degree to which they are similar or related.
We present a novel morphological analysis technique which induces a morphological and syntactic symmetry between two languages with highly asymmetrical morphological structures to improve statistical machine translation qualities.The technique pre-supposes fine-grained segmentation of a word in the morphologically rich language into sequence of and part-of-speech tagging of the parallel corpus.The algorithm to be the morphologically rich language to induce the desired morphological and syntactic symmetry.The technique improves Arabic-to-English translation qualities significantly when applied to IBM Model 1 and Phrase Translation Models trained on the training corpus size ranging from 3,500 to 3.3 million sentence pairs.
Lebanese violate warplanes Israeli airspace A l T A } r A t A l H r b y P y l y P A l A s r A } t n t h k A l m j A l A l j w y A l l b n A n y Abstract In this paper, we present a unigram segmentation model for statistical machine translation where the segmentation units are blocks: pairs of phrases without internal structure.The segmentation model uses a novel orientation component to handle swapping of neighbor blocks.During training, we collect block uncounts with we count how often a block occurs to the left or to the right of some predecessor block.The orientation model is shown to improve translation performance over two models: 1) no block re-ordering is used, and 2) the block swapping is controlled only by a language model.We show experimental results on a standard Arabic-English translation task.
Parallel corpora are crucial for training SMT systems.However, for many language pairs they are available only in very limited quantities.For these language pairs a huge portion of phrases encountered at run-time will be unknown.We show how techniques from paraphrasing can be used to deal with these otherwise unknown source language phrases.Our results show that augmenting a stateof-the-art SMT system with paraphrases leads to significantly improved coverage and translation quality.For a training corpus with 10,000 sentence pairs we increase the coverage of unique test set unigrams from 48% to 90%, with more than half of the newly covered items accurately translated, as opposed to none in current approaches.
This paper advocates a new architecture for textual inference in which finding a good alignment is separated from evaluating entailment.Current approaches to semantic inference in question answering and textual entailment have approximated the entailment problem as that of computing the best alignment of the hypothesis to the text, using a locally decomposable matching score.We argue that there are significant weaknesses in this approach, including flawed assumptions of monotonicity and locality.Instead we propose a pipelined approach where alignment is followed by a classification step, in which we extract features representing high-level characteristics of the entailment problem, and pass the resulting feature vector to a statistical classifier trained on development data.We report results on data from the 2005 Pascal RTE Challenge which surpass previously reported results for alignment-based systems.
Named Entity recognition (NER) is an important part of many natural language processing tasks.Most current approaches employ machine learning techniques and require supervised data.However, many languages lack such resources.This paper presents an algorithm to automatically discover Named Entities (NEs) in a resource free language, given a bilingual corpora in which it is weakly temporally aligned with a resource rich language.We observe that NEs have similar time distributions across such corpora, and that they are often transliterated, and develop an algorithm that exploits both iteratively.The algorithm makes use of a new, frequency based, metric for time distributions and a resource free discriminative approach to transliteration.We evaluate the algorithm on an English-Russian corpus, and show high level of NEs discovery in Russian.
We present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models.Compared to the standard practice of intersecting predictions of independently-trained models, joint training provides a 32% reduction in AER.Moreover, a simple and efficient pair of HMM aligners provides a 29% reduction in AER over symmetrized IBM model 4 predictions.
We present a simple, but surprisingly effective, method of self-training a twophase parser-reranker system using readily available unlabeled data.We show that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker. improved model achieves an of 92.1%, an absolute 1.1% improvement (12% error reduction) over the previous best result for Wall Street Journal parsing.Finally, we provide some analysis to better understand the phenomenon.
In this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources.These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels.We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns.
Systems based on synchronous grammars and tree transducers promise to improve the quality of statistical machine translation output, but are often very computationally intensive.The complexity is exponential in the size of individual grammar rules due to arbitrary re-orderings between the two languages, and rules extracted from parallel corpora can be quite large.We devise a linear-time algorithm for factoring syntactic re-orderings by binarizing synchronous rules when possible and show that the resulting rule set significantly improves the speed and accuracy of a state-of-the-art syntax-based machine translation system.
surface text patterns for a question answering system. of the 40th Annual Meeting of the As
investigate for primarily unsupervised sequence modeling.Prior knowledge is specified declaratively, by providing a few canonical examples of each target annotation label.This sparse prototype information is then propagated across a corpus using distributional similarity features in a log-linear generative model.On part-of-speech induction in English and Chinese, as well as an information extraction task, prototype features provide substantial error rate reductions over competitive baselines and outperform previous work.For example, we can achieve an English part-of-speech tagging accuracy of 80.5% using only three examples of each tag and no dictionary constraints.We also compare to semi-supervised learning and discuss the system’s error trends.
We present a novel statistical approach to parsing, for constructing a complete, formal meaning representation of a sentence.A semantic parser is learned given a set of sentences annotated with their correct meaning represen- The main innovation of is its use of state-of-the-art statistical machine translation techniques.A word alignment model is used for lexical acquisition, and the parsing model itself can be seen as a syntax-based translation model. show that favorably in terms of both accuracy and coverage compared to existing learning methods requiring similar amount of supervision, and shows better robustness to variations in task complexity and word order.
This paper studies the impact of paraphrases on the accuracy of automatic evaluation.Given a reference sentence and a machine-generated sentence, we seek to find a paraphrase of the reference sentence that is closer in wording to the machine output than the original reference.We apply our paraphrasing method in the context of machine translation evaluation.Our experiments show that the use of a paraphrased synthetic reference refines the accuracy of automatic evaluation.We also found a strong connection between the quality of automatic paraphrases as judged by humans and their contribution to automatic evaluation.
In this paper, we study the effect of different word-level preprocessing decisions for Arabic on SMT quality.Our results show that given large amounts of training data, splitting off only proclitics performs best.However, for small amounts of training data, it is best to apply English-like tokenization using part-of-speech tags, and sophisticated morphological analysis and disambiguation.Moreover, choosing the appropriate preprocessing produces a significant increase in BLEU score if there is a change in genre between training and test data.
hovy mitch martha.palmer lance.ramshaw weischedel @isi.edu @cis.upenn.edu @colorado.edu @bbn.com @bbn.com We describe the OntoNotes methodology and its result, a large multilingual richly-annotated corpus constructed at 90% interannotator agreement.An initial portion (300K words of English newswire and 250K words of Chinese newswire) will be made available to the community during 2007.
We present a novel parser combination scheme that works by reparsing input sentences once they have already been parsed by several different parsers.We apply this idea to dependency and constituent parsing, generating results that surpass state-of-theart accuracy levels for individual parsers.
Traditional noun phrase coreference resolution systems represent features only of pairs of noun phrases.In this paper, we propose a machine learning method enables features over noun phrases, resulting in a first-order probabilistic model for coreference.We outline a set of approximations that make this approach practical, and apply our method to the ACE coreference dataset, achieving a 45% error reduction over a comparable method that only considers features of pairs of noun phrases.This result demonstrates an example of how a firstorder logic representation can be incorporated into a probabilistic model and scaled efficiently.
This paper presents two Markov chain Monte Carlo (MCMC) algorithms forBayesian inference of probabilistic context free grammars (PCFGs) from ter minal strings, providing an alternative to maximum-likelihood estimation usingthe Inside-Outside algorithm.We illus trate these methods by estimating a sparse grammar describing the morphology ofthe Bantu language Sesotho, demonstrat ing that with suitable priors Bayesian techniques can infer linguistic structure in situations where maximum likelihoodmethods such as the Inside-Outside algo rithm only produce a trivial grammar.
We present a sentence compression system based on synchronous context-free grammars (SCFG), following the successful noisy-channel approach of (Knight and Marcu, 2000).We define a headdriven Markovization formulation of SCFG deletion rules, which allows us to lexicalize probabilities of constituent deletions.We also use a robust approach for tree-to-tree alignment between arbitrary document-abstract parallel corpora, which lets us train lexicalized models with much more data than previous approaches relying exclusively on scarcely available document-compression corpora.Finally, we evaluate different Markovized models, and find that our selected best model is one that exploits head-modifier bilexicalization to accurately distinguish adjuncts from complements, and that produces sentences that were judged more grammatical than those generated by previous work.
Currently there are several approaches to machine translation (MT) based on different paradigms; e.g., phrasal, hierarchical and syntax-based.These three approaches yield similar translation accuracy despite using fairly different levels of linguistic knowledge.The availability of such a variety of systems has led to a growing interest toward finding better translations by combining outputs from multiple systems.This paper describes three different approaches to MT system combination.These combination methods operate on sentence, phrase and word level exploiting information from -best lists, system scores and target-to-source phrase alignments.The word-level combination provides the most robust gains but the best results on the development test sets (NIST MT05 and the newsgroup portion of GALE 2006 dry-run) were achieved by combining all three methods.
Standard pairwise coreference resolution systems are subject to errors resulting from their performing anaphora identification as an implicit part of coreference resolution.In this paper, we propose an integer linear programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task, such that each local model informs the other for the final assignments. joint ILP formulation provides score improvements of 3.7-5.3% over a base coreference classifier on the ACE datasets.
We address the problem of analyzing multiple related opinions in a text.For instance, in a restaurant review such opinions may include food, ambience and service.We formulate this task as a multiple aspect ranking problem, where the goal is to produce a set of numerical scores, one for each aspect.We present an algorithm that jointly learns ranking models for individual aspects by modeling the dependencies between assigned ranks.This algorithm guides the prediction of individual rankers by analyzing meta-relations between opinions, such as agreement and contrast.We prove that our agreementbased joint model is more expressive than individual ranking models.Our empirical results further confirm the strength of the model: the algorithm provides significant improvement over both individual rankers and a state-of-the-art joint ranking model.
Letter-to-phoneme conversion generally requires aligned training data of letters and phonemes.Typically, the alignments are limited to one-to-one alignments.We present a novel technique of training with many-to-many alignments.A letter chunking bigram prediction manages double letters and double phonemes automatically as opposed to preprocessing with fixed lists.We also apply an HMM method in conjunction with a local classification model to predict a global phoneme sequence given a word.The many-to-many alignments result in significant improvements over the traditional one-to-one approach.Our system achieves state-of-the-art performance on several languages and data sets.
We present several improvements to unlexicalized parsing with hierarchically state-split PCFGs.First, we present a novel coarse-to-fine method in which a grammar’s own hierarchical projections are used for incremental pruning, including a method for efficiently computing projections of a grammar without a treebank.In our experiments, hierarchical pruning greatly accelerates parsing with no loss in empirical accuracy.Second, we compare various inference procedures for state-split PCFGs from the standpoint of risk minimization, paying particular attention to their practical tradeoffs.Finally, we present multilingual experiments which show that parsing with hierarchical state-splitting is fast and accurate in multiple languages and domains, even without any language-specific tuning.
This paper presents and compares WordNetbased and distributional similarity approaches.The strengths and weaknesses of each approach regarding similarity and relatedness tasks are discussed, and a combination is presented.Each of our methods independently provide the best results in their class on the RG and WordSim353 datasets, and a supervised combination of them yields the best published results on all datasets.Finally, we pioneer cross-lingual similarity, showing that our methods are easily adapted for a cross-lingual task with minor losses.
We present a family of priors over probabilistic grammar weights, called the shared logistic normal distribution.This family extends the partitioned logistic normal distribution, enabling factored covariance between the probabilities of different derivation events in the probabilistic grammar, providing a new way to encode prior knowledge about an unknown grammar.We describe a variational EM algorithm for learning a probabilistic grammar based on this family of priors.We then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learnand with a non-parallel, multilingual corpus.
Unsupervised grammar induction models tend to employ relatively simple models of syntax when compared to their supervised counterparts.Traditionally, the unsupervised models have been kept simple due to tractability and data sparsity concerns.In this paper, we introduce basic valence frames and lexical information into an unsupervised dependency grammar inducer and show how this additional information can be leveraged via smoothing.Our model produces state-of-theart results on the task of unsupervised grammar induction, improving over the best previous work by almost 10 percentage points.
We introduce a novel precedence reordering approach based on a dependency parser to statistical machine translation systems.Similar to other preprocessing reordering approaches, our method can efficiently incorporate linguistic knowledge into SMT systems without increasing the complexity of decoding.For a set of five subject-object-verb (SOV) order languages, we show significant improvements in BLEU scores when translating from English, compared to other reordering approaches, in state-of-the-art phrase-based SMT systems.
One of the reasons nonparametric Bayesian inference is attracting attention in computational linguistics is because it provides a principled way of learning the units of generalization together with their probabilities.Adaptor grammars are a framework for defining a variety of hierarchical nonparametric Bayesian models.This paper investigates some of the choices that arise in formulating adaptor grammars and associated inference procedures, and shows that they can have a dramatic impact on performance in an unsupervised word segmentation task.With appropriate adaptor grammars and inference procedures we achieve an 87% word token f-score on the standard Brent version of the Bernstein- Ratner corpus, which is an error reduction of over 35% over the best previously reported results for this corpus.
For many language technology applications, such as question answering, the overall system runs several independent processors over the data (such as a named entity recognizer, a coreference system, and a parser).This easily results in inconsistent annotations, which are harmful to the performance of the aggregate system.We begin to address this problem with a joint model of parsing and named entity recognition, based on a discriminative feature-based constituency parser.Our model produces a consistent output, where the named entity spans do not conflict with the phrasal spans of the parse tree.The joint representation also allows the information from each type of annotation to improve performance on the other, and, in experiments with the OntoNotes corpus, we found improvements of to absolute F1 for parsing, and up to F1 for named entity recognition.
We present an exploration of generative probabilistic models for multi-document summarization.Beginning with a simple word frequency based model (Nenkova and Vanderwende, 2005), we construct a sequence of models each injecting more structure into the representation of document set content and exhibiting ROUGE gains along the way.Our model, utilizes a hierarchical LDA-style model (Blei et al., 2004) to represent content specificity as a hierarchy of topic vocabulary distributions.At the task of producing generic DUC-style summaries, state-of-the-art ROUGE performance and in pairwise user evaluation strongly outperforms Toutanova et al. (2007)’s state-of-the-art discriminative system.We explore capacity to produce multiple ‘topical summaries’ in order to facilitate content discovery and navigation.
Recent work has shown that translating segmentation lattices (lattices that encode alternative ways of breaking the input to an MT system into words), rather than text in any particular segmentation, improves translation quality of languages whose orthography does not mark morpheme boundaries.However, much of this work has relied on multiple segmenters that perform differently on the same input to generate sufficiently diverse source segmentation lattices.In this work, we describe a maximum entropy model of compound word splitting that relies on a few general features that can be used to generate segmentation lattices for most languages with productive compounding.Using a model optimized for German translation, we present results showing significant improvements in translation quality in German-English, Hungarian-English, and Turkish-English translation over state-ofthe-art baselines.
We present results on a novel hybrid semantic SMT model that incorporates the strengths of both semantic role labeling and phrase-based statistical machine translation.The approach avoids major complexity limitations via a two-pass architecture.The first pass is performed using a conventional phrase-based SMT model.The second pass is performed by a re-ordering strategy guided by shallow semantic parsers that produce both semantic frame and role labels.Evaluation on a Wall Street Journal newswire genre test set showed the hybrid model to yield an improvement of roughly half a point in BLEU score over a strong pure phrase-based SMT baseline – to our knowledge, the first successful application of semantic role labeling to SMT.
Current vector-space models of lexical semantics create a single “prototype” vector to represent the meaning of a word.However, due to lexical ambiguity, encoding word meaning with a single vector is problematic.This paper presents a method that uses clustering to produce multiple “sense-specific” vectors for each word.This approach provides a context-dependent vector representation of word meaning that naturally accommodates homonymy and polysemy.Experimental comparisons to human judgements of semantic similarity for both isolated words as well as words in sentential contexts demonstrate the superiority of this approach over both prototype and exemplar based vector-space models.
We present results from a range of experiments on article and preposition error correction for non-native speakers of English.We first compare a language model and errorspecific classifiers (all trained on large English corpora) with respect to their performance in error detection and correction.We then combine the language model and the classifiers in a meta-classification approach by combining evidence from the classifiers and the language model as input features to the metaclassifier.The meta-classifier in turn is trained on error-annotated learner data, optimizing the error detection and correction performance on this domain.The meta-classification approach results in substantial gains over the classifieronly and language-model-only scenario.Since the meta-classifier requires error-annotated data for training, we investigate how much training data is needed to improve results over the baseline of not using a meta-classifier.All evaluations are conducted on a large errorannotated corpus of learner English.
We propose the first unsupervised approach to the problem of modeling dialogue acts in an open domain.Trained on a corpus of noisy Twitter conversations, our method discovers dialogue acts by clustering raw utterances.Because it accounts for the sequential behaviour of these acts, the learned model can provide insight into the shape of communication in a new medium.We address the challenge of evaluating the emergent model with a qualitative visualization and an intrinsic conversation ordering task.This work is inspired by a corpus of 1.3 million Twitter conversations, which will be made publicly available.This huge amount of data, available only because Twitter blurs the line between chatting and publishing, highlights the need to be able to adapt quickly to a new medium.
We report on work in progress on extractlexical simplifications (e.g., focusing on utilizing edit histories in Simple English Wikipedia for this task.We consider two main approaches: (1) deriving simplification probabilities via an edit model that accounts for a mixture of different operations, and (2) using metadata to focus on edits that are more likely to be simplification operations.We find our methods to outperform a reasonable baseline and yield many high-quality lexical simplifications not included in an independently-created manually prepared list.
Coreference resolution is governed by syntactic, semantic, and discourse constraints.We present a generative, model-based approach in which each of these factors is modularly encapsulated and learned in a primarily unsupervised manner.Our semantic representation first hypothesizes an underlying set of latent which generate specific entities that in turn render individual mentions.By sharing lexical statistics at the level of abstract entity types, our model is able to substantially reduce semantic compatibility errors, resulting in the best results to date on the complete end-to-end coreference task.
The quality of a statistical machine translation (SMT) system is heavily dependent upon the amount of parallel sentences used in training.In recent years, there have been several approaches developed for obtaining parallel sentences from non-parallel, or comparable data, such as news articles published within the same time period (Munteanu and Marcu, 2005), or web pages with a similar structure (Resnik and Smith, 2003).One resource not yet thoroughly explored is Wikipedia, an online encyclopedia containing linked articles in many languages.We advance the state of the art in parallel sentence extraction by modeling the document level alignment, motivated by the observation that parallel sentence pairs are often found in close proximity.We also include features which make use of the additional annotation given by Wikipedia, and features using an automatically induced lexicon model.Results for both accuracy in sentence extraction and downstream improvement in an SMT system are presented.
We examine the viability of building large polarity lexicons semi-automatically from the web.We begin by describing a graph propagation framework inspired by previous work on constructing polarity lexicons from lexical graphs (Kim and Hovy, 2004; Hu and Liu, 2004; Esuli and Sabastiani, 2009; Blair- Goldensohn et al., 2008; Rao and Ravichandran, 2009).We then apply this technique to build an English lexicon that is significantly larger than those previously studied.Crucially, this web-derived lexicon does not require WordNet, part-of-speech taggers, or other language-dependent resources typical of sentiment analysis systems.As a result, the lexicon is not limited to specific word classes – e.g., adjectives that occur in WordNet – and in fact contains slang, misspellings, multiword expressions, etc.We evaluate a lexicon derived from English documents, both qualitatively and quantitatively, and show that it provides superior performance to previously studied lexicons, including one derived from
There has been a proliferation of recent work on SMT tuning algorithms capable of handling larger feature sets than the traditional MERT approach.We analyze a number of these algorithms in terms of their sentencelevel loss functions, which motivates several new approaches, including a Structured SVM.We perform empirical comparisons of eight different tuning strategies, including MERT, in a variety of settings.Among other results, we find that a simple and efficient batch version of MIRA performs at least as well as training online, and consistently outperforms other options.
It has been established that incorporating word cluster features derived from large unlabeled corpora can significantly improve prediction of linguistic structure.While previous work has focused primarily on English, we extend these results to other languages along two dimensions.First, we show that these results hold true for a number of languages across families.Second, and more interestingly, we provide an algorithm for inducing cross-lingual clusters and we show that features derived from these clusters significantly improve the accuracy of cross-lingual structure prediction.Specifically, we show that by augmenting direct-transfer systems with cross-lingual cluster features, the relative error of delexicalized dependency parsers, trained on English treebanks and transferred to foreign languages, can be reduced by up to 13%.When applying the same method to direct transfer of named-entity recognizers, we observe relative improvements of up to 26%.
We present a novel method for evaluating grammatical error correction.The core of method, which we call is an algorithm for efficiently computing the sequence of phrase-level edits between a source sentence and a system hypothesis that achieves the highest overlap with the goldstandard annotation.This optimal edit seis subsequently scored using mea- We test our on the Helping Our Own (HOO) shared task data and show that our method results in more accurate evaluation for grammatical error correction.
author: Online conversational text, typified by microblogs, and text is a challenge for natural language processing.Unlike the highly edited genres that conventional NLP tools have been developed for, conversational text contains many nonstandard lexical items and syntactic patterns.These are the result of unintentional errors, dialectal variation, conversational ellipsis, topic diversity, and creative use of language and orthography (Eisenstein, 2013).An example is shown in Fig.1.As a result of this widespread variation, standard modeling assumptions that depend on lexical, syntactic, and orthographic regularity are inappropriate.There Abstract We consider the problem of part-of-speech tagging for informal, online conversational text.We systematically evaluate the use of large-scale unsupervised word clustering and new lexical features to improve tagging accuracy.With these features, our system achieves state-of-the-art tagging results on both Twitter and IRC POS tagging tasks; Twitter tagging is improved from 90% to 93% accuracy (more than 3% absolute).Qualitative analysis of these word clusters yields insights about NLP and linguistic phenomena in this genre.Additionally, we contribute the first POS annotation guidelines for such text and release a new dataset of English language tweets annotated using these guidelines.Tagging software, annotation guidelines, and large-scale word clusters are available at:
We introduce an annotation scheme for temporal expressions, and describe a method for resolving temporal expressions in print and broadcast news.The system, which is based on both hand-crafted and machine-learnt rules, achieves an 83.2% accuracy (Fmeasure) against hand-annotated data.Some initial steps towards tagging event chronologies are also described.
This paper presents a comprehensive empirical comparison between two approaches for developing a base noun phrase chunker: human rule writing and active learning using interactive realtime human annotation.Several novel variations on active learning are investigated, and underlying cost models for cross-modal machine learning comparison are presented and explored.Results show that it is more efficient and more successful by several measures to train a system using active learning annotation rather than hand-crafted rule writing at a comparable level of human labor investment.
with genetic algorithms.Workshop on Empirical Learning of NLP Tasks.K. Koskenniemi, 1983.A general computation model word-form recognition and production.11, of General Linguistics. of Helsinki.C.X.Ling, 1994.Learning the past tense of English verbs: The symbolic pattern associator vs. connecmodels.Art.Intel.Res., R. Mooney and M. Califf, 1995.Induction of firstorder decision lists: Results on learning the past of English verbs.Art.Intel.Res., K. Oflazer and S. Nirenburg, 1999.Practical bootof morphological analyzers. of the Conference on Natural Language Learning.D. Rumelhart and J. McClelland, 1986.On learning the past tense of English verbs.In J. McClel- D. Rumelhart, and the Group, Parallel distributed processing: Explorations in the of cognition, 2.MIT Press.P. Theron and I. Cloete, 1997.Automatic acquisition two-level morphological rules. of the Fifth Conference on Applied Natural Language Propages 103-110.A. Voutilainen, 1995.Morphological disambiguation.In F. Karlsson, A. Voutilainen, J. Heikkila, and A.(eds.) grammar - A language independent system for parsing unrestricted text, pages 165-284.The Hague: Mouton de Gruyter.
The noisy channel model has been applied to a wide range of problems, including spelling correction.These models consist of two components: a source model and a channel model.Very little research has gone into improving the channel model for spelling correction.This paper describes a new channel model for spelling correction, based on generic string to string edits.Using this model gives significant performance improvements compared to previously proposed models.
Extractive summarization techniques cannot generate document summaries shorter than a single sentence, something that is often required.An ideal summarization system would understand each document and generate an appropriate summary directly from the results of that understanding.A more practical approach to this problem results in the use of an approximation: viewing summarization as a problem analogous to statistical machine translation.The issue then becomes one of generating a target document in a more concise language from a source document in a more verbose language.This paper presents results on experiments using this approach, in which statistical models of the term selection and term ordering are jointly applied to produce summaries in a style learned from a training corpus.
In this paper, we present and compare various single-word based alignment models for statistical machine translation.We discuss the five IBM alignment models, the Hidden- Markov alignment model, smoothing techniques and various modifications.We present different methods to combine alignments.As evaluation criterion we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment.We show that models with a first-order dependence and a fertility model lead to significantly better results than the simple models IBM-1 or IBM-2, which are not able to go beyond zero-order dependencies.
We discuss the advantages of lexicalized tree-adjoining grammar as an alternative to lexicalized PCFG for statistical parsing, describing the induction of a probabilistic LTAG model from the Penn Treebank and evaluating its parsing performance.We find that this induction method is an improvement over the EM-based method of (Hwa, 1998), and that the induced model yields results comparable to lexicalized PCFG.
We present a system for identifythe semantic relationships, or sefilled by constituents of a sentence within a semantic frame.Various lexical and syntactic features are derived from parse trees and used to derive statistical classifiers from hand-annotated training data.
answers in large collections of texts: paragraph + abductive inference.Notes of the Fall AAAI Symposium on Question An
The amount of readily available on-line text has reached hundreds of billions of words and continues to grow.Yet for most core natural language tasks, algorithms continue to be optimized, tested and compared after training on corpora consisting of only one million words or less.In this paper, we evaluate the performance of different learning methods on a prototypical natural language disambiguation task, confusion set disambiguation, when trained on orders of magnitude more labeled data than has previously been used.We are fortunate that for this particular application, correctly labeled training data is free.Since this will often not be the case, we examine methods for effectively exploiting very large corpora when labeled data comes at a cost.
While paraphrasing is critical both for interpretation and generation of natural language, current systems use manual or semi-automatic methods to collect paraphrases.We present an unsupervised learning algorithm for identification of paraphrases from a corpus of multiple English translations of the same source text.Our approach yields phrasal and single word lexical paraphrases as well as syntactic paraphrases.
We present two language models based upon an “immediate-head” parser — our name for a parser that conditions events below a constituent head of While all of the most accurate statistical parsers are of the immediate-head variety, no previous grammatical language model uses this technology.The perplexity for both of these models significantly improve upon the trigram model base-line as well as the best previous grammarbased language model.For the better of our two models these improvements are 24% and 14% respectively.We also suggest that improvement of the underlying parser should significantly improve the model’s perplexity and that even in the near term there is a lot of potential for improvement in immediatehead language models. are what we will call parsers in that all of the properties of the immedescendants of a constituent assigned probabilities that are conditioned on the lexical of For example, in Figure 1 the probability the into np pp conditioned on head of the “put”, as are the choices of the under the i.e., “ball” (the head of and “in” (the head of the It is the experience of the statistical parsing community that immediate-head parsers are the most accurate we can design.It is also worthy of note that many of these [1,3,6,7] are that is, for a try to find the parse by Equation 1: = arg (1) This is interesting because insofar as they comthese parsers define a language-model in that they can (in principle) assign a probability to all possible sentences in the language by com
We develop a framework for formalizing semantic construction within grammars expressed in typed feature struclogics, including The approach provides an alternative to the lambda calculus; it maintains much of the desirable flexibility of unificationbased approaches to composition, while constraining the allowable operations in order to capture basic generalizations and improve maintainability.
This paper presents methods for a qualitative, unbiased comparison of lexical association measures and the results we have obtained for adjective-noun pairs and preposition-noun-verb triples extracted from German corpora.In our approach, we compare the entire list of candidates, sorted according to the particular measures, to a reference set of manually identified “true positives”.We also show how estimates for the very large number of hapaxlegomena and double occurrences can be inferred from random samples.
A good decoding algorithm is critical to the success of any statistical machine translation system.The decoder’s job is to find the translation that is most likely according to set of previously learned parameters (and a formula for combining them).Since the space of possible translations is extremely large, typical decoding algorithms are only able to examine a portion of it, thus risking to miss good solutions.In this paper, we compare the speed and output quality of a traditional stack-based decoding algorithm with two new decoders: a fast greedy decoder and a slow but optimal decoder that treats decoding as an integer-programming optimization problem.
We propose a statistical method that finds the maximum-probability segmentation of a given text.This method does not require training data because it estimates probabilities from the given text.Therefore, it can be applied to any text in any domain.An experiment showed that the method is more accurate than or at least as accurate as a state-of-the-art text segmentation system.
We present a syntax-based statistical translation model.Our model transforms a source-language parse tree into a target-language string by applying stochastic operations at each node.These operations capture linguistic differences such as word order and case marking.Model parameters are estimated in polynomial time using an EM algorithm.The model produces word alignments that are better than those
algebraic path problem (shortest paths; matrix inver- 34(3):191–219.Richard Sproat and Michael Riley.1996.Compilation of weighted finite-state transducers from decision trees. of the 34th Annual Meeting of the Andreas Stolcke and Stephen M. Omohundro.1994.Best-first model merging for hidden Markov model induction.Tech.Report ICSI TR-94-003, Berkeley, CA.Robert Endre Tarjan.1981a.A unified approach to path of the 28(3):577–593, July.Robert Endre Tarjan.1981b.Fast algorithms for solving problems. of the 28(3):594–614, July.G. van Noord and D. Gerdemann.2001.An extendible regular expression compiler for finite-state approaches natural language processing.In Impleno.22 in Springer Lecture Notes in CS.
In this paper we explore the power of surface text patterns for open-domain question answering systems.In order to obtain an optimal set of patterns, we have developed a method for learning such patterns automatically.A tagged corpus is built from the Internet in a bootstrapping process by providing a few hand-crafted examples of each question type to Altavista.Patterns are then automatically extracted from the returned documents and standardized.We calculate the precision of each pattern, and the average precision for each question type.These patterns are then applied to find answers to new questions.Using the TREC-10 question set, we report results for two cases: answers determined from the TREC-10 corpus and from the web.
We present a noun phrase coreference system that extends the work of Soon et al. (2001) and, to our knowledge, produces the best results to date on the MUC- 6 and MUC-7 coreference resolution data sets — F-measures of 70.4 and 63.4, respectively.Improvements arise from two sources: extra-linguistic changes to the learning framework and a large-scale expansion of the feature set to include more sophisticated linguistic knowledge.
We present a generative distributional model for the unsupervised induction of natural language syntax which explicitly models constituent yields and contexts.Parameter search with EM produces higher quality analyses than previously exhibited by unsupervised systems, giving the best published unparsing results on the Experiments on Penn treebank sentences of comparalength show an even higher 71% on nontrivial brackets.We compare distributionally induced and actual part-of-speech tags as input data, and examine extensions to the basic model.We discuss errors made by the system, compare the system to previous models, and discuss upper bounds, lower bounds, and stability for this task.
This paper describes a simple patternmatching algorithm for recovering empty nodes and identifying their co-indexed antecedents in phrase structure trees that do not contain this information.The patterns are minimal connected tree fragments containing an empty node and all other nodes co-indexed with it.This paper also proposes an evaluation procedure for empty node recovery procedures which is independent of most of the details of phrase structure, which makes it possible to compare the performance of empty node recovery on parser output with the empty node annotations in a goldstandard corpus.Evaluating the algorithm on the output of Charniak’s parser (Charniak, 2000) and the Penn treebank (Marcus et al., 1993) shows that the patternmatching algorithm does surprisingly well on the most frequently occuring types of empty nodes given its simplicity.
This paper presents a method for incorporating word pronunciation information in a noisy channel model for spelling correction.The proposed method builds an explicit error model for word pronunciations.By modeling pronunciation similarities between words we achieve a substantial performance improvement over the previous best performing models for spelling correction.
peg,. pyscropyrssexa pa...Inn/bee(Esrey:,we6onee nocnemere)seressartba 43csrassartsmeuroto-ssoprbonourseLproronparserrna flaw) sror spe6yer — ,r1 Transducerloacleo a erkirlE.FOL.*I PD.021 fP21111“enwialitneI r Figure 2: Unicode text in Gate2 witnessed on the software scene with the emergence of Unicode as a universal standard for representing textual data.GATE supports multilingual data processing using Unicode as its default text encoding.It also provides a means of entering text in various languages, using virtual keyboards where the language is not supported by the underlying operating platform.(Note that although Java represents characters as Unicode, it doesn't support input in many of the languages covered by Unicode.)Currently 28 languages are supported, and more are planned for future releases.Because GATE is an open architecture, new virtual keyboards can be defined by the users and added to the system as needed.For displaying the text, GATE relies on the rendering facilities offered by the Java implementation for the platform it runs on.Figure 2 gives an example of text in various languages displayed by GATE.The ability to handle Unicode data, along with the separation between data and implementation, allows LE systems based on GATE to be ported to new languages with no additional overhead apart from the development of the resources needed for the specific language.These facilities have been developed as part of the EMILLE project (McEnery et al., 2000), which focuses on the construction a 63 million word electronic corpus of South Asian languages.3 Applications One of GATE's strengths is that it is flexible and robust enough to enable the development of a wide range of applications within its framework.In this section, we describe briefly some of the NLP applications we have developed using the GATE architecture.3.1 MUSE The MUSE system (Maynard et al., 2001) is a multi-purpose Named Entity recognition system which is capable of processing texts from widely different domains and genres, thereby aiming to reduce the need for costly and time-consuming adaptation of existing resources to new applications and domains.The system aims to identify the parameters relevant to the creation of a name recognition system across different types of variability such as changes in domain, genre and media.For example, less formal texts may not follow standard capitalisation, punctuation and spelling formats, which can be a problem for many generic NE systems.Current evaluations with this system average around 93% precision and 95% recall across a variety of text types.3.2 ACE The MUSE system has also been adapted to take part in the current ACE (Automatic Content Extraction) program run by NIST.This requires systems to perform recognition and tracking tasks of named, nominal and pronominal entities and their mentions across three types of clean news text (newswire, broadcast news and newspaper) and two types of degraded news text (OCR output and ASR output).3.3 MUMIS The MUMIS (MUltiMedia Indexing and Searching environment) system uses Information Extraction components developed within GATE to produce formal annotations about essential events in football video programme material.This IE system comprises versions of the tokenisation, sentence detection, POS-tagging, and semantic tagging modules developed as part of GATE's standard resources, but also includes morphological analysis, full syntactic parsing and discourse interpretation modules, thereby enabling the production of annotations over text encoding structural, lexical, syntactic and semantic information.The semantic tagging module currently achieves around 91% precision and 76% recall, a significant improvement on a baseline named entity recognition system evaluated against it.4 Processing Resources Provided with GATE is a set of reusable processing resources for common NLP tasks.(None of them are definitive, and the user can replace and/or extend them as necessary.)These are packaged together to form ANNIE, A Nearly- New IE system, but can also be used individually or coupled together with new modules in order to create new applications.For example, many other NLP tasks might require a sentence splitter and POS tagger, but would not necessarily require resources more specific to IE tasks such as a named entity transducer.The system is in use for a variety of IE and other tasks, sometimes in combination with other sets of application-specific modules.ANNIE consists of the following main processing resources: tokeniser, sentence splitter, POS tagger, gazetteer, finite state transducer (based on GATE's built-in regular expressions over annotations language (Cunningham et al., 2002)), orthomatcher and coreference resolver.The resources communicate via GATE's annotation API, which is a directed graph of arcs bearing arbitrary feature/value data, and nodes rooting this data into document content (in this case text). text into simple tokens, such as numbers, punctuation, symbols, and words of different types (e.g. with an initial capital, all upper case, etc.).The aim is to limit the work of the tokeniser to maximise efficiency, and enable greater flexibility by placing the burden of analysis on the grammars.This means that the tokeniser does not need to be modified for different applications or text types. splitter a cascade of finitestate transducers which segments the text into sentences.This module is required for the tagger.Both the splitter and tagger are domainand application-independent. a modified version of the Brill tagger, which produces a part-of-speech tag as an annotation on each word or symbol.Neither the splitter nor the tagger are a mandatory part of the NE system, but the annotations they produce can be used by the grammar (described below), in order to increase its power and coverage. of lists such as cities, organisations, days of the week, etc.It not only consists of entities, but also of names of useful as typical company designators (e.g.'Ltd:), titles, etc.The gazetteer lists are compiled into finite state machines, which can match text tokens. tagger of handcrafted rules written in the JAPE (Java Annotations Pattern Engine) language (Cunningham et al., 2002), which describe patterns to match and annotations to be created as a result.JAPE is a version of CPSL (Common Pattern Specification Language) (Appelt, 1996), which provides finite state transduction over annotations based on regular expressions.A JAPE grammar consists of a set of phases, each of which consists of a set of pattern/action rules, and which run sequentially.Patterns can be specified by describing a specific text string, or annotations previously created by modules such as the tokeniser, gazetteer, or document format analysis.Rule prioritisation (if activated) prevents multiple assignment of annotations to the same text string. another optional module for the IE system.Its primary objective is to perform co-reference, or entity tracking, by recognising relations between entities.It also has a secondary role in improving named entity recognition by assigning annotations to previously unclassified names, based on relations with existing entities. identity relations between entities in the text.For more details see (Dimitrov, 2002).4.1 Implementation The implementation of the processing resources is centred on robustness, usability and the clear distinction between declarative data representations and finite state algorithms The behaviour of all the processors is completely controlled by external resources such as grammars or rule sets, which makes them easily modifiable by users who do not need to be familiar with programming languages.The fact that all processing resources use finite-state transducer technology makes them quite performant in terms of execution times.Our initial experiments show that the full named entity recognition system is capable of processing around 2.5KB/s on a PITT 450 with 256 MB RAM (independently of the size of the input file; the processing requirement is linear in relation to the text size).Scalability was tested by running the ANNIE modules over a randomly chosen part of the British National Corpus (10% of all documents), which contained documents of up to 17MB in size.5 Language Resource Creation Since many NLP algorithms require annotated corpora for training, GATE's development environment provides easy-to-use and extendable facilities for text annotation.In order to test their usability in practice, we used these facilities to build corpora of named entity annotated texts for the MUSE, ACE, and MUMIS applications.The annotation can be done manually by the user or semi-automatically by running some processing resources over the corpus and then correcting/adding new annotations manually.Depending on the information that needs to be annotated, some ANNIE modules can be used or adapted to bootstrap the corpus annotation task.For example, users from the humanities created a gazetteer list with 18th century place names in London, which when supplied to the ANNIE gazetteer, allows the automatic annotation of place information in a large collection of 18th century court reports from the Old Bailey in London.Since manual annotation is a difficult and error-prone task, GATE tries to make it simple to use and yet keep it flexible.To add a new annotation, one selects the text with the mouse (e.g., &quot;Mr. Clever&quot;) and then clicks on the desired annotation type (e.g., Person), which is shown in the list of types on the right-handside of the document viewer (see Figure 1).If however the desired annotation type does not already appear there or the user wants to associate more detailed information with the annotation (not just its type), then an annotation editing dialogue can be used.6 Evaluation A vital part of any language engineering application is the evaluation of its performance, and a development environment for this purpose would not be complete without some mechanisms for its measurement in a large number of test cases.GATE contains two such mechanisms: an evaluation tool (AnnotationDiff) which enables automated performance measurement and visualisation of the results, and a benchmarking tool, which enables the tracking of a system's progress and regression testing.6.1 The AnnotationDiff Tool Gate's AnnotationDiff tool enables two sets of annotations on a document to be compared, in order to either compare a system-annotated text with a reference (hand-annotated) text, or to compare the output of two different versions of the system (or two different systems).For each annotation type, figures are generated for precision, recall, F-measure and false positives.The AnnotationDiff viewer displays the two sets of annotations, marked with different colours (similar to 'visual diff' implementations such as in the MKS Toolkit or TkDiff).Annotations in the key set have two possible colours depending on their state: white for annotations which have a compatible (or partially compatible) annotation in the response set, and orange for annotations which are missing in the response set.Annotations in the response set have three possible colours: green if they are compatible with the key annotation, blue if they Figure 3: Fragment of results from benchmark tool are partially compatible, and red if they are spurious.In the viewer, two annotations will be positioned on the same row if they are co-extensive, and on different rows if not.6.2 Benchmarking tool GATE's benchmarking tool differs from the AnnotationDiff in that it enables evaluation to be carried out over a whole corpus rather than a single document.It also enables tracking of the system's performance over time.The tool requires a clean version of a corpus (with no annotations) and an annotated reference corpus.First of all, the tool is run in generation mode to produce a set of texts annotated by the system.These texts are stored for future use.The tool can then be run in three ways: 1.Comparing the annotated set with the reference set; 2.Comparing the annotated set with the set produced by a more recent version of the system resources (the latest set); 3.Comparing the latest set with the reference set.In each case, performance statistics will be provided for each text in the set, and overall statistics for the entire set, in comparison with the reference set.In case 2, information is also provided about whether the figures have increased or decreased in comparison with the annotated set.The annotated set can be updated at any time by rerunning the tool in generation mode with the latest version of the system resources.Furthermore, the system can be run in verbose mode, where for each figure below a certain threshold (set by the user), the non-coextensive annotations (and their corresponding text) will be displayed.The output of the tool is written to an HTML file in tabular form, as shown in Figure 3.Current evaluations for the MUSE NE system are producing average figures of 90-95% Precision and Recall on a selection of different text types (spoken transcriptions, emails etc.).The default ANNIE system produces figures of between 80-90% Precision and Recall on news texts.This figure is lower than for the MUSE system, because the resources have not been tuned to a specific text type or application, but are intended to be adapted as necessary.Work on resolution of anaphora is currently averaging 63% Precision and 45% Recall, although this work is still very much in progress, and we expect these figures to improve in the near future.7 Related Work GATE draws from a large pool of previous work on infrastructures, architectures and development environments for representing and processing language resources, corpora, and annotations.Due to space limitations here we will discuss only a small subset.For a detailed review and its use for deriving the desiderata for this architecture see (Cunningham, 2000).Work on standard ways to deal with XML data is relevant here, such as the LT XML work at Edinburgh (Thompson and McKelvie, 1997), as is work on managing collections of documents and their formats, e.g.(Brugman et al., 1998; Grishman, 1997; Zajac, 1998).We have also drawn from work on representing information about text and speech, e.g.(Brugman et al., 1998; Mikheev and Finch, 1997; Zajac, 1998; Young et al., 1999), as well as annotation standards, such as the ATLAS project (an architecture for linguistic annotation) at LDC (Bird et kirlactunerkterripNerl ABC19980430.1830.0858.sgm Annotation tope., GPE RecallIncreaseon Atonaltmarked iron 06371426571426571la10 type Organization 1.0 increaseon hurnan-markedto 1 0 09444444444444444 Mug 07, limEll.ncreaseon tom0345to 07, 14153ING ANNOTATIONSIt To automatetetteABC Ir.NNOTATKMISinTo embroil,bath PARTIALLYCORRECT Pl4071&quot;ATIC*15nhe automateIDA, PratotationType Annotation type Person Precision increase on human-marked from 08947368421052632 lc 09444444444444444 03444444444444444 al., 2000).Our approach is also related to work on user interfaces to architectural facilities such as development environments, e.g.(Brugman et al., 1998) and to work on comparing different versions of information, e.g.(Sparck-Jones and Galliers, 1996; Paggio, 1998).This work is particularly novel in that it addresses the complete range of issues in NLP application development in a flexible and extensible way, rather than focusing just on some particular aspect of the development process.In addition, it promotes robustness, re-usability, and scalability as important principles that help with the construction of practical NLP systems.8 Conclusions In this paper we have described an infrastructure for language engineering software which aims to assist the develeopment of robust tools and resources for NLP.One future direction is the integration of processing resources which learn in the background while the user is annotating corpora in GATE's visual environment.Currently, statistical models can be integrated but need to be trained separately.We are also extending the system to handle language generation modules, in order to enable the construction of applications which require language production in addition to analysis, e.g. intelligent report generation from IE data.
tion using statistical models of Roget's categories on large corpora.In of the
This paper introduces new learning al gorithms for natural language processing based on the perceptron algorithm.We show how the algorithms can be efficientlyapplied to exponential sized representations of parse trees, such as the ?all sub trees?(DOP) representation described by (Bod 1998), or a representation tracking all sub-fragments of a tagged sentence.We give experimental results showing significant improvements on two tasks: parsing Wall Street Journal text, and named entity extraction from web data.
We present a stochastic parsing system consisting of a Lexical-Functional Grammar (LFG), a constraint-based parser and a stochastic disambiguation model.We report on the results of applying this system to parsing the UPenn Wall Street Journal (WSJ) treebank.The model combines full and partial parsing techniques to reach full grammar coverage on unseen data.The treebank annotations are used to provide partially labeled data for discriminative statistical estimation using exponential models.Disambiguation performance is evaluated by measuring matches of predicate-argument relations on two distinct test sets.On a gold standard of manually annotated f-structures for a subset of the WSJ treebank, this evaluation reaches 79% F-score.An evaluation on a gold standard of dependency relations for
We present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source-channel approach as a special case.All knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible hidden variables.This approach allows a baseline machine translation system to be extended easily by adding new feature functions.We show that a baseline statistical machine translation system is significantly improved using this approach.
This paper describes a decoding algorithm for a syntax-based translation model (Yamada and Knight, 2001).The model has been extended to incorporate phrasal translations as presented here.In contrast to a conventional word-to-word statistical model, a decoder for the syntaxbased model builds up an English parse tree given a sentence in a foreign language.As the model size becomes huge in a practical setting, and the decoder considers multiple syntactic structures for each word alignment, several pruning techniques are necessary.We tested our decoder in a Chinese-to-English translation system, and obtained better results than IBM Model 4.We also discuss issues concerning the relation between this decoder and a language model.
Human evaluations of machine translation are extensive but expensive.Human evaluations can take months to finish and involve human labor that can not be reused.We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run.We present this method as an automated understudy to skilled human judges which substitutes for them when there is for quick or frequent
This paper describes a wide-coverage statistical parser that uses Combinatory Categorial Grammar (CCG) to derive dependency structures.The parser differs from most existing wide-coverage treebank parsers in capturing the long-range dependencies inherent in constructions such as coordination, extraction, raising and control, as well as the standard local predicate-argument dependencies.A set of dependency structures used for training and testing the parser is obtained from a treebank of CCG normal-form derivations, which have been derived (semi-) automatically from the Penn Treebank.The parser correctly recovers over 80% of labelled dependencies, and around 90% of unlabelled dependencies.
disambiguation rivaling supervised methods.In Proceedings of the 33rd Annual Meeting of the for Computational pages 189–196. see that view independence does not imply preindepence, consider an example in which always.This is compatible with rule independence, but implies that = 1 and = 0, violating precision independence.A
We present an unsupervised approach to discourse relations of hold between arbitrary spans of texts.We show that discourse relation classifiers trained on examples that are automatically extracted from massive amounts of text can be used to distinguish between some of these relations with accuracies as high as 93%, even when the relations are not explicitly marked by cue phrases.
resource acquisition.In of the Workshop on Linguistic Knowledge Acquisition and Representation: Bootstrapping Anno- Language Data. appear.I. Dan Melamed.1998.Annotation style guide for the blinker project.Technical Report IRCS 98-06, University of Pennsylvania.Arul Menezes and Stephen D. Richardson.2001.A best-first alignment algorithm for automatic extraction of transfer mappings from bilingual cor- In of the 39th Annual Meeting of the Association for Computational Linguistics DDMT Workshop, France.Anoop Sarkar.2001.Applying co-training methods statistical parsing.In of NAACL, Shieber.1994.Restricting the weakgenerative capacity of synchronous treegrammars.Intelligence,
Named entity phrases are some of the most difficult phrases to translate because new phrases can appear from nowhere, and because many are domain specific, not to be found in bilingual dictionaries.We present a novel algorithm for translating named entity phrases using easily obtainable monolingual and bilingual resources.We report on the application and evaluation of this algorithm in translating Arabic named entities to English.We also compare our results with the results obtained from human translations and a commercial system for the same task.
This paper presents a simple unsupervised learning algorithm for classifying reviews up) or recdown).The classification of a review is predicted by the orientation the phrases in the review that contain adjectives or adverbs.A phrase has a positive semantic orientation when it has good associations (e.g., “subtle nuances”) and a negative semantic orientation when it has bad associations (e.g., “very cavalier”).In this paper, the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word “excellent” minus the mutual information between the given phrase and the word “poor”.A review is classified as recommended if the average semantic orientation of its phrases is positive.The algorithm achieves an average accuracy of 74% when evaluated on 410 reviews from Epinions, sampled from four different domains (reviews of automobiles, banks, movies, and travel destinations).The accuracy ranges from 84% for automobile reviews to 66% for movie reviews.
This paper proposes a Hidden Markov Model (HMM) and an HMM-based chunk tagger, from which a named entity (NE) recognition (NER) system is built to recognize and classify names, times and numerical quantities.Through the HMM, our system is able to apply and integrate four types of internal and external evidences: 1) simple deterministic internal feature of the words, such as capitalization and digitalization; 2) internal semantic feature of important triggers; 3) internal gazetteer feature; 4) external macro context feature.In this way, the NER problem can be resolved effectively.Evaluation of our system on MUC-6 and MUC-7 English NE tasks achieves F-measures of 96.6% and 94.1% respectively.It shows that the performance is significantly better than reported by any other machine-learning system.Moreover, the performance is even consistently better than those based on handcrafted rules.
Recent work in Question Answering has focused on web-based systems that answers using simple lexicosyntactic patterns.We present an alternative strategy in which patterns are used to extract highly precise relational information offline, creating a data repository that is used to efficiently answer questions.We evaluate our strategy on a challenging subset of questions, i.e.“Who is ...” questions, against a state of the art web-based Question Answering system.Results indicate that the extracted relations answer 25% more questions correctly and do so three orders of magnitude faster than the state of the art system.
In this paper we present a novel, customizable IE paradigm that takes advantage of predicate-argument structures.We also introduce a new way of automatically identifying predicate argument structures, which is central to our IE paradigm.It is based on: (1) an extended set of features; and (2) inductive decision tree learning.The experimental results prove our claim that accurate predicate-argument structures enable high quality IE results.
We introduce a probabilistic noisychannel model for question answering and we show how it can be exploited in the context of an end-to-end QA system.Our noisy-channel system outperforms a stateof-the-art rule-based QA system that uses similar resources.We also show that the model we propose is flexible enough to accommodate within one mathematical framework many QA-specific resources and techniques, which range from the exploitation of WordNet, structured, and semi-structured databases to reasoning, and paraphrasing.
Kernel-based learning (e.g., Support Vector Machines) has been successfully applied to many hard problems in Natural Language Processing (NLP).In NLP, although feature combinations are crucial to improving performance, they are heuristically selected.Kernel methods change this situation.The merit of the kernel is that feature combinaimplicitly expanded without loss of generality and increasing the computational costs.Kernel-based text analysis shows an excellent performance in terms in accuracy; however, these methods are usually too slow to apply to large-scale text analysis.In this paper, we extend Mining to convert a kernel-based classifier into a simple and fast linear classifier.Experimental results on English BaseNP Chunking, Japanese Word Segmentation and Japanese Dependency Parsing show that our new classifiers are about 30 to 300 times faster than the standard kernel-based classifiers.
Previous research has demonstrated the utility of clustering in inducing semantic verb classes from undisambiguated corpus data.We describe a new approach which involves clustering subcategorizaframe distributions using the Information Bottleneck and nearest neighbour methods.In contrast to previous work, we particularly focus on clustering polysemic verbs.A novel evaluation scheme is proposed which accounts for the effect of polysemy on the clusters, offering us a good insight into the potential and limitations of semantically classifying
We have aligned Japanese and English news articles and sentences to make a large parallel corpus.We first used a method based on cross-language information retrieval (CLIR) to align the Japanese and English articles and then used a method based on dynamic programming (DP) matching to align the Japanese and English sentences in these articles.However, the results included many incorrect alignments.To remove these, we propose two measures (scores) that evaluate the validity of alignments.The measure for article alignment uses similarities in sentences aligned by DP matching and that for sentence alignment uses similarities in articles aligned by CLIR.They enhance each other to improve the accuracy of alignment.Using these measures, we have successfully constructed a largescale article and sentence alignment corpus available to the public.
We augment a model of translation based on re-ordering nodes in syntactic trees in order to allow alignments not conforming to the original tree structure, while keeping computational complexity polynomial in the sentence length.This is done by adding a new subtree cloning operation to either tree-to-string or tree-to-tree alignment algorithms.
Word alignment plays a crucial role in statistical machine translation.Word-aligned corpora have been found to be an excellent source of translation-related knowledge.We present a statistical model for computing the probability of an alignment given a sentence pair.This model allows easy integration of context-specific features.Our experiments show that this model can be an effective tool for improving an existing word alignment.
We present a probabilistic parsing model for German trained on the Negra treebank.We observe that existing lexicalized parsing models using head-head dependencies, while successful for English, fail to outperform an unlexicalized baseline model for German.Learning curves show that this effect is not due to lack of training data.We propose an alternative model that uses sister-head dependencies instead of head-head dependencies.This model outperforms the baseline, achieving a labeled precision and recall of up to 74%.This indicates that sister-head dependencies are more appropriate for treebanks with very flat structures such as Negra.
In statistical machine translation, the generation of a translation hypothesis is computationally expensive.If arbitrary wordreorderings are permitted, the search problem is NP-hard.On the other hand, if we restrict the possible word-reorderings in an appropriate way, we obtain a polynomial-time search algorithm.In this paper, we compare two different reordering constraints, namely the ITG constraints and the IBM constraints.This comparison includes a theoretical discussion on the permitted number of reorderings for each of these constraints.We show a connection between the ITG constraints and the since 1870 known We evaluate these constraints on two tasks: the Verbmobil task and the Canadian Hansards task.The evaluation consists of two parts: First, we check how many of the Viterbi alignments of the training corpus satisfy each of these constraints.Second, we restrict the search to each of these constraints and compare the resulting translation hypotheses.The experiments will show that the baseline ITG constraints are not sufficient on the Canadian Hansards task.Therefore, we present an extension to the ITG constraints.These extended ITG constraints increase the alignment coverage from about 87% to 96%.
Often, the training procedure for statistical machine translation models is based on maximum likelihood or related criteria.A general problem of this approach is that there is only a loose relation to the final translation quality on unseen text.In this paper, we analyze various training criteria which directly optimize translation quality.These training criteria make use of recently proposed automatic evaluation metrics.We describe a new algorithm for efficient training an unsmoothed error count.We show that significantly better results can often be obtained if the final evaluation criterion is taken directly into account as part of the training procedure.
We apply a decision tree based approach to pronoun resolution in spoken dialogue.Our system deals with pronouns with NPand non-NP-antecedents.We present a set of features designed for pronoun resolution in spoken dialogue and determine the most promising features.We evaluate the system on twenty Switchboard dialogues and show that it compares well to Byron’s (2002) manually tuned system.
Several approaches have been described for the automatic unsupervised acquisition of patterns for information extraction.Each approach is based on a particular model for the patterns to be acquired, such as a predicate-argument structure or a dependency chain.The effect of these alternative models has not been previously studied.In this paper, we compare the prior models and introduce a new model, the Subtree model, based on arbitrary subtrees of dependency trees.We describe a discovery procedure for this model and demonstrate experimentally an improvement in recall using Subtree patterns.
This paper presents a Chinese word segmentation system that uses improved sourcechannel models of Chinese sentence generation.Chinese words are defined as one of the following four types: lexicon words, morphologically derived words, factoids, and named entities.Our system provides a unified approach to the four fundamental features of word-level Chinese language processing: (1) word segmentation, (2) morphological analysis, (3) factoid detection, and (4) named entity recognition.The performance of the system is evaluated on a manually annotated test set, and is also compared with several state-ofthe-art systems, taking into account the fact that the definition of Chinese words often varies from system to system.
This paper presents a method for unsupervised discovery of semantic patterns.Semantic patterns are useful for a variety of text understanding tasks, in particular for locating events in text for information extraction.The method builds upon previously described approaches to iterative unsupervised pattern acquisition.One common characteristic of prior approaches is that the output of the algorithm is a continuous stream of patterns, with gradually degrading precision.Our method differs from the previous pattern acquisition algorithms in that it introduces competition among several scenarios simultaneously.This provides natural stopping criteria for the unsupervised learners, while maintaining good precision levels at termination.We discuss the results of experiments with several scenarios, and examine different aspects of the new procedure.
Figure 3: Size and devset performance of the cumulatively annotated models, starting with the markovized baseline.The two columns show the change in the baseline for each annotation introduced, both cumulatively and for each single annotation applied to the baseline in isolation.history models similar in intent to those described in Ron et al. (1994).For variable horizontal histories, we did not split intermediate states below 10 occurrences of a symbol.For example, if the symbol too rare, we would colit to For vertical histories, we used a cutoff which included both frequency and mutual information between the history and the expansions (this was not appropriate for the horizontal because unreliable at such low counts).Figure 2 shows parsing accuracies as well as the number of symbols in each markovization.These symbol counts include all the intermediate states which represent partially completed constituents.The general trend is that, in the absence of further annotation, more vertical annotation is better – even exhaustive grandparent annotation.This is not true for horizontal markovization, where the variableorder second-order model was superior.The best has an 79.74, already a substantial improvement over the baseline.In the remaining sections, we discuss other annotations which increasingly split the symbol space.Since we expressly do not smooth the grammar, not all splits are guaranteed to be beneficial, and not all sets of useful splits are guaranteed to co-exist well. particular, while markovization is good on its own, it has a large number of states and does not tolerate further splitting well.Therefore, base all further exploration on the ROOT S&quot;ROOT 4: An error which can be resolved with the (incorrect baseline parse shown). grammar.Although it does not necessarily jump out of the grid at first glance, this point represents the best compromise between a compact grammar and useful markov histories.3 External vs. Internal Annotation The two major previous annotation strategies, parent annotation and head lexicalization, can be seen as instances of external and internal annotation, respectively.Parent annotation lets us indicate an important feature of the external environment of a node which influences the internal expansion of that node.On the other hand, lexicalization is a (radical) method of marking a distinctive aspect of the otherwise hidden internal contents of a node which influence the external distribution.Both kinds of annotation can be useful.To identify split states, we suffixes of the form mark internal content and mark external features.To illustrate the difference, consider unary productions.In the raw grammar, there are many unaries, and once any major category is constructed over a span, most others become constructible as well using unary chains (see Klein and Manning (2001) for discussion).Such chains are rare in real treebank trees: unary rewrites only appear in very specific for example of verbs where an empty, controlled subject.Figure 4 shows an erroneous output of the parser, using the baseline markovized grammar.Intuitively, there are several reasons this parse should be ruled out, but is that the lower which is intended prifor of communication verbs, is not a unary rewrite position (such complements usually have subjects).It would therefore be natural to annotate the trees so as to confine unary productions to the contexts in which they are actually ap- We tried two annotations.First, .NP&quot;S VP&quot;S NP&quot;VP VBD , NN SˆVP .VPˆS QP , NP&quot;VP $ CD CD VBG 444.9 million including , CONJP NP&quot;NP NP&quot;NP $ QP JJ NN , RB RB IN net interest down slightly from CD $ 450.7 million was Revenue $ CD (with a any nonterminal node which has only one child.In isolation, this resulted in an absolute gain of 0.55% (see figure 3).The same sentence, parsed using only the baseline and is parsed correctly, because the in the incorrect parse ends with an very low marked nodes had no siblings with It was similar to solo benefit (0.01% worse), but provided far less marginal benefit on top of later features (none at all on top of our top models), and was One restricted place where external unary annotation was very useful, however, was at the preterminal level, where internal annotation was meaningless.One distributionally salient tag conflation in the Penn treebank is the identification of demonstraand regular determiners based on whether they were only captured this distinction.The same external unary annotation was even more efwhen applied to adverbs disfor example, well Beyond these cases, unary tag marking was detrimen- The 78.86%.4 Tag Splitting The idea that part-of-speech tags are not fine-grained enough to abstract away from specific-word behaviour is a cornerstone of lexicalization.The for example, showed that the determiners which occur alone are usefully distinguished from those which occur with other nomimaterial.This marks the with a single bit about their immediate external context: whether there are sisters.Given the success of parent annotation for nonterminals, it makes sense to parent antags, as well In fact, as figure 3 shows, exhaustively marking all preterminals with their parent category was the most effective single annotation we tried.Why should this be useful?Most tags have a canonical category.For example, occur under (only 234 of 70855 do not, mostly mistakes).However, when a tag that when we show such trees, we generally only show one annotation on top of the baseline at a time.Moreover, we do not explicitly show the binarization implicit by the horizontal markovization. two are not equivalent even given infinite data.5: An error resolved with the (of the (a) the incorrect baseline parse and (b) the correct resolves this error. somewhat regularly occurs in a non-canonical position, its distribution is usually distinct.For example, most common adverbs directly under and Under they are and Under and and so on. substantially, to 80.62%.In addition to the adverb case, the Penn tag set conflates various grammatical distinctions that are commonly made in traditional and generative grammar, and from which a parser could hope to get useful information.For example, subordinating conas, complementizers prepositions in, all get the tag of these distinctions are captured by conjunctions occur under under but are not (both subordinating conjunctions and complementizers appear Also, there are exclusively nounprepositions predominantly verbones and so on.The annotation a linguistically motivated 6-way split the and brought the total to 81.19%.Figure 5 shows an example error in the baseline is equally well fixed by either In this case, the more common nominal of preferred unless the is annoto allow prefer We also got value from three other annotations which subcategorized tags for specific lexemes. we split off auxiliary verbs with the which appends all forms all forms of More miconjunction tags to indicate is an extended uniform version of the partial auxiliary annotation of Charniak (1997), wherein all auxiliaries are as a added to gerund auxiliaries and VP&quot;S VP&quot;S TO VP&quot;VP TO&quot;VP VP&quot;VP to VB PP&quot;VP to VB&quot;VP SBAR&quot;VP see NP&quot;PP see IN&quot;SBAR S&quot;SBAR IN (a) (b) NNS NN if VP&quot;S VBZ&quot;VP works works NN&quot;NP advertising advertising or not they were the strings &, each of which have distinctly different distributions from other conjunctions.Finally, we gave the percent sign (%) its own tag, in line with the dollar sign ($) already having its own.Together these three anbrought the 81.81%.5 What is an Unlexicalized Grammar?Around this point, we must address exactly what we by an To the extent that go about subcategorizing many of them might come to represent a single word.One might thus feel that the approach of this paper is to walk down a slippery slope, and that we are merely arguing degrees.However, we believe that there is a fundamental qualitative distinction, grounded in linguistic practice, between what we see as permitted an unlexicalized against what one finds hopes to exploit in lexicalized The division rests on the traditional distinction between words closed-class words) and open class or lexical words).It is standard practice in linguistics, dating back decades, to annotate phrasal nodes with important functiondistinctions, for example to have a a whereas content words are not part of grammatical structure, and one would not have sperules or constraints for an for example.We follow this approach in our model: various closed classes are subcategorized to better represent important distinctions, and important features commonly expressed by function words are annotated phrasal nodes (such as whether a finite, or a participle, or an infinitive clause).However, no use is made of lexical class words, to provide either or bilexical At any rate, we have kept ourselves honest by estimating our models exclusively by maximum likelihood estimation over our subcategorized grammar, without any form of interpolation or shrinkage to unsubcategorized categories (although we do rules, as explained above).This effecshould be noted that we started with four tags in the Penn tagset that rewrite as a single word: and some of the punctuation tags, which rewrite as barely more.To the extent that we subcategorize tags, there will be more such cases, but many of them already exist in other tag sets.For instance, many tag sets, such as the Brown and tagsets give a separate sets of tags to each form of verbal auxiliaries and most of which rewrite as only a single word (and any corresponding contractions).6: An error resolved with the (a) incorrect baseline parse and (b) the correct tively means that the subcategories that we break off must themselves be very frequent in the language.In such a framework, if we try to annotate categories with any detailed lexical information, many sentences either entirely fail to parse, or have only extremely weird parses.The resulting battle against sparsity means that we can only afford to make a few distinctions which have major distributional impact.Even with the individual-lexeme annotations in this section, the grammar still has only 9255 states compared to the 7619 of the baseline model.6 Annotations Already in the Treebank At this point, one might wonder as to the wisdom of stripping off all treebank functional tags, only to heuristically add other such markings back in to the grammar.By and large, the treebank out-of-the tags, such as have negative utility.Recall that the raw treebank gramwith no annotation or markovization, had an of 72.62% on our development set.With the functional annotation left in, this drops to 71.49%.The v markovization baseline of 77.77% dropped even further, all the way to 72.87%, when these annotations were included.Nonetheless, some distinctions present in the raw trees were valuable.For example, an an could be either a temporal a For the annotation we retained the on and, furthermore, propathe tag down to the tag of the head of the This is illustrated in figure 6, which also shows an of its utility, clarifying that last night is not a plausible compound and facilitating the othunusual high attachment of the smaller the cumulative 82.25%.Note that this technique of pushing the functional tags down to preterminals might be useful more generfor example, locative expand roughly the VPˆVP VPˆVP VB to NPˆVP appear NNˆTMP NPˆNP PPˆNP JJ PPˆNP NPˆNP appear NPˆVP VB NP-TMPˆVP to IN NNS last night JJ times three NNP NN on CD on times three last CNN night NPˆPP NNP CNN NPˆPP IN NNS CD (a) (b) ROOT Distance SˆROOT SˆROOT 7: An error resolved with the (a) incorrect baseline parse and (b) the correct way as all other (usually as but do tend to have different prepositions below A second kind of information in the original trees is the presence of empty elements.Following (1999), the annotation nodes which have an empty subject (i.e., raising and constructions).This brought 82.28%.7 Head Annotation The notion that the head word of a constituent can affect its behavior is a useful one.However, often the head tag is as good (or better) an indicator of how constituent will We found several head annotations to be particularly effective.First, poshave a very different distribution than – in particular, are only used in the treebank when the leftmost child is possessive (as opposed to other imaginable uses like for York which is left flat).To address this, all possessive This brought total 83.06%.Second, the is very overloaded in the Penn treebank, most severely in that there is no distinction between finite and in- An example of the damage this conflation can do is given in figure 7, where one needs to capture the fact that present-tense verbs do not take bare infinitive To allow the finite/non-finite distinction, and other verb distinctions, all with their head tag, merging all finite forms to a sintag In particular, this also accomplished This was extremely bringing the cumulative 85.72%, 2.66% absolute improvement (more than its solo improvement over the baseline). is part of the explanation of why (Charniak, 2000) finds that early generation of head tags as in (Collins, 1999) is so beneficial.The rest of the benefit is presumably in the availability of the tags for smoothing purposes.Error analysis at this point suggested that many remaining errors were attachment level and conjunction scope.While these kinds of errors are undoubtedly profitable targets for lexical preference, most attachment mistakes were overly high attachments, indicating that the overall right-branching tendency of English was not being captured.Indeed, this tenis a difficult trend to capture in a because often the high and low attachments involve the very same rules.Even if not, attachment height is modeled by a it is somehow explicitly encoded into category labels.More complex parsing models have indirectly overcome this by modeling distance (rather than height). distance is difficult to encode in a – marking nodes with the size of their yields masmultiplies the state Therefore, we wish to find indirect indicators that distinguish high from low ones.In the case of two a with the question of whether the a second modifier of the leftmost should attach lower, inside the first the important distinction is usually that the lower site is a base Collins (1999) captures this by introducing the notion of a base in any dominates only preterminals is with a Further, if an not have non-base it is given one with a unary production.This was helpful, but substantially less than marking base the unary, whose presence actually erased a useful indicator – base are more frequent in subject position than object position, for example.In isolation, the Collins method actually hurt the base- (absolute cost to 0.37%), while skipping the unary insertion added an absolute 0.73% to the and brought the cumulative 86.04%. the case of attachment of a an eiabove or inside a relative clause, the high is distinct from the low one in that the already modified one contains a verb (and the low one may be base well).This is a partial explanation of the utility of verbal distance in Collins (1999).To inability to encode distance naturally in a naive somewhat ironic.In the heart of any the fundamental table entry or chart item is a label over a span, for exan position 0 to position 5.The concrete use of a grammar rule is to take two adjacent span-marked labels and them (for example and into Yet, only the labels are used to score the combination.“ DT “ NPˆS VBZ VPˆVP VPˆS !.” ” “ “ NPˆS DT NPˆVP ” .!” VPˆS-VBF VBZ (a) (b) NPˆVP buying This is This NN NN panic buying LP LR Exact CB 0 CB Magerman (1995) 84.9 84.6 1.26 56.6 Collins (1996) 86.3 85.8 1.14 59.9 this paper 86.9 85.7 86.3 30.9 1.10 60.3 Charniak (1997) 87.4 87.5 1.00 62.1 Collins (1999) 88.7 88.6 0.90 67.1 Figure 8: Results of the final model on the test set (section 23). this, all nodes which any verbal node with a This the cumulative 86.91%.We also tried marking nodes which dominated prepositions and/or conjunctions, but these features did not help the cumulative hill-climb.The final distance/depth feature we used was an explicit attempt to model depth, rather than use distance and linear intervention as a proxy.With we marked all which contained their right periphery (i.e., as a rightmost descendant).This captured some further attachment trends, and brought us to a final develop- 87.04%.9 Final Results We took the final model and used it to parse section 23 of the treebank.Figure 8 shows the re- The test set 86.32% for words, already higher than early lexicalized models, though of course lower than the state-of-the-art parsers.10 Conclusion The advantages of unlexicalized grammars are clear enough – easy to estimate, easy to parse with, and timeand space-efficient.However, the dismal performance of basic unannotated unlexicalized grammars has generally rendered those advantages irrelevant.Here, we have shown that, surprisingly, the maximum-likelihood estimate of a compact unlexiparse on par with early lexicalized parsers.We do not want to argue that lexical selection is not a worthwhile component of a state-ofthe-art parser – certain attachments, at least, require it – though perhaps its necessity has been overstated.Rather, we have shown ways to improve parsing, some easier than lexicalization, and others of which are orthogonal to it, and could presumably be used to benefit lexicalized parsers as well.
Table 3: Frequency of parse error types. hancements that can be used to address it.4.1 Analysis by error type and PCFG-enrichment fixes Multilevel VP adjunction errors (Figure 5) are common in models without parent annotation, although even with parent annotation the presence of VP coordination would give multilevel VP adjunction nonzero probability.We address this error by taking advantage of the CTB's principled VP annotation practices, marking adjunction, complementation, and coordination VP levels, which builds the flat adjunction constraint back into the structure of the head daughter.NP-NP modification, depicted in NNM± in Figure 4, was the most common error seen; the greater prevalence of false positives is likely a result of the overall PCFG parsing preference for flatter structures.This type of parse ambiguity is grounded in the semantic ambiguity of compound noun interpretation.This semantic ambiguity exists in English as well, as in the NP-NP modification false positive/negative PNM{+/—} (non-NP) prenominal mod. false positive/negative CRD{H,L}{V,N} incorrect {high/low} coordination attachment of righthand {verbal/nominal} material X/Y incorrect adjunction into X; rect site was Y mistag X/Y category Y mistagged as X that only mistaggings leading to constituentlevel parse errors were tallied.VP (-ADJ) ADVP ADVP VP (-COMP) NP AD AD VV NP 1 1 1 NP i• Itg 4k 4g* A A Fi' positively investigate profession NR NN NR I I I NN VP i4 A * 01 ADVP VP Shanghai customs Chorigm rig office AD ADVP VP I NR NN NR NN i• AD VV NP I I I I i4 A * 01 044' Itg 4k 4g* A A Fi' Shanghai customs Chorigming office positively investigate high-risk profession NP NNM±* Figure 5: Flat (corpus) versus multilevel (incorrect-parse) adjunction.Parenthesized material is category-modification.CP NP I exports NP VP -** NP VP -** VV NP realized I gross exports CRDHN* NP PU NP NP problems CRDLN CP NP NP PU NP unmet • fig_ conditions ë problems VP NP ADVP VP nationwide most long NP VP ADVP VP nationwide -------most long Figure 4: Major parse ambiguities.Starred examples are correct in corpus; alternates are parse errors. string speculator Richard Denthese structures are typically bracketed flat in the ETB, underspecifying the semantic relations relative to the CTB.In CTB parsing, this type of ambiguity is difficult to resolve; different compound NP parses differ in dependency structure, so the dependency model resolves errors when word frequencies are large enough to be reliable, but this is often not possible.We found that the internal distributions of (i) NP modifiers of NPs and (ii) left-modified NPs both differ from the internal distribution of NPs in general; we take advantage of this in the PCFG model by marking both types (i) and (ii), which reduces the bias against NP-NP modification in compound NPs.Prenominal modification errors, illustrated in PNM of Figure 4, are rather infrequent, despite the natural parallel with PP attachment ambiguity in English.Due to the highly articulated structure of prenominal modifiers, it seems difficult to address this problem directly; one measure we found somewhat successful is to mark IP daughters of prenominal modification.Coordination scope errors occured in two major varieties: those where the misattached right conjunct is verbal (a VP or IP), and those where it is nominal the latter case is illustrated in and CRDLN in Figure The equivverbal coordination is generally marked with commas, whereas nominal coordination is marked with conjoiners or the mostly noun-conjoining punctuation mark &quot;, &quot; IP NP heretofore unmet conditions NP IP IP NP VP IP PU IP PU IP NP VP he I I president may VV him say president may he I meet him 120, say Figure 6: Ambiguity between communication verb subcategorization frame (left; corpus) and high coordination attachment (right; incorrect parse). ocal majority of low over high verbal attachment errors contrasts qualitatively with ETB parsing, where low attachment is more common and parsers tend to err toward high attachment.There are two major sources of ambiguous attachment sites: (i) any VP can be parsed as an IP plus a unary IP—NP, so due to pro-drop any VP coordination is ambiguous with a higher IP coordination; (ii) VPs are multilevel, giving rise to ambiguities of scope over adjuncts.It seems that (i) is a difficult problem; in some cases, certain &quot;discourse-level&quot; adverbs such as IP modification and are thus strong indicators of high attachment.To capture this we mark those adverbs possessing an IP grandparent.We address (ii) to some extent by marking VPs as adjunction or complementation structures, as shown before in Figure 5; in training data, only like-type VPs are coordinated.With nominal coordination scope errors, the situation is different: we found no false low attachments.False high scopings can be reduced by marking NP conjuncts.(Charniak, 2000) claims that a similar strategy proved effective for WSJ parsing.
A central problem of word sense disambiguation (WSD) is the lack of manually sense-tagged data required for supervised learning.In this paper, we evaluate an approach to automatically acquire sensetagged training data from English-Chinese parallel corpora, which are then used for disambiguating the nouns in the SENSEVAL-2 English lexical sample task.Our investigation reveals that this method of acquiring sense-tagged data is promising.On a subset of the most difficult SENSEVAL-2 nouns, the accuracy difference between the two approaches is only 14.0%, and the difference could narrow further to 6.5% if we disregard the advantage that manually sense-tagged data have in their sense coverage.Our analysis also highlights the importance of the issue of domain dependence in evaluating WSD programs.
Ordering information is a critical task for natural language generation applications.In this paper we propose an approach to information ordering that is particularly suited for text-to-text generation.We describe a model that learns constraints on sentence order from a corpus of domainspecific texts and an algorithm that yields the most likely order among several alternatives.We evaluate the automatically generated orderings against authored texts from our corpus and against human subjects that are asked to mimic the model’s task.We also assess the appropriateness of such a model for multidocument summarization.
This paper describes a method of detecting grammatical and lexical errors made by Japanese learners of English and other techniques that improve the accuracy of error detection with a limited amount of training data.In this paper, we demonstrate to what extent the proposed methods hold promise by conducting experiments using our learner corpus, which contains information on learners’ errors.
Often one may wish to learn a tree-to-tree mapping, training it on unaligned pairs of trees, or on a mixture of trees and strings.Unlike previous statistical formalisms (limited to isomorphic TSG local distortion of the tree topology.We reformulate it to permit dependency trees, and sketch EM/Viterbi algorithms for alignment, training, and decoding.
This paper describes a noisy channel model of speech repairs, which can identify and correct repairs in speech transcripts.A syntactic parser is used as the source model, and a novel type of TAG-based transducer is the channel model.The use of TAG is motivated by the intuition that the reparandum is a “rough copy” of the repair.The model is trained and tested on the Switchboard disfluency-annotated corpus.
Discriminative methods have shown significant improvements over traditional generative methods in many machine learning applications, but there has been difficulty in extending them to natural language parsing.One problem is that much of the work on discriminative methods conflates changes to the learning method with changes to the parameterization of the problem.We show how a parser can be trained with a discriminative learning method while still parameterizing the problem according to a generative probability model.We present three methods for training a neural network to estimate the probabilities for a statistical parser, one generative, one discriminative, and one where the probability model is generative but the training criteria is discriminative.The latter model outperforms the previous two, achieving state-ofthe-art levels of performance (90.1% F-measure on constituents).
This paper describes and evaluates log-linear parsing models for Combinatory Categorial A parallel implementation of algorithm is described, which runs on a Beowulf cluster allowing the complete Penn Treebank to be used for estimation.We also develop a new efficient parsing for maximises expected recall of dependencies.We compare models use all including nonstandard derivations, with normal-form models.The performances of the two models are comparable and the results are competitive with ex
This paper describes an incremental parsing approach where parameters are estimated using a variant of the perceptron algorithm.A beam-search algorithm is used during both training and decoding phases of the method.The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn treebank.We demonstrate that training a perceptron model to combine with the generative model during search provides a 2.1 percent F-measure improvement over the generative model alone, to 88.8 percent.
This paper proposes a new approach for resolution which uses the tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the Bell tree to the leaf nodes.A Maximum Entropy model is used to rank these paths.The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported.We also train a coreference system using the MUC6 data and competitive results are obtained.
Most foreign names are transliterated into Chinese, Japanese or Korean with approximate phonetic equivalents.The transliteration is usually achieved through intermediate phonemic mapping.This paper presents a new framework that allows direct orthographical mapping (DOM) between two different languages, through a joint source-channel model, also transliteration model (TM). the TM model, we automate the orthographic alignment process to derive the aligned transliteration units from bilingual dictionary.The TM under the DOM framework greatly reduces system development effort and provides a quantum leap in improvement in transliteration accuracy over that of other state-of-the-art machine learning algorithms.The modeling framework is validated through several experiments for English-Chinese language pair.
analysis to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as “thumbs up” “thumbs down”.To determine this powe propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document.Extracting these portions can be implemented using efficient for finding cuts in this greatly facilitates incorporation of cross-sentence contextual constraints.
This paper shows how finite approximations of long distance dependency (LDD) resolution can be obtained automatically for wide-coverage, robust, probabilistic Lexical-Functional Grammar (LFG) resources acquired from treebanks.We extract LFG subcategorisation frames and paths linking LDDreentrancies from f-structures generated automati cally for the Penn-II treebank trees and use them in an LDD resolution algorithm to parse new text.Unlike (Collins, 1999; Johnson, 2002), in our ap proach resolution of LDDs is done at f-structure (attribute-value structure representations of basicpredicate-argument or dependency structure) with out empty productions, traces and coindexation in CFG parse trees.Currently our best automaticallyinduced grammars achieve 80.97% f-score for f structures parsing section 23 of the WSJ part of the Penn-II treebank and evaluating against the DCU1051 and 80.24% against the PARC 700 Depen dency Bank (King et al, 2003), performing at the same or a slightly better level than state-of-the-art hand-crafted grammars (Kaplan et al, 2004).
Discovering the significant relations embedded in documents would be very useful not only for information retrieval but also for question answering and summarization.Prior methods for relation discovery, however, needed large annotated corpora which cost a great deal of time and effort.We propose an unsupervised method for relation discovery from large corpora.The key idea is clustering pairs of named entities according to the similarity of context words intervening between the named entities.Our experiments using one year of newspapers reveals not only that the relations among named entities could be detected with high recall and precision, but also that appropriate labels could be automatically provided for the relations.
We extend previous work on tree kernels to estimate the similarity between the dependency trees of sentences.Using this kernel within a Support Vector Machine, we detect and classify relations between entities in the Automatic Content Extraction (ACE) corpus of news articles.We examine the utility of different features such as Wordnet hypernyms, parts of speech, and entity types, and find that the dependency tree kernel achieves a 20% F1 improvement over a “bag-of-words” kernel.
Most information extraction (IE) systems treat separate potential extractions as independent.However, in many cases, considering influences potential extractions could improve overall accuracy.Statistical methods on models, such as random fields have been shown to be an effective approach to learning accurate IE systems.We present a new IE method that employs Relational Markov Networks (a generalization of CRFs), which can represent arbitrary dependencies between extractions.This allows for &quot;collective information extraction&quot; that exploits the mutual influence between possible extractions.Experiments on learning to extract protein names from biomedical text demonstrate the advantages of this approach.
present a generative model for the learning of dependency structures.We also describe the multiplicative combination of this dependency model with a model of linear constituency.The product model outperforms both components on their respective evaluation metrics, giving the best published figures for undependency parsing constituency parsing.We also demonstrate that the combined model works and is robust cross-linguistically, being able to exploit either attachment or distributional regularities that are salient in the data.
We investigate a number of simple methods for improving the word-alignment accuracy of IBM Model 1.We demonstrate reduction in alignment error rate of approximately 30% resulting from (1) giving extra weight to the probability of alignment to the null word, (2) smoothing probability estimates for rare words, and (3) using a simple heuristic estimation method to initialize, or replace, EM training of model parameters.
GTM30 0.87 0.81 0.91 0.79 0.67 0.90 0.83 0.77 0.87 0.73 0.62 0.83 0.83 0.77 0.88 0.71 0.60 0.83 1.Pearson’s Spearman’s of automatic evaluation measures vs. 4, and 12 are maximum of 1, 4, and 12 grams, NIST is the NIST ROUGE-L is LCS-based F-measure 1), ROUGE-W is weighted LCS-based F-measure = 1).ROUGE-S* is skip-bigram-based co-occurrence statistics with any skip distance limit, ROUGEis skip-bigram-based F-measure 1) with maximum skip distance of N, PER is position independent word error rate, and WER is word error rate.GTM 10, 20, and 30 are general text matcher exponents of 1.0, 2.0, and 3.0.(Note, only 4, and 12 are shown here to preserve space.) limit and with skip distant limits of 0, 4, and 9.Correlation analysis based on two different correlastatistics, Pearson’s Spearman’s with respect to adequacy and fluency are shown in Table 1.Pearson’s correlation measures the and direction of a between any two variables, i.e. automatic metric score and human assigned mean coverage score in our case.It ranges from +1 to -1.A correlation of 1 means that there is a perfect positive linear relationship between the two variables, a correlation of -1 means that there is a perfect negative linear relationship between them, and a correlation of 0 means that there is no linear relationship between them.Since we would like to use automatic evaluation metric not only in comparing systems a quick overview of the Pearson’s coefficient, see: http://davidmlane.com/hyperstat/A34739.html. but also in in-house system development, a good linear correlation with human judgment would enable us to use automatic scores to predict corresponding human judgment scores.Therefore, Pearson’s correlation coefficient is a good measure to look at. correlation coefficient 6is also a measure of correlation between two variables.It is a non-parametric measure and is a special case of the Pearson’s correlation coefficient when the values of data are converted into ranks before computing the coefficient.Spearman’s correlation coefficient does not assume the correlation between the variables is linear.Therefore it is a useful correlation indicator even when good linear correlation, for example, according to Pearson’s correlation coefficient between two variables could a quick overview of the Spearman’s coefficient, see: http://davidmlane.com/hyperstat/A62436.html. not be found.It also suits the NIST MT evaluation scenario where multiple systems are ranked according to some performance metrics.To estimate the significance of these correlation statistics, we applied bootstrap resampling, generating random samples of the 919 different sentence segments.The lower and upper values of 95% confidence interval are also shown in the table.Dark (green) cells are the best correlation numbers in their categories and light gray cells are statistically equivalent to the best numbers in their categories.Analyzing all runs according to the adequacy and fluency table, we make the following observations: Applying the stemmer achieves higher correlation with adequacy but keeping case information achieves higher correlation with fluency except for (only is shown).For example, Pearson’s correlation of ROUGE-S* with adequacy increases from 0.85 (Case) to 0.95 while its Pearson’s with fluency drops from 0.84 (Case) to 0.78 (Stem).We will focus our discussions on the Stem set in adequacy and Case set in fluency.Pearson's values in the Stem set of the Adequacy Table, indicates that ROUGE- L and ROUGE-S with a skip distance longer than 0 correlate highly and linearly with adequacy and NIST.ROUGE-S* achieves best correlation with a Pearson’s 0.95.Measures favoring consecutive matches, i.e. and 12, ROUGE-W, GTM20 and 30, ROUGE-S0 (bigram), and WER have lower Pear- Among them WER (0.48) that tends to penalize small word movement is the worst performer.One interesting observation is that longer lower correlation with adequacy. generally agree with Pearhave more equivalents.Pearson's values in the Stem of the Fluency Table, indicates that has the highest correlation (0.93) with fluency.However, it is statistically indistinguishable with 95% confidence from all other metrics shown in the Case set of the Fluency Table except for WER and GTM10.GTM10 has good correlation with human judgments in adequacy but not fluency; while GTM20 and GTM30, i.e.GTM with exponent larger than 1.0, has good correlation with human judgment in fluency but not adequacy.ROUGE-L and ROUGE-S*, 4, and 9 are good automatic evaluation metric candidates since they as well as fluency correlation and outperform and 12 significantly in adequacy.Among them, ROUGE-L is the best metric in both adequacy and fluency correlation with human judgment according to Spearman’s correlation coefficient and is statistically indistinguishable from the best metrics in both adequacy and fluency correlation with human judgment according to Pearson’s correlation coefficient.In this paper we presented two new objective automatic evaluation methods for machine translation, ROUGE-L based on longest common subsequence (LCS) statistics between a candidate translation and a set of reference translations.Longest common subsequence takes into account sentence level structure similarity naturally and identifies longest co-occurring in-sequence ngrams automatically while this is a free parameter To give proper credit to shorter common sequences that are ignored by LCS but still retain the flexibility of non-consecutive matches, we proposed counting skip bigram co-occurrence.The skip-bigram-based ROUGE-S* (without skip disrestriction) had the best Pearson's correlation of 0.95 in adequacy when all words were lower case and stemmed.ROUGE-L, ROUGE-W, ROUGE-S*, ROUGE-S4, and ROUGE-S9 were performers to measuring fluency.However, they have the advantage that we can apthem on sentence level while longer would not differentiate any sentences with length shorter than 12 words (i.e. no 12-gram matches).We plan to explore their correlation with human judgments on sentence-level in the future.We also confirmed empirically that adequacy and fluency focused on different aspects of machine translations.Adequacy placed more emphasis on terms co-occurred in candidate and reference translations as shown in the higher correlations in Stem set than Case set in Table 1; while the reverse was true in the terms of fluency.The evaluation results of ROUGE-L, ROUGE- W, and ROUGE-S in machine translation evaluation are very encouraging.However, these measures in their current forms are still only applying string-to-string matching.We have shown that better correlation with adequacy can be reached by applying stemmer.In the next step, we plan to extend them to accommodate synonyms and paraphrases.For example, we can use an existing thesaurus such as WordNet (Miller 1990) or creating a customized one by applying automated synonym set discovery methods (Pantel and Lin 2002) to identify potential synonyms.Paraphrases can also be automatically acquired using statistical methods as shown by Barzilay and Lee (2003).Once we have acquired synonym and paraphrase data, we then need to design a soft matching function that assigns partial credits to these approximate matches.In this scenario, statistically generated data has the advantage of being able to provide scores reflecting the strength of similarity between synonyms and paraphrased.ROUGE-L, ROUGE-W, and ROUGE-S have also been applied in automatic evaluation of summarization and achieved very promising results (Lin 2004).In Lin and Och (2004), we proposed a framework that automatically evaluated automatic MT evaluation metrics using only manual translations without further human involvement.According to the results reported in that paper, ROUGE-L, ROUGE-W, and ROUGE-S also outperformed NIST.
In an ordinary syntactic parser, the input is a string, and the grammar ranges over strings.This paper explores generalizations of ordinary parsing algorithms that allow the input to consist of string tuples and/or the grammar to range over string tuples.Such algorithms can infer the synchronous structures hidden in parallel texts.It turns out that these generalized parsers can do most of the work required to train and apply a syntax-aware statistical machine translation system.
We describe a statistical approach for modeling agreements and disagreements in conversational interaction.Our approach first identifies adjacency pairs using maximum entropy ranking based on a set of lexical, durational, and structural features that look both forward and backward in the discourse.We then classify utterances as agreement or disagreement using these adjacency pairs and features that represent various pragmatic influences of previous agreement or disagreement on the current utterance.Our approach achieves 86.9% accuracy, a 4.9% increase over previous work.
Extracting semantic relationships between entities is challenging because of a paucity of annotated data and the errors induced by entity detection modules.We employ Maximum Entropy models to combine diverse lexical, syntactic and semantic features derived from the text.Our system obtained competitive results in the Automatic Content Extraction (ACE) evaluation.Here we present our general approach and describe our ACE results.
In machine learning, whether one can build a more accurate classifier by using data is an important issue.Although a number of semi-supervised methods have been proposed, their effectiveness on NLP tasks is not always clear.This paper presents a novel semi-supervised method that employs a learning paradigm which we call The idea is to find “what good classifiers are like” by learning from thousands of automatically generated auxiliary classification problems on unlabeled data.By doing so, the common predictive structure shared by the multiple classification problems can be discovered, which can then be used to improve performance on the target problem.The method produces performance higher than the previous best results on CoNLL’00 syntactic chunking and CoNLL’03 named entity chunking (English and German).
This paper defines a generative probabilistic model of parse trees, which we call PCFG-LA.This model is an extension of PCFG in which non-terminal symbols are augmented with latent variables.Finegrained CFG rules are automatically induced from a parsed corpus by training a PCFG-LA model using an EM-algorithm.Because exact parsing with a PCFG-LA is NP-hard, several approximations are described and empirically compared.In experiments using the Penn WSJ corpus, our automatically trained model gave a performance of 86.6% (F, sentences 40 words), which is comparable to that of an unlexicalized PCFG parser created using extensive manual feature selection.
This paper reports the development of loglinear models for the disambiguation in wide-coverage HPSG parsing.The estimation of log-linear models requires high computational cost, especially with widecoverage grammars.Using techniques to reduce the estimation cost, we trained the models using 20 sections of Penn Treebank.A series of experiments empirically evaluated the estimation techniques, and also examined the performance of the disambiguation models on the parsing of real-world sentences.
We present an effective training algorithm for linearly-scored dependency parsers that implements online largemargin multi-class training (Crammer and Singer, 2003; Crammer et al., 2003) on top of efficient parsing techniques for dependency trees (Eisner, 1996).The trained parsers achieve a competitive dependency accuracy for both English and Czech with no language specific enhancements.
address the wherein rather than simply decide whether a review is “thumbs up” or “thumbs down”, as in previous sentiment analysis work, one must determine an author’s evaluation with respect to a multi-point scale (e.g., one to five “stars”).This task represents an interesting twist on standard multi-class text categorization because there are several different degrees of similarity between class labels; for example, “three stars” is intuitively closer to “four stars” than to “one star”.We first evaluate human performance at task.Then, we apply a based on a labeling formulation of the problem, that alters a given-ary classifier’s output in an explicit attempt to ensure that similar items receive similar labels.We show that the meta-algorithm can provide significant improvements over both multi-class and regression versions of SVMs when we employ a novel similarity measure appropriate to the problem.
We propose a method for extracting semantic orientations of words: desirable or undesirable.Regarding semantic orientations as spins of electrons, we use the mean field approximation to compute the approximate probability function of the system instead of the intractable actual probability function.We also propose a criterion for parameter selection on the basis of magnetization.Given only a small number of seed words, the proposed method extracts semantic orientations with high accuracy in the experiments on English lexicon.The result is comparable to the best value ever reported.
This paper considers the problem of automatic assessment of local coherence.We present a novel entity-based representation of discourse which is inspired by Centering Theory and can be computed automatically from raw text.We view coherence assessment as a ranking learning problem and show that the proposed discourse representation supports the effective learning of a ranking function.Our experiments demonstrate that the induced model achieves significantly higher accuracy than a state-of-the-art coherence model.
In this paper, we view coreference resolution as a problem of ranking candidate partitions generated by different coreference systems.We propose a set of partition-based features to learn a ranking model for distinguishing good and bad partitions.Our approach compares favorably to two state-of-the-art coreference systems when evaluated on three standard coreference data sets.
Discriminative reranking is one method for constructing high-performance statistical parsers (Collins, 2000).A discriminative reranker requires a source of candidate parses for each sentence.This paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse-to-fine generative parser (Charniak, 2000).This method generates 50-best lists that are of substantially higher quality than previously obtainable.We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less.
We present a statistical phrase-based translamodel that uses phrases that contain subphrases.The model is formally a synchronous context-free grammar but is learned from a bitext without any syntactic information.Thus it can be seen as shift to the of syntaxtranslation systems without any lin- In our experiments using BLEU as a metric, the hierarchical phrasebased model achieves a relative improvement of 7.5% over Pharaoh, a state-of-the-art phrase-based system.
Summarization - Step Sentence Knight and Marcu (Knight and Marcu, 2000) (K&M) present a noisy-channel model for sentence compression.The main difficulty in using this method is the lack of data; Knight and Marcu use a corpus of 1035 training sentences.More data is not easily available, so in addition to improving the original K&M noisy-channel model, we create unsupervised and semi-supervised models of the task.Finally, we point out problems with modeling the task in this way.They suggest areas for future research.
Conditional random fields (Lafferty et al., 2001) are quite effective at sequence labeling tasks like shallow parsing (Sha and Pereira, 2003) and namedentity extraction (McCallum and Li, 2003).CRFs allowing the incorporation of arbifeatures into the model.To train on we require methods for log-linear models; few exist.We describe a novel We show that the new technique can be intuitively understood as exnegative evidence is computationally efficient.Applied to a sequence labeling problem—POS tagging given a tagging dictionary and unlabeled text—contrastive estimation outperforms EM (with the same feature set), is more robust to degradations of the dictionary, and can largely recover by modeling additional features.
Most current statistical natural language processing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use.We how to solve this dilemma with sama simple Monte Carlo method used to perform approximate inference in factored probabilistic models.By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference.We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints.This technique results in an error reduction of up to 9% over state-of-the-art systems on two established information extraction tasks.
This paper presents a novel algorithm for the acquisition of Information Extraction patterns.The approach makes the assumption that useful patterns will have similar meanings to those already identified as relevant.Patterns are compared using a variation of the standard vector space model in which information from an ontology is used to capture semantic similarity.Evaluation shows this algorithm performs well when compared with a previously reported document-centric approach.
We present a framework for word alignment based on log-linear models.All knowledge sources are treated as feature functions, which depend on the source langauge sentence, the target language sentence and possible additional variables.Log-linear models allow statistical alignment models to be easily extended by incorporating syntactic information.In this paper, we use IBM Model 3 alignment probabilities, POS correspondence, and bilingual dictionary coverage as features.Our experiments show that log-linear models significantly outperform IBM translation models.
We present a version of Inversion Transduction Grammar where rule probabilities are lexicalized throughout the synchronous parse tree, along with pruning techniques for efficient training.Alignment results improve over unlexicalized ITG on short sentences for which full EM is feasible, but pruning seems to have a negative impact on longer sentences.
Reading proficiency is a fundamental component of language competency.However, finding topical texts at an appropriate reading level for foreign and second language learners is a challenge for teachers.This task can be addressed with natural language processing technology to assess reading level.Existing measures of reading level are not well suited to this task, but previous work and our own pilot experiments have shown the benefit of using statistical language models.In this paper, we also use support vector machines to combine features from traditional reading level measures, statistical language models, and other language processing tools to produce a better method of assessing reading level.
We describe a method for incorporating syntactic information in statistical machine translation systems.The first step of the method is to parse the source language string that is being translated.The second step is to apply a series of transformations to the parse tree, effectively reordering the surface string on the source language side of the translation system.The goal of this step is to recover an underlying word order that is closer to the target language word-order than the original string.The reordering approach is applied as a pre-processing step in both the training and decoding phases of a phrase-based statistical MT system.We describe experiments on translation from German to English, showing an improvement from 25.2% Bleu score for a baseline system to 26.8% Bleu score for the system with reordering, a statistically significant improvement.
Syntax-based statistical machine translation (MT) aims at applying statistical models to structured data.In this paper, we present a syntax-based statistical matranslation system based on a probabilistic synchronous dependency insertion grammar.Synchronous dependency insertion grammars are a version of synchronous grammars defined on dependency trees.We first introduce our approach to inducing such a grammar from parallel corpora.Second, we describe the graphical model for the machine translation task, which can also be viewed as a stochastic tree-to-tree transducer.We introduce a polynomial time decoding algorithm for the model.We evaluate the outputs of our MT system using the NIST and Bleu automatic MT evaluation software.The result shows that our system outperforms the baseline system based on the IBM models in both translation speed and quality.
Semantic role labeling is the process of annotating the predicate-argument structure in text with semantic labels.In this paper we present a state-of-the-art baseline semantic role labeling system based on Support Vector Machine classifiers.We show improvements on this system by: i) adding new features including features extracted from dependency parses, ii) performing feature selection and calibration and iii) combining parses obtained from semantic parsers trained using different syntactic views.Error analysis of the baseline system showed that approximately half of the argument identification errors resulted from parse errors in which there was no syntactic constituent that aligned with the correct argument.In order to address this problem, we combined semantic parses from a Minipar syntactic parse and from a chunked syntactic representation with our original baseline system which was based on Charniak parses.All of the reported techniques resulted in performance improvements.
Despite much recent progress on accurate semantic role labeling, previous work has largely used independent classifiers, possibly combined with separate label sequence models via Viterbi decoding.This stands in stark contrast to the linguistic observation that a core argument frame is with strong dependencies between arguments.We show how to build a joint model of argument frames, incorporating novel features that model these interactions into discriminative loglinear models.This system achieves an reduction of all arguments core arguments over a stateof-the art independent classifier for goldstandard parse trees on PropBank.
Previous work has used monolingual parallel corpora to extract and generate paraphrases.We show that this task can be done using bilingual parallel corpora, a much more commonly available resource.Using alignment techniques from phrasebased statistical machine translation, we show how paraphrases in one language can be identified using a phrase in another language as a pivot.We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities, and show how it can be refined to take contextual information into account.We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments, and contrast the quality with paraphrases extracted from automatic alignments.
In this paper, we explore the power of randomized algorithm to address the challenge of working with very large amounts of data.We apply these algorithms to generate noun similarity lists from 70 million pages.We reduce the running time from quadratic to practically linear in the number of elements to be computed.
Sentiment Classification seeks to identify a piece of text according to its author’s general feeling toward their subject, be it positive or negative.Traditional machine learning techniques have been applied to this problem with reasonable success, but they have been shown to work well only when there is a good match between the training and test data with respect to topic.This paper demonstrates that match with respect to domain and time is also important, and presents preliminary experiments with training data labeled with emoticons, which has the potential of being independent of domain, topic and time.
We describe a new approach for synthetically combining the output of several different Machine Translation (MT) engines operating on the same input.The goal is to produce a synthetic combination that surpasses all of the original systems in translation quality.Our approach uses the individual MT engines as “black boxes” and does not require any explicit cooperation from the original MT systems.A decoding algorithm uses explicit word matches, in conjunction with confidence estimates for the various engines and a trigram language model in order to score and rank a collection of sentence hypotheses that are synthetic combinations of words from the various original engines.The highest scoring sentence hypothesis is selected as the final output of our system.Experiments, using several Arabicto-English systems of similar quality, show a substantial improvement in the quality of the translation output.
We consider the task of unsupervised lecture segmentation.We formalize segmentation as a graph-partitioning task that optimizes the normalized cut criterion.Our approach moves beyond localized comparisons and takes into account longrange cohesion dependencies.Our results demonstrate that global analysis improves the segmentation accuracy and is robust in the presence of speech recognition errors.
We present an approach to pronoun resolution based on syntactic paths.Through a simple bootstrapping procedure, we learn the likelihood of coreference between a pronoun and a candidate noun based on the path in the parse tree between the two entities.This path information enables us to handle previously challenging resolution instances, and also robustly addresses traditional syntactic coreference constraints.Highly coreferent paths also allow mining of precise probabilistic gender/number information.We combine statistical knowledge with well known features in a Support Vector Machine pronoun resolution classifier.Significant gains in performance are observed on several datasets.
In this paper we present a novel approach for inducing word alignments from sentence aligned data.We use a Conditional Random Field (CRF), a discriminative model, which is estimated on a small supervised training set.The CRF is conditioned on both the source and target texts, and thus allows for the use of arbitrary and overlapping features over these data.Moreover, the CRF has efficient training and decoding processes which both find globally optimal solutions.We apply this alignment model to both French-English and Romanian-English language pairs.We show how a large number of highly predictive features can be easily incorporated into the CRF, and demonstrate that even with only a few hundred word-aligned training sentences, our model improves over the current state-ofthe-art with alignment error rates of 5.29 and 25.8 for the two tasks respectively.
In this paper we investigate Chinesename transliteration using compacorpora where texts in the two languages deal in some of the same topics — and therefore share references to named entities — but are not translations of each other.We present two distinct methods for transliteration, one approach using phonetic transliteration, and the second using the temporal distribution of candidate pairs.Each of these approaches works quite well, but by combining the approaches one can achieve even better results.We then propose a novel score propagation method that utilizes the co-occurrence of transliteration pairs within document pairs.This propagation method achieves further improvement over the best results from the previous step.
We present a novel method for extracting parallel sub-sentential fragments from comparable, non-parallel bilingual corpora.By analyzing potentially similar sentence pairs using a signal processinginspired approach, we detect which segments of the source sentence are translated into segments in the target sentence, and which are not.This method enables us to extract useful machine translation training data even from very non-parallel corpora, which contain no parallel sentence pairs.We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system.
Fine-grained sense distinctions are one of the major obstacles to successful Word Sense Disambiguation.In this paper, we present a method for reducing the granularity of the WordNet sense inventory based on the mapping to a manually crafted dictionary encoding sense hierarchies, namely the Oxford Dictionary of English.We assess the quality of the mapping and the induced clustering, and evaluate the performance of coarse WSD systems in the Senseval-3 English all-words task.
this paper, we present a weakly-supervised, general-purpose, and accurate algorithm for harvesting semantic relations.The main contributions are: i) a method for exploiting generic patterns by filtering incorrect instances using the Web; and ii) a principled measure of pattern and instance reliability enabling the filtering algorithm.We present an empirical comof various state of the art systems, on different size and genre corpora, on extracting various general and specific relations.Experimental results show that our exploitation of generic patterns substantially increases system recall with small effect on overall precision.
This paper presents a pilot study of the use of phrasal Statistical Machine Translation (SMT) techniques to identify and correct writing errors made by learners of English as a Second Language (ESL).Using examples of mass noun errors in the Learner Error Cor- (CLEC) guide creation of an engineered training set, we show that application of the SMT paradigm can capture errors not well addressed by widely-used proofing tools designed for native speakers.Our system was able to correct 61.81% of mistakes in a set of naturallyoccurring examples of mass noun errors found on the World Wide Web, suggesting that efforts to collect alignable corpora of preand post-editing ESL writing samples offer can enable the development of SMT-based writing assistance tools capable of repairing many of the complex syntactic and lexical problems found in the writing of ESL learners.
We present a novel approach for discovering word categories, sets of words sharing a significant aspect of their meaning.We utilize meta-patterns of highfrequency words and content words in order to discover pattern candidates.Symmetric patterns are then identified using graph-based measures, and word categories are created based on graph clique sets.Our method is the first pattern-based method that requires no corpus annotation or manually provided seed patterns or words.We evaluate our algorithm on very large corpora in two languages, using both human judgments and WordNetbased evaluation.Our fully unsupervised results are superior to previous work that used a POS tagged corpus, and computation time for huge corpora are orders of magnitude faster than previously reported.
Statistical parsers trained and tested on the Wall Street Journal treebank have shown vast improvements over the last 10 years.Much of this improvement, however, is based upon an ever-increasing number of features to be trained on (typithe data.This has led to concern that such parsers may be too finely tuned to this corpus at the expense of portability to other genres.Such worries have merit.The standard “Charniak parser” checks in at a labeled precisionof 89.7% on the Penn set, but only 82.9% on the test set from the Brown treebank corpus.This paper should allay these fears.In particular, we show that the reranking parser described in Charniak and Johnson (2005) improves performance of the parser on Brown to 85.2%.Furthermore, use of the self-training techniques described in (Mc- Closky et al., 2006) raise this to 87.8% (an error reduction of 28%) again without any use of labeled Brown data.This is remarkable since training the parser and reranker on labeled Brown data achieves only 88.4%.
We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank.Starting with a simple Xbar grammar, we learn a new grammar whose nonterminals are subsymbols of the original nonterminals.In contrast with previous work, we are able to split various terminals to different degrees, as appropriate to the actual complexity in the data.Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation.On the other hand, our grammars are much more compact and substantially more accurate than previous work on automatic annotation.Despite its simplicity, our best grammar achieves 90.2% on the Penn Treebank, higher than fully lexicalized systems.
We propose a novel reordering model for phrase-based statistical machine translation (SMT) that uses a maximum entropy (MaxEnt) model to predicate reorderings of neighbor blocks (phrase pairs).The model provides content-dependent, hierarchical phrasal reordering with generalization based on features automatically learned from a real-world bitext.We present an algorithm to extract all reordering events of neighbor blocks from bilingual data.In our experiments on Chineseto-English translation, this MaxEnt-based reordering model obtains significant improvements in BLEU score on the NIST MT-05 and IWSLT-04 tasks.
In this paper, we argue that n-gram language models are not sufficient to address word reordering required for Machine Translation.We propose a new distortion model that can be used with existing phrase-based SMT decoders to address those n-gram language model limitations.We present empirical results in Arabic to English Machine Translation that show statistically significant improvements when our proposed model is used.We also propose a novel metric to measure word order similarity (or difference) between any pair of languages based on word alignments.
first show how a structural bias improve the accuracy of state-of-the-art dependency grammar induction models trained by EM from unannotated examples (Klein and Manning, 2004).Next, by annealing the free parameter that controls this bias, we achieve further improvements.We then describe an alternative kind of structural bias, toward “broken” hypotheses consisting of partial structures over segmented sentences, and show a similar pattern of improvement.We relate this approach to contrastive estimation (Smith and Eisner, 2005a), apply the latter to grammar induction in six languages, and show that our new approach improves accuracy by 1–17% (absolute) over CE (and 8–30% over EM), achieving to our knowledge the best results on this to date.Our method, is a general technique with broad applicability to hidden-structure discovery problems.
We present a novel translation model on alignment template (TAT) which describes the alignment between a source parse tree and a target string.A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels.The model is linguistically syntaxbased because TATs are extracted automatically from word-aligned, source side parsed parallel texts.To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string.Our experiments show that the TAT-based model significantly outperforms Pharaoh, a state-of-the-art decoder for phrase-based models.
Morphological disambiguation is the process of assigning one set of morphological features to each individual word in a text.When the word is ambiguous (there are several possible analyses for the word), a disambiguation procedure based on the word context must be applied.This paper deals with morphological disambiguation of the Hebrew language, which combines morphemes into a word in both agglutinative and fusional ways.We present an unsupervised stochastic model – the only resource we use is a morphological analyzer – which deals with the data sparseness problem caused by the affixational morphology of the Hebrew language.We present a text encoding method for languages with affixational morphology in which the knowledge of word formation rules (which are quite restricted in Hebrew) helps in the disambiguation.We adapt HMM algorithms for learning and searching this text representation, in such a way that segmentation and tagging can be learned in parallel in one step.Results on a large scale evaluation indicate that this learning improves disambiguation for complex tag sets.Our method is applicable to other languages with affix morphology.
This paper presents a novel training algorithm for a linearly-scored block sequence translation model.The key component is a new procedure to directly optimize the global scoring function used by a SMT decoder.No translation, language, or distortion model probabilities are used as in earlier work on SMT.Therefore our method, which employs less domain specific knowledge, is both simpler and more extensible than previous approaches.Moreover, the training procedure treats the decoder as a black-box, and thus can be used to optimize any decoding scheme.The training algorithm is evaluated on a standard Arabic-English translation task.
This paper investigates a machine learning approach for temporally ordering and anchoring events in natural language texts.To address data sparseness, we used temporal reasoning as an oversampling method to dramatically expand the amount of training data, resulting in predictive accuracy on link labeling as high as 93% using a Maximum Entropy classifier on human annotated data.This method compared favorably against a series of increasingly sophisticated baselines involving expansion of rules derived from human intuitions.
We introduce a semi-supervised approach to training for statistical machine translation that alternates the traditional Expectation Maximization step that is applied on a large training corpus with a discriminative step aimed at increasing word-alignment quality on a small, manually word-aligned sub-corpus.We show that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality.
Named Entity recognition (NER) is an important part of many natural language processing tasks.Current approaches often employ machine learning techniques and require supervised data.However, many languages lack such resources.This paper presents an (almost) unsupervised learning algorithm for automatic discovery of Named Entities (NEs) in a resource free language, given a bilingual corpora in which it is weakly temporally aligned with a resource rich language.NEs have similar time distributions across such corpora, and often some of the tokens in a multi-word NE are transliterated.We develop an algorithm that exploits both observations iteratively.The algorithm makes use of a new, frequency based, metric for time distributions and a resource free discriminative approach to transliteration.Seeded with a small number of transliteration pairs, our algorithm discovers multi-word NEs, and takes advantage of a dictionary (if one exists) to account for translated or partially translated NEs.We evaluate the algorithm on an English-Russian corpus, and show high level of NEs discovery in Russian.
This paper proposes a novel composite kernel for relation extraction.The composite kernel consists of two individual kernels: an entity kernel that allows for entity-related features and a convolution parse tree kernel that models syntactic information of relation examples.The motivation of our method is to fully utilize the nice properties of kernel methods to explore diverse knowledge for relation extraction.Our study illustrates that the composite kernel can effectively capture both flat and structured features without the need for extensive feature engineering, and can also easily scale to include more features.Evaluation on the ACE corpus shows that our method outperforms the previous best-reported methods and significantly outperforms previous two dependency tree kernels for relation extraction.
We investigate generalizations of the allsubtrees &quot;DOP&quot; approach to unsupervised parsing.Unsupervised DOP models assign all possible binary trees to a set of sentences and next use (a large random subset of) all subtrees from these binary trees to compute the most probable parse trees.We will test both a relative frequency estimator for unsupervised DOP and a maximum likelihood estimator which is known to be statistically consistent.We report state-ofthe-art results on English (WSJ), German (NEGRA) and Chinese (CTB) data.To the best of our knowledge this is the first paper which tests a maximum likelihood estimator for DOP on the Wall Street Journal, leading the surprising result that unsupervised parsing model beats a widely used supervised model (a treebank PCFG).
Work on the semantics of questions has argued that the relation between a question and its answer(s) can be cast in terms of logical entailment.In this paper, we demonstrate how computational systems to recognize entailment can be used to enhance the accuracy of current open-domain automatic question answering (Q/A) systems.In our experiments, we show that when textual entailment information is used to either filter or rank answers returned by a Q/A system, accuracy can be increased by as much as 20% overall.
We present a new approach for mappingnatural language sentences to their formal meaning representations using string kernel-based classifiers.Our system learns these classifiers for every production in theformal language grammar.Meaning representations for novel natural language sen tences are obtained by finding the most probable semantic parse using these stringclassifiers.Our experiments on two real world data sets show that this approachcompares favorably to other existing sys tems and is particularly robust to noise.
Statistical MT has made great progress in the last few years, but current translation models are weakon re-ordering and target language fluency.Syn tactic approaches seek to remedy these problems.In this paper, we take the framework for acquiring multi-level syntactic translation rules of (Gal ley et al, 2004) from aligned tree-string pairs, and present two main extensions of their approach: first, instead of merely computing a single derivation that minimally explains a sentence pair, we constructa large number of derivations that include contextually richer rules, and account for multiple interpretations of unaligned words.Second, we pro pose probability estimates and a training procedure for weighting these rules.We contrast differentapproaches on real examples, show that our esti mates based on multiple derivations favor phrasal re-orderings that are linguistically better motivated, and establish that our larger rules provide a 3.63 BLEU point increase over minimal rules.
This paper describes a study of the patterns of translational equivalence exhibited by a variety of bitexts.The study found that the complexity of these patterns in every bitext was higher than suggested in the literature.These findings shed new light on why “syntactic” constraints have not helped to improve statistical translation models, including finitestate phrase-based models, tree-to-string models, and tree-to-tree models.The paper also presents evidence that inversion transduction grammars cannot generate some translational equivalence relations, even in relatively simple real bitexts in syntactically similar languages with rigid word order.Instructions for replicating our experiments are at
We propose a new hierarchical Bayesian model of natural languages.Our model makes use of a generalization of the commonly used Dirichlet distributions called Pitman-Yor processes which produce power-law distributions more closely resembling those in natural languages.We show that an approximation to the hierarchical Pitman-Yor language model recovers the exact formulation of interpolated Kneser-Ney, one of the best smoothmethods for language models.Experiments verify that our model gives cross entropy results superior to interpolated Kneser-Ney and comparable to modified Kneser-Ney.
Subjectivity and meaning are both important properties of language.This paper explores their interaction, and brings empirical evidence in support of the hypotheses that (1) subjectivity is a property that can be associated with word senses, and (2) word sense disambiguation can directly benefit from subjectivity annotations.
Jelinek.1991. language modeling speech In A. Waibel and K.F.Lee, editors, Readings in Speech Recognition, pages 450-506.Morgan Kaufmann, 1991 D. Kernighan, K Church and W. Gale.1990. spelling correction program based on a noisy model.Kukich.1992. for automatically corwords in ACM Computing Surveys,
We evaluate the accuracy of an unlexicalized statistical parser, trained on 4K treebanked sentences from balanced data and tested on the PARC DepBank.We demonstrate that a parser which is competitive in accuracy (without sacrificing processing speed) can be quickly tuned without reliance on large in-domain manuallyconstructed treebanks.This makes it more practical to use statistical parsers in applications that need access to aspects of predicate-argument structure.The comparison of systems using DepBank is not straightforward, so we extend and validate DepBank and highlight a number of representation and scoring issues for relational evaluation schemes.
Word alignment methods can gain valuable guidance by ensuring that their alignments maintain cohesion with respect to the phrases specified by a monolingual dependency tree.However, this hard constraint can also rule out correct alignments, and its utility decreases as alignment models become more complex.We use a publicly available structured output SVM to create a max-margin syntactic aligner with a soft cohesion constraint.The resulting aligner is the first, to our knowledge, to use a discriminative learning method to train an ITG bitext parser.
Syntactic parsing requires a fine balance between expressivity and complexity, so that naturally occurring structures can be accurately parsed without compromising efficiency.In dependency-based parsing, several constraints have been proposed that restrict the class of permissible structures, such as projectivity, planarity, multi-planarity, well-nestedness, gap degree, and edge degree.While projectivity is generally taken to be too restrictive for natural language syntax, it is not clear which of the other proposals strikes the best balance between expressivity and complexity.In this paper, we review and compare the different constraints theoretically, and provide an experimental evaluation using data from two treebanks, investigating how large a proportion of the structures found in the treebanks are permitted under different constraints.The results indicate that a combination of the well-nestedness constraint and a parametric constraint on discontinuity gives a very good fit with the linguistic data.
At present, adapting an Information Extraction system to new topics is an expensive and slow process, requiring some knowledge engineering for each new topic.We propose a new paradigm of Information Extraction which operates 'on demand' in response to a user's query.On-demand Information Extraction (ODIE) aims to completely eliminate the customization effort.Given a user’s query, the system will automatically create patterns to extract salient relations in the text of the topic, and build tables from the extracted information using paraphrase discovery technology.It relies on recent advances in pattern discovery, paraphrase discovery, and extended named entity tagging.We report on experimental results in which the system created useful tables for many topics, demonstrating the feasibility of this approach.
10 restarts 1 restart 793 Optimization Procedure labeled dependency acc.[%] Slovenian Bulgarian Dutch Max. like.27.78 47.23 36.78 Min. error 22.52 54.72 36.78 Ann. min. risk 31.16 54.66 36.71 2: dependency accuracy on parsing 200sentence test corpora, after training 10 experts on 1000 senand fitting their weights 200 more.For Slovenian, minimum risk annealing is significantly better than the other training methods, while minimum error is significantly worse.For Bulgarian, both minimum error and annealed minimum risk training achieve significant gains over maximum likelihood, but are indistinguishable from each other.For Dutch, the three methods are indistinguishable. than either and in some cases significantly helped.Note, however, that annealed minimum risk training results in a deterministic classifier just as these other training procedures do.The orthogonal of Bayes risk decoding achieved gains on parsing (Goodman, 1996) and machine translation (Kumar and Byrne, 2004).In speech recognition, researchers have improved decoding by smoothing probability estimates numerically on heldout data in a manner reminiscent of annealing (Goel and Byrne, 2000).We are interested in applying our techniques for approximating nonlinear loss functions to MBR by performing the risk minimization inside the dynamic programming or other decoder.Another training approach that incorporates arloss functions is found in the in the margin-based-learning community (Taskar et al., 2004; Crammer et al., 2004).Like other max-margin techniques, these attempt to make the best hypothesis far away from the inferior ones.The distinction is in using a loss function to calculate the required margins.8 Conclusions Despite the challenging shape of the error surface, we have seen that it is practical to optimize task-specific error measures rather than optimizing likelihood—it produces lower-error systems.Different methods can be used to attempt this global, non-convex optimization.We showed that for MT, and sometimes for dependency parsing, an annealed minimum risk approach to optimization performs significantly better than a previous line-search method that does not smooth the error surface.It never does significantly worse.With such improved methods for minimizing error, we can hope to make better use of task-specific training criteria in NLP.References L. R. Bahl, P. F. Brown, P. V. de Souza, and R. L. Mercer.1988.A new algorithm for the estimation of hidden model parameters.In pages 493–496.E. Charniak and M. Johnson.2005.Coarse-to-fine n-best and maxent discriminative reranking.In pages 173–180.S. F. Chen and R. Rosenfeld.1999.A gaussian prior for smoothing maximum entropy models.Technical report, CS Dept., Carnegie Mellon University.K. Crammer, R. McDonald, and F. Pereira.2004.New large algorithms for structured prediction.In Structured Outputs M. Dreyer, D. A. Smith, and N. A. Smith.2006.Vine parsing and minimum risk reranking for speed and precision.In G. Elidan and N. Friedman.2005.Learning hidden variable The information bottleneck approach.6:81–127.V. Goel and W. J. Byrne.2000.Minimum Bayes-Risk auspeech recognition.Speech and Lan- 14(2):115–135.J. T. Goodman.1996.Parsing algorithms and metrics.In pages 177–183.Hinton.1999.Products of experts.In of volume 1, pages 1–6.K.-U.Hoffgen, H.-U.Simon, and K. S. Van Horn.1995. trainability of single neurons. of Computer and 50(1):114–125.D. S. Johnson and F. P. Preparata.1978.The densest hemiproblem.Comp.6(93–107).S. Katagiri, B.-H. Juang, and C.-H. Lee.1998.Pattern recognition using a family of design algorithms based upon the probabilistic descent method.86(11):2345–2373, November.P. Koehn, F. J. Och, and D. Marcu.2003.Statistical phrasetranslation.In pages 48–54.S. Kumar and W. Byrne.2004.Minimum bayes-risk decodfor statistical machine translation.In J. Lafferty, A. McCallum, and F. C. N. Pereira.2001.Conditional random fields: Probabilistic models for segmenting labeling sequence data.In F. J. Och.2003.Minimum error rate training in statistical translation.In pages 160–167.K. Papineni, S. Roukos, T. Ward, and W.-J.Zhu.2002.A method for automatic evaluation of machine In pages 311–318.K. A. Papineni.1999.Discriminative training via linear In A. Rao and K. Rose.2001.Deterministically annealed deof Hidden Markov Model speech recognizers. on Speech and Audio 9(2):111–126.K. Rose.1998.Deterministic annealing for clustering, compression, classification, regression, and related optimizaproblems.86(11):2210–2239.N. A. Smith and J. Eisner.2004.Annealing techniques for statistical language learning.In pages 486–493.
An unsupervised part-of-speech (POS) tagging system that relies on graph clustering methods is described.Unlike in current state-of-the-art approaches, the kind and number of different tags is generated by the method itself.We compute and merge two partitionings of word graphs: one based on context similarity of high frequency words, another on log-likelihood statistics for words of lower frequencies.Using the resulting word clusters as a lexicon, a Viterbi POS tagger is trained, which is refined by a morphological component.The approach is evaluated on three different languages by measuring agreement with existing taggers.
We describe the new release of the RASP (robust accurate statistical parsing) system, designed for syntactic annotation of free text.The new version includes a revised and more semantically-motivated output representation, an enhanced grammar and part-of-speech tagger lexicon, and a more flexible and semi-supervised training method for the structural parse ranking model.We evaluate the released version on the WSJ using a relational evaluation scheme, and describe how the new release allows users to enhance performance using (in-domain) lexical information.
Extracting tree transducer rules for syntactic MT systems can be hindered by word alignment errors that violate syntactic correspondences.We propose a novel model for unsupervised word alignment which explicitly takes into account target language constituent structure, while retaining the robustness and efficiency of the HMM alignment model.Our model’s predictions improve the yield of a tree transducer extraction system, without sacrificing alignment quality.We also discuss the impact of various posteriorbased methods of reconciling bidirectional alignments.
Statistical machine translation systems are usually trained on large amounts of bilingual text and monolingual text in the target language.In this paper we explore the use of transductive semi-supervised methods for the effective use of monolingual data from the source language in order to improve translation quality.We propose several algorithms with this aim, and present the strengths and weaknesses of each one.We present detailed experimental evaluations on the French–English EuroParl data set and on data from the NIST Chinese–English largedata track.We show a significant improvement in translation quality on both tasks.
Recent research presents conflicting evidence on whether word sense disambiguation (WSD) systems can help to improve the performance of statistical machine translation (MT) systems.In this paper, we successfully integrate a state-of-the-art WSD system into a state-of-the-art hierarchical phrase-based MT system, Hiero.We show for the first time that integrating a WSD system improves the performance of a state-ofthe-art statistical MT system on an actual translation task.Furthermore, the improvement is statistically significant.
When a word sense disambiguation (WSD) system is trained on one domain but applied to a different domain, a drop in accuracy is frequently observed.This highlights the importance of domain adaptation for word sense disambiguation.In this paper, we first show that an active learning approach can be successfully used to perform domain adaptation of WSD systems.Then, by using the predominant sense predicted by expectation-maximization (EM) and adopting a count-merging technique, we improve the effectiveness of the original adaptation process achieved by the basic active learning approach.
Efficient decoding has been a fundamental problem in machine translation, especially with an integrated language model which is essential for achieving good translation quality.We develop faster approaches for problem based on parsing algorithms and demonstrate their effectiveness on both phrase-based and syntax-based MT systems.In both cases, our methods achieve significant speed improvements, often by more than a factor of ten, over the conventional beam-search method at the same levels of search error and translation accuracy.
We propose a new, simple model for the automatic induction of selectional preferences, using corpus-based semantic similarity metrics.Focusing on the task of semantic role labeling, we compute selectional preferences for semantic roles.In evaluations the similarity-based model shows lower error rates than both Resnik's WordNet-based model and the EM-based clustering model, but has coverage problems.
We present a web mining method for discovering and enhancing relationships in which a specified concept (word class) participates.We discover a whole range of relationships focused on the given concept, rather than generic known relationships as in most previous work.Our method is based on clustering patterns that contain concept words and other words related to them.We evaluate the method on three different rich concepts and find that in each case the method generates a broad variety of relationships with good precision.
The Penn Treebank does not annotate base noun phrases committing only to flat structures that ignore the of English This means that tools trained on Treebank data cannot the correct internal structure of This paper details the process of adding gold-standard bracketing within each noun phrase in the Penn Treebank.We then examine the consistency and reliability of our annotations.Finally, we use resource to determine using several statistical approaches, thus demonstrating the utility of the corpus.This adds detail to the Penn Treebank that necessary for many
A key question facing the parsing community is how to compare parsers which use different grammar formalisms and produce different output.Evaluating a parser on the same resource used to create it can lead to non-comparable accuracy scores and an over-optimistic view of parser performance. this paper we evaluate a on DepBank, and demonstrate the difficulties in converting the parser output into Dep- Bank grammatical relations.In addition we present a method for measuring the effectiveness of the conversion, which provides an upper bound on parsing accuracy.The obtains an F-score of 81.9% on labelled dependencies, against an upper of 84.8%.We compare the against the outperformover 5% overall and on the majority of dependency types.
Domain adaptation is an important problem in natural language processing (NLP) due to the lack of labeled data in novel domains.In this paper, we study the domain adaptation problem from the instance weighting perspective.We formally analyze and characterize the domain adaptation problem from a distributional view, and show that there are two distinct needs for adaptation, corresponding to the different distributions of instances and classification functions in the source and the target domains.We then propose a general instance weighting framework for domain adaptation.Our empirical results on three NLP tasks show that incorporating and exploiting more information from the target domain through instance weighting is effective.
Over the last few years, two of the main research directions in machine learning of natural language processing have been the study of semi-supervised learning algorithms as a way to train classifiers when the labeled data is scarce, and the study of ways to exploit knowledge and global information in structured learning tasks.In this paper, we suggest a method for incorporating domain knowledge in semi-supervised learning algorithms.Our novel framework unifies can exploit several kinds of specific The experimental results presented in the information extraction domain demonstrate that applying constraints helps the model to generate better feedback during learning, and hence the framework allows for high performance learning with significantly less training data than was possible before on these tasks.
Until quite recently, extending Phrase-based Statistical Machine Translation (PBSMT) with syntactic structure caused system performance to deteriorate.In this work we show that incorporating lexical syntactic descriptions in the form of supertags can yield significantly better PBSMT systems.We describe a novel PBSMT model that integrates supertags into the target language model and the target side of the translation model.Two kinds of supertags are employed: those from Lexicalized Tree-Adjoining Grammar and Combinatory Categorial Grammar.Despite the differences between these two approaches, the supertaggers give similar improvements.In addition to supertagging, we also explore the utility of a surface global grammaticality measure based on combinatory operators.We perform various experiments on the Arabic to English NIST 2005 test set addressing issues such as sparseness, scalability and the utility of system subcomponents.Our best result (0.4688 BLEU) improves by 6.1% relative to a state-of-theart PBSMT model, which compares very favourably with the leading systems on the NIST 2005 task.
This paper describes an incremental parser and an unsupervised learning algorithm for inducing this parser from plain text.The parser uses a representation for syntactic structure similar to dependency links which is well-suited for incremental parsing.In contrast to previous unsupervised parsers, the parser does not use part-of-speech tags and both learning and parsing are local and fast, requiring no explicit clustering or global optimization.The parser is evaluated by converting its output into equivalent bracketing and improves on previously published results for unsupervised parsing from plain text.
In this paper we investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity.Inference in the model is based on standard sequence classification techniques using constrained Viterbi to ensure consistent solutions.The primary advantage of such a model is that it allows classification decisions from one level in the text to influence decisions at another.Experiments show that this method can significantly reduce classification error relative to models trained in isolation.
Automatic sentiment classification has been extensively studied and applied in recent years.However, sentiment is expressed differently in different domains, and annotating corpora for every possible domain of interest is impractical.We investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products.First, we extend to sentiment classification the recently-proposed structural correspondence learning (SCL) algorithm, reducing the relative error due to adaptation between domains by an average of 30% over the original SCL algorithm and 46% over a supervised baseline.Second, we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another.This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains.
We present an approach to query expansion in answer retrieval that uses Statistical Machine Translation (SMT) techniques to bridge the lexical gap between questions and answers.SMT-based query expansion is done by i) using a full-sentence paraphraser to introduce synonyms in context of the entire query, and ii) by translating query terms into answer terms using a full-sentence SMT model trained on question-answer pairs.We evaluate these global, context-aware query expansion techon from 10 million question-answer pairs extracted from FAQ pages.Experimental results show that SMTbased expansion improves retrieval performance over local expansion and over retrieval without expansion.
A Bloom filter (BF) is a randomised data structure for set membership queries.Its space requirements are significantly below lossless information-theoretic lower bounds but it produces false positives with some quantifiable probability.Here we explore the use of BFs for language modelling in statistical machine translation. show how a BF containing can enable us to use much larger corpora and higher-order models complementing a con- LM within an SMT system.We also consider (i) how to include approximate frequency information efficiently within a BF and (ii) how to reduce the error rate of these models by first checking for sub-sequences in candidate grams.Our solutions in both cases retain the one-sided error guarantees of the BF while taking advantage of the Zipf-like distribution of word frequencies to reduce the space requirements.
We present a new approach to relation extraction that requires only a handful of training examples.Given a few pairs of named entities known to exhibit or not exhibit a particular relation, bags of sentences containing the pairs are extracted from the web.We extend an existing relation extraction method to handle this weaker form of supervision, and present experimental results demonstrating that our approach can reliably extract relations from web documents.
chl, dozhang@microsoft.com mhli@insun.hit.edu.cn muli, mingzhou@microsoft.com guanyi@insun.hit.edu.cn Abstract Inspired by previous preprocessing approaches to SMT, this paper proposes a novel, probabilistic approach to reordering which combines the merits of syntax and phrase-based SMT.Given a source sentence and its parse tree, our method generates, tree operations, an list of reordered inputs, which are then fed to standard phrase-based decoder to produce the optimal translation.Experiments show that, for the NIST MT-05 task of Chinese-to- English translation, the proposal leads to BLEU improvement of 1.56%.
Current phrase-based SMT systems perform poorly when using small training sets.This is a consequence of unreliable translation estimates and low coverage over source and target phrases.This paper presents a method which alleviates this problem by exploiting multiple translations of the same source Central to our approach is triangulathe process of translating from a source to a target language via an intermediate third language.This allows the use of a much wider range of parallel corpora for training, and can be combined with a standard phrase-table using conventional smoothing methods.Experimental results demonstrate BLEU improvements for triangulated models over a standard phrase-based system.
Unsupervised learning of linguistic structure is a difficult problem.A common approach is to define a generative model and maximize the probability of the hidden structure given the observed data.Typically, this is done using maximum-likelihood estimation (MLE) of the model parameters.We show using part-of-speech tagging that a fully Bayesian approach can greatly improve performance.Rather than estimating a single set of parameters, the Bayesian approach integrates over all possible parameter values.This difference ensures that the learned structure will have high probability over a range of possible parameters, and permits the use of priors favoring the sparse distributions that are typical of natural language.Our model has the structure of a standard trigram HMM, yet its accuracy is closer to that of a state-of-the-art discriminative model (Smith and Eisner, 2005), up to 14 percentage points better than MLE.We find improvements both when training from data alone, and using a tagging dictionary.
In this paper, we propose guided learning, a new learning framework for bidirectional sequence classification.The tasks of learning the order of inference and training the local classifier are dynamically incorporated into a single Perceptron like learning algorithm.We apply this novel learning algorithm to POS tagging.It obtains an error rate of 2.67% on the standard PTB test set, which represents 3.3% relative error reduction over the previous best result on the same data set, while using fewer features.
We study the impact of syntactic and shallow semantic information in automatic classification of questions and answers and answer re-ranking.We define (a) new tree structures based on shallow semantics encoded in Predicate Argument Structures (PASs) and (b) new kernel functions to exploit the representational power of such structures with Support Vector Machines.Our experiments suggest that syntactic information helps tasks such as question/answer classification and that shallow semantics gives remarkable contribution when a reliable set of PASs can be extracted, e.g. from answers.
Standard approaches to Chinese word segmentation treat the problem as a tagging task, assigning labels to the characters in the sequence indicating whether the character marks a word boundary.Discriminatively trained models based on local character features are used to make the tagging decisions, with Viterbi decoding finding the highest scoring segmentation.In this paper we propose an alternative, word-based segmentor, which uses features based on complete words and word sequences.The generalized perceptron algorithm is used for discriminative training, and we use a beamsearch decoder.Closed tests on the first and show that our system is competitive with the best in the literature, achieving the highest reported F-scores for a number of corpora.
We present an unsupervised, nonparametric Bayesian approach to coreference resolution which models both global entity identity across a corpus as well as the sequential anaphoric structure within each document.While most existing coreference work is driven by pairwise decisions, our model is fully generative, producing each mention from a combination of global entity properties and local attentional state.Despite being unsupervised, our system achieves a 70.3 on the MUC-6 test set, broadly in the range of some recent supervised results.
This paper presents the first empirical results to our knowledge on learning synchronous grammars that generate logical forms.Using statistical machine translation techniques, a semantic parser based on a synchronous grammar augmented with operators is learned given a set of training sentences and their correct logical forms.The resulting parser is shown to be the bestperforming system so far in a database query domain.
This paper explores methods for generating subjectivity analysis resources in a new language by leveraging on the tools and resources available in English.Given a bridge between English and the selected target language (e.g., a bilingual dictionary or a parallel corpus), the methods can be used to rapidly create tools for subjectivity analysis in the new language.
We investigate automatic classification of speculative language (‘hedging’), in biomedical text using weakly supervised machine learning.Our contributions include a precise description of the task with annotation guidelines, analysis and discussion, a probabilistic weakly supervised learning model, and experimental evaluation of the methods presented.We show that hedge classification is feasible using weakly supervised ML, and point toward avenues for future research.
where many sources of information about the project can be found.Moses was the subject of this year’s Johns Hopkins University Workshop on Machine Translation (Koehn et al. 2006).The decoder is the core component of Moses.To minimize the learning curve for many researchers, the decoder was developed as a drop-in replacement for Pharaoh, the popular phrase-based decoder.In order for the toolkit to be adopted by the community, and to make it easy for others to contribute to the project, we kept to the following principles when developing the decoder: • Accessibility • Easy to Maintain • Flexibility • Easy for distributed team development • Portability It was developed in C++ for efficiency and followed modular, object-oriented design.3 Factored Translation Model Non-factored SMT typically deals only with the surface form of words and has one phrase table, as shown in Figure 1.Translate: i am buying you a green cat vous achète un vert using phrase dictionary: i am buying vous green chat Figure 1.Non-factored translation In factored translation models, the surface forms may be augmented with different factors, such as POS tags or lemma.This creates a factored representation of each word, Figure 2.⎛⎞⎛ ⎞⎛ vous achet ⎞⎛ chat ⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟ ⎜⎟⎜ ⎟⎜ PRO PRO VB ART NN ⎟⎜ ⎟⎜ ⎟ ⎜⎟⎜ ⎟⎜ je vous acheter ⎟⎜ chat ⎟⎜ ⎟⎜ ⎟⎜ ⎝⎠⎝ ⎠⎝ st present masc / ⎠⎝ ⎛ ⎞⎛ buy ⎞⎛ ⎞⎛ ⎞ you a cat ⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟ ⎜⎟⎜ VB ⎟⎜ ⎟ PRO ART NN ⎜ ⎟⎜ tobuy ⎟⎜ ⎟⎜ ⎟ you a cat ⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟ ⎝⎠⎝ Figure 2.Factored translation Mapping of source phrases to target phrases may be decomposed into several steps.Decomposition of the decoding process into various steps means that different factors can be modeled separately.Modeling factors in isolation allows for flexibility in their application.It can also increase accuracy and reduce sparsity by minimizing the number dependencies for each step.For example, we can decompose translating from surface forms to surface forms and lemma, as shown in Figure 3. je achète you a un a une vert cat 178 Figure 3.Example of graph of decoding steps By allowing the graph to be user definable, we can experiment to find the optimum configuration for a given language pair and available data.The factors on the source sentence are considered fixed, therefore, there is no decoding step which create source factors from other source factors.However, Moses can have ambiguous input in the form of confusion networks.This input type has been used successfully for speech to text translation (Shen et al. 2006).Every factor on the target language can have its own language model.Since many factors, like lemmas and POS tags, are less sparse than surface forms, it is possible to create a higher order language models for these factors.This may encourage more syntactically correct output.In Figure 3 we apply two language models, indicated by the shaded arrows, one over the words and another over the lemmas.Moses is also able to integrate factored language models, such as those described in (Bilmes and Kirchhoff 2003) and (Axelrod 2006).4 Confusion Network Decoding Machine translation input currently takes the form of simple sequences of words.However, there are increasing demands to integrate machine translation technology into larger information processing systems with upstream NLP/speech processing tools (such as named entity recognizers, speech recognizers, morphological analyzers, etc.).These upstream processes tend to generate multiple, erroneous hypotheses with varying confidence.Current MT systems are designed to process only one input hypothesis, making them vulnerable to errors in the input.In experiments with confusion networks, we have focused so far on the speech translation case, where the input is generated by a speech recognizer.Namely, our goal is to improve performance of spoken language translation by better integrating speech recognition and machine translation models.Translation from speech input is considered more difficult than translation from text for several reasons.Spoken language has many styles and genres, such as, formal read speech, unplanned speeches, interviews, spontaneous conversations; it produces less controlled language, presenting more relaxed syntax and spontaneous speech phenomena.Finally, translation of spoken language is prone to speech recognition errors, which can possibly corrupt the syntax and the meaning of the input.There is also empirical evidence that better translations can be obtained from transcriptions of the speech recognizer which resulted in lower scores.This suggests that improvements can be achieved by applying machine translation on a large set of transcription hypotheses generated by the speech recognizers and by combining scores of acoustic models, language models, and translation models.Recently, approaches have been proposed for improving translation quality through the processing of multiple input hypotheses.We have implemented in Moses confusion network decoding as discussed in (Bertoldi and Federico 2005), and developed a simpler translation model and a more efficient implementation of the search algorithm.Remarkably, the confusion network decoder resulted in an extension of the standard text decoder.5 Efficient Data Structures for Translation Model and Language Models With the availability of ever-increasing amounts of training data, it has become a challenge for machine translation systems to cope with the resulting strain on computational resources.Instead of simply buying larger machines with, say, 12 GB of main memory, the implementation of more efficient data structures in Moses makes it possible to exploit larger data resources with limited hardware infrastructure.A phrase translation table easily takes up gigabytes of disk space, but for the translation of a single sentence only a tiny fraction of this table is needed.Moses implements an efficient representation of the phrase translation table.Its key properare a tree for source words and demand i.e. only the fraction of the phrase table that is needed to translate a sentence is loaded into the working memory of the decoder.179 For the Chinese-English NIST task, the memory requirement of the phrase table is reduced from 1.7 gigabytes to less than 20 mega bytes, with no loss in translation quality and speed (Zens and Ney 2007).The other large data resource for statistical machine translation is the language model.Almost unlimited text resources can be collected from the Internet and used as training data for language modeling.This results in language models that are too large to easily fit into memory.The Moses system implements a data structure for language models that is more efficient than the canonical SRILM (Stolcke 2002) implementation used in most systems.The language model on disk is also converted into this binary format, resulting in a minimal loading time during start-up of the decoder.An even more compact representation of the model is the result of the the word prediction and back-off probabilities of the language model.Instead of representing these probabilities with 4 byte or 8 byte floats, they are sorted into bins, resulting in (typically) 256 bins which can be referenced with a single 1 byte index.This quantized language model, albeit being less accurate, has only minimal impact on translation performance (Federico and Bertoldi 2006).6 Conclusion and Future Work This paper has presented a suite of open-source tools which we believe will be of value to the MT research community.We have also described a new SMT decoder which can incorporate some linguistic features in a consistent and flexible framework.This new direction in research opens up many possibilities and issues that require further research and experimentation.Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).
We combine the strengths of Bayesian modeling and synchronous grammar in unsupervised learning of basic translation phrase pairs.The structured space of a synchronous grammar is a natural fit for phrase pair probability estimation, though the search space can be prohibitively large.Therefore we explore efficient algorithms for pruning this space that lead to empirically effective results.Incorporating a sparse prior using Variational Bayes, biases the models toward generalizable, parsimonious parameter sets, leading to significant improvements in word alignment.This preference for sparse solutions together with effective pruning methods forms a phrase alignment regimen that produces better end-to-end translations than standard word alignment approaches.
Among syntax-based translation models, the which takes as input a parse tree of the source sentence, is a promising direction being faster and simpler than its string-based counterpart.However, current tree-based systems suffer from a major drawback: they only use the 1-best parse to direct the translation, which potentially introduces translation mistakes due to parsing errors.We a that translates a packed forest of exponentially many parses, which encodes many more alternatives standard lists.Large-scale experiments show an absolute improvement of 1.7 BLEU points over the 1-best baseline.This result is also 0.8 points higher than decoding with 30-best parses, and takes even less time.
Large-scale discriminative machine translation promises to further the state-of-the-art, but has failed to deliver convincing gains over current heuristic frequency count systems.We argue that a principle reason for this failure is not dealing with multiple, equivalent translations.We present a translation model which models derivations as a latent variable, in both training and decoding, and is fully discriminative and globally optimised.Results show that accounting for multiple derivations does indeed improve performance.Additionally, we show that regularisation is essential for maximum conditional likelihood models in order to avoid degenerate solutions.
We apply the hypothesis of “One Sense Per Discourse” (Yarowsky, 1995) to information extraction (IE), and extend the scope of “discourse” from one single document to a cluster of topically-related documents.We employ a similar approach to propagate consistent event arguments across sentences and documents.Combining global evidence from related documents with local decisions, we design a simple scheme to conduct cross-document inference for improving the ACE event ex- Without using any additional labeled data this new approach obtained 7.6% higher F-Measure in trigger labeling and 6% higher F-Measure in argument labeling over a state-of-the-art IE system which extracts events independently for each sentence.
Online reviews are often accompanied with numerical ratings provided by users for a set of service or product aspects.We propose a statistical model which is able to discover corresponding topics in text and extract textual evidence from reviews supporting each of these aspect ratings – a fundamental problem in aspect-based sentiment summarization (Hu and Liu, 2004a).Our model achieves high accuracy, without any explicitly labeled data except the user provided opinion ratings.The proposed approach is general and can be used for segmentation in other applications where sequential data is accompanied with correlated signals.
mzhang@i2r.a-star.edu.sg hfjiang@mtlab.hit.edu.cn tancl@comp.nus.edu.sg aaiti@i2r.a-star.edu.sg lisheng@hit.edu.cn hli@i2r.a-star.edu.sg Abstract This paper presents a translation model that is based on tree sequence alignment, where a tree sequence refers to a single sequence of subtrees that covers a phrase.The model leverages on the strengths of both phrase-based and linguistically syntax-based method.It automatically learns aligned tree sequence pairs with mapping probabilities from word-aligned biparsed parallel texts.Compared with previous models, it not only captures non-syntactic phrases and discontinuous phrases with linguistically structured features, but also supports multi-level structure reordering of tree typology with larger span.This gives our model stronger expressive power than other reported models.Experimental results on the NIST MT-2005 Chinese-English translation task show that our method statistically significantly outperforms the baseline systems.
In this paper, we propose a novel string-todependency algorithm for statistical machine translation.With this new framework, we employ a target dependency language model during decoding to exploit long distance word relations, which are unavailable with a traditional n-gram language model.Our experiments show that the string-to-dependency decoder achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to a standard hierarchical string-tostring system on the NIST 04 Chinese-English evaluation set.
reranking techniques ofsuffer from the limited scope of the best list, which rules out many potentially alternatives.We instead propose a method that reranks a packed forest of exponentially many parses.Since exact inference is intractable with non-local features, we present an approximate algorithm inspired by forest rescoring that makes discriminative training practical over the whole Treebank.Our final result, an F-score of 91.7, outperforms both 50-best and 100-best reranking baselines, and is better than any previously reported systems trained on the Treebank.
We present a simple and effective semisupervised method for training dependency parsers.We focus on the problem of lexical representation, introducing features that incorporate word clusters derived from a large unannotated corpus.We demonstrate the effectiveness of the approach in a series of dependency parsing experiments on the Penn Treebank and Prague Dependency Treebank, and we show that the cluster-based features yield substantial gains in performance across a wide range of conditions.For example, in the case of English unlabeled second-order parsing, we improve from a baseline accuof in the case of Czech unlabeled second-order parsing, we from a baseline accuracy of addition, we demonstrate that our method also improves performance when small amounts of training data are available, and can roughly halve the amount of supervised data required to reach a desired level of performance.
This paper provides evidence that the use of more unlabeled data in semi-supervised learning can improve the performance of Natural Language Processing (NLP) tasks, such as part-of-speech tagging, syntactic chunking, and named entity recognition.We first propose a simple yet powerful semi-supervised discriminative model appropriate for handling large scale unlabeled data.Then, we describe experiments performed on widely used test collections, namely, PTB III data, CoNLL’00 and ’03 shared task data for the above three NLP tasks, respectively.We incorporate up to 1G-words (one billion tokens) of unlabeled data, which is the largest amount of unlabeled data ever used for these tasks, to investigate the performance improvement.In addition, our results are superior to the best reported results for all of the above test collections.
For centuries, the deep connection between languages has brought about major discoveries about human communication.In this paper we investigate how this powerful source of information can be exploited for unsupervised language learning.In particular, we study the task of morphological segmentation of multiple languages.We present a nonparametric Bayesian model that jointly induces morpheme segmentations of each language under consideration and at the same time identifies cross-lingual morpheme pator We apply our model to three Semitic languages: Arabic, Hebrew, Aramaic, as well as to English.Our results demonstrate that learning morphological models in tandem reduces error by up to 24% relative to monolingual models.Furthermore, we provide evidence that our joint model achieves better performance when applied to languages from the same family.
We address the task of unsupervised POS tagging.We demonstrate that good results can be obtained using the robust EM-HMM learner when provided with good initial conditions, even with incomplete dictionaries.We present a family of algorithms to compute effective estimations We test the method on the task of full morphological disambiguation in Hebrew achieving an error reduction of 25% over a strong uniform distribution baseline.We also test the same method on the standard WSJ unsupervised POS tagging task and obtain results competitive with recent state-ofthe-art methods, while using simple and efficient learning methods.
In statistical language modeling, one technique to reduce the problematic effects of data sparsity is to partition the vocabulary into equivalence classes.In this paper we investigate the effects of applying such a technique to highermodels trained on large corpora.We introduce a modification of the exchange clustering algorithm with improved efficiency for certain partially class-based models and a distributed version of this algorithm to efficiently obtain automatic word classifications large vocabularies million words) ussuch large training corpora billion tokens).The resulting clusterings are then used in training partially class-based language models.We show that combining them with wordmodels in the log-linear model of a state-of-the-art statistical machine translation system leads to improvements in translation quality as indicated by the BLEU score.
We present a method for learning bilingual translation lexicons from monolingual corpora.Word types in each language are characterized by purely monolingual features, such as context counts and orthographic substrings.Translations are induced using a generative model based on canonical correlation analysis, which explains the monolingual lexicons in terms of latent matchings.We show that high-precision lexicons can be learned in a variety of language pairs and from a range of corpus types.
used in the 1970-80s as knowledge backbones that enabled inference and other NLP tasks requiring deep semantic knowledge.We propose unsupervised of similar schemata called chains raw newswire text.A narrative event chain is a partially ordered set of events related by a common protagonist.We describe a three step process to learning narrative event chains.The first uses unsupervised distributional methods to learn narrative relations between events sharing coreferring arguments.The second applies a temporal classifier to partially order the connected events.Finally, the third prunes and clusters self-contained chains from the space of events. introduce two evaluations: the evaluate event relatedness, and an orcoherence to evaluate narrative order. show a over baseline narrative prediction and temporal coherence. tate learning, and thus this paper addresses the three of chain induction: event ordering of events selection (pruning the event space into discrete sets).Learning these prototypical schematic sequences of events is important for rich understanding of text.Scripts were central to natural language understanding research in the 1970s and 1980s for proposed tasks such as summarization, coreference resolution and question answering.For example, Schank and Abelson (1977) proposed that understanding text about restaurants required knowledge about the Restaurant Script, including the participants (Customer, Waiter, Cook, Tables, etc.), the events constituting the script (entering, sitting down, asking for menus, etc.), and the various preconditions, ordering, and results of each of the constituent actions.Consider these two distinct narrative chains.
Chinese word segmentation is a preliminary step.To avoid error propagation and improve segmentation by utilizing segmentation and tagging can be performed simultaneously.A challenge for this joint approach is the large combined search space, which makes efficient decoding very hard.Recent research has explored integration of segmentation and tagging, by decoding under restricted versions of the full combined search space.In this paper, propose a joint segmentation and tagging model that does not impose any hard constraints on the interaction between word and Fast decoding is achieved by using a novel multiple-beam search algorithm.The system uses a discriminative statistical model, trained using the generalized perceptron algorithm.The joint model gives an error reduction in segmentation accuracy of an error reduction in tagging acof compared to the traditional pipeline approach.
Previous studies of data-driven dependency parsing have shown that the distribution of parsing errors are correlated with theoretical properties of the models used for learning and inference.In this paper, we show how these results can be exploited to improve parsing accuracy by integrating a graph-based and a transition-based model.By letting one model generate features for the other, we consistently improve accuracy for both models, resulting in a significant improvement of the state of the art when evaluated on data sets from the CoNLL-X shared task.
Discriminative feature-based methods are widely used in natural language processing, but sentence parsing is still dominated by generative methods.While prior feature-based dynamic programming parsers have restricted training and evaluation to artificially short sentences, we present the first general, featurerich discriminative parser, based on a conditional random field model, which has been successfully scaled to the full WSJ parsing data.Our efficiency is primarily due to the use of stochastic optimization techniques, as well as parallelization and chart prefiltering.On WSJ15, we attain a state-of-the-artF-score a 14% relative reduction in error over previous models, while being two orders of magnitude faster.On sentences of length our system achieves an F-score of a 36% relative reduction in error over a generative baseline.
In adding syntax to statistical MT, there is a tradeoff between taking advantage of linguistic analysis, versus allowing the model to exploit linguistically unmotivated mappings learned from parallel training data.A number of previous efforts have tackled this tradeoff by starting with a commitment to linguistically motivated analyses and then finding appropriate ways to soften that commitment.We present an approach that explores the tradeoff from the other direction, starting with a context-free translation model learned directly from aligned parallel text, and then adding soft constituent-level constraints based on parses of the source language.We obtain substantial improvements in performance for translation from Chinese and Arabic to English.
Word lattice decoding has proven useful in spoken language translation; we argue that it provides a compelling model for translation of text genres, as well.We show that prior work in translating lattices using finite state techniques can be naturally extended to more expressive synchronous context-free grammarbased models.Additionally, we resolve a significant complication that non-linear word lattice inputs introduce in reordering models.Our experiments evaluating the approach demonstrate substantial gains for Chinese- English and Arabic-English translation.
We present a novel approach to weakly supervised semantic class learning from the web, using a single powerful hyponym pattern combined with graph structures, which capture two properties associated with pattern-based Ina candidate is it was discovered many times by other instances in the pattern.A candidate is if it frequently leads to the discovery of other instances.Together, these two measures capture not only frequency of occurrence, but also cross-checking that the candidate occurs both near the class name and near other class members.We developed two algorithms that begin with just a class name and one seed instance and then automatically generate a ranked list of new class instances.We conducted experiments on four semantic classes and consistently achieved high accuracies.
Many phrase alignment models operate over combinatorial space of phrase We prove that finding an optimal alignment in this space is NP-hard, while computing alignment expectations is #P-hard.On the other hand, we show that the problem of finding an optimal alignment can be cast as an integer linear program, which provides a simple, declarative approach to Viterbi inference for phrase alignment models that is empirically quite efficient.
A desirable quality of a coreference resolution system is the ability to handle transitivity constraints, such that even if it places high likelihood on a particular mention being coreferent with each of two other mentions, it will also consider the likelihood of those two mentions being coreferent when making a final assignment.This is exactly the kind of constraint that integer linear programming (ILP) is ideal for, but, surprisingly, previous work applying ILP to coreference resolution has not encoded this type of constraint.We train a coreference classifier over pairs of mentions, and show how to encode this type of constraint on top of the probabilities output from our pairwise classifier to extract the most probable legal entity assignments.We present results on two commonly used datasets which show that enforcement of transitive closure consistently improves performance, including imof up to 3.6% using the and up to 16.5% using cluster f-measure.
Parser self-training is the technique of taking an existing parser, parsing extra data and then creating a second parser by treating the extra data as further training data.Here we apply this technique to parser adaptation.In particular, we self-train the standard Charniak/Johnson Penn-Treebank parser using unlabeled biomedical abstracts.This an of 84.3% on a standard test set of biomedical abstracts from the Genia corpus.This is a 20% error reduction over the best previous result on biomedical data (80.2% on the same test set).
In this paper, we present a reinforcement learning approach for mapping natural language instructions to sequences of executable actions.We assume access to a reward function that defines the quality of the executed actions.During training, the learner repeatedly constructs action sequences for a set of documents, executes those actions, and observes the resulting reward.We use a policy gradient algorithm to estimate the parameters of a log-linear model for action selection.We apply our method to interpret instructions in two domains — Windows troubleshooting guides and game tutorials.Our results demonstrate that this method can rival supervised learning techniques while requiring few or no annotated training exam
A central problem in grounded language acquisition is learning the correspondences between a rich world state and a stream of text which references that world state.To deal with the high degree of ambiguity present in this setting, we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state.We show that our model generalizes across three domains of increasing difficulty—Robocup sportscasting, weather forecasts (a new domain), and NFL recaps.
Minimum Error Rate Training (MERT) and Minimum Bayes-Risk (MBR) decoding are used in most current state-of-theart Statistical Machine Translation (SMT) systems.The algorithms were originally to work with lists of translations, and recently extended to lattices that encode many more hypotheses typical lists.We here extend lattice-based MERT and MBR algorithms to work with hypergraphs that encode a vast number of translations produced by MT systems based on Synchronous Context Free Grammars.These algorithms are more efficient than the lattice-based versions presented earlier.We show how MERT can be employed to optimize parameters for MBR decoding.Our experiments show speedups from MERT and MBR as well as performance improvements from MBR decoding on several language pairs.
This paper presents an unsupervised opinanalysis method for clasi.e., recognizing which stance a person is taking in an online debate.In order to handle the complexities of this genre, we mine the web to learn associations that are indicative of opinion stances in debates.We combine this knowledge with discourse information, and formulate the debate side classification task as an Integer Linear Programming problem.Our results show that our method is substantially better than challenging baseline methods.
The lack of Chinese sentiment corpora limits the research progress on Chinese sentiment classification.However, there are many freely available English sentiment corpora on the Web.This paper focuses on the problem of cross-lingual sentiment classification, which leverages an available English corpus for Chinese sentiment classification by using the English corpus as training data.Machine translation services are used for eliminating the language gap between the training set and test set, and English features and Chinese features are considered as two independent views of the classification problem.We propose a cotraining approach to making use of unlabeled Chinese data.Experimental results show the effectiveness of the proposed approach, which can outperform the standard inductive classifiers and the transductive classifiers.
We formulate the problem of nonprojective dependency parsing as a polynomial-sized integer linear program.Our formulation is able to handle non-local output features in an efficient manner; not only is it compatible with prior knowledge encoded as hard constraints, it can also learn soft constraints from data.In particular, our model is able to learn correlations among neighboring arcs (siblings and grandparents), word valency, and tendencies toward nearlyprojective parses.The model parameters are learned in a max-margin framework by employing a linear programming relaxation.We evaluate the performance of our parser on data in several natural languages, achieving improvements over existing state-of-the-art methods.
We present a novel transition system for dependency parsing, which constructs arcs only between adjacent words but can parse arbitrary non-projective trees by swapping the order of words in the input.Adding the swapping operation changes the time complexity for deterministic parsing from linear to quadratic in the worst case, but empirical estimates based on treebank data show that the expected running time is in fact linear for the range of data attested in the corpora.Evaluation on data from five languages shows state-of-the-art accuracy, with especially good results for the labeled exact match score.
Broad-coverage annotated treebanks necessary to train parsers do not exist for many resource-poor languages.The wide availability of parallel text and accurate parsers in English has opened up the possibility of grammar induction through partial transfer across bitext.We consider generative and discriminative models for dependency grammar induction that use word-level alignments and a source language parser (English) to constrain the space of possible target trees.Unlike previous approaches, our framework does not require full projected parses, allowing partial, approximate transfer through linear expectation constraints on the space of distributions over trees.We consider several types of constraints that range from generic dependency conservation to language-specific annotation rules for auxiliary verb analysis.We evaluate our approach on Bulgarian and Spanish CoNLL shared task data and show that we consistently outperform unsupervised methods and can outperform supervised learning for limited training data.
We describe a novel method for the task of unsupervised POS tagging with a dictionary, one that uses integer programming to explicitly search for the smallest model that explains the data, and then uses EM to set parameter values.We evaluate our method on a standard test corpus using different standard tagsets (a 45-tagset as well as a smaller 17-tagset), and show that our approach performs better than existing state-of-the-art systems in both settings.
In this paper, we present a discriminative word-character hybrid model for joint Chinese word segmentation and POS tagging.Our word-character hybrid model offers high performance since it can handle both known and unknown words.We describe our strategies that yield good balance for learning the characteristics of known and unknown words and propose an errordriven policy that delivers such balance by acquiring examples of unknown words from particular errors in a training corpus.We describe an efficient framework for training our model based on the Margin Infused Relaxed Algorithm (MIRA), evaluate our approach on the Penn Chinese Treebank, and show that it achieves superior performance compared to the state-ofthe-art approaches reported in the literature.
We describe an unsupervised system for learncoherent sequences or sets events whose arguments are filled with participant semantic roles defined over words jury, Unlike most previous work in event structure or semantic role learning, our system does not use supervised techniques, hand-built knowledge, or predefined classes of events or roles.Our unsupervised learning algorithm uses coreferring arguments in chains of verbs to learn both rich narrative event structure and argument roles.By jointly addressing both tasks, we improve on previous results in narrative/frame learning and induce rich frame-specific semantic roles.
We aim to shed light on the state-of-the-art in NP coreference resolution by teasing apart the differences in the MUC and ACE task definitions, the assumptions made in evaluation methodologies, and inherent differences in text corpora.First, we examine three subproblems that play a role in coreference resolution: named entity recognition, anaphoricity determination, and coreference element detection.We measure the impact of each subproblem on coreference resolution and confirm that certain assumptions regarding these subproblems in the evaluation methodology can dramatically simplify the overall task.Second, we measure the performance of a state-of-the-art coreference resolver on several classes of anaphora and use these results to develop a quantitative measure for estimating coreference resolution performance on new data sets.
We present a series of experiments on auidentifying the sense of imrelations, i.e. relations that are not marked with a discourse connective such as “but” or “because”.We work with a corpus of implicit relations present in newspaper text and report results on a test set that is representative of the naturally occurring distribution of senses.We use several linguistically informed features, including polarity tags, Levin verb classes, length of verb phrases, modality, context, and lexical features.In addition, we revisit past approaches using lexical pairs from unannotated text as features, explain some of their shortcomings and propose modifications.Our best combination of features outperforms the baseline from data intensive approaches by 4% for comparison and 16% for contingency.
We present a phrasal synchronous grammar model of translational equivalence.Unlike previous approaches, we do not resort to heuristics or constraints from a word-alignment model, but instead directly induce a synchronous grammar from parallel sentence-aligned corpora.We use a hierarchical Bayesian prior to bias towards compact grammars with small translation units.Inference is performed using a novel Gibbs sampler over synchronous derivations.This sampler side-steps the intractability issues of previous models which required inference over derivation forests.Instead each sampling iteration is highly efficient, allowing the model to be applied to larger translation corpora than previous approaches.
Paraphrase generation (PG) is important in plenty of NLP applications.However, the research of PG is far from enough.In this paper, we propose a novel method for statistical paraphrase generation (SPG), which can (1) achieve various applications based on a uniform statistical model, and (2) naturally combine multiple resources to enhance the PG performance.In our experiments, we use the proposed method to generate paraphrases for three different applications.The results show that the method can be easily transformed from one application to another and generate valuable and interesting paraphrases.
This work investigates supervised word alignment methods that exploit inversion transduction grammar (ITG) constraints.We consider maximum margin and conditional likelihood objectives, including the presentation of a new normal form grammar for canonicalizing derivations.Even for non-ITG sentence pairs, we show that it is possible learn ITG alignment models by simple relaxations of structured discriminative learning objectives.For efficiency, we describe a set of pruning techniques that together allow us to align sentences two orders of magnitude faster than naive bitext CKY parsing.Finally, we introduce many-to-one block alignment features, which significantly improve our ITG models.Altogether, our method results in the best reported AER numbers for Chinese-English and a performance improvement of 1.1 BLEU over GIZA++ alignments.
Modern models of relation extraction for tasks like ACE are based on supervised learning of relations from small hand-labeled corpora.We investigate an alternative paradigm that does not require labeled corpora, avoiding the domain dependence of ACEstyle algorithms, and allowing the use of corpora of any size.Our experiments use Freebase, a large semantic database of several thousand relations, to For each pair of entities that appears in some Freebase relation, we find all sentences containing those entities in a large unlabeled corpus and extract textual features to train a relation classifier.Our algorithm combines the advantages of supervised IE (combining 400,000 noisy pattern features in a probabilistic classifier) and unsupervised IE (extracting large numbers of relations from large corpora of any domain).Our model is able to extract 10,000 instances of 102 relations at a precision of 67.6%.We also analyze feature performance, showing that syntactic parse features are particularly helpful for relations that are ambiguous or lexically distant in their expression.
We present a simple and scalable algorithm for clustering tens of millions of phrases and use the resulting clusters as features in discriminative classifiers.To demonstrate the power and generality of this approach, we apply the method in two very different applications: named entity recognition and query classification.Our results show that phrase clusters offer significant improvements over word clusters.Our NER system achieves the best current result on the widely used CoNLL benchmark.Our query classifier is on par with the best system in KDDCUP 2005 without resorting to labor intensive knowledge engineering efforts.
Discourse connectives are words or such as and contrary explicitly signal the presence of a discourse relation.There are two types of ambiguity that need to be resolved during discourse processing.First, a word can be ambiguous between discourse or non-discourse usage.For be either a temporal discourse connective or a simply a word meaning “formerly”.Secondly, some connectives are ambiguous in terms of the they mark.For example can serve as either a temporal or causal connective.We demonstrate that syntactic features improve performance in both disambiguation tasks.We report state-ofthe-art results for identifying discourse vs. non-discourse usage and human-level performance on sense disambiguation.
Tree substitution grammars (TSGs) offer many advantages over context-free grammars (CFGs), but are hard to learn.Past approaches have resorted to heuristics.In this paper, we learn a TSG using Gibbs sampling with a nonparametric prior to control subtree size.The learned grammars perform significantly better than heuristically extracted ones on parsing accuracy.
If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features.We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking.We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines.We find further by combining word representations.You can download word features, for use in existing NLP systems, as well as our here:
computation of preferthe admissible argument values for a relation, is a well-known NLP task with applicability.We present which utilizes LinkLDA (Erosheva et al., 2004) to model selectional preferences.By simultaneously inferring latent topics and topic distributions over relations, the benefits of previous approaches: like traditional classbased approaches, it produces humaninterpretable classes describing each relation’s preferences, but it is competitive with non-class-based methods in predictive power. compare several state-ofthe-art methods achieving an 85% increase in recall at 0.9 precision over mutual information (Erk, 2007).We also evaleffectiveness at filtering improper applications of inference rules, where we show substantial improvement Pantel system (Pantel et al.,
Incremental parsing techniques such as shift-reduce have gained popularity thanks to their efficiency, but there remains a problem: the search is only explores a tiny fraction of the whole space (even with beam search) as opposed to dynamic programming.We show that, surprisingly, dynamic programming is in fact possible for many shift-reduce parsers, by merging “equivalent” stacks based on feature values.Empirically, our algorithm yields up to a five-fold speedup over a state-of-the-art shift-reduce dependency parser with no loss in accuracy.Better search also leads to better learning, and our final parser outperforms all previously reported dependency parsers for English and Chinese, yet is much faster.
Statistical translation models that try to capture the recursive structure of language have been widely adopted over the last few years.These models make use of varying amounts of information from linguistic theory: some use none at all, some use information about the grammar of the target language, some use information about the grammar of the source language.But progress has been slower on translation models that are able to learn the relationship between the grammars of both the source and target language.We discuss the reasons why this has been a challenge, review existing attempts to meet this challenge, and show how some old and new ideas can be combined into a simple approach that uses both source and target syntax for significant improvements in translation accuracy.
We address the problem of selecting nondomain-specific language model training data to build auxiliary language models for use in tasks such as machine translation.Our approach is based on comparing the cross-entropy, according to domainspecific and non-domain-specifc language models, for each sentence of the text source used to produce the latter language model.We show that this produces better language models, trained on less data, than both random data selection and two other previously proposed methods.
present an open source framework for decoding, aligning with, and training a number of statistical machine translation models, including word-based models, phrase-based models, and models based on synchronous context-free grammars.Using a single unified internal representation for translation forests, the decoder strictly separates model-specific translation logic from general rescoring, pruning, and inference algorithms.From this unified representation, the decoder can not only the 1or translations, but also alignments to a reference, or the quantities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques.Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders.
Sentiment analysis on Twitter data has attracted much attention recently.In this paper, we focus on target-dependent Twitter sentiment classification; namely, given a query, we classify the sentiments of the tweets as positive, negative or neutral according to whether they contain positive, negative or neutral sentiments about that query.Here the query serves as the target of the sentiments.The state-ofthe-art approaches for solving this problem always adopt the target-independent strategy, which may assign irrelevant sentiments to the given target.Moreover, the state-of-the-art approaches only take the tweet to be classified into consideration when classifying the sentiment; they ignore its context (i.e., related tweets).However, because tweets are usually short and more ambiguous, sometimes it is not enough to consider only the current tweet for sentiment classification.In this paper, we propose to improve target-dependent Twitter sentiment classification by 1) incorporating target-dependent features; and 2) taking related tweets into consideration.According to the experimental results, our approach greatly improves the performance of target-dependent sentiment classification.
We demonstrate how supervised discriminative machine learning techniques can be used to automate the assessment of ‘English as a Second or Other Language’ (ESOL) examination scripts.In particular, we use rank preference learning to explicitly model the grade relationships between scripts.A number of different features are extracted and ablation tests are used to investigate their contribution to overall performance.A comparison between regression and rank preference models further supports our method.Experimental results on the first publically available dataset show that our system can achieve levels of performance close to the upper bound for the task, as defined by the agreement between human examiners on the same corpus.Finally, using a set of ‘outlier’ texts, we test the validity of our model and identify cases where the model’s scores diverge from that of a human examiner.
A lack of standard datasets and evaluation has prevented the field of paraphrasmaking the kind of rapid progress enjoyed by the machine translation community over the last 15 years.We address both problems by presenting a novel data collection framework that produces highly parallel text data relatively inexpensively and on a large scale.The highly parallel nature of this data allows us to use simple n-gram comparisons to measure both the semantic adequacy and lexical dissimilarity of paraphrase candidates.In addition to being simple and efficient to compute, experiments show that these metrics correlate highly with human judgments.
hanb@student.unimelb.edu.au tb@ldwin.net Abstract Twitter provides access to large volumes of data in real time, but is notoriously noisy, hampering its utility for NLP.In this paper, we target out-of-vocabulary words in short text messages and propose a method for identifying and normalising ill-formed words.Our method uses a classifier to detect ill-formed words, and generates correction candidates based on morphophonemic similarity.Both word similarity and context are then exploited to select the most probable correction candidate for the word.The proposed method doesn’t require any annotations, and achieves state-of-the-art performance over an SMS corpus and a novel dataset based on Twitter.
Information extraction (IE) holds the promise of generating a large-scale knowledge base from the Web’s natural language text.Knowledge-based weak supervision, using structured data to heuristically label a training corpus, works towards this goal by enabling the automated learning of a potentially unbounded number of relation extractors.Recently, researchers have developed multiinstance learning algorithms to combat the noisy training data that can come from heuristic labeling, but their models assume are for example they extract the pair This paper presents a novel approach for multi-instance learning with overlapping relations that combines a sentence-level extraction model with a simple, corpus-level component for aggregating the individual facts.We apply our model to learn extractors for NY Times text using weak supervision from Freebase.Experiments show that the approach runs quickly and yields surprising gains in accuracy, at both the aggregate and sentence level.
Standard algorithms for template-based information extraction (IE) require predefined template schemas, and often labeled data, to learn to extract their slot fillers (e.g., an the a template).This paper describes an approach to template-based IE that removes this requirement and performs extraction without knowing the template structure in advance.Our algorithm instead learns the template structure automatically from raw text, inducing template schemas as sets of linked events (e.g., include off, deassociated with semantic roles.We also solve the standard IE task, using the induced syntactic patterns to extract role fillers from specific documents.We evaluate on the MUC-4 terrorism dataset and show that we induce template structure very similar to handcreated gold structure, and we extract role fillers with an F1 score of .40, approaching the performance of algorithms that require full knowledge of the templates.
Disambiguating concepts and entities in a context sensitive way is a fundamental problem in natural language processing.The comprehensiveness of Wikipedia has made the online encyclopedia an increasingly popular target for disambiguation.Disambiguation to Wikipedia is similar to a traditional Word Sense Disambiguation task, but distinct in that the Wikipedia link structure provides additional information about which disambiguations are compatible.In this work we analyze approaches that utilize this information to arrive at coherent sets of disambiguations for a given document (which we call “global” approaches), and compare them to more traditional (local) approaches.We show that previous approaches for global disambiguation can be improved, but even then the local disambiguation provides a baseline which is very hard to beat.
We address the problem of part-of-speech tagging for English data from the popular microblogging service Twitter.We develop a tagset, annotate data, develop features, and report tagging results nearing 90% accuracy.The data and tools have been made available to the research community with the goal of enabling richer text analysis of Twitter and related social media data sets.
In statistical machine translation, a researcher seeks to determine whether some innovation (e.g., a new feature, model, or inference algorithm) improves translation quality in comparison to a baseline system.To answer this question, he runs an experiment to evaluate the behavior of the two systems on held-out data.In this paper, we consider how to make such experiments more statistically reliable.We provide a systematic analysis of the effects of optimizer instability—an extraneous variable that is seldom controlled for—on experimental outcomes, and make recommendations for reporting results more accurately.
Transition-based dependency parsers generally use heuristic decoding algorithms but can accommodate arbitrarily rich feature representations.In this paper, we show that we can improve the accuracy of such parsers by considering even richer feature sets than those employed in previous systems.In the standard Penn Treebank setup, our novel features improve attachment score form 91.4% to 92.9%, giving the best results so far for transitionbased parsing and rivaling the best results overall.For the Chinese Treebank, they give a signficant improvement of the state of the art.An open source release of our parser is freely available.
Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems.However, most of these models arebuilt with only local context and one represen tation per word.This is problematic becausewords are often polysemous and global con text can also provide useful information for learning word meanings.We present a new neural network architecture which 1) learnsword embeddings that better capture the se mantics of words by incorporating both local and global document context, and 2) accountsfor homonymy and polysemy by learning mul tiple embeddings per word.We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate ourmodel on it, showing that our model outper forms competitive baselines and other neural language models.1
Natural language parsing has typically been done with small sets of discrete categories such as NP and VP, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases or splitting categories only partly address the problem at the cost of huge feature spaces and sparseness.Instead, we introduce a Compositional Vector Grammar (CVG), which combines PCFGs with a syntactically untied recursive neural network that learns syntactico-semantic, compositional vector representations.The CVG improves the PCFG of the Stanford Parser by 3.8% to obtain an F1 score of 90.4%.It is fast to train and implemented approximately as an efficient reranker it is about 20% faster than the current Stanford factored parser.The CVG learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as PP attachments.
discourse utterances that combine into of the discourse, namely, units discourse that are typically larger than a single sentence, but smaller than the complete discourse.However, the constituent structure is not determined solely by the linear sequence of utterances.It is common for two contiguous utterances to be members of different subconstituents of the discourse (as with breaks between phrases in the syntactic analysis of a sentence); likewise, it is common for two utterances that are not contiguous to be members of the same subconstituent.An individual subconstituent of a discourse exhibits both internal coherence and coherence with the other subconstituents.That is, discourses have been shown to of coherence. coherence to the ways in which the larger segments of discourse relate to one another.It depends on such things as the function of a discourse, its subject matter, and rhetorical schema 1977, 1981; Reichman, 19811. coherence refers to the ways in which individual sentences bind together to form larger discourse segments.It depends on such things as the syntactic structure of an utterance, and the use of referring expressions 1Sidner, 19811.The two levels of discourse coherence correspond to two of focusing—global centering.Participants are said to be globally focused on a set of entities relevant to the overall discourse.These entities may either have been explicitly introduced into the discourse or be sufficiently closely related to such entities to be considered implicitly in focus [Grosz, 19811.In contrast. centering refers to a more local focusing process, one relates to identifying the single entity that an individual utterance most centrally concerns (Sidner, 1979; Joshi and Weinstein, 19811.44 The two levels of focusing/coherence have different effects on the processing of pronominal and nonpronominal definite noun phrases.Global coherence and focusing are major factors in the generation and interpretation of nonpronominal definite referring expressions.- Local coherence and centering have greater effect on the processing of pronominal expressions.In Section 5 we shall describe the rules governing the use of these kinds of expressions and shall explain why additional processing by the hearer (needed for drawing additional inferences) is involved when pronominal expressions are used to refer to globally focused entities or nonpronominal expressions are used to refer to centered entities.Many approaches to language interpretation have ignored these differences, depending instead on powerful inference mechanisms to identify the referents of referring expressions.Although such approaches may suffice, especially for well-formed texts, they are insufficient in general.In particular, such approaches will not work for generation.Here the relationships among focusing, coherence, and referring expressions are essential and must be explicitly provided for.Theories—and systems based on them--will generate unacceptable uses of referring expressions if they do not take these into 3.Centering and Anaphora theory, the centers of a sentence in a discourse serve to integrate that sentence into the discourse.Each S, has a single center, a set of centers, Cb(S) serves to link S to the preceding discourse, while Cf(S) provides a set of entities to which the succeeding discourse may be linked.To avoid confusion, the phrase 'the center' will he used to refer only to Cb(S).To clarify the notion of center, we will consider a number of discourses illustrating the various factors that combined in (abstractly) and in its identification in a discourse.In Section 5 we define center more precisely, show how it relates to Sidner's [19811 immediate focus and potential foci, and discuss how the linkages established by the centers of a sentence help to determine the degree of intelligibility of a discourse.We begin by showing that the center cannot be defined in syntactic terms alone.The interaction of semantics and centering is more complex and is discussed in Section 4.The following examples, drawn from Reinhart [19821, illustrate the point that the notion of center is not i.e., the syntax of a sentence S not determine which of its NPs (The differ in other respects also.Reichman [19811 and Grosz 110811 discuss some of these. attempts to incorporate focusing mechanisms in generation systems are described in [Appelt, 1981 and McKeown, 19821. can obviously affect the interpretation; for of this paper, it may be regarded as part of a for the use of this terminology discussed (la) Who did Max see yesterday?(lb) Max saw Rosa.(2a) Did anyone see Rosa yesterday?(2b) Max saw Rosa.(lb) and (2b) are identical, Cb(lb) Max and Cb(2b) is Rosa.This can be seen in part by noticing that saw Rosa' seems more natural than (Ib) *Max saw her' than (2b) (a fact consistent with the centering rule introduced in Section 5.)The subject NP is the center in one context, the object NP in the other. when the NP used to realize Cb(S) syntactically determined, the Cb(S) itself is not yet fully determined, for Cb(S) is typically not a linguistic entity (i.e., it is not a particular linguistic expression).Rosa, not 'Rosa' is the Cb(2b).Consider. the discourse: (3a) How is Rosa?(3b) Did anyone see her yesterday? saw her.Here, Cb(3c) is Rosa, but clearly would not be in other contexts where the expression 'her' still realized the backward-looking center of 'Max saw her.'This is seen most simply by considering the discourse that would result if &quot;How is Joan?' replaced (3a).In the discourse that resulted, Joan, not Rosa, would be the center of (3c).4.Centering and Realization The interactions of semantic and pragmatic factors with centering and their effects on referring expressions are more complex than the preceding discussion suggests.In the examples given above, the NPs that realize Cb(S) also denote it, but this is not always the case: we used the term 'realize' in the above discussion advisedly.In this section, we consider two kinds of examples in which the center of a sentence is not simply the denotation of some noun phrase occurring in the sentence.First, we will examine several examples in which the choice of and interaction among different kinds of interpretations of definite noun phrases are affected by the local discourse context (i.e., centering).Second, the role of pragmatic factors in some problematic cases of referential uses of definite descriptions [Donnellan 19661 is discussed.4.1.Realization and Value-Free and Value-Loaded Interpretations distinction between semantic denotation is necessary to treat the interaction between value-free and value-loaded interpretations [Barwise and Perry, 19821 of definite descriptions, as they occur in extended discourse.Consider, for example, the following sequence: (4a) The vice president of the United States is also president of the Senate.(4b) Historically, he is the president's key man in negotiations with Congress. to China, he handled tricky negotiations, so he prepared for this Cb(4b) and Cb(4b') are each realized by the anaphoric element 'he.'But (4b) expresses the same thing as 'Historically, the vice president of the United States is the president's key man in negotiations with Congress' (in which it is clear that no single individual vice president is being referred to) whereas (4b1 expresses the same thing as, 'As ambassador to China, the [person who is now] vice president of the United States handled many tricky negotiations,...' This can be accounted for by observing that 'the vice president of the United States' contributes both its value-free interpretation and its value-loading at the world type to Cf(4a).Cb(4b) is then the value-free interpretation and Cb(4b') is the valueloading, i.e., George Bush.In this example, both value-free and value-loaded interpretations are shown to stern from the same full definite noun phrase.It is also possible for the movement of the center from a value-free interpretation (for Cb(S)) to a value-loaded interpretation (for Cb of the next sentence)--or vice versa—to be accomplished solely with pronouns.That is, although (4b)-(4b1 is (at least for some readers) not a natural dialogue, similar sequences are possible.There appear to be strong constraints on the kinds of transitions that are allowed.In particular, if a given sentence forces either the value-free or value-loaded interpretation, then only that interpretation becomes possible in a subsequent sentence.However, if some sentence in a given context merely prefers one interpretation while allowing the other, then either one is possible in a subsequent sentence.For example, the sequence (6a) The vice president of the United States also president Senate. the president's key aan in negotiations vith Congress. in which 'he' may be interpreted as either value-free or (VL), may be followed by either of following As to China, he zany tricky negotiations. is required to be at least old.However, if we change (5b) to force the value-loaded interpretation, as in (5b&quot;), then only (5c) is possible.(Sb') Right now he is the president's key man in negotiations with Congress.Similarly, if (5b) is changed to force the value-free interpretation, as in (4b), then only (5c') is possible.If an intermediate sentence allows both interpretations but prefers one in a given context, then either is possible in the third sentence.A use with preference for a valueloaded interpretation followed by a use indicating the value-free interpretation is illustrated in the sequence: John thinks that the telephone is a toy. it every day.(11 preferred; ok) He doesn't realize that it is an invention changed the world. preference a value-free interpretation that is by value-loaded one is easiest to see in a dialogue situation; vice president of the United States is also president of the Senate.I he played some role in the House.(VF preferred; VL did, but that was before he was Realization and Use these examples, might appear that the concepts of value-free and value-loaded interpretation are identical to DonneIlan's 119661 attributive and referential uses of noun phrases.However, there is an important difference between these two distinctions.The importance to our theory is that the referential use of definite noun phrases introduces the need to take pragmatic factors (in particular speaker intention) into account, not just semantic factors.Donnellan [1966] describes the referential and uses of descriptions in the following way: 'A speaker who uses a definite description attributively in an assertion states something whoever or whatever is the so-and-so. speaker who uses a definite description referentially in an assertion, on the other hand, uses the description to enable his audience to pick out whom or what he is talking about and states something about that person or thing.In the first case the definite description might be said to occur essentially, for the speaker wishes to assert something about whatever or whoever fits that description; but in the referential use the definite description is merely one tool for doing a certain job--calling attention to a person or thing--and in general any other device for doing the same job, another description or a name, would do as well.In the attributive use, the attribute of being the so-and-so is all important, while it is not in the referential use.'The distinction Donnellan suggests can be formulated in terms of the different propositions a sentence S containing a definite description D may be used to express on different occasions of use.When D is used referentially, it contributes its denotation to the proposition expressed by 46 S; when it is used attributively, it contributes to the proposition expressed by S a semantic interpretation related to the descriptive content of D. The identity of this semantic interpretation is not something about which Donnellan is explicit.Distinct formal treatments of the semantics of definite descriptions in natural language would construe the appropriate interpretation differently.In semantic treatments based on possible worlds, the appropriate interpretation would be a (partial) function from possible worlds to objects; in the situation semantics expounded by Barwise and Perry, the appropriate interpretation is a (partial) function from resource to objects.As just described, the referential-attributive distinction appears to be exactly the distinction that Barwise and Perry formulate in terms of the value-loaded and valuefree interpretations of definite noun phrases.But this gloss omits an essential aspect of the referentialattributive distinction as elaborated by Donnellan.In view, a speaker may use referentially to refer to an object distinct from the semantic denotation of the description, and, moreover, to refer to an object even when the description has no semantic denotation.In one sense, this phenomenon arises within the framework of Barwise and Perry's treatment of descriptions.If we understand the semantic denotation of a description to be the unique object that satisfies the content of the description, if there is one, then Barwise and Perry would allow that there are referential uses of a description D that contribute objects other than the semantic denotation of D to the propositions expressed by uses of sentences in which D occurs.But this is only because Barwise and Perry allow that a description may be evaluated at a resource situation other than the complete situation in order to arrive at its denotation on a given occasion of use.Still, the denotation of the description relative to a given resource situation is the unique object in the situation that satisfies the description relative to that situation.The referential uses of descriptions that Donnellan gives of do seem to arise by evaluation of descriptions at alternative resource situations, but rather through the *referential intentions' of the speaker in his of the description. aspect of referential use is a rather a semantic phenomenon and is best analyzed in terms of the distinction between semantic reference and speaker's reference elaborated in Kripke [10771. the following discourses from Kripke [10771: 'any situation on which the speaker can focus attention is a potential candidate for a resource situation with to which the speaker may value load uses of descriptions.Such resource situations must contain a unique object satisfies description. husband is kind to her.NO. isn't.The can you're referring to isn't her husband.Her husband to He her isn't her husband.With (6a) and (7a), Kripke has in mind a case like the one discussed in Donnellan [19661, in which a speaker uses a description to refer to something other than the referent of that description, the unique thing that satisfies the description (if there is one).Kripke analyzes this case as an instance of the general phenomenon of a clash of intentions in language use.In the case at hand, the speaker has a general intention to use the description to refer to its semantic referent; his intention, distinct from general semantic intention, is to use it to refer to a particular individual.He incorrectly believes that these two intentions coincide this gives rise to a use of referring expression in which the speaker's reference reference are (The speaker's referent is presumably the woman's lover).From our point of view, the importance of the case resides in its showing that Cf(S) may include more than one entity, that is realized by a single NP in S. In this case, 'her husband' contributes both the husband and the lover to Cf(6a) and Cf(7a).This can be seen by observing that both discourses seem equally appropriate and that the backward-looking centers of (6b) and (7b) are the husband and the lover, respectively, realized by their anaphoric elements.Hence, the forward-looking centers of a sentence may be related not semantically but to the that realize Hence, the importance of the referential/attributive distinction from our point of view is that it leads to cases in which the centers of a sentence may be pragmatically rather than semantically related to the noun phrases that realize them.5.Center Movement and Center Realization-- Constraints En the foregoing sections we have discussed a number of examples to illustrate two essential points.First, the noun phrase that realizes the backward-looking center of an utterance in a discourse cannot be determined from the of the utterance alone.Second, the relation c noun phrases centers solely a semantic a pragmatic relation.This discussion has proceeded at a rather intuitive level, without explicit elaboration of the framework we regard as appropriate for dealing with centering and its role in explaining discourse phenomena.Before going on to describe constraints on the realization relation that are, of course, several alternative explanations; e.g., the may believe that the description is more likely than to be interpreted correctly by the hearer.Ferreting out the in a given situation requires of belief and the like.A discussion of issues is beyond the this paper.67 explain certain phenomena in discourse, we should be somewhat more explicit about the notions of center and realization.We have said that each utterance S in a discourse has associated with it a backward-looking center, Cb(S), and a set of forward-looking centers, Cf(S).What manner of objects are these centers?They are the sort of objects that can serve as the semantic interpretations of singular That is, either they are objects in the world (e.g., planets, people, numbers) or they are functions from possible worlds (situations, etc.) to objects in the world that can be used to interpret definite descriptions.That is, whatever serves to interpret a definite noun phrase can be a center.For the sake of concreteness in many of the examples in the preceding discussion, we have relied on the situation semantics of Barwise and Perry.The theory we are developing does not depend on this particular semantical treatment of definite noun phrases, but it does require several of the distinctions that treatment provides.In particular, our theory requires a semantical treatment that accommodates the distinction between interpretations of definite noun phrases that contribute their content to the propositions expressed by sentences in which they occur and interpretations that contribute only their denotation—in other words, the distinction between value-free and value-loaded interpretations.As noted, a distinction of this sort can be effected within the framework of 'possible-worlds' approaches to the semantics of natural language.In addition, we see the need for interpretations of definite noun phrases to be dependent on their discourse context.Once again, this is a feature of interpretations that is accommodated in the relational approach to semantics advocated by Barwise and Perry, but it might be accommodated within other as Given that Cb(S), the center of sentence S in a discourse, is the interpretation of a definite noun phrase, how does it become related to S?In a typical example, S will contain a full definite noun phrase or pronoun that realizes the center.The realization relation is neither nor pragmatic.For example, realizes c in cases where a definite description and is interpretation, or an object related to it by a 'speaker's reference.'More when is pronoun, the principles that which c are such that realizes c from neither semantics nor pragmatics exclusively.They are principles that must be elicited from the study of itself.A tentative formulation of some principles is given below. it is typical that, when a center of S, S an that realizes c, is by no means necessary.In particular, for sentences containing noun treatment of our theory we will consider centers that are realized by constituents in other syntactic categories.119831 discusses some of these issues and compares several of with Montague semantics. phrases that express functional relations (e.g., 'the door,' 'the owner') whose arguments are not exhibited explicitly (e.g., a house is the current center, but so far its door nor its owner has been it is the case that such argument can be backward-looking center of the sentence.We are studying such and expect to integrate that into our theory of discourse The basic rule that constrains the realization of the backward-looking center of an utterance is a constraint on the speaker, namely: the Cb the current utterance is the same as the of the previous utterance, a pronoun should be are two things to about this rule.First, it not preclude using for other entities as long as one is used for the center.Second, it is not a hard but rather principle, like a Gricean maxim, that violated.However, such violations lead at best to in which the is forced to draw additional inferences. simple example, consider the following sequence, assuming at the outset that John is the center of the discourse: (8a) He called up Mike yesterday.(he=John) (8b) He vas annoyed by John's call.
128 24% 161 29% 47 9% 148 27% 32 6% 17 3% 11 2% 6.Discussion the rules for Fidditch are written as deterministic pattern-action rules of the same sort as the rules in the parsing grammar, their operation is in a sense isolable.The patterns of the self-correction rules are checked first, before any of the grammar rule patterns are checked, at each step in the parse.Despite this independence in terms of rule ordering, the operation of the self-corr,:ction component is closely tied to the grammar of the parser; for it is the parsing grammar that specifies what sort of constituents count as the same for copying. example, if the grammar did not treat a noun phrase when it is subject of a sentence, the self-correction rules could not properly resolve a sentence like People-a people from Kennsington the editing rules would never recognize that the same sort of element.(Note (13) treated as a Restart because lexical trigger is not present.)Thus, the observed pattern of self-correction introduces empirical constraints on the set of features that are available for syntactic rules.The self-correction rules impose constraints not only on what linguistic elements must count as the same, but also on what must count as different.For example, in sentence be recognized as different sorts of elements in the grammar for the AUX node to be correctly the grammar assigned the words exactly the same part of speech, then the Category C;.7y Editor necessarily apply, incorrectly expunging (14) Kid could-be a brain in school.It appears therefore that the pattern of self-corrections that occur represents a potentially rich source of evidence about the nature of syntactic categories. the patterns of self-correction count as about the nature of categories for the linguist, then this data must be equally available to the language learner.This would suggest that, far from being an impediment to language learning, non-fluencies may in fact facilitate language acquisition by highlighting equivalent classes. expunction of edit signal only surface copy category copy stack copy restart failures remaining unclear and ungrammatical 127 This raises the general question of how children can acquire a language in the face of unrestrained non-fluency.How can a language learner sort out the grammatical from the ungrammatical strings?(The non-fluencies of speech are of course but one aspect of the degeneracy of input that makes language acquisition a puzzle.)The self-correction system I have described suggests that many non-fluent strings can be resolved with little detailed linguistic knowledge.As Table 1 shows, about a quarter of the editing signals result in expunction of only non-linguistic material.This requires only an ability to distinguish linguistic from nonlinguistic stuff, and it introduces the idea that edit signals signal an expunction site.Almost a third are resolved by the Surface Copying rule, which can be viewed simply as an instance of the general non-linguistic rule that multiple instances of the same thing count as a single instance.The category copying rules are generalizations of simple copying, applied to a knowledge of linguistic categories.Making the transition from surface copies to category copies is aided by the fact that there is considerable overlap in coverage, defining a path of expanding generalization.Thus at the earliest stages of learning, only the simplest, non-linguistic self-correction rules would come into play, and gradually the more syntactically integrated would be acquired.Contrast this self-correction system to an approach that handles non-fluencies by some general problem solving routines, for example Granger (1982), who proposes reasoning from what a speaker might be expected to say.Besides the obvious inefficiencies of general problem solving approaches, it is worth giving special emphasis to the problem with learnability.A general problem solving approach depends crucially on evaluating the likelihood of possible deviations from the norms.But a language learner has by definition only partial and possibly incorrect knowledge of the syntax, and is therefore unable to consistently identify deviations from the grammatical system.With the editing system I describe, the learner need not have the ability to recognize deviations from grammatical norms, but merely the non-linguistic ability to recognize copies of the same thing. far, I have considered the selfcorrection component from the standpoint of parsing.However, it is clear that the origins are in the process of generation.The mechanism for editing self-corrections that I have proposed has as its essential operation expunging one two identical is unable to expunge a sequence of two elements.(The Surface Copy Editor might be viewed as a counterexample to this claim, but see below.)Consider expunction now from the standpoint of the generator.Suppose self-correction bears a one-to-one relationship to a possible action of the generator (initiated by some monitoring component) which could be called ABANDON CONSTRUCT X.And suppose that this action can be initiated at any time up until CONSTRUCT X is completed, when a signal is returned that the construction is complete.Further suppose that ABANDON CONSTRUCT X causes an editing signal.When the speaker decides in the middle of some linguistic element to abandon it and start again, an editing signal is produced.If this is an appropriate model, then the elements which are self-corrected should be exactly those elements that xist at some stage in the generation process.Thus, we should be able to find evidence for the units involved in generation by looking at the data of self-correction.And indeed, such evidence should be available to the language learner as well.Summary I have described the nature of self-corrected speech (which is a major source of spoken non-fluencies) and how it can be resolved by simple editing rules within the context of a deterministic parser.Two features are essential to the self-correction system: 1) every self-correction site (whether it results in the expunction of words or not) is marked by a phonetically identifiable signal placed at the right edge of the potential expunction site; and 2) the expunged part is the left-hand member of a pair of copies, one on each side of the editing signal.The copies may be of three types: 1) identical surface strings, which are edited by a matching rule that applies before syntactic analysis begins; 2) complete constituents, when two constituents of the same type appear in the parser's buffer; or 3) incomplete constituents, when the parser finds itself trying to complete a constituent of the same type as a constituent it has just completed.Whenever two such copies appear in such a configuration, and the first one ends with an editing signal, the first is expunged from further analysis.This editing system has been implemented as part of a deterministic parser, and tested on a wide range of sentences from transcribed speech.Further study of the self-correction system promises to provide insights into the units of production and the nature of linguistic categories.
Linguists, including computational linguists, have always been fond of talking about trees.In this paper, we outline a theory of linguistic structure which talks about talking about trees; we call theory theory While important issues must be resolved before a complete picture of D-theory emerges (and also before we can build programs which utilize it), we believe that this theory will ultimately provide a framework for explaining the syntax and semantics of natural in a manner which is This paper will focus primarily on one set of motivations for this theory, those engendered by attempts to handle certain syntactic phenomena within the framework of deterministic parsing.
By exploring the relationship between parsing and deduction, a new and more general view of chart parsing is obtained, which encompasses parsing for grammar formalisms based on unification, and is the basis of the Earley Deduction proof procedure for definite clauses. efficiency of this approach for an interesting class grammars is discussed.
The paper discusses the linguistic aspects of a new general purpose facility for computing with features.The program was developed in connection with the course I taught at the University of Texas in the fall of 1983.It is a generalized and expanded version of a system that Stuart Shieber originally designed for the PATR-II project at SRI in the spring of 1983 with later modifications by Fernando Pereira and me.Like its predecessors, the new Texas version of the &quot;DG (directed graph)&quot; package is primarily intended for representing morphological and syntactic information but it may turn out to be very useful for semantic representa
Arg = (Sense Var) P2 = Arg = (Sense Var)1 This, in turn, is readily interpretable as a description of the logical expression Vq.dog(q)AP(g) It remains to provide verbs with a sense that provides a suitable for is, for (Sense Prop P2 Pred).An example would be the following: Cat = V Lex = barks Tense = Pres Pers = 3 Subj = {Num = Sing Anim = + Obj = NONE = [P2 = [Fred = bark]]] IV Conclusion It has not been possible in this paper to give more than an impression of how an experimental machine translation system might be constructed based on FUG.I hope, however, that it has been possible to convey something of the value of monotonic systems for this purpose.Implementing FUG in an efficient way requires skill and a variety of little known techniques.However, the programs, though subtle, are not large and, once written, they provide the grammarian and lexicographer with an emmense wealth of expressive devices.Any system implemented strictly within this framework will be reversible in the sense that, if it translates from language A to language B the, to the same extent, translates from B to A.If the set among the translations it delivers for a, then a will be among the translations of each of I of no system that comes close to providing these advantages and I know of no facility provided for in any system proposed hitherto that it not subsumable under FUG =
A considerable body of accumulated knowledge about the design of languages for communicating information to computers has been derived from the subfields of programming language design and semantics.It has been the goal of the PATR group at SRI to utilize a relevant portion of this knowledge in implementing tools to facilitate communication of linguistic information to computers.The PATR-TI formalism is our current computer language for encoding linguistic information.This paper, a brief overview of that formalism, attempts to explicate our design decisions in terms of a set of properties that effective computer languages should incorporate.
A correct structural analysis of a discourse is a prerequisite for understanding it.This paper sketches the outline of a discourse grammar which acknowledges several different levels of structure.This grammar, the &quot;Dynamic Discourse Model&quot;, uses an Augmented Transition Network parsing mechanism to build a representation of the semantics of a discourse in a stepwise fashion, from left to right, on the basis of the semantic representations of the individual clauses which constitute the discourse.The intermediate states of the parser model the intermediate states of the social situation which generates the discourse.The paper attempts to demonstrate that a discourse may indeed be viewed as constructed by means of sequencing and recursive nesting of discourse constituents.It gives rather detailed examples of discourse structures at various levels, and shows how these structures are described in the framework proposed here.STRUCTURES AT DIFFERENT LEVELS If a discourse understanding system is to be able to assemble the meaning of a complex discourse fragment (such as a story or an elaborate description) out of the meanings of the utterances constituting the fragment, it needs a correct structural analysis of it.Such an analysis is also necessary to assign a correct semantic interpretation to clauses as they occur in the discourse; this is seen most easily in cases where this interpretation depends on phenomena such as the discourse scope of temporal and locative adverbials, the movement of the reference time in a narrative, or the interpretation of discourse anaphora.The Dynamic Discourse Model, outlined in this paper, is a discourse grammar under development which analyses the structure of a discourse in order to be able to deal adequately with its semantic aspects.It should be emphasized at the outset that this system is a formal model of discourse syntax and semantics, but not a computer implementation of such a model.For a system to be able to understand a discourse, it must be able to analyse it at several different levels.1.Any piece of talk must be assigned to one Interaction-i.e., to a socially constructed verbal exchange which has, at any moment, awell-defined set of participants.2.Virtually every interaction is viewed by its as belonging to a particular predefined genre -be it a doctor-patient interaction, a religious ceremony, or a casual chat.Depending on the genre, certain participants may have specific roles in the verbal exchange, and there may be a predefinedagenda specifying consecutive parts of the interaction.An interaction which is socially &quot;inin such a fashion is called a Event(Hymes,1967,1972).3.A stretch of talk within one Speech Event may be as dealing with one Topic.4.Within a Topic, we may find one or more Dis- Units(DU's) -socially acknowledged units of talk which have a recognizable &quot;point&quot; or purpose, while at the same time displaying a specific syntactic/semantic structure.Clear examples are stories, procedures, descriptions, and jokes.5.When consecutive clauses are combined into one syntactic/semantic unit, we call this unit a constituent unit(dcu).Examples are: lists, narrative structures, and various binary structures (&quot;A but B&quot;, &quot;A because B&quot;, etc.).Adjacency Structuresmay well be viewed as a kind of dcu, but they deserve special mention.They are two or three part conversational routines involving speaker change.The clearest examples are question-answer pairs and exchanges of greetings.7.The smallest units which we shall deal with at discourse level are clausesand operators.Operators include &quot;connectors&quot; like &quot;and&quot;, &quot;or&quot;, &quot;because&quot;, as well as &quot;discourse markers&quot; like &quot;well&quot;, &quot;so&quot;, &quot;incidentally&quot;.The levels of discourse structure just discussed are hierarchically ordered.For instance, any DU must be part of a Speech Event, while it must be built up out of dcu's.The levels may thus be viewed as an expansion of the familiar linguistic hierarchy of phoneme, morpheme, word and clause.This does not mean, however, that every discourse is to be analysed in terms of a five level tree structure, with levels corresponding to dcu, DU, Topic, Speech Event and Interaction.To be able to describe discourse as it actually occurs, discourse constituents of various types must be allowed to be embedded in constituents of the same and other types.We shall see various examples of this in later sections.It is worth emphasizing here already that &quot;high level constituents&quot; may be embedded in &quot;low level constituents&quot;.For instance, a dcu may be interrupted by a clause which initiates another Interaction.Thus, a structural description of the unfolding discourse would include an Interaction as embedded in the dcu.In 413 this way, we can describe &quot;intrusions&quot;, &quot;asides to third parties&quot;, and other interruptions of one Interaction by another.In the description of discourse semantics, the level of the dcu's (including the adjacency structures) plays the most central role: at this level the system defines how the semantic representation of a complex discourse constituent is constructed out of the semantic representations of its parts.The other levels of structure are also of some relevance, however: - The Discourse Unit establishes higher level semantic coherence.For instance, the semantics of different episodes of one story are integrated at this level.- The Topic provides a frame which determines the interpretation of many lexical items and descriptions.- The Speech Event provides a script which describes the conventional development of the discourse, and justifies assumptions about the purposesofdiscourse participants.- The Interaction specifies referents for indexicals like &quot;I&quot;, &quot;you&quot;, &quot;here&quot;, &quot;now&quot;.II THE DYNAMIC DISCOURSE MODEL Dealina with linguistic structures above the clause level is an enterprise which differs in an essential way from the more common variant of linguistic activity which tries to describe the internal structure of the verbal symbols people exchange.Discourse linguistics does not study static verbal objects, but must be involved with the social process which produces the discourse -with the ways in which the discourse participants manipulate the obligations and possibilities of the discourse situation, and with the ways in which their talk is constrained and framed by the structure of this discourse situation which they themselves created.The structure one may assign to the text of a discourse is but a reflection of the structure of the process which produced it.Because of this, the Dynamic Discourse Model that we are developing is only indirectly involved in trying to account for the a posteriori structure of a finished discourse; instead, it tries to trace the relevant states of the social space in terms of which the discourse is constructed.This capability is obviously of crucial importance if the model is to be applied in the construction of computer systems which can enter into actual dialogs.The Dynamic Discourse Model, therefore, must construct the semantic interpretation of a discourse on a clause by clause basis, from left to right, yielding intermediate semantic representations of unfinished constituents, as well as setting the semantic parameters whose values influence the interpretation of subsequent constituents.A syntactic/semantic system of this sort may very well be fromulated as an Augmented Transition Network grammar (Woods, 1970), a non-deterministic parsing system specified by a set of transition networks which may call each other recursively.Every Speech Event type, DU type and dcu type is associated with a transition network specifying its internal structure.As a transition network processes the consecutive constituents of a discourse segment, it builds up, step by step, a representation of the meaning of the segment.This representation is stored in a register associated with the network.At any stage of the Process, this register contains a representation of the meaning of the discourse segment so far.An ATN parser of this sort models important aspects of the discourse process.After each clause, the system is in a well-defined state, characterized by the stack of active transition networks and, for each of them, the values in its registers and the place where it was interrupted.When we say that discourse participants know &quot;where they are&quot; in a complicated discourse, we mean that they know which discourse constituent is being initiated or continued, as well as which discourse constituents have been interrupted where and in what order -in other words, they are aware of the embedding structure and other information captured by the ATN configuration.The meaning of most clause utterances cannot be determined on the basis of the clause alone, but involves register values of the embedding dcu -as when a question sets up a frame in terms of which its answer is interpreted (cf.Scha, 1983) or when, to determine the temporal reference of a clause in a narrative, one needs a &quot;reference time&quot; which is established by the foregoing part of the narrative (section III B 2).From such examples, we see that the discourse constituent unit serves as a framework for the semantic interpretation of the clauses which constitute the text.By the same token, we see that the semantics of an utterance is not exhaustively described by indicating its illocutionary force and its propositional content.An utterance may also cause an update in one or more semantic registers of the dcu, and thereby influence the semantic interpretation of the following utterances.This phenomenon also gives us a useful perspective on the notion of interruption which was mentioned before.For instance, we can now see the difference between the case of a story being interrupted by a discussion, and the superficially similar case of a story followed by a discussion which is, in its turn, followed by another story.In the first case, the same dcu is resumed and all its register values are still available; in the second case, the first story has been finished before the discussion and the re-entry into a storyworld is via a different story.The first story has been closed off and its register values are no longer avilable for re-activation; the teller of the second story must re-initialize the variables of time, place and character, even if the events of the second story concern exactly the same characters and situations as the first.Thus, the notions of interruption and resumption have not only a social reality which is experienced by the interactants involved.They also have semantic consequences for the building and interpretation of texts.Interruption and resumption are often explicitly signalled by the occurrence of &quot;discourse markers&quot;.Interruption is signalled by a PUSHmarker such as &quot;incidentally&quot;, &quot;by the way&quot;, &quot;you or &quot;like&quot;.Resumption is signalled by a POP- 414 -markers such as &quot;O.K.&quot;, &quot;well&quot;, &quot;so&quot; or &quot;anyway&quot;.(For longer lists of discourse marking devices, and somewhat more discussion of their functioning, see Reichman (1981) and Polanyi and Scha(1983b).)In terms of our ATN description of discourse structure, the PUSHand POP-markers do almost exactly what their names suggest.A PUSH-marker signals the creation of a new embedded discourse constituent, while a POP-marker signals a return to an embedding constituent (though not necessarily the immediately embedding one), closing off the current constituent and all the intermediate ones.The fact that one POP-marker may thus create a whole cascade of discourse-POPs was one of Reichman's (1981) arguments for rejecting the ATN model of discourse structure.We have indicated before, however, that accommodating this phenomenon is at worst a matter of minor technical extensions of the ATNformalism (Polanyi and Scha, 1983b); in the present paper, we shall from now on ignore it.
To facilitate work in discourse interpretation, the logical form of English sentences should be both close to English and syntactically simple.In this paper I propose • logical notation which is first-order and nonintensional, and for which semantic translation can be naively compositional.The key move is to expand what kinds of entities one allows in one's ontology, rather than complicating the logical notation, the logical form of sentences, or the semantic translation process.Three classical problems adverbials, the distinction between and dicto belief reports, and the problem of identity in intensional contexts are examined for the difficulties they pose for this logical notation, and it is shown that the difficulties can be overcome.The paper closes with a statement about the view of semantics that is presupposed by this approach.The real problem in natural language processing is the interpretation of discourse.Therefore, the other aspects of the total process should be in the service of discourse interpretation.This includes the semantic translation of sentences into a logical form, and indeed the logical notation itself.Discourse interpretation processes, as I see them, are inferential processes that manipulate or perform deductions on logical expressions encoding the information in the text and on other logical expressions encoding the speaker's and hearer's background knowledge.These considerations lead to two principal criteria for • logical notation.Criterion I: The notation should be as close to English as possible.This makes it easier to specify the rules for translation between English and the formal language, and also makes it easier to encode in logical notation facts we normally think of in English.The ideal choice by this criterion is English itself, but it fails monumentally on the second criterion.Criterion II: The notation should be syntactically simple.Since discourse processes are to be defined primarily in terms of manipulations performed on expressions in the logical notation, the simpler that notation, the easier it will be to define the discourse operations.The development of such a logical notation is usually taken to be a very hard problem.I believe this is because researchers have imposed upon themselves several additional constraints to adhere to stringent ontological scruples, to explain a number of mysterious syntactic facts as a by-product of the notation, and to encode efficient deduction techniques in the notation.Most representational difficulties go away if one rejects these constraints, and there are good reasons for rejecting each of the constraints.Ontological scruples: Researchers in philosophy and linguistics have typically restricted themselves to very few (although a strange assortment of) kinds of entities physical objects, numbers, sets, times, possible worlds, propositions, events, and situations and all of these but the first have been controversial.Quine has been the greatest exponent of ontological chastity.His argument is that in any scientific theory, we adopt, at least insofar as we are reasonable, the simplest conceptual scheme into which the disordered fragments of our experience can be fitted arranged.&quot; (Quine, 1953, P.16.)But he goes on to say that 'simplicity ... is not a clear and unambiguous idea; and it is quite capable of presenting a double or multiple standard.&quot; 17.)Minimizing kinds of entities is not the only way to achieve simplicity in a theory.The aim in this enterprise is achieve simplicity by minimizing the complexity of the in the system.It turns out this can be achieved by multiplying kinds of entities, bY allowing as an entity everything that can be referred to by a noun phrase.Syntactic explanation: The argument here is easy.It would be pleasant if an explanation of, say, the syntactic behavior of count nouns and mass nouns fell out of our underlying ontological stricture at no extra cost, but if the extra cost is great complication in statements of discourse operations, it would be quite unpleasant.In constructing a theory of discourse interpretation, it doesn't make sense for us to tie our hands by requiring syntactic explanations as well.The problem of discourse is at least an order of magnitude harder than the problem of syntax, and syntax shouldn't be in the driver's seat.Efficient deduction: There is a long tradition in artificial intelligence of building control information into the notation, and indeed much work in knowledge representation is driven by consideration.Semantic networks and other notational sysaround hierarchies (Quillian, 1968; Simmons, Hendrix, 1975) implicitly assign a low cost to certain types of syllogistic reasoning.The KL-ONE representation language and Brachman, 1982) has a variety of notational dewith an associated efficient deduction procedure.Hayes (1979) has argued that frame representations (Minsky, 1975; Bobrow and Winograd, 1977) should be viewed as sets of predicate calculus axioms together with a control component for drawing certain kinds of inferences quickly.In quite a different vein, Moore (1980) uses a possible worlds notation to model and action in part to avoid inefficiencies in theorem- 61 proving.By contrast, I would argue against building efficiencies into the notation.From a psychological point of view, this allows us to abstract away from the details of implementation on a particular computational device, increasing the generality of the theory.From a technological point of view, it reflects a belief that we must first determine empirically the most common classes of inferences required for discourse processing and only then seek algorithms for optimizing them.In this paper I propose a flat logical notation with an ontologically promiscuous semantics.One's first naive guess as to how to represent a simple sentence like A boy builds a boat. is as follows: This simple approach seems to break down when we encounter the more difficult phenomena of natural language, like tense, intensional contexts, and adverbials, as in the sentence A boy wanted to build a boat quickly.These phenomena have led students of language to introduce significant complications in their logical notations for representing sentences.My approach will be to maintain the syntactic simplicity of the logical notation and expand the theory of the world implicit in the semantics to accommodate this simplicity.The representation of the above sentence, as is justified below, is es, es, y)Paagel z, es)Aquickqes, es) y) occurred in the past, where is es, is the quickness of es, which is z's building of a boy and a boat.In brief, the logical form of natural language sentences will be a conjunction of atomic predications in which all variables are existentially quantified with the widest possible scope.Predicates will be identical or nearly identical to natural language morphemes.There will be no functions, functional., nested quantifiers, disjunction., negations, or modal or intensional operators.2 The Logical Notation Davidson (1967) proposed a treatment of action sentences in which events are treated as individuals.This facilitated the representation of sentences with time and place adverbials.Thus we can view the sentences John ran on Monday.John ran in San Francisco. as asserting the existence of a running event by John and asserting a relation between the event and Monday or San Francisco.We can similarly view the sentence John ran slowly. as expressing an attribute about a running event.Treating events as individuals is also useful because they can be arguments of statements about causes: Because he wanted to get there first, John ran.Because John ran, he arrived sooner than anyone else.They can be the objects of propositional attitudes: Bill was surprised that John ran. this approach accomodates the facts that events can nominalized and can be referred to pronominally: John's running tired him out.John ran, and Bill saw it.But virtually every predication that can be made in natural language can be specified as to time and place, be modified adverbially, function as a cause or effect of something else, be the object of a propositional attitude, be nominalized, and be referred to by a pronoun.It is therefore convenient to extend Davidson's approach to all predications.That is, corresponding any predication that be made in natural language, we will say there is an event, or state, or condition, or situation, or *eventuality&quot;, or whatever, in the world that it refers to. approach might be called 'ontological One ontological scruples.Thus we would like to have in our logical notation the possibility of an extra argument in each predication referring to the &quot;condition&quot; that exists when that predication is true.However, especially for expository convenience, we would like to retain the option of not specifying that extra argument when it is not needed and would only get in our way.Hence, I propose a logical that provides two predicates that are systemby introducing might be called a '.Corresponding io n-ary predicate there will he an n + first. argument be thought of as condition that holds when p is true the subsequent arguments.Thus, run(J) means that John ( E J) that running event by John. or running.If slippery(F) means that floor slippery, F) that condition of F's being slippery, or F's slipperiness.The effect of this notational mato provide handles by which various can grasped by higher predications. similar approach has been used in many Al systems. discourse one not only makes such epheevents, states and conditions. also refers to entities do not actually Our notation must thus have a way referring to such entities.We therefore take model to be a Platonic universe which contains everything that can be spoken of objects, events, states, conditions whether they exist in the real world or not.It then may or may not be a property of such entities that they exist in the real world.In the sentence (1) John worships Zeus, the worshipping event and John, but not Zeus, exist in the real world, but all three exist in the (overpopulated) Platonic universe.Similarly, in John wants to fly.62 John's flying exists in the Platonic universe but not in the real The logical notation then is just first-order predicate calculus, where the universe of discourse is a rich set of individuals, which are real, possible and even impossible objects, events, conditions, eventualities, and so on.Existence and truth in the actual universe are treated as predications about individuals in the Platonic universe.For this purwe use a predicate formula J 0 N ) says the individual in the Platonic universe denoted JOHN in the actual The formula Exist(E) runi(E, the condition John's running exists the actual universe, or more simply that &quot;John runs&quot; is true, or still more simply, that John runs.A shorter way to write it is N).Although for a simple sentence like 'John runs&quot;, a logical form like (2) seems a bit overblown, when we come to real sentences in English discourse with their variety of tenses, modalities and adverbial modifiers, the more elaborated logical form is necessary.Adopting the notation of (2) has the effect of splitting a into its propositional content rtire(E, and assertional claim - E xist( E).This frequently out to be useful, as the latter is often in doubt until substantial work has been done by discourse interpretation processes.An entire sentence may be embedded within an indirect proof or other extended counterfactual.We are now in a position to state formally the systematic relation between the unprimed and primed predicates as an axiom schema.For every n-ary predicate p, (Vi, D (36)Ezisf(c)AptIc, is, p is true of , there is a condition e of p's true of and e exists.Conversely, , , , is. if e is the condition of p's being true of then p is true of can compress these axiom schemas into one formula: :=7 A sentence in English asserts the existence of one or more eventualities in the real world, and this may or may not imply the existence of other individuals.The logical form of sentence (1) is JOHN, ZEUS) H N) but not Exist(Z S).Similarly. form of &quot;John wants to fly&quot; is need not adhere to Platonism to accept Platonic universe.It be viewrti as a socially constituted, or conventional, construction, which is nevertheless highly constrained by the way the (not directly accessible) world is.The degree of constraint is variable.We constrained by the material world to believe in trees and chairs, less so to believe in patriotism or ghosts.'The reader might choose to think of the Platonic universe as the universe possible individuals, although I do not want to exclude impossible individuals, such as the condition John believes to exist when he believes 6 + 7 = 15.'McCarthy (1977) employs a similar technique.JOHN) Exist(J0 N) but not When the existence of the condition corresponding to some predication implies the existence of one of the arguments of the predication, we say that the predicate is transparent in that argument, worship and want are transparent in their first arguments and opaque in their second arguments.In if a predicate p is transparent in its nth argument this can be encoded by the axiom is, if e is p's being true of x e then Equivalently, z, ...) In the absence of such axioms, predicates are assumed to be opaque.The following sentence illustrates the extent to which we must have a way of representing existent and nonexistent states and ordinary discourse.(4) The government has repeatedly refused to deny that Prime Minister Margaret Thatcher vetoed the ltannel Tunnel at summit meeting with President Mitterand on 18 Scientist last week.&quot; In addition to the ordinary individuals Margaret Thatcher and Mitterand and the corporate entity are intervals of timr 18 May and the entity, the 'hannel Tunnel. an iii (i', event and the complex event if I he ing, actually occurred, a set, of real refusals distributed across time in a particular way, a denial event which did not occur, and a event which may or may not us Post( mean that existed in t he past f ect( mean what the perfect roughly. existed in the past. and may not yet be iltlj liii The representation of just the verb, nominalit ations. adverbials and of ) is as El) K 1) f denyl (;OUT, E3) A ) A ( ) 18M U )A VS. Ks) week( Of the variotis entities referred to, the sentence. via Imprinted asserts the existence of a typiriel refusal in a ,et refusals anti the revelation Tile ext,tence the existence (if the ov,'rnmIni,'iiI Ii (,,', not howl% the of the denial; (nine the oppte-tie Ii way •■14..■!..7t.-1 of the veto, but certainly doe, Hot i pl■ it .The revelaimplies the existence of both the Scientist properly, we should say &quot;existentially transparent&quot; and opaque&quot;, since this notion not coincide exactly with referential transparency. in this notation is aiways entities in the Platonic uni• Existence in real world is expressed by predicates, in particular predicate sentence is taken from the 1%2 indebted to Paul Martin calling to attention.63 the at relation E4, which in turn implies the existence of the veto and the meeting.These then imply the existence of Thatcher President Mitterand not Channel course, we know about the existence of some of these entities, such as Margaret Thatcher and President Mitterand, for reasons other than the transparency of predicates.Sentence (4) shows that virtually anything can be embedded in a higher predication.This is the reason, in the logical notation, for flattening everything into predications about individuals.There are four serious problems that must be dealt with if this approach is to work quantifiers, opaque adverbials, the between and dicto of belief reports, and the problem of identity in intensional contexts.I have described a solution to the quantifier problem elsewhere (Hobbs, 1983).Briefly, universally quantified variables are reified as typical elements of sets, existential quantification inside the scope of universally quantified variables are handled by means of dependency functions, and the quantifier structure of sentences is encoded in indices on predicates.In this paper I will address only the other three problems in detail.3 Opaque Adverbials It seems reasonably natural to treat transparent adverbials as properties of events.For opaque adverbials, like 'almost&quot;, it seems less natural, and one is inclined to follow Reichenbach (1947) in treating them as functionals mapping predicates into predicates.Thus, John is almost a man. would be represented almost(man)(/) That is, almost maps the predicate man into the predicate &quot;almost a man', which is then applied to John.This representation is undesirable for our purposes since it is not first-order.It would be preferable to treat opaque operators as we do transparent ones, as properties of events or conditions.The sentence would be represented J) But does this get us into difficulty?First note that this representation does not imply that John is a man, for we have not asserted E's existence in the real world, and almost is opaque and does not imply its argument's existence. is there enough information in allow one to determine truth value of isolation; without appeal to other facts?The answer is that there could be.We can construct model in which for every functional is a corresponding predicate that z)) The existence of the model shows that this condition is not necessarily contradictory. the universe of discourse the class of finite sets built out of a finite set of urelements.The interpretation of a constant be some element of call it I(X). interpretation a monadic predicate p will a subset of call it I(p). such that plE, X), we define the interpretation of < 1(p), >. suppose we have a functional predicates into We can define the corresponding predicate be such that true if there are a predicate p and a constant the interpretation of is < 1(p), I(X) > true. fact that we can define such a predicate a moderately rich model means that we are licensed to treat opaque adverbials as properties of events and conditions.The purpose of this exercise is only to show the viability of approach.I am not claiming that a running event pair of the runner and the all runners, although it should be harmless enough for those irredeemably committed to set-theoretic semantics to view it like that.It should be noted that this treatment of adverbials has confor the individuating eventualities.We can say &quot;John is almost a man&quot; without wishing to imply 'John is almost a mammal,&quot; so we would not want to say that John's bea man is the being mammal.We are forced, though not unwillingly, into a position of individuating eventualities according to very fine-grained criteria.Re Dicto Reports The next problem concerns the distinction (due to Quine (1956)) ditto reports.A belief report like (5) John believes a man at the next table is a spy. two interpretations.The diet° likely in the circumstance in which John and some man are at adjacent and John observes suspicious behavior. re interlikely if some man is sitting at the table next the speaker of the sentence, and John is nowhere around but knows man otherwise and suspects him to be a A sentence very nearly forces the re believes Bill's mistress is whereas the sentence believes Russian consulate are spies. indicates a dicto the de re reading (5), John is not necessarily taken to know that the man is in at the next table, but he is assumed to to the man somehow.More on below.In the John believes there is a who is both at the table and a spy, but may be unable to identify man.The re (5) usually taken to support the inference There is someone John believes to be a the supports the weaker inference (7) John believes that someone is a spy. is due to Moore and Hendrix (1982).64 As Quine has pointed out, as usually interpreted, the first of these sentences is false for most of us, the second one true.A common notational maneuver (though one that Quine rejects) is to represent this distinction as a scope ambiguity.Sentence (6) is encoded as (8) and (7) as (9): (8) (3x)believe(J , spy(x)) (9) believe(J,(3x)spy(x)) If one adopts this notation and stipulates what the expressions mean, then there are certainly distinct ways of representing the two sentences.But the interpretation of the two expressions is not obvious.It is not obvious for example that (8) could not cover the case where there is an individual such that John believes him to be a spy but has never seen him and knows absolutely nothing else about him not his name, nor his appearance. nor his location at any point in time beyond the fact that he is a spy.In fact, the notation we propose takes (8) to be the most representation.Since quantification over in the Platonic universe, (8) says that there is some entity in the Platonic universe such that John believes of that entity that it is a spy.Expression (8) commits us to no other beliefs on the part of John.When understood in this way, expression (8) is representation of what is conveyed in de dirto report.Translated into the flat notation and introducing a constant for the existentially quantified variable, (8) becomes believe( J P) S) Anything else that John believes about this entity must be explicitly.In particular, the dicto of be represented by something like believe(J, P) S) J , Q) , S,T) the next table.That is, John believes that a and that the table.John may know many other about still fall short of knowing is.There is a range of possibilities for John's knowledge, from hare statements of (10) and (11) that correspond to a to the full-blown knowledge of S's identity that is present in a In fact, an FBI agent would progress through just such a range of belief states on his way to identifying the spy.To state John's knowledge of S's identity properly, we would have to state explicitly John's belief in a potentially very large collection of properties of the spy.To arrive at a succinct way of representing knowledge of identity in our notation, let its contwo of equivalent sentences: that?Identify that.The FBI doesn't know who the spy is.The FBI doesn't know the spy's identity.The answer to the question 'Who are you?&quot; and what is rebefore we can say that we know is or that we know their identity is a highly context-dependent matter.Several years ago, before I had ever seen Kripke, if someone had me whether I knew who Saul was, 1 have 'Yes.He's the author of Naming and once I was at a workshop which I knew was being attended by Kripke, but I didn't yet know what he looked like.If someone had asked me whether I knew who Kripke was, I would have had to say, &quot;No.&quot; The relevant property in that context was not his authorship of some paper, but any property that distinguished him from the others present, such as the man in the back row holding a cup of coffee&quot;.Knowledge of a person's identity is then a matter of knowing some context-dependent essential property that serves to identify that person for present purposes that is, a matter of or she is.Therefore, we need a kind of place-holder predicate to stand for this essential property, that in any particular context can be specified more precisely.It happens that English has a morthat serves just this function morpheme Let posit a predicate stands for the context ially determined property or conjunction of properties that would count identification in that particular context. reading of generally taken to John's of the identity of the spy.Assuming report would be as a conjunction of two beliefs, one for the tnaM predication and the other expressing knowledge of the essential property. the what-ness, of the of predication.J P) .spy'( P, X) A al] I. ci wh'((j, is, John a spy and John knows who let us probe just a little more deeply in particular call into quest it knowledge of is really part of the meaning of the sentence in the re The representation of of F).I said. is P) P, S) J , S • T) us represent the reading as .1 Apy'l P. Exi.0)(,)) A ,IP(y.13 J. common (12) (1:') t he I.11.P, 5) There ambil.Tuit y exists the real world is merely believed John dicta). addition, ri) includes conjuncts J, R) line (13b). are these part of the int erpretat 5?The following example (10111)1 his ..;tilipose entire Rotary (lob is at t he table nest iit he 5. but Joliddoesn't know t his.John belie% i' —.me memof the rhib a sp■. but 111, i),' a 111(11 5 this lint ion. and only ( holds. not (12).Judgments are uncertain to sentence 5 is appropriate in circumstances, it is certain that the sentence John believes someone at the next table is a spy. is appropriate, and that is sufficient for the argument. seems then that the conjuncts R) R. S) are not part of what we want in the initial logical form of the sentence,' but only a very common conversational implicature.The reason the implicature is very common is that if of it: they are not part of the meaning of sentence.65 John doesn't know that the man is at the next table, there must be some other description under which John is familiar with the man.The story I just told provides such a description, but not one sufficient for identifying the man. analysis is attractive since it allows us to view the re dicto problem as just one instance of a much more general problem, namely, the existential status of the grammatically subordinated material in sentences.Generally, such material takes on the tense of the sentence.Thus, in The boy built the boat. building event by z of place in the past, and we assume a boy in the past, at the time of the building.But in Many rich men studied computer science in college. the most natural reading is not that the men were rich when they were studying computer science but that they are rich now.In The flower is artificial. there is an entity z which is described as a flower, and z exists, its 'flower-nese does not exist in the real world. is a which is embedded in the opaque predicate was stated above that the representation (10) for the conveys no properties of than that John believes him to be a spy.In particular, it does not convey S's in the real world. refers to a possible individual, may turn out to if, for example.John ever comes to be able to identify the person whom he believes to be the spy, or if there is some actual spy who has given John good cause for his suspicions. not be actual, only possible.Suppose this is the case.One common objection to possible individuals is that they may seem to violate the Law of the Excluded Middle.Is or not married?Our intuition is that the question is inappropriate, and indeed the answer given in our formalism this flavor.By axiom is really just an abfor marrierr(E,S) is false, for the of the real world would imply the existence of also false.But its falsity has nothing to do with S's marital status, only his existential status.The predication unmarried(S) is false for the same reason.The primed predicates are basic, and for them the problem of the excluded does not arise.The predication married( S) true false depending on whether is condition of S's being married.An unprimed, transparent predicate carries along with it the existence of its arguments, and it can fail to be true of an entity either through the entity being actual but not having that property or through the nonexistence of the entity.5 Identity in Belief Contexts final problem I will consider arises in dicto reports.It is the problem of identity in intensional contexts, raised by (1892).One way of stating the problem is this.Why it that if (14) John believes the Evening Star is rising. and if the Evening Star is identical to the Morning Star, it is not necessarily true that (15) John believes the Morning Star is rising.Leibniz's Law, we ought to to substitute for an entity any entity that is identical to it.This puzzle survives translation into the logical notation, if John knows of the existence of the Morning Star and if proper are unique.The representation for (the dicto of) sentence (14) is believe(J, rise( ES) John's belief in the Morning Star would he represented A existence of the Evening Star and the Morning Star expressed by uniqueness of the proper name 'Evening is expressed by the axiom Ettentrig-Star(y) x y The identity of the Evening Star and the Morning Star is expressed all of this we can infer that the Morning also Evening Star and hence identical to hence can into rise( give have is a representation for the paradoxical (15).There are three possibilities for dealing with this problem. first is to or restrict Leibniz's Law.The second is to deny that the Evening Star and the Morning Star are identical as entities in the Platonic universe; they only happen to he identical in the real world, and that is not sufficient for intersubstitutivity. third is to deny that expression (16) represents (14) because 'the Evening Star&quot; in (14) does not refer to what it seems to refer to.The first possibility is the approach of researchers who treat belief as an operator rather than as a predicate, and then re• strict substitution inside the operator?We cannot avail ourselves of this. solution because of the flatness of our notation. predicate rise is surely referentially transparent, so if S identical, MS can he substituted for the , give riselP,„t/S).Then the exnot even require substitution to he belief about the Morning In any case, this approach does not seem wise in view of the central importance played in discourse interpretation by the identity of differently presented entities, i.e. by coreference.Free intersubstitutibility of identicals seems a desirable property to preserve.The second possible answer to Frege's problem is to say that in the Platonic universe, the Morning Star and the Evening Star 'This 4 a purely syntactic approach, and when one tries to construct a semantics for it, one is generally driven to the third possibility.66 are different entities.It just happens that in the real world they identical.But it is not true that = MS, equality, like quantification, is over entities in the Platonic universe.The fact identical in the real world (call this relation rw-identieel) must be stated explicitly, say, by the expression MS) properly, rwidentieal(z, y) For reasoning about 're-identical* entities, that is, Platonic entities that are identical in the real world, we may take the following approach.Substitution in referentially transparent contexts would be achieved by use of the axiom schema , ...)pi , ...) A ...) A rin-identical(es, is the kth argument of p and p is referentially transin its That is, if p's being true of and identical in the real world, then there is a es of p's being true of 64, and es is identical to the real world.Substitution of &quot;rw-identicals' in a condition results not in the same condition but in an &quot;rw-identical' condition. would be such an axiom for the first argument believe but not for its referentially opaque second argument.Axioms will express the fact that rw-identieal is an equivalence relation: y) D 14v-identical(y, z) y, s)rvridentieagz, y) z) ,v-identieal(z. z) Finally, the following axiom, together with axiom (17), would express Leibniz's Law: es) (Ezist(et all we can prove that if the Evening Star rises then the Morning Star rises, but we cannot prove from John's that the Evening that John believes the Morning Star rises.If John knows the Morning Star and the Evening Star are identical, and he knows axiom (17), then his belief that the Morning Star rises can be proved as one would prove his belief in the consequences of any other syllogism whose premises he believed, in accordance with a treatment of reasoning about developed in a longer version this This solution is in the spirit of our whole representational approach in that it forces us to be painfully explicit about everything.The notation does no magic for us.There is a significant cost associated with this solution, however.When proper names are represented as predicates and not as constants, the natural way to state the uniqueness of proper names is by means of foilowing sort: tar(z) tar(y) z since from the axioms for we can show that tar( M S), it follow that We thus restate the axiom for the uniqueness of proper names as rel-identieal(z, y) A similar modification must be made for functions.Since we are using only predicates, the uniqueness of the value of a function must be encoded with an axiom like y, ather(y, z = and y are both fathers of z and y are the same.This have to by the axiom z)A f ather(y, z) y) The very common problems involving reasoning about equality, which can be done efficiently, are thus translated into problems involving reasoning about the predicate rw-identical, which is very cumbersome. way to view this second solution is as a fix to the first solution.For &quot;=1&quot; we substitute the relation rw-identical, and by means of axiom schema (17), we force substitutions to propagate to the eventualities they occur in, and we force the distinction between referentially transparent and referentially opaque predicates to be made explicitly.It is thus an indirect way of rejecting Leibniz' Law. third solution is to say that 'the Evening Star' sentence (14) does not really refer to the Evening Star, but to some abstract entity somehow related to the Evening Star.That is, sentence (14) is really an example of metonymy.This may seem counterintuitive, and even bizarre, at first blush.But in fact the most widely accepted classical solutions to the problem of are of this flavor.For Frege (1892) 'the Evening in sentence (14) does not refer to the Evening Star but to the the phrase 'the Evening Star'.In a more recent approach.Zalta (1983) takes such noun phrases to refer to 'abstract objects' related to the real object.In both approaches noun phrases in intensional contexts refer to senses or abstract objects, while other noun phrases refer to actual entities, and so it is necessary to specify which predicates are intensional.In a Montagovian approach, 'the Evening Star' would be taken to refer to the intension of the Evening Star, .not its eztenaion in real world, and noun phrases would to refer to intensions, although for nonintensional predicates there would be meaning postulates that make this equivalent to reference to extensions.Thus, in all these approaches intensional and extensional predmust be distinguished explicitly, and noun phrases in intensional contexts are systematically interpreted metonymically.It would be easy enough in our framework to implement these approaches.We can define a function a of three arguments the entity, and the condition used to entity the sense, or intension, abstract corresponding to actual for that cognizer and Sentence would be represented, not as (16), but as A ri eel( , o( ES , J, J ,Q AEveninpS tari(Q , ES) tend to prefer to think of the value as abstract Whatever it is, it is necessary that the of J, Qi)be something different from the value Qs) where is, different objects must correspond to the condition of being Star and the condition of being the Morning It because of this feature that we escape the problem 67 intersubstitutivity of identicals, for substitution of (18) yields `... 41)) rather than A S, J, A , would be the representation of sentence (1$).The difficulty with this approach is that it makes the interpretation of noun phrases dependent on their embedding context: Intensional context metonymic interpretation Extensional context — nonmetonymic interpretation It thus violates, though not seriously, the naive compositionality that I have been at so many pains to preserve.Metonymy is a very common phenomenon in discourse, but I prefer to think of it as occurring irregularly, and not as signalled systematically by other elements in the sentence.Having laid out the three possible solutions and their shortcomings, I find that I would like to avoid the problem of identity altogether.The third approach suggests a ruse for doing so.We can assume that, in general, (16) is the representation of sentence (14).We invoke no extra complications where we don't have to.When, in interpreting the text, we encounter a difficulty resulting (rota the problem of identity, we can go back and revise our interpretation of (14), by assuming the reference must have been • metonymic one to the abstract entity and not the actual entity.In those cases it would be as are saying, 'John couldn't believe about the Evening Star itself that it is rising.The paradox shows that he is insufficiently acquainted with the Evening Star to refer to it directly.He must be talking about an abstract entity related to the Evening Star.* My guess is that we will not have to resort to this ruse often, for I suspect the problem rarely arises in actual discourse interpretation.6 The Role of Semantics Let me close by making some comments about ways of doing Semantics is the attempted specification relation between language and the world.However, this requires theory of the world.There of choices one can in this regard. one of the spectrum let's say the right end one can adopt the 'correct' theory of the world, the theory given by quantum mechanics and the other sciences.If one does this, semantics becomes impossible because it is no less than all of science, a fact that has led Fodor (1980) to express some despair.There's too much of • mismatch between the way we view the world and the way the world really is.At the left one can assume a theory world that is isomorphic to way we talk about it.What been doing in this paper, fact, is an effort to work out the details such theory.In this case, semantics becomes very nearly trivial.Most activity in semantics today is slightly to the right of the extreme left end of this spectrum.One makes certain assumptions about the nature of the world that closely redeet language, and doesn't make other sesumptiosui.Where one has failed to necessary assumptions, puzzles appear, and semantics becomes effort to solve those puzzles.Nevertheless, rails move far enough away from language to represent significant progress toward the right end of the spectrum.The position I advocate is that there is no reason to make our tisk more difficult.We will have puzzles enough to solve when we get to discourse.
This paper describes the SDC PUNDIT, (Prolog UNDerstands Integrated Text), system for processing language PUNDIT, written in Prolog, is a highly modular system consisting of distinct syntactic, semantic and pragmatics components.Each component draws on one or more sets of data, including a lexicon, a broad-coverage grammar of English, semantic verb decompositions, rules mapping between syntactic and semantic constituents, and a domain model.This paper discusses the communication between the syntactic, semantic and pragmatic modules that is necessary for making implicit linguistic information explicit.The key is letting syntax and semantics recognize missing linguistic entities as implicit entities, so that they can be labelled as such, and reference resolution can be directed to find specific referents for the entities.In this way the task of making implicit linguistic information explicit becomes a subset of the tasks performed by reference resolution.The success of this approach is dependent on marking missing syntactic constituents as missing semantic roles as that reference resolution can know when to look for referents.
A constraint is proposed in the Centering approach to pronoun resolution in discourse.This &quot;property-sharing&quot; constraint requires that two pronominal expressions that retain the same Cb across adjacent utterances share a certain common grammatical property.This property is expressed along the dimension of the grammatical function SUBJECT for both Japanese and English discourses, where different pronominal forms are primarily used to realize the Cb.It is the zero pronominal in Japanese, and the (unstressed) overt pronoun in English.The resulting constraint complements the original Centering, accounting for its apparent violations and providing a solution to the interpretation of multi-pronominal utterances.It also provides an alternative account of anaphora interpretation that appears to be due to structural parallelism.This reconciliation of centering/focusing and parallelism is a major advantage.I will then add another dimension called the &quot;speaker identification&quot; to the constraint to handle a group of special cases in Japanese discourse.It indicates a close association between centering and the speaker's viewpoint, and sheds light on what underlies the effect of perception reports on pronoun resolution in general.These results, by drawing on facts in two very different languages, demonstrate the cross-linguistic applicability of the centering framework. this Centers are semantic objects--(sets of) individuals, objects, states, actions, or events--represented in complex ways so that a strict coreference need not hold between related A center mentioned in the current utterance may be mentioned again in the next utterance (by the same or a different speaker).In this sense, a center is &quot;forward-looking&quot; (Cf).Crucially, one of the centers may be identified as &quot;backward-looking&quot; (Cb).Cb the entity an utterance centrally Its main role is to connect the current utterance to the preceding The term Center is used for the Cb.Thus an utterance may be associated with any number of Cfs, one of which may be the Cb.These Cfs are given a default Cb order, that &quot;how much each center is expected to be the next Cb&quot;.I regard Cb to be optional for It comes into exsistence by way of a that is, the process in which a previous non-Cb becomes the new Cb in discourse.(1981, 1983) focus foci local focusing correspond to Cb and Cfs, respectively.The difference is that Sidner uses two immediate foci (Discourse Focus and Actor Focus) while centering uses only one (Cb) (see Grosz et. al.1983 for discussion).Various factors --syntactic, semantic, and pragmatic-are combined for the identification of the Cb.One of them the of pronominal expressions, expressed in the
In this paper we present a formalization of the centering approach to modeling attentional structure in discourse and use it as the basis for an algorithm to track discourse context and bind pronouns.As described in [GJW86], the process of centering attention on entities in the discourse gives rise to the intersentential states of, retaining shiftpropose an extension to these states which handles some additional cases of multiple ambiguous pronouns.The algorithm has been implemented in an HPSG natural language system which serves as the interface to a database query application.
Although disjunction has been used in several unificationbased grammar formalisms, existing methods of unification have been unsatisfactory for descriptions containing large quantities of disjunction, because they require exponential time.This paper describes a method of unification by successive approximation, resulting in better average performance.
To interpret a sentence: An approach to abductive inference developed in the TAC- ITUS project has resulted in a dramatic simplification of how the problem of interpreting texts is conceptualized.Its use in solving the local pragmatics problems of reference, compound nominals, syntactic ambiguity, and metonymy is described and illustrated.It also suggests an elegant and thorough integration of syntax, semantics, and pragmatics.
We conducted an empirical analysis into the relation between control and discourse structure.We applied control criteria to four dialogues and identified 3 levels of discourse structure.We investigated the mechanism for changing control between these structures and found that utterance type and not cue words predicted shifts of control.Participants used certain types of signals when discourse goals were proceeding successfully but resorted to interruptions when they were not.
The relations are used as plans; their intended effects are interpreted as the goals they achieve.In other words, in order to bring about the state which both speaker and hearer know that the purpose of know that they both know it, etc.), the structurer uses Purpose as a plan and tries to satisfy its constraints.In this system, constraints and goals are interfor example, in the event that believed not by the satellite constraint of the resimply becomes the goal to achieve (BMB (RESULT Similarly, the propo- Ow s SCAN-1 ?ACT-2)) (DMB (08.1 are interpreted as the goal to find some element that could legitimately take place of In order to enable the relations to nest recursome relations' nucleuses and satellites contain requirements that specify additional relations, such as examples, contrasts, etc.Of course, these additional requirements may only be included if such material can coherently follow the content of the nucleus or satellite.The question of ordering such additional constituents is still under investigation.The question of whether such additional material should be included at all is not addressed; the structurer tries to say everything it is given.The structurer produces all coherent paragraphs (that is, coherent as defined by the relations) that satisfy the given goal(s) for any set of input elements.For example, paragraph (b) is produced to the initial goal S H (SEQUENCE goal is produced by PEA, together with the appropriate representation ele- (ASK-I. in response to the does the system a program?.Different initial goals will result in different paragraphs.Each paragraph is represented as a tree in which branch points are RST relations and leaves are input elements.Figure 1 is the tree for para- (b).It contains the relations (signalled by 'then' and 'finally&quot;), Elaboration (&quot;in particular&quot;), and Purpose (&quot;in order to&quot;).In the corresponding paragraph produced by Penman, the relations' characteristic words or phrases (boldfaced below) appear between the blocks of text they relate: [The system asks the user to tell it the characteristic of the program to be system applies to the progran2.](0 system scans the proo order to opportunities to apply transformations to the system resolves [It confirms the enhancewith the [it performs the enhancement.ho 166 input sentence generator --ot update agenda choose final plan get next bud RST relations expand bud grow tree Figure 2: Hierarchical Planning Structurer 6-The Structurer As stated above, the structurer is a simplified top-down hierarchical expansion planner (see Figure 2).It operates as follows: given one or more communicative goals, it finds RST relations whose intended effects match (some of) these goals; it then inspects which of the input elements match the nucleus and subgoal constraints for each relation.Unmatched constraints become subgoals which are posted on an agenda for the next level of planning.The tree can be expanded in either depth-first or breadth-first fashion.Eventually, process bottoms out when either: (a) all input elements have been used and unsatisfied subgoals remain (in which case the structurer could request more input with desired properties from the encapsulating system); or (b) all goals are satisfied.If more than one plan (i.e., paragraph tree structure) is produced, the results are ordered by preferring trees with the minimum unused number of input elements and the minimum number of remaining unsatisfied subgoals.The best tree is then traversed in left-to-right order; leaves provide input to Penman to be generated in English and relations at branch points provide typical interclausal relation words or phrases.In this way the structurer performs top-down goal refinement down to the level of the input elements. and Further Work This work is also being tested in a completely separate domain: the generation of text in a multimedia system that answers database queries.Penman produces the following description of the ship Knox (where CTG 070.10 designates a group of ships): Knox is en route in rendezvous with CTG 070.10, arriving in Pearl Harbor on 4/24, for port visit until 4/30.In this text, each clause (en route, rendezvous, arrive, visit) is a separate input element; the structurer linked them using the relations Sequence and Purpose (the same Purpose as shown above; it is signalled by &quot;in order to&quot;).However, Penman can also be made to produce (d).Knox is en route in order to rendezvous with CTG 070.10.It will arrive in Pearl Harbor on 4/24.It will be on port visit until 4/30.The problem is clear: how should sentences in the paragraph be scoped?At present, avoiding any claims about a theory, the structurer can feed 167 Penman either extreme: make everything one sentence, or make each input element a separate sentence.However, neither extreme is satisfactory; as is clear from paragraph (b), &quot;short&quot; spans of text can be linked and &quot;long&quot; ones left separate.A simple way to implement this is to count the number of leaves under each branch (nucleus or satellite) in the paragraph structure tree.Another shortcoming is the treatment of input elements as indivisible entities.This shortcoming is a result of factoring out the problem of aggregation as a separate text planning task.Chunking together input elements (to eliminate detail) or taking them apart (to be more detailed) has received scant mention — see Ellovy 871, and for the related problem of paraphrase see [Schank 75] — but this task should interact with text structuring in order to provide text that is both optimally detailed and coherent.At the present time, only about 20% of the RST relations have been formalized to the extent that they can be used by the structurer.This formalization process is difficult, because it goes handin-hand with the development of terms with which to characterize the relations' goals/constraints.Though the formalization can never be completely finalized — who can hope to represent something like motivation or justification complete with all ramifications?— the hope is that, by having the requirements stated in rather basic terms, the relations will be easily adaptable to any new representation scheme and domain.(It should be noted, of course, that, to be useful, these formalizations need only be as specific and as detailed as the domain model and representation requires.)In addition, the availability of a set of communicative goals more detailed than just say or ask (for example), should make it easier for programs that require output text to interface with the generator.This is one focus of current text planning work at ISI.
We present an algorithm for generating strings from logical form encodings that improves upon previous algorithms in that it places fewer restrictions on the class of grammars to which it is applicable.In particular, unlike an Earley deduction generator (Shieber, 1988), it allows use of semantically nonmonotonic grammars, yet unlike topdown methods, it also permits left-recursion.The enabling design feature of the algorithm is its implicit traversal of the analysis tree for the string being generated in a semantic-head-driven fashion.
rs Sunday, calling for greater economic reforms to mtniasion asserted that &quot;the Postal Service could Then, she said, the family hopes to e out-of-work steelworker.&quot;because that doesn't &quot;We suspend reality when we say we'll scientists has won the first round in an effort to about three children in a mining town who plot to GM executives say the shutdowns will rtment as receiver, instructed officials to try to The package, which is to newly enhanced image as the moderate who moved to million offer from chairman Victor Posner to help after telling a delivery-room doctor not to try to h birthday Tuesday, cheered by those who fought to at he had formed an alliance with Moslem rebels to &quot; Basically we could We worked for a year to their expensive mirrors, just like in wartime, to ard of many who risked their own lives in order to We must increase the amount Americans save China from poverty. save enormous sums of money in contracting out individual c save enough for a down payment on a home. save jobs, that costs jobs.&quot; save money by spending $10,000 in wages for a public works save one of Egypt's great treasures, the decaying tomb of R save the &quot;pit ponies &quot;doomed to be slaughtered. save the automaker $500 million a year in operating costs a save the company rather than liquidate it and then declared save the country nearly $2 billion, also includes a program save the country. save the financially troubled company, but said Posner stil save the infant by inserting a tube in its throat to help i save the majestic Beaux Arts architectural masterpiece. save the nation from communism. save the operating costs of the Pershings and pound-launch save the site at enormous expense to us, &quot;said Leveillee. save them from drunken Yankee brawlers, &quot;Tess said. save those who were passengers.&quot; save.&quot; Figure 2: Some AP 1987 Concordance lines to 'save ... from,' roughly sorted into categories save X from Y (65 concordance lines) 1 save PERSON from Y (23 concordance lines) 1.1 save PERSON from BAD (19 concordance lines) ( Robert DeNiro ) to &quot;We wanted to Murphy was sacrificed to &quot;God sent this man to Pope John Paul II to&quot; save Indian tribes(PERSON] from genocide[DESTRUCT[BAD]] at the hands of save hirn(PERSON] from undue trouble(BADI and loss[BAD] of money, &quot; save more powerful Democrats[PERSON] from harm(BAD] . save my five diikiren[PERSON] from being burned to death[DESTRUCT(BAD]] and save us(PERSON1 from sin[BAD] .&quot; 1.2 save PERSON from (BAD) LOC(ATION) (4 concordance lines) rescuers who helped save the toddler(PERSON] from an abandoned well[LOC] will be feted with a parade while attempting to save two drowning boys(PERSON] from a turbulent(BAD] creek[LOC] in Ohio(LOC) 2. save INST(ITUTION) from (ECON) BAD (27 concordance lines) member states to help should be sought &quot;to law was necessary to operation &quot;to were not needed to his efforts to save the BEC[INST] from possible bankruptcy[ECONPAD] this year. save the company[CORP[DIST]] from bankruptcy(ECON)(BAD] . save the country[NATIONINSTD from dismter(BAD1 • save the nation[NATION[INST]J from Conununism[BADNPOLITICAL] save the system from bankruptcy[ECONHBAD] . save the world(INST] from the likes of Lothar and the Spider Woman 3. save ANIMAL from DESTRUCT(ION) (5 concordance lines) give them the money to save the dogs[ANIMAL] from being destroyed(DESTRUC11 , program intended to save the giant birds[ANIMAL] from extinction[DESTRUCT] , UNCLASSIFIED (10 concordance lines) walnut and ash trees to save them from the axes and saws of a logging company. after the attack to save the ship from a terrible[BAD] fire , Navy reports concluded Thursday.
In order to take steps towards establishing a methodology for evaluating Natural Language systems, we conducted a case study.We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues.We present the quantitative results of handsimulating these algorithms, but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general.We illustrate the general difficulties encountered with quantitative evaluation.These are problems with: (a) allowing for underlying assumptions, (b) determining how to handle underspecifications, and (c) evaluating the contribution of false positives and error chaining.
present a new grammatical formalism called Con- Dependency Grammar in which every rule is given constraint on wordto-word modifications.CDG parsing is formalized as a constraint satisfaction problem over a finite domain so that efficient constraint-propagation algorithms can be employed to reduce structural ambiguity without generating individual parse trees.The weak generative capacity and the computational complexity of CDG parsing are also discussed_
Collocational knowledge is necessary for language gener- The problem collocations come in a large variety of forms.They can involve two, three or more words, these words can be of different syntactic categories and they can be involved in more or less rigid ways.This leads to two main difficulties: collocational knowledge has to be acquired and it must be represented flexibly so that it can be used for language generation.We address both problems in this paper, focusing on the acquisition problem.We describe a program, Xtract, that automatically acquires a range of collocations from large textual corpora and we describe how they can be represented in a flexible lexicon using a unification based formalism.
A method of determining the similarity of nouns on the basis of a metric derived from the distribution of subject, verb and object in a large text corpus is described.The resulting quasi-semantic classification of nouns demonstrates the plausibility of the distributional hypothesis, and has potential application to a variety of tasks, including automatic indexing, resolving nominal compounds, and determining the scope of modification.
This paper presents a new approach for resolving lexical ambiguities in one language using statistical data on lexical relations in another language.This approach exploits the differences between mappings of words to senses in different languages.We concentrate on the problem of target word selection in machine translation, for which the approach is directly applicable, and employ a statistical model for the selection mechanism.The model was evaluated using two sets of Hebrew and German examples and was found to be very useful for disambiguation.
In this paper we describe a statistical technique for aligning sentences with their translations in two parallel corpora.In addition to certain anchor points that are available in our data, the only information about the sentences that we use for calculating alignments is the number of tokens that they contain.Because we make no use of the lexical details of the sentence, the alignment computation is fast and therefore practical for application to very large collections of text.We have used this technique to align several million sentences in the English-French Hansard corpora and have achieved an accuracy in excess of 99% in a random selected set of 1000 sentence pairs that we checked by hand.We show that even without the benefit of anchor points the correlation between the lengths of aligned sentences is strong enough that we should expect to achieve an accuracy of between 96% and 97%.Thus, the technique may be applicable to a wider variety of texts than we have yet tried.
Researchers in both machine translation (e.g., al., and bilingual lexicography (e.g., Klavans and Tzoulcermann, 1990) have recently become interested in studying parallel texts, texts such as the Canadian Hansards (parliamentary proceedings) which are available in multiple languages (French and English).This paper describes a method for aligning sentences in these parallel texts, based on a simple statistical model of character lengths.The method was developed and tested on a small trilingual sample of Swiss economic reports.A much larger sample of 90 million words of Canadian Hansards has been aligned and donated to the ACL/DCI.
This paper describes an implemented program that takes a raw, untagged text corpus as its only input (no open-class dictionary) and generates a partial list of verbs occurring in the text and the subcategorization frames (SFs) in which they occur.Verbs are detected by a novel technique based on the Case Filter of Rouvret and Vergnaud (1980).The completeness of the output list increases monotonically with the total number of occurrences of each verb in the corpus.False positive rates are one to three percent of observations.Five SFs are currently detected and more are planned.Ultimately, I expect to provide a large SF dictionary to the NLP community and to train dictionaries for specific corpora.
We propose that ambiguous prepositional phrase attachment can be resolved on the basis of the relative strength of association of the preposition with noun and verb, estimated on the basis of word distribution in a large corpus.This work suggests that a distributional approach can be effective in resolving parsing problems that apparently call for complex reasoning.
We describe a statistical technique for assigning senses to words.An instance of a word is assigned a sense by asking a question about the context in which the word appears.The question is constructed to have high mutual information with the translation of that instance in another language.When we incorporated this method of assigning senses into our statistical machine translation system, the error rate of the system decreased by thirteen percent.
hiyan@cam. sri . corn ralcam. sri. corn ABSTRACT Aspects of semantic interpretation, such as quantifier scoping and reference resolution, are often realised computationally by non-monotonic operations involving loss of information and destructive manipulation of semantic representations.The paper describes how monotonic reference resolution and scoping can be carried out using a revised Quasi Logical Form (QLF) representation.Semantics for QLF are presented in which the denotations of formulas are extended monotonically as QLF expressions are resolved.
We have analyzed 607 sentences of spontaneous human-computer speech data containing repairs, drawn from a total corpus of 10,718 sentences.We present here criteria and techniques for automatically detecting the presence of a repair, its location, and making the appropriate correction.The criteria involve integration of knowledge from several sources: pattern matching, syntactic and semantic analysis, and acoustics.
K. Lan i and S. J.Young.1990.The estimation of stochastic context-free grammars using the Insidealgorithm.Speech and Lan- K. Lan i and S. J.Young.1991.Applications of stochastic context-free grammars using the Insidealgorithm.Speech and Lan- David Magerman and Mitchell Marcus.1990.Parsing a natural language using mutual informastatistics.In MA.Yves Schabes.1992.Stochastic lexicalized treegrammars.In 92.Forthcoming.
We have recently reported on two new word-sense disambiguation systems, one trained on bilingual material (the Canadian Hansards) and the other trained on monolingual material (Roget's Thesaurus and Grolier's Encyclopedia).After using both the monolingual and bilingual classifiers for a few months, we have convinced ourselves that the performance is remarkably good.Nevertheless, we would really like to be able to make a stronger statement, and therefore, we decided to try to develop some more objective evaluation measures.Although there has been a fair amount of literature on sense-disambiguation, the literature does not offer much guidance in how we might establish the success or failure of a proposed solution such as the two systems mentioned in the previous paragraph.Many papers avoid quantitative evaluations altogether, because it is so difficult to come up with credible estimates of performance.This paper will attempt to establish upper and lower bounds on the level of performance that can be expected in an evaluation.An estimate of the lower bound of 75% (averaged over ambiguous types) is obtained by measuring the performance produced by a baseline system that ignores context and simply assigns the most likely sense in all cases.An estimate of the upper bound is obtained by assuming that our ability to measure performance is largely limited by our ability obtain reliable judgments from human informants.Not surprisingly, the upper bound is very dependent on the instructions given to the judges.Jorgensen, for example, suspected that lexicographers tend to depend too much on judgments by a single informant and found considerable variation over judgments (only 68% agreement), as she had suspected.In our own experiments, we have set out to find word-sense disambiguation tasks where the judges can agree often that we could show that they were outperforming the baseline system.Under quite different conditions, we have found 96.8% agreement over judges.
have been a number of recent papers on aligning parallel texts at the sentence level, e.g., Brown al and Church (to appear), Isabelle (1992), Kay and Rosenschein (to appear), Simard al Warwick— Armstrong and Russell (1990).On clean inputs, such as the Canadian Hansards, these methods have been very successful (at least 96% correct by sentence).Unfortunately, if the input is noisy (due to OCR and/or unknown markup conventions), then these methods tend to break down because the noise can make it difficult to find boundaries, let alone sentences.This paper describes a new program, aligns texts at the level rather than at the sentence/paragraph level, based on the cognate approach proposed by Simard al.
In this paper, we describe a fast algorithm for aligning sentences with their translations in a bilingual corpus.Existing efficient algorithms ignore word identities and only consider sentence (Brown al., Gale and Church, 1991).Our algorithm constructs a simple statistical word-to-word translation model on the fly during alignment.We find the alignment that maximizes the probability of generating the corpus with this translation model.We have achieved an error rate of approximately 0.4% on Canadian Hansard data, which is a significant improvement over previous results.The algorithm is language independent.
The paper describes an algorithm that employs English and French text taggers to associate noun phrases in an aligned bilingual corpus.The taggers provide part-of-speech categories which are used by finite-state recognizers to extract simple noun phrases for both languages.Noun phrases are then mapped to each other using an iterative re-estimation algorithm that bears similarities to the Baum-Welch algorithm which is used for training the taggers.The algorithm provides an alternative to other approaches for finding word correspondences, with the advantage that linguistic structure is incorporated.Improvements to the basic algorithm are described, which enable context to be accounted for when constructing the noun phrase mappings.
While a different order for these predictions is possible, we only experimented with this one.Parameter Estimation We only have built a decision tree to the rule probability component (3) of the model.For the mowe are using with the usual interpolation smoothing for the other four components of the model.We have assigned bit strings to the syntactic and semantic categories and to the rules manually.Our intention is that bit strings differing in the least significant bit positions correspond to categories of non-terminals or rules that are similar.We also have assigned bitstrings for the words in the vocabulary (the lexical heads) using automatic clustering algorithms using the bigram mutual information clustering algorithm (see (5)).Given the bitsting of a history, we then designed a decision tree for modeling the probability that a rule will be used for rewriting a node in the parse tree.Since the grammar produces parses which may be more detailed than the Treebank, the decision tree was built using a training set constructed in the following manner.Using the grammar with the P-CFG model we determined the most likely parse that is consistent with the Treebank and considered the resulting sentence-tree pair as an event.Note that the grammar parse will also provide the lexical head structure of the parse.Then, we extracted using leftmost derivation order tuples of a history (truncated to the definition of a history in the HBG model) and the corresponding rule used in expanding a node.Using the resulting data set we built a decision tree by classifying histories to locally minimize the entropy of the rule template.With a training set of about 9000 sentencetree pairs, we had about 240,000 tuples and we grew a tree with about 40,000 nodes.This required 18 hours on a 25 MIPS RISC-based machine and the resulting decision tree was nearly 100 megabytes.Immediate vs. Functional Parents model employs two types of parents, the and the The a list Figure 3: Sample representation of &quot;with a list&quot; in HBG model.R: PP1 Syn: PP H1: list with R: NBAR4 Syn: NP Sem: Data H1: list H2: a R: N1 Syn: N Sem: Data H1: list H2: * 35 immediate parent is the constituent that immediately dominates the constituent being predicted.If the immediate parent of a constituent has a different syntactic type from that of the constituent, then the immediate parent is also the functional parent; otherwise, the functional parent is the functional parent of the immediate parent.The distinction between functional parents and immediate parents arises primarily to cope with unit productions.When unit productions of the form XP2 ---> XP1 occur, the immediate parent of XP1 is XP2.But, in general, the constituent XP2 does not contain enough useful information for ambiguity resolution.In particular, when considering only immediate parents, unit rules such as NP2 —■ NP1 prevent the probabilistic model from allowing the NP1 constituent to interact with the VP rule which is the functional parent of NP1.When the two parents are identical as it often happens, the duplicate information will be ignored.However, when they differ, the decision tree will select that parental context which best resolves ambiguities.Figure 3 shows an example of the representation of a history in HBG for the prepositional phrase &quot;with a list.&quot; In this example, the immediate parent of the Ni node is the NBAR4 node and the functional parent of Ni is the PP1 node.Results We compared the performance of HBG to the &quot;broad-coverage&quot; probabilistic context-free gram- P-CFG.The of the grammar is 90% on test sentences of 7 to 17 words.The of P-CFG is 60% on the same test corpus of 760 sentences used in our experiments.On the same test sentences, the HBG model has a of 75%.This is a reduction of 37% in error rate.Accuracy P-CFG 59.8% HBG 74.6% Error Reduction 36.8% Figure 4: Parsing accuracy: P-CFG vs. HBG In developing HBG, we experimented with similar models of varying complexity.One discovery made during this experimentation is that models which incorporated more context than HBG performed slightly worse than HBG.This suggests that the current training corpus may not contain enough sentences to estimate richer models.Based on the results of these experiments, it appears likely that significantly increasing the size of the training corpus should result in a corresponding improvement in the accuracy of HBG and richer HBG-like models.To check the value of the above detailed history, we tried the simpler model: 1.2.3. p(Syn p(Sem ISyn, p(R ISyn, Sem, This model corresponds to a P-CFG with NTs that are the crude syntax and semantic categories with the lexical heads.The in this case was 66%, a small improvement over the P-CFG model indicating the value of using more context from the derivation tree.Conclusions The success of the HBG model encourages future development of general history-based grammars as a more promising approach than the usual P-CFG.More experimentation is needed with a larger Treebank than was used in this study and with different aspects of the derivation history.In addition, this paper illustrates a new approach to grammar development where the parsing problem is divided (and hopefully conquered) into two subproblems: one of grammar coverage for the grammarian to address and the other of statistical modeling to increase the probability of picking the correct parse of a sentence.
Overgeneration is the main source of computational complexity in previous principle-based parsers.This paper presents a message passing algorithm for principle-based parsing that avoids the overgeneration problem.This algorithm has been implemented in C++ and successfully tested with example sen
Certain spans of utterances in a discourse, referred here as widely assumed to form coherent units.Further, the segmental structure of discourse has been claimed to constrain and be constrained by many phenomena.However, there is weak consensus on the nature of segments and the criteria for recognizing or generating them.We present quantitative results of a two part study using a corpus of spontaneous, narrative monologues.The first part evaluates the statistical reliability of human segmentation of our corpus, where speaker intention is the segmentation criterion.We then use the subjects' segmentations to evaluate the correlation of discourse segmentation with three linguistic cues (referential noun phrases, cue words, and pauses), using information retrieval metrics.
In recent years there is much interest in word cooccurrence relations, such as n-grams, verbobject combinations, or cooccurrence within a limited context.This paper discusses how to estimate the probability of cooccurrences that do not occur in the training data.We present a method that makes local analogies between each specific unobserved cooccurrence and other cooccurrences that contain similar words, as determined by an appropriate word similarity metric.Our evaluation suggests that this method performs better than existing smoothing methods, and may provide an alternative to class based models.
In this paper we present a method to group adjectives according to their meaning, as a first step towards the automatic identification of adjectival scales.We discuss the properties of adjectival scales and of groups of semantically related adjectives and how they imply sources of linguistic knowledge in text corpora.We describe how our system exploits this linguistic knowledge to compute a measure of similarity between two adjectives, using statistical techniques and without having access to any semantic information about the adjectives.We also show how a clustering algorithm can use these similarities to produce the groups of adjectives, and we present results produced by our system for a sample set of adjectives.We conclude by presenting evaluation methods for the task at hand, and analyzing the significance of the results obtained.
We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts.Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering.Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership.In many cases, the clusters can be thought of as encoding coarse sense distinctions.Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical &quot;soft&quot; clustering of the data.Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data.
This paper presents a new method for producing a dictionary of subcategorization frames from unlabelled text corpora.It is shown that statistical filtering of the results of a finite state parser running on the output of a stochastic tagger produces high quality results, despite the error rates of the tagger and the parser.Further, it is argued that this method can be used to learn all subcategorization frames, whereas previous methods are not extensible to a general solution to the problem.
In this paper we describe a new technique for parsing free text: a transformational grammar' is automatically learned that is capable of accurately parsing text into binary-branching syntactic trees with nonterminals unlabelled.The algorithm works by beginning in a very naive state of knowledge about phrase structure.By repeatedly comparing the results of bracketing in the current state to proper bracketing provided in the training corpus, the system learns a set of simple structural transformations that can be applied to reduce error.After describing the algorithm, we present results and compare these results to other recent results in automatic grammar induction.
This paper proposes a new indicator of text structure, called the lexical cohesion profile (LCP), which locates segment boundaries in a text.A text segment is a coherent scene; the words in a segment are linked together via lexical cohesion relations.LCP records mutual similarity of words in a sequence of text.The similarity of words, which represents their cohesiveness, is computed using a semantic network.Comparison with the text segments marked by a number of subjects shows that LCP closely correlates with the human judgments.LCP may provide valuable information for resolving anaphora and ellipsis.
This paper describes TextTiling, an algorithm for partitioning expository texts into coherent multi-paragraph discourse units which reflect the subtopic structure of the texts.The algorithm uses domain-independent lexical frequency and distribution information to recognize the interactions of multiple simultaneous themes.Two fully-implemented versions of the algorithm are described and shown to produce segmentation that corresponds well to human judgments of the major subtopic boundaries of thirteen lengthy texts.
This paper presents a statistical decision procedure for lexical ambiguity resolution.The algorithm exploits both local syntactic patterns and more distant collocational evidence, generating an efficient, effective, and highly perspicuous recipe for resolving a given ambiguity.By identifying and utilizing only the single best disambiguating evidence in a target context, the algorithm avoids the problematic complex modeling of statistical dependencies.Although directly applicable to a wide class of ambiguities, the algorithm is described and evaluated in a realistic case study, the problem of restoring missing accents in Spanish and French text.Current accuracy exceeds 99% on the full task, and typically is over 90% for even the most difficult ambiguities.
This paper will focus on the semantic representation of verbs in computer systems and its impact on lexical selection problems in machine translation (MT).Two groups of English and Chinese verbs are examined to show that lexical selection must be based on interpretation of the sentence as well as selection restrictions placed on the verb arguments.A novel representation scheme is suggested, and is compared to representations with selection restrictions used in transfer-based MT.We see our approach as closely aligned with knowledge-based MT approaches (KBMT), and as a separate component that could be incorporated into existing systems.Examples and experimental results will show that, using this scheme, inexact matches can achieve correct lexical selection.
is composed of interdependent variables.The test used to evaluate a model gives preference to those that have the fewest number of interdependencies, thereby selecting models expressing only the most systematic variable interactions.To summarize the method, one first identifies informative contextual features (where &quot;informative&quot; is a well-defined notion, discussed in Section 2).Then, out of all possible decomposable models characterizing interdependency relationships among the selected variables, those that are found to produce good approximations to the data are identified (using the test mentioned above) and one of those models is used to perform disambiguation.Thus, we are able to use multiple contextual features without the need for untested assumptions regarding the form of the model.Further, approximating the joint distribution of all variables with a model identifying only the most important systematic interactions among variables limits the number of parameters to be estimated, supports computational efficiency, and provides an understanding of the data.The biggest limitation associated with this method is the need for large amounts of sense-tagged data.Because asymptotic distributions of the test statistics are used, the validity of the results obtained using this approach are compromised when it is applied to sparse data (this point is discussed further in Section 2).To test the method of model selection presented in this paper, a case study of the disambiguation of the performed. selected because it has been shown in previous studies to be a difficult word to disambiguate.We selected as the set of tags all non-idiomatic noun senses of defined in the electronic version of Longman's Dictionary of Contemporary English (LDOCE) ([23]).Using the models produced in this study, we are able to assign an sense tag to every usage of a heldout test set with 78% accuracy.Although it is difficult to compare our results to those reported for previous disambiguation experiments, as will be discussed later, we feel these results are encouraging.The remainder of the paper is organized as follows.Section 2 provides a more complete definition of the Abstract Most probabilistic classifiers used for word-sense disambiguation have either been based on only one contextual feature or have used a model that is simply assumed to characterize the interdependencies among multiple contextual features.In this paper, a different approach to formulating a probabilistic model is presented along with a case study of the performance of models produced in this manner for the disambiguation of the noun describe a method for formulating probabilistic models that use multiple contextual features for word-sense disambiguation, without requiring untested assumptions regarding the form of the model.Using this approach, the joint distribution of all variables is described by only the most systematic variable interactions, thereby limiting the number of parameters to be estimated, supporting computational efficiency, and providing an understanding of the data.
Tagged Dependency -.— Tagged Adjacency -e— L. Pattern 3 5 Training scheme (integers denote window widths) Figure 5: Accuracy using a tagged corpus for various training schemes data would make a difference, the freely available Brill tagger (Brill, 1993) was applied to the corpus.Since no manually tagged training data is available for our corpus, the tagger's default rules were used (these rules were produced by Brill by training on the Brown corpus).This results in rather poor tagging accuracy, so it is quite possible that a manually tagged corpus would produce better results.Three training schemes have been used and the tuned analysis procedures applied to the test set.Figure 5 shows the resulting accuracy, with accuracy values from figure 3 displayed with dotted lines.If anything, admitting additional training data based on the tagger introduces more noise, reducing the accuracy.However, for the pattern training scheme an improvement was made to the dependency model, producing the highest overall accuracy of 81%.4 Conclusion The experiments above demonstrate a number of important points.The most general of these is that even quite crude corpus statistics can provide information about the syntax of compound nouns.At the very least, this information can be applied in broad coverage parsing to assist in the control of search.I have also shown that with a corpus of moderate size it is possible to get reasonable results without using a tagger or parser by employing a customised training pattern.While using windowed co-occurrence did not help here, it is possible that under more data sparse conditions better performance could be achieved by this method.The significance of the use of conceptual association deserves some mention.I have argued that without it a broad coverage system would be impossible.This is in contrast to previous work on conceptual association where it resulted in little improvement on a task which could already be performed.In this study, not only has the technique proved its worth by supporting generality, but through generalisation of training information it outperforms the equivalent lexical association approach given the same information.Amongst all the comparisons performed in these experiments one stands out as exhibiting the greatest contrast.In all experiments the dependency model provides a substantial advantage over the adjacency model, even though the latter is more prevalent in proposals within the literature.This result is in accordance with the informal reasoning given in section 1.3.The model also has the further commendation that it predicts correctly the observed proportion of left-branching compounds found in two independently extracted test sets.In all, the most accurate technique achieved an accuracy of 81% as compared to the 67% achieved by guessing left-branching.Given the high frequency of occurrence of noun compounds in many texts, this suggests that the use of these techniques in probabilistic parsers will result in higher performance in broad coverage natural language processing.5 Acknowledgements This work has received valuable input from people too numerous to mention.The most significant contributions have been made by Richard Buckland, Robert Dale and Mark Dras.I am also indebted to Vance Gledhill, Mike Johnson, Philip Resnik, Richard Sproat, Wilco ter Stal, Lucy Vanderwende and Wobcke.Financial support is gratefully ack- 53 nowledged from the Microsoft Institute and the Australian Government.
designed to share some of the advantages of TAG while overcoming some its limitations. two composition operations called subsertion and sister-adjunction.The most distinctive feaof that, unlike TAG, there is complete uniformity in the way that the relate lexical items: subsertion always corresponds to complementation and sister-adjunction to modi- Furthermore, unlike TAG, provide a uniform analysis for whmovement in English and Kashmiri, despite the fact that the wh element in Kashmiri appears in sentence-second position, and not sentence-initial position as in English.We define a new grammar formalism, called D-Tree arises from work on Tree- Adjoining Grammars (TAG) (Joshi et al., 1975).A salient feature of TAG is the extended domain of locality it provides.Each elementary structure can be associated with a lexical item (as in Lexicalized (LTAG) (Joshi & 1991)).Properties related to the lexical item (such as subcategorization, agreement, certain types of word order variation) can be expressed within the elementary struc- (Kroch, 1987; Frank, 1992).In addition, remain tractable, yet their generative capacity is sufficient to account for certain syntactic phenomena that, it has been argued, lie beyond Context-Free Grammars (CFG) (Shieber, 1985).TAG, however, has two limitations which provide the motivation for this The first problem (discussed in Section that the of substitution and adjunction do not map cleanly onto the relations of complementation and modification.A second problem (discussed in Section 1.2) has to do with the of provide analyses for certain syntactic phenomena.In developing DTG we have tried to overcome these problems while remaining faithto what we see as the key advantages of particular, its enlarged domain of locality).In Section 1.3 we introduce some of the key features of explain how they are intended to address problems that we have identified with 1.1 Derivations and Dependencies operations of substitution and adjunction relate two lexical items.It is therefore natural to interpret these operations as establishing a direct linguistic relation between the two lexical items, namely a relation of complementation (predicateargument relation) or of modification.In purely CFG-based approaches, these relations are only implicit.However, they represent important linguistic intuition, they provide a uniform interface to semantics, and they are, as Schabes & Shieber (1994) argue, important in order to support statistical parameters in stochastic frameworks and appropriate constraints in many frameworks, complementation and modification are in fact made & Kaplan, 1982) provides a separate functional (f-) structure, and dependency grammars (see e.g.Mel'euk (1988)) use these notions as the principal basis for syntactic representation.We will follow the dependency literature in referring to complementation and modification as syntactic dependency.As observed by Rambow and Joshi (1992), for TAG, the importance of the dependency structure means that not only the derived phrase-structure tree is of interest, but also the operations by which we obtained it from elementary structures.This information is encoded in the derivation tree (Vijay-Shanker, 1987).However, as Vijay-Shanker (1992) observes, the operations are not used uniformly: while substitution is used only to add a (nominal) complement, adjunction is used both for modification and (clausal) complementation.Clausal complementation could not be handled uniformly by substitution because of the existence of syntactic such as in English.Furthermore, there is an inconsistency in 151 the directionality of the operations used for complementation in TAG@: nominal complements are substituted into their governing verb's tree, while the governing verb's tree is adjoined into its own clausal complement.The fact that adjunction and substitution are used in a linguistically heterogeneous manner means that (standard) TAG derivation trees do not provide a good representation of the dependencies between the words of the sentence, i.e., of the predicate-argument and modification structure. adore SUBJ COMP Figure 1: Derivation trees for (1): original definition (left); Schabes & Shieber definition (right) For instance, English sentence (1) gets the derivation structure shown on the left in Figure 1'. spicy hotdogs he claims Mary seems to adore When comparing this derivation structure to the dependency structure in Figure 2, the following problems become apparent.First, both adjectives deon in the derivation structure is daughter of addition, deon does its nominal argument, on the derivation strucis daughter of direction does express the actual dependency), and also daughter of neither is an argument of the other). claim SUBOMP he seem I COMP adore Mary hotdog </LOD spicy small Figure 2: Dependency tree for (1) Schabes & Shieber (1994) solve the first problem 'For clarity, we depart from standard TAG notational practice and annotate nodes with lexemes and arcs with grammatical function. by distinguishing between the adjunction of modifiers and of clausal complements.This gives us the derivation structure shown on the right in Figure 1.While this might provide a satisfactory treatment of modification at the derivation level, there are now three types of operations (two adjunctions and substitution) for two types of dependencies (arguments and modifiers), and the directionality problem for embedded clauses remains unsolved. defining have attempted to resolve these problems with the use of a single operation (that we call subsertion) for handling all complementation and a second operation (called sisteradjunction) for modification.Before discussion these operations further we consider a second problem with TAG that has implications for the design of these new composition operations (in particular, subsertion).1.2 Problematic Constructions for TAG be used to provide suitable analyses for certain syntactic phenomena, including longdistance scrambling in German (Becker et al., 1991), Romance Clitics (Bleam, 1994), wh-extraction out of complex picture-NPs (Kroch, 1987), and Kashmiri wh-extraction (presented here).The problem in describing these phenomena with TAG arises from the fact (observed by Vijay-Shanker (1992)) that adjoining is an overly restricted way of combining structu- We illustrate the problem by considering Kashon Bhatt (1994). extraction in Kashmiri proceeds as in English, except that the wh-word ends up in sentence-second position, with a topic from the matrix clause in sentence-initial position.This is illustrated in (2a) for a simple clause and in (2b) for a complex clause.(2) rameshan kyaa dyutnay tse RameshERG whatNom gave youoAr What did you give Ramesh? b. rameshan kyaa, chu baasaan what is believeNPertthat kor 'ERG do What does Ramesh believe that I did?Since the moved element does not appear in sentence-initial position, the TAG analysis of English wit-extraction of Kroch (1987; 1989) (in which the matrix clause is adjoined into the embedded clause) cannot be transferred, and in fact no linguistically plausible TAG analysis appears to be available.In the past, variants of TAG have been developed to extend the range of possible analyses.In Multi-Component TAG (MCTAG) (Joshi, 1987), trees are grouped into sets which must be adjoined to- (multicomponent adjunction).However, MCexpressive power since, while syntactic relations are invariably subject to c-command or dominance constraints, there is no way to state that Mary OBJ hotdog claim SUBJ spicyhe I MOD small adore COMP COMP Mary OBJ seem hotdog claim MOD SUBJ spicy small he COMP seamSUBJ 152 two trees from a set must be in a dominance relain the derived tree.Domination et al., 1991) are multicomponent systems that allow for the expression of dominance constraints.However, MCTAG-DL share a further problem with MCTAG: the derivation structures cannot be given a linguistically meaningful interpretation.Thus, they fail to address the first prowe discussed (in Section 1.3 The DTG Approach Vijay-Shanker (1992) points out that use of adjunction for clausal complementation in TAG corresponds, at the level of dependency structure, to substitution at the foot node' of the adjoined tree.However, adjunction (rather than substitution) is used since, in general, the structure that is substituted may only form part of the clausal complement: the remaining substructure of the clausal complement appears above the root of the adjoined tree.Unfortunately, as seen in the examples given in Section 1.2, there are cases where satisfactory analyses cannot be obtained with adjunction.In particular, using adjunction in this way cannot handle cases in which parts of the clausal complement are required to be placed within the structure of the adjoined tree. of subsertion is designed to overcome this limitation.Subsertion can be viewed as a generalization of adjunction in which components of the clausal complement (the subserted structure) which are not substituted can be interspersed within the structure that is the site of the subsertion.Following earlier work (Becker et al., Vijay-Shanker, 1992), a mechanism involving the use of domination links (d-edges) that ensure that parts of the subserted structure that are not substituted dominate those parts that are.Furthermore, there is a need to constrain the way in which the non-substituted components can be interspersed'.This is done by either using appropriate feature constraints at nodes or by means of subsertion-insertion constraints (see Section 2).We end this section by briefly commenting on the of sister-adjunction.In TAG, modification is performed with adjunction of modifier trees that have a highly constrained form.In particular, the foot nodes of these trees are always daughters of the root and either the leftmost or rightmost frontier nodes.The effect of adjoining a these cases the foot node is an argument node of the lexical anchor.'This was also observed by Rambow (1994a), where integrity constraint (first defined for an of TAG (Becker etal., 1991)) is defined for a MCTAG-DL version called V-TAG.However, this was found to be insufficient for treating both long-distance scrambling and long-distance topicalization in German.V-TAG retains adjoining (to handle topicalization) for this reason. tree of this form corresponds (almost) exactly to the addition of a new (leftmost or rightmost) subtree below the nede that was the site of the adjunction. this reason, we have equipped an operation (sister-adjunction) that does exactly this and more.From the definition of Section 2 it can be seen that the essential aspects of Schabes & Shieber (1994) treatment for modification, including multiple modifications of a phrase, be captured by using this defining Section 2, we discuss, in Section 3, DTG analyses for the English and Kashmiri data presented in this section.Section 4 briefly algorithms.2 Definition of D-Tree Grammars d-tree is a tree with two types of domination edges (d-edges) and immediate domination edges (i-edges).D-edges and i-edges express domination and immediate domination relations between nodes.These relations are never rescinded when dtrees are composed.Thus, nodes separated by an i-edge will remain in a mother-daughter relationship throughout the derivation, whereas nodes separated by an d-edge can be equated or have a path of any length inserted between them during a derivation.D-edges and i-edges are not distributed arbitrarily in d-trees.For each internal node, either all of its daughters are linked by i-edges or it has a single daughter that is linked to it by a d-edge.Each node is labelled with a terminal symbol, a nonterminal or the empty string. containing n can be decomposed into n + 1 containing only i-edges.D-trees can be composed using two operations: subsertion and sister-adjunction.When a d-tree is subserted into another d-tree component of a is substituted at a frontier nonterminal node (a node) and all components of a that are above the substituted component are inserted into d-edges above the substituted node or placed above the root node.For example, consider the d-trees a and shown in Figure 3.Note that components are shown as triangles.In the composed d-tree 7 the component a(5) is substituted at a substitution node in /3.The components, a(1), a(2), and a(4) of a above a(5) drift up the path in which runs from the substitution node.These are then into in # or above the root of #.In general, when a component some d-tree a is inserted into a d-edge betnodes and two new d-edges are created, the first of which relates and the root node of the second of which relates the frontier 'Santorini and Mahootian (1995) provide additional evidence against the standard TAG approach to modification from code switching data, which can be accounted for by using sister-adjunction.153 Figure 3: Subsertion node of a(i) that dominates the substituted comto is possible for components above the substituted node to drift arbitrarily far up the d-tree and distribute themselves within domination edges, or above the root, in any way that is compatible with the domination relationships present in the d-tree. a mechanism called constraints control what can appear within d-edges (see below).The second composition operation involving dtrees is called sister-adjunction.When a d-tree a is at a node ri in a d-tree the comd-tree from the addition to fl of a as a new leftmost or rightmost sub-d-tree below that sister-adjunction involves the addition of exactly one new immediate domination edge and that several sister-adjunctions can occur at the same constraints where d-trees can be sister-adjoined and whether they will be rightor left-sister-adjoined (see below).DTG a four tuple G = , VT S, D) the usual nonterminal and termialphabets, a distinguished nonterand a finite set of DTG said to be each d-tree in the grammar has at least one terminal node.The d-trees of a grammar two additional annotations: subsertion-insertion constraints and sister-adjoining constraints.These will be described below, but first we define simultaneously and subsertion-adjoining trees (SAtrees), which are partial derivation structures that can be interpreted as representing dependency in
This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations.The algorithm is based on two powerful constraints — that words tend to have one sense per discourse and one sense per collocation — exploited in an iterative bootstrapping procedure.Tested accuracy exceeds 96%.
Large-scale natural language generation requires the integration of vast amounts of knowledge: lexical, grammatical, and conceptual.A robust generator must be able to well pieces of knowledge are missing.It must also be robust against incomplete or inaccurate inputs.To attack these problems, we have built a hybrid generator, in which gaps in symbolic knowledge are filled by statistical methods.We describe algorithms and show experimental results.We also discuss how the hybrid generation model can be used to simplify current generators and enhance their portability, even when perfect knowledge is in principle obtainable.
Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text, as is evidenced by their poor performance on domains like the Wall Street Journal, and by the movement away from parsing-based approaches to textprocessing in general.In this paper, I describe SPATTER, a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result.This work is based on the following premises: (1) grammars are too complex and detailed to develop manually for most interesting domains; (2) parsing models must rely heavily on lexical and contextual information to analyze sentences accurately; and (3) existing n-grain modeling techniques are inadequate for parsing models.In experiments comparing SPATTER with IBM's computer manuals parser, SPATTER significantly outperforms the grammar-based parser.Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures, SPATachieves 86% precision, and 1.3 crossing brackets per sentence for sentences of 40 words or less, and 91% precision, 90% recall, and 0.5 crossing brackets for sentences between 10 and 20 words in length.
Common algorithms for sentence and word-alignment allow the automatic identification of word translations from parallel texts.This study suggests that the identification of word translations should also be possible with non-parallel and even unrelated texts.The method proposed is based on the assumption that there is a correlation between the patterns of word cooccurrences in texts of different languages.
In this paper, we present a new approach for word sense disambiguation (WSD) using an exemplar-based learning algorithm.This approach integrates a diverse set of knowledge sources to disambiguate word sense, including part of speech of neighboring words, morphological form, the unordered set of surrounding words, local collocations, and verb-object syntactic relation.We tested our WSD program, named LEXAS, on both a common data set used in previous work, as well as on a large sense-tagged corpus that we separately constructed.LEXAS achieves a higher accuracy on the common data set, and performs better than the most frequent heuristic on the highly ambiguous words in the large corpus tagged with the refined senses of WORDNET.
Under categorial grammars that have powerful rules like composition, a simple n-word sentence can have exponentially many parses.Generating all parses is inefficient and obscures whatever true semantic ambiguities are in the input.This paper addresses the problem for a fairly general form of Combinatory Categorial Grammar, by means of an efficient, correct, and easy to implement normal-form parsing tech- The parser is proved to find exone in each semantic equivalence class of allowable parses; that is, spurious ambiguity (as carefully defined) is shown to be both safely and completely eliminated.
Hong Kong's stabilize boom is us life styles's pillar.Our prosperity and stability underpin our way of life.44NMINVitta (Ben gang de jing ji qian jing yu zhang gu6, te bie shi guang dong sheng de jing ji qian jing xi xi xiang guan.)Hong Kong's economic foreground with China, particular Guangdong province's economic foreground vitally interrelated.Our economic future is inextricably bound up with China, and with Guangdong Province in particular. firdtittifirg.g.(WO win quin zhi chi ta de yi jian.)I absolutely uphold his views.I fully support his views.Mt (Zhe xie an pai ke jia qiing wo men ri hOu wei chi jin r6ng wen ding de neng 11.)These arrangements can enforce us future kept financial stabilization's competency.These arrangements will enhance our ability to maintain monetary stability in the years to come. tWt, ftRAMT.A1t: ZOM, ftrig*IliTtAtIttM3R/OIAMPfiEfissi R. wa zai ke yl ken ding de shuO, wO men jiang hul ti gong wei di dao ge xiang zhii yao mu biao suO xil de jing fei.)However, I now can certainty's say, will provide for us attain various dominant goal necessary's current expenditure.The consultation process is continuing but I can confirm now that the necessary funds will be made available to meet the key targets.Figure 4: Example translation outputs. translation accuracy was performed on a random sample drawn from Chinese sentences of fewer than 20 words from the parallel corpus, the results of which are shown in Figure 3.We have judged only whether the correct meaning (as determined by the corresponding English sentence in the parallel corpus) is conveyed by the translation, paying particular attention to word order, but otherwise ignoring morphological and function word choices.For comparison, the accuracies from the A*-based systems are also shown.There is no significant difference in the accuracy.Some examples of the output are shown in Figure 4.On the other hand, the new algorithm has indeed proven to be much faster.At present we are unable to use direct measurement to compare the speed of the systems meaningfully, because of vast implementational differences between the systems.However, the order-of-magnitude improvements are immediately apparent.In the earlier system, translation of single sentences required on the order of hours (Sun Sparc 10 workstations).In contrast the new algorithm generally takes less than one minute—usually substantially less—with no special optimization of the code.6 Conclusion We have introduced a new algorithm for the runtime optimization step in statistical machine translation systems, whose polynomial-time complexity addresses one of the primary obstacles to practicality facing statistical MT.The underlying model for the algorithm is a combination of the stochastic BTG and bigram models.The improvement in speed does not appear to impair accuracy significantly.We have implemented a version that accepts ITGs rather than BTGs, and plan to experiment with more heavily structured models.However, it is important to note that the search complexity rises exponentially rather than polynomially with the size of the grammar, just as for context-free parsing (Barton, Berwick, and Ristad, 1987).This is not relevant to the BTG-based model we have described since its grammar size is fixed; in fact the BTG's minimal grammar size has been an important advantage over more linguistically-motivated ITG-based models.157 We have also implemented a generalized version that accepts arbitrary grammars not restricted to normal form, with two motivations.The pragmatic benefit is that structured grammars become easier to write, and more concise.The expressiveness benefit is that a wider family of probability distributions can be written.As stated earlier, the normal form theorem guarantees that the same set of shapes will be explored by our search algorithm, regardless of whether a binary-branching BTG or an arbitrary BTG is used.But it may sometimes be useful to place probabilities on n-ary productions that vary with n in a way that cannot be expressed by composing binary productions; for example one might wish to encourage longer straight productions.The generalized version permits such strategies.Currently we are evaluating robustness extensions of the algorithm that permit words suggested by the language model to be inserted in the output sentence, which the original A* algorithms permitted.Acknowledgements Thanks to an anonymous referee for valuable comments, and to the SILC group members: Xuanyin Xia, Eva Wai-Man Fong, Cindy Ng, Hong-sing Wong, and Daniel Ka-Leung Chan.Many thanks also to Kathleen McKeown and her group for discussion, support, and assistance.
Many different metrics exist for evaluating parsing results, including Viterbi, Crossing Brackets Rate, Zero Crossing Brackets Rate, and several others.However, most parsing algorithms, including the Viterbi algorithm, attempt to optimize the same metric, namely the probability of getting the correct labelled tree.By choosing a parsing algorithm appropriate for the evaluation metric, better performance can be achieved.We present two new algorithms: the &quot;Labelled Recall Algorithm,&quot; which maximizes the expected Labelled Recall Rate, and the &quot;Bracketed Recall Algorithm,&quot; which maximizes the Bracketed Recall Rate.Experimental results are given, showing that the two new algorithms have improved performance over the Viterbi algorithm on many criteria, especially the ones that they optimize
This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree.Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words.Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Magerman 95; Jelinek et al. 94), which has the best published results for a statistical parser on this task.The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes.With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy.
Charts constitute a natural uniform architecture for parsing and generation provided string position is replaced by a notion more appropriate to logical forms and that measures are taken to curtail generation paths containing semantically incomplete phrases.1 Charts Shieber (1988) showed that parsing charts can be also used in generation and raised the question, which we take up again here, of whether they constitute a natural uniform architecture for parsing and generation.In particular, we will be interested in the extent to which they bring to the generation process advantages comparable to those that make them attractive in parsing.Chart parsing is not a well defined notion.The usual conception of it involves at least four related ideas: edges. context-free grammar, all phrases of a given category that cover a given part of the string are equivalent for the purposes of constructing larger phrases.Efficiency comes from collecting equivalent of phrases into (inactive) constructing edges from edges rather than phrases from phrases. edges. phrases of whatever size can be built by considering existing edges pair-wise if provision is made for partial phrases.Partial phrases are collected edges that are said to be they can be thought of as actively seeking material to complete them. algorithm schema. created edges are placed an are moved from the agenda to the by one until none remains to be moved.When an edge is moved, all interactions between it and edges already in the chart are considered and any new edges that they give rise to are added to the agenda. positions in the string at which phrases begin and end can be used to index edges so that the algorithm schema need consider interactions only between adjacent pairs.Chart parsing is attractive for the analysis of natural languages, as opposed to programming languages, for the way in which it treats ambiguity.Regardless of the number of alternative structures for a particular string that a given phrase participates in, it will be constructed once and only once.Although the number of structures of a string can grow exponentially with the length of the string, the number of edges that needs to be constructed grows only with the square of the string length and the whole parsing process can be accomplished in cubic time.Innumerable variants of the basic chart parsing scheme are possible.For example, if there were languages with truly free word order, we might attempt to characterize them by rules like those of context-free grammar, but with a somewhat different interpretation.Instead of replacing nonterminal symbols in a derivation with strings from the righthand side of corresponding rules, we would remove the nonterminal symbol and insert the symbols from the righthand side of the rule at arbitrary places in the string.A chart parser for languages with free word order would be a minor variant of the standard one.An edge would take the form where v is a vector with a bit for every word in the string and showing which of those words the edge covers.There is no longer any notion of adjacency so that there would be no indexing by string position.Interesting interactions occur between pairs of edges whose bit vectors have empty intersections, indicating that they cover disjoint sets of words.There can now be as many edges as bit-vectors and, not surprisingly, the computational complexity of the parsing process increases accordingly.2 Generation A parser is a transducer from strings to structures or logical forms.A generator, for our purposes, is the inverse.One way to think of it, therefore, is as a parser of structures or logical forms that delivers analyses in the form of strings.This view has the apparent disadvantage of putting insignificant differences in the syntax of a logical forms, such as the relative order of the arguments to symmetric operators, on the same footing as more significant facts about them.We know that it will not generally be possible to reduce 200 logical expressions to a canonical form but this does not mean that we should expect our generator to be compromised, or even greatly delayed, by trivial distinctions.Considerations of this kind were, in part, responsible for the recent resurgence of interest in &quot;flat&quot; representations of logform (Copestake 996) and for the representations used for transfer in Shake-and-Bake translation (Whitelock, 1992).They have made semantic formalisms like those now usually associated with Davison (Davidson, 1980, Parsons, 1990) attractive in artificial intelligence for many years (Hobbs 1985, Kay, 1970).Operationally, the attraction is that the notations can be analyzed largely as free word-order languages in the manner outlined above.Consider the expression (I) (1) r: run(r), past(r), fast(r), argl(r, j), name(j, John) which we will take as a representation of the logical form of sentences ran fast ran quickly. consists of a distinguished index (r) and a list of predicates whose relative order is immaterial.The distinguished index identifies this as a sentence that makes a claim about a running event.&quot;John&quot; is the name of the entity that stands in the `argl ' relation to the running which took place in the past and which was fast.Nothing turns on these details which will differ with differing ontologies, logics, and views of semantic structure.What concerns us here is a procedure for generating a sentence from a structure of this general kind.Assume that the lexicon contains entries like those in (2) in which the italicized arguments to the semantic predicates are variables.(2) Words Cat Semantics John np(x) John) ran vp(x, y) argl(x, y), past(x) fast adv(x) quickly adv(x) x: fast(x) facie for the utility of these particular words for expressing ( I) can be made simply by noting that, instantiation of the variables, the semantics of each of these words subsumes (1).3 The Algorithm Schema The entries in (2), with their variables suitably instantiated, become the initial entries of an agenda and we begin to move them to the chart in accordance with the algorithm schema, say in the order given.The variables in the 'Cat' and 'Semantics' columns of (2) provide the essential link between syntax and semantics.The predicates that represent the semantics of a phrase will simply be the union of those representing the constituents.The rules that sanction a phrase (e.g.(3) below) show which variables from the two parts are to be identified. the entry for moved, no interactions are because the chart is empty.When moved, the ran considered as a possible phrase on the basis of rule (3).(3) s(x) —> np(y), vp(x, y).With appropriate replacements for variables, this maps onto the subset (4) of the original semantic specification in (1).(4) r: run(r), past(r), argl(r, j), name(j, John) Furthermore it is a complete sentence.However, it does not count as an output to the generation process as a whole because it subsumes some but not all of (1).It therefore simply becomes a new edge on the agenda. string fast a verb phrase by virtue rule (5) giving the semantics (6), and the phrase the same semantics is put on the agenda when is move to the chart.(5) vp(x) —> vp(x) adv(x) (6) r: run(r), past(r), fast(r), argl(r, y) agenda now contains the entries in Words Cat Semantics John ran s(r) r: run(r), past(r), arg I (r, j), name(j, John) ran fast vp(r, j) r: run(r), past(r), fast(r), argl(r, j) ran quickly vp(r, j) r: run(r), past(r), fast(r), arg 1 (r, j) Assuming that adverbs modify verb phrases and not senthere will be no interactions when the ran is moved to the chart. the edge for fast moved, the possibility of creating the phrase fast quickly well as fast. are rejected, however, on the grounds that they would involve using a predicate from the original semantic specification more than once.This would be similar to allowing a given word to be covered by overlapping phrases in free word-order parsing.We proposed eliminating this by means of a bit vector and the same technique applies here.The fruitful interactions that occur here are fast quickly the one hand, and 201 on the other.Both give sentences whose semantics subsumes the entire input.Several things are noteworthy about the process just outlined.1.Nothing turns on the fact that it uses a primitive version of event semantics.A scheme in which the indices were handles referring to subexpressions in any variety of flat semantics could have been treated in the same way.Indeed, more conventional formalisms with richly recursive syntax could be converted to this form on the fly.2.Because all our rules are binary, we make no use of active edges.3.While it fits the conception of chart parsing given at the beginning of this paper, our generator does not involve string positions centrally in the chart representation.In this respect, it differs from the proposal of Shieber (1988) which starts with all word edges leaving and entering a single vertex.But there is essentially no information in such a representation.Neither the chart nor any other special data structure is required to capture the fact that a new phrase may be constructible out of any given pair, and in either order, if they meet certain syntactic and semantic criteria.4.Interactions must be considered explicitly between new edges and all edges currently in the chart, because no indexing is used to identify the existing edges that could interact with a given new one.5.The process is exponential in the worst case because, if a sentence contains a word with k modifiers, then a it will be generated with each of the subsets of those modifiers, all but one of them being rejected when it is finally discovered that their semantics does not subsume the entire input.If the relative orders of the modifiers are unconstrained, matters only get worse.Points 4 and 5 are serious flaws in our scheme for which we shall describe remedies.Point 2 will have some importance for us because it will turn out that the indexing scheme we propose will require the use of distinct active and inactive edges, even when the rules are all binary.We take up the complexity issue first, and then turn to how the efficiency of the generation chart might be enhanced through indexing.4 Internal and External Indices The exponential factor in the computational complexity of our generation algorithm is apparent in an example like (8).(8) Newspaper reports said the tall young Polish athlete ran fast The same set of predicates that generate this sentence clearly also generate the same sentence with deletion of all of the words young. a total of 8 strings.Each is generated in its entirety, though finally rejected because it fails to account for all of the semantic The words also be deleted independently giving a grand total of 32 strings. concentrate on the phrase young Polish athlete which we assumed would be combined with the verb phrase fast the rule (3).The distinguished index of the noun call it p, is identified with the variable the rule, but this variable is not associated with the syntactic category, s, on the left-hand side of the rule.The grammar has access to indices only through the variables that annotate grammatical categories in its rules, so that rules that incorporate this sentence into larger phrases can have no further to the index p. We therefore say that p is sentence tall young Polish athlete ran fast.The index p would, of course, also be internal to the young Polish athlete ran fast, the tall Polish ran fast, However, in these cases, the semantic material remaining to be expressed contains predicates that refer to this internal index, say tall(p)' , and `young(p)'.While the lexicon may have words to express these predicates, the grammar has no way of associating their referents with the above noun phrases because the variables corresponding to those referents are internal.We conclude that, as a matter of principle, no edge should be constructed if the result of doing so would be to make internal an index occurring in part of the input semantics that the new phrase does not subsume.In other words, the semantics of a phrase must contain all predicates from the input specification that refer to any indices internal to it.This strategy does not prevent the generation of an exponential number of variants of phrases containing modifiers.It limits proliferation of the ill effects, however, by allowing only the maximal one to be incorporated in larger phrases.In other words, if the final has phrases with respectively, then of the first and of the second will be created, but only one of each set will be incorporated into larger and no factor of will be introduced into the cost of the process.5 Indexing String positions provide a natural way to index the strings input to the parsing process for the simple reason that there are as many of them as there are words but, for there to be any possibility of interaction between a pair of edges, they must come together at just one index.These are the natural points of articulation in the domain of strings.They cannot fill this role in generation because they are not natural properties of the semantic expressions that are the input to the process.The corresponding natural points of articulation in 202 flat semantic structures are the entities that we have already referring to as In the modified version of the procedure, whenever a new inactive edge is created with label B(b ...). then for all rules of the form in (9), an active edge is also created with label A(...)/C(c ...).A(...) ---> ...) C(c ...) This represents a phrase of category A that requires a phrase of category Con the right for its completion.In these labels, (variables representing) the first, or distinassociated with B and C. By analogy with parsing charts, an inactive edge labeled B(b ...) can be of as from means simply it is efficiently accessible through the index active ...) be thought of as incident from, or through, the index key property of this scheme is that active and inactive edges interact by virtue of indices that they share and, by letting vertices correspond to indices, we collect together sets of edges that could interact.We illustrate the modified procedure with the sentence (10) whose semantics we will take to be (11), the grammar rules (12)-(14), and the lexical entries in (15).(10) The dog saw the cat.(11) dog(d), def(d), saw(s), past(s), cat(c), def(c), argl(s. d), arg2(s, c).(12) s(x) np(y) vp(x, y) (13) vp(x, --> v(x, Y, z) np(z) (14) np(x) ---> det(x) n(x) (15) Words Cat Semantics cat n(x) saw z) x: see(x), past(x), argl(x, y), arg2(x,z) dog n(x) the det(x) The procedure will be reminiscent of left-corner parsing.Arguments have been made in favor of a head-driven strategy which would, however, have been marginally more (e.g. in Kay (1989), Shieber, el. and the differences are, in any case, not germane to our current concerns.The initial agenda, including active edges, and collecting edges by the vertices that they are incident from, is given in (16).The grammar is consulted only for the purpose of creating active edges and all interactions in the chart are between active and inactive pairs of edges incident from the same vertex.(16) Vert Words Cat Semantics d the det(d) d: def(d) the np(d)/n(d) d: def(d) dog n(d) d: dog(d) s saw v(s, d, c) s: see(s). past(s), d), arg2(s, c saw vp(s, d)/np(c) r: see(s), past(s), argl(r, j) the det(c) c: def(c) the np(c)/n(c) c: def(c) cat n(c) c: dog(c) (17) Vert Words Cat Semantics d the dog np(d) d: dog(d), def(d) saw the cat vp(s, d)/np(d) s: see(s), past(s), argl(s, d), arg2(s, c), cat(c), def(c) c the cat np(c) c: cat(c), def(c) s saw the cat vp(s, d) s: see(s), past(s), argl(s, d), arg2(s, c), cat(c), def(c) Among the edges in (16), there are two interactions, one at vertices c and d. They cause the first and third edges in (17) to be added to the agenda.The first interacts with the active edge originally introduced by the verb &quot;saw&quot; producing the fourth entry in (17).The label on this edge matches the first item on the right-hand side of rule (12) and the active edge that we show in the second entry is also introduced.The final interaction is between the first and second edges in (17) which give rise to the edge in (18).This procedure confirms perfectly to the standard algorithm schema for chart parsing, especially in the version that makes predictions immediately following the recognition of the first constituent of a phrase, that is, in the version that is essentially a caching left-corner parser.203 (18) Vert Words Cat Semantics s The dog saw the cat s(s) dog(d), def(d), see(s), past( s),argl(s , d), arg2(s, c), cat(c), def(c).6 Acknowledgments Whatever there may be of value in this paper owes much to the interest, encouragement, and tolerance of my colleagues Marc Dymetman, Ronald Kaplan, John Maxwell, and Hadar Shem Toy.I am also indebted to the anonymous reviewers of this paper.
This paper reports on corpus-based research into the relationship between intonational variation and discourse structure.We examine the effects of speaking style (read versus spontaneous) and of discourse segmentation method (text-alone versus text-and-speech) on the nature of this relationship.We also compare the acoustic-prosodic features of initial, medial, and final utterances in a discourse segment.
We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling, including those described by Jelinek and Mercer (1980), Katz (1987), and Church and Gale (1991).We investigate for the first time how factors such as training data corpus versus Wall Street Journal), and n-gram order (bigram versus trigram) affect the relative performance of these methods, which we measure through the cross-entropy of test data.In addition, we introduce two novel smoothing techniques, one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique, both of which outperform existing methods.
Corpus-based methods for natural language processing often use supervised training, requiring expensive manual annotation of training corpora.This paper investigates methods for reducing annotacost by selection. this approach, during training the learning program examines many unlabeled examples and selects for labeling (annotation) only those that are most informative at each stage.This avoids redundantly annotating examples that contribute little new information.This paper extends our previous on sample selection for probabilistic classifiers.We describe a family of methods for committee-based sample selection, and report experimental results for the task of stochastic part-ofspeech tagging.We find that all variants achieve a significant reduction in annotation cost, though their computational efficiency differs.In particular, the simplest method, which has no parameters to tune, gives excellent results.We also show that sample selection yields a significant reduction in the size of the model used by the tagger.
In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar.We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement.Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).
As the text databases available to users become larger and more heterogeneous, genre becomes increasingly important for computational linguistics as a complement to topical and structural principles of classification.We propose a theory of genres as of correlate with various surface cues, and argue that genre detection based on surface cues is as successful as detection based on deeper structural properties.
We derive the rhetorical structures of texts by means of two new, surface-form-based algorithms: one that identifies discourse usages of cue phrases and breaks sentences into clauses, and one that produces valid rhetorical structure trees for unrestricted natural language texts.The algorithms use information that was derived from a corpus analysis of cue phrases.
It is challenging to translate names and technical terms across languages with different alphabets and sound inventories.These items are commonly transliterated, i.e., replaced with approximate phonetic equivalents. example, English comes out :/ — in Japanese.Translating such items from Japanese back to English is even more challenging, and of practical interest, as transliterated items make up the bulk of text phrases not found in bilingual dictionaries.We describe and evaluate a method for performing backwards transliterations by machine.This method uses a generative model, incorporating several distinct stages in the transliteration process.
We identify and validate from a large corpus constraints from conjunctions on the positive or negative semantic orientation of the conjoined adjectives.A log-linear regression model uses these constraints to predict whether conjoined adjectives are of same or different orientations, achieving 82% accuracy in this task when each conjunction is considered independently.Combining the constraints across many adjectives, a clustering algorithm separates the adjectives into groups of different orientations, and finally, adjectives are labeled positive or negative.Evaluations on real data and simulation experiments indicate high levels of performance: classification precision is more than 90% for adjectives that occur in a modest number of conjunctions in the corpus.
This paper presents PARADISE (PARAdigm for DIalogue System Evaluation), a general framework for evaluating spoken rlialogue agents.The framework decouples task requirements from an agent's dialogue behaviors, supports comparisons among dialogue strategies, enables the calculation of performance over subdialogues and whole dialogues, specifies the relative contribution of various factors to performance, and makes it possible to compare agents performing different tasks by normalizing for task complexity.
This paper presents a trainable rule-based algorithm for performing word segmentation.The algorithm provides a simple, language-independent alternative to large-scale lexical-based segmenters requiring large amounts of knowledge engineering.As a stand-alone segmenter, we show our algorithm to produce high performance Chinese segmentation.In addition, we show the transformation-based algorithm to be effective in improving the output of several existing word segmentation algorithms in three different languages.
Many multilingual NLP applications need to translate words between different languages, but cannot afford the computational expense of inducing or applying a full translation model.For these applications, we have designed a fast algorithm for estimating a partial translation model, which accounts for translational equivalence only at the word level .The model's precision/recall trade-off can be directly controlled via one threshold parameter.This feature makes the model more suitable for applications that are not fully statistical.The model's hidden parameters can be easily conditioned on information extrinsic to the model, providing an easy way to integrate pre-existing knowledge such as partof-speech, dictionaries, word order, etc.. Our model can link word tokens in parallel texts as well as other translation models in the literature.Unlike other translation models, it can automatically produce dictionary-sized translation lexicons, and it can do so with over 99% accuracy.
Recognizing shallow linguistic patterns, such as basic syntactic relationships between words, is a common task in applied natural language and text processing.The common practice for approaching this task is by tedious manual definition of possible pattern structures, often in the form of regular expressions or finite automata.This paper presents a novel memory-based learning method that recognizes shallow patterns in new text based on a bracketed training corpus.The training data are stored as-is, in efficient suffix-tree data structures.Generalization is performed on-line at recognition time by comparing subsequences of the new text to positive and negative evidence in the corpus.This way, no information in the training is lost, as can happen in other learning systems that construct a single generalized model at the time of training.The paper presents experimental results for recognizing noun phrase, subject-verb and verb-object patterns in English.Since the learning approach enables easy porting to new domains, we plan to apply it to syntactic patterns in other languages and to sub-language patterns for information extraction.
Cross-document coreference occurs when the same person, place, event, or concept is discussed in more than one text source.Computer recognition of this phenomenon is important because it helps break &quot;the document boundary&quot; by allowing a user to examine information about a particular entity from multiple text sources at the same time.In this paper we describe a cross-document coreference resolution algorithm which uses the Vector Space Model to resolve ambiguities between people having the same name.In addition, we also describe a scoring algorithm for evaluating the cross-document coreference chains produced by our system and we compare our algorithm to the scoring algorithm used in the MUC- 6 (within document) coreference task.
is a NSF-supported project in corpus-based computational lexicography, now in its second year (NSF IRI-9618838, &quot;Tools for Lexicon Building&quot;).The project's key features are (a) a commitment to corpus evidence for semantic and syntactic generalizations, and (b) the representation of the valences of its target words (mostly nouns, adjectives, and verbs) in which the semantic portion makes use of frame semantics.The resulting database will contain (a) descriptions of the semantic frames underlying the meanings of the words described, and (b) the valence representation (semantic and syntactic) of several thousand words and phrases, each accompanied by (c) a representative collection of annotated corpus attestations, which jointly exemplify the observed linkings between &quot;frame elements&quot; and their syntactic realizations (e.g. grammatical function, phrase type, and other syntactic traits).This report will present the project's goals and workflow, and information about the computational tools that have been adapted or created in-house for this work.
One of the most exciting recent directions in machine learning is the discovery that the combination of multiple classifiers often results in significantly better performance than what can be achieved with a single classifier.In this paper, we first show that the errors made from three different state of the art part of speech taggers are strongly complementary.Next, we show how this complementary behavior can be used to our advantage.By using contextual cues to guide tagger combination, we are able to derive a new tagger that achieves performance significantly greater than any of the individual taggers.
Finding simple, non-recursive, base noun phrases is an important subtask for many natural language processing applications.While previous empirical methods for base NP identification have been rather complex, this paper instead proposes a very simple algorithm that is tailored to the relative simplicity of the task.In particular, we present a corpus-based approach for finding base NPs by matching part-ofspeech tag sequences.The training phase of the algorithm is based on two successful techniques: first the base NP grammar is read from a &quot;treebank&quot; corpus; then the grammar is improved by selecting rules with high &quot;benefit&quot; scores.Using this simple algorithm with a naive heuristic for matching rules, we achieve surprising accuracy in an evaluation on the
The paper presents a language model that develops syntactic structure and uses it to extract meaningful information from the word history, thus enabling the use of long distance dependencies.The model assigns probability to every joint sequence of words-binary-parse-structure with headword annotation and operates in a left-to-right manner — therefore usable for automatic speech recognition.The model, its probabilistic parameterization, and a set of experiments meant to evaluate its predictive power are presented; an improvement over standard trigram modeling is achieved.
representations of context for the detection and corof malapropisms.An electronic lexical database and some of its applications.
Bootstrapping semantics from text is one of the greatest challenges in natural language learning.We first define a word similarity measure based on the distributional pattern of words.The similarity measure allows us to construct a thesaurus using a parsed corpus.We then present a new evaluation methodology for the automatically constructed thesaurus.The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.
There are obvious reasons for trying to automate the production of multilingual documentation, especially for routine subject-matter in restricted domains (e.g. technical instructions).Two approaches have been adopted: Machine Translation (MT) of a source text, and Multilingual Natural Language Generation (M-NLG) from a knowledge base.For MT, information extraction is a major difficulty, since the meaning must be derived by analysis of the source text; M-NLG avoids this difficulty but seems at first sight to require an expensive phase of knowledge engineering in order to encode the meaning.We introduce here a new technique which employs M-NLG during the phase of knowledge editing.A 'feedback text', generated from a possibly incomplete knowledge base, describes in natural language the knowledge encoded so far, and the options for extending it.This method allows anyone speaking one of the supported languages to produce texts in all of them, requiring from the author only expertise in the subject-matter, not expertise in knowledge engineering.
several unsupervised statistical models for the prepositional phrase attachment task that approach the accuracy of the best supervised methods for this task.Our unsupervised approach uses a heuristic based on attachment proximity and trains from raw text that is annotated with only part-of-speech tags and morphological base forms, as opposed to attachment information.It is therefore less resource-intensive and more portable than previous corpus-based algorithm proposed for this task.We present results for prepositional phrase attachment in both English and Spanish.
As a lexical knowledge base constructed automatically from the definitions and example sentences in two machine-readable dictionaries (MRDs), MindNet embodies several features that distinguish it from prior work with MRDs.It is, however, more than this static resource alone.MindNet represents a general methodology for acquiring, structuring, accessing, and exploiting semantic information from natural language text.This paper provides an overview of the distinguishing characteristics of MindNet, the steps involved in its creation, and its extension beyond dictionary text.
semantic lexicons semiautomatically could be a great time saver, relative to creating them by hand.In this paper, we present an algorithm for extracting potential entries for a category from an on-line corpus, based upon a small set of exemplars.Our algorithm finds more correct terms and fewer incorrect ones than previous work in this area.Additionally, the entries that are generated potentially provide broader coverage of the category than would occur to an individual coding them by hand.Our algorithm finds many terms not included within Wordnet (many more than previous algorithms), and could be viewed as an &quot;enhancer&quot; of existing broad-coverage resources.
I propose a model for determining the hearer's attentional state which depends solely on a list of salient discourse entities (S-list).The ordering among the elements of the S-list covers also the of the center the centering model.The ranking criteria for the S-list based on the distinction between entities and incorporate preferences for interand intra-sentential anaphora.The model is the basis for an algorithm which operates incrementally, word by word.
distributional similarity measures for the purpose of improving probability estimation for unseen cooccurrences.Our contributions are three-fold: an empirical comparison of a broad range of measures; a classification
We present a method for extracting parts of objects from wholes (e.g.&quot;speedometer&quot; from &quot;car&quot;).Given a very large corpus our method finds part words with 55% accuracy for the top 50 words as ranked by the system.The part list could be scanned by an end-user and added to an existing ontology (such as WordNet), or used as a part of a rough semantic lexicon.
We present a technique for automatic induction of slot annotations for subcategorization frames, based on induction of hidden classes in the EM framework of statistical estimation.The models are empirically evalutated by a general decision test.Induction of slot labeling for subcategorization frames is accomplished by a further application of EM, and applied experimentally on frame observations derived from parsing large corpora.We outline an interpretation of the learned representations as theoretical-linguistic decompositional lexical entries.
Previous work has shown that automatic methods can be used in building semantic lexicons.This work goes a step further by automatically creating not just clusters of related words, but a hierarchy of nouns and their hypernyms, akin to the hand-built hierarchy in WordNet.
This paper presents a case study of analyzing and improving intercoder reliability in discourse using statistical techniques. corrected tags are formulated and successfully used to guide a revision of the coding manual and develop an automatic classifier.
Non-compositional expressions present a special challenge to NLP applications.We present a method for automatic identification of non-compositional expressions using their statistical properties in a text corpus.Our method is based on the hypothesis that when a phrase is non-composition, its mutual information differs significantly from the mutual informations of phrases obtained by substituting one of the word in the phrase with a similar word.
paper describes initial work on Read, an automated reading comprehension system that accepts arbitrary text input (a story) and answers questions about it.We have acquired a corpus of 60 and 60 test stories of to grade material; each story is followed by short-answer questions (an answer key was also provided).We used these to construct and evaluate a baseline system that uses pattern matching (bag-of-words) techniques augmented with additional automated linguistic processing (stemming, name identification, semantic class identification, and pronoun resolution).This simple system retrieves the sentence containing the answer 30-40% of the time.
Coreference resolution involves finding antecedents for anaphoric discourse entities, such as definite noun phrases.But many definite noun phrases are not anaphoric because their meaning can be understood from general world knowledge (e.g., &quot;the White House&quot; or &quot;the news media&quot;).We have developed a corpus-based algorithm for automatically identifying definite noun phrases that are non-anaphoric, which has the potential to improve the efficiency and accuracy of coreference resolution systems.Our algorithm generates lists of nonanaphoric noun phrases and noun phrase patterns from a training corpus and uses them to recognize non-anaphoric noun phrases in new texts.Using 1600 MUC-4 terrorism news articles as the training corpus, our approach achieved 78% recall and 87% precision at identifying such noun phrases in 50 test documents.
stochastic parsers use grammars, where each word type idiosyncratically prefers particular complements with parhead words.We present parsing algorithms for two bilexical formalisms, improvthe prior upper bounds of For a comspecial case that was known to allow (Eisner, 1997), we present an algorithm with an improved grammar constant.
Algorithms for the alignment of words in translated texts are well established.However, only recently new approaches have been proposed to identify word translations from non-parallel or even unrelated texts.This task is more difficult, because most statistical clues useful in the processing of parallel texts cannot be applied to non-parallel texts.Whereas for parallel texts in some studies up to 99% of the word alignments have been shown to be correct, the accuracy for non-parallel texts has been around 30% up to now.The current study, which is based on the assumption that there is a correlation between the patterns of word co-occurrences in corpora of different languages, makes a significant improvement to about 72% of word translations identified correctly.
STRAND (Resnik, 1998) is a languageindependent system for automatic discovery of text in parallel translation on the World Wide Web.This paper extends the preliminary STRAND results by adding automatic language identification, scaling up by orders of magnitude, and formally evaluating performance.The most recent end-product is an automatically acquired parallel corpus comprising 2491 English-French document pairs, approximately 1.5 million words per language.
Log-linear models provide a statistically sound framework for Stochastic &quot;Unification-Based&quot; Grammars (SUBGs) and stochastic versions of other kinds of grammars.We describe two computationally-tractable ways of estimating the parameters of such grammars from a training corpus of syntactic analyses, and apply these to estimate a stochastic version of Lexical
We present a method to automatically generate a concise summary by identifying and synthesizing similar elements across related text from a set of multiple documents.Our approach is unique in its usage of language generation to reformulate the wording of the summary.
Tempeval-2 comprises evaluation tasks for time expressions, events and temporal relations, the latter of which was split up in four sub tasks, motivated by the notion that smaller subtasks would make both data preparation and temporal relation extraction easier.Manually annotated data were
This paper presents the description and evaluation framework of SemEval-2010 Word Sense Induction & Disambiguation task, as well as the evaluation results of 26 participating systems.In this task, participants were required to induce the senses of 100 target words using a training set, and then disambiguate unseen instances of the same words using the induced senses.Systems’ answers were evaluated in: (1) an unsupervised manner by using two clustering evaluation measures, and (2) a supervised manner in a WSD task.
This paper presents the first round of the on Textual Entailment for organized within SemEval-2012.The task was designed to promote research on semantic inference over texts written in different languages, targeting at the same time a real application scenario.Participants were presented with datasets for different language pairs, where multi-directional entailment relations (“forward”, “backward”, “bidirectional”, “no entailment”) had to be identified.We report on the training and test data used for evaluation, the process of their creation, the participating systems (10 teams, 92 runs), the approaches adopted and the results achieved.
We present a multi-document summarizer, called MEAD, which generates summaries using cluster centroids produced by a topic detection and tracking system.We also describe two new techniques, based on sentence utility and subsumption, which we have applied to the evaluation of both single and multiple document summaries.Finally, we describe two user studies that test our models of multi-document summarization.
Morphology induction is a subproblem of important tasks like automatic learning of machine-readable dictionaries and grammar induction.Previous morphology induction approaches have relied solely on statistics of hypothesized stems and affixes to choose which affixes to consider legitimate.Relying on stemand-affix statistics rather than semantic knowledge leads to a number of problems, such as the inappropriate use of valid affixes (&quot;ally&quot; stemming to &quot;all&quot;).We introduce a semantic-based algorithm for learning morphology which only proposes affixes when the stem and stem-plusaffix are sufficiently similar semantically.We implement our approach using Latent Semantic Analysis and show that our semantics-only approach provides morphology induction results that rival a current state-of-the-art system.
This paper addresses the issue of the automatic induction of syntactic categories from unannotated corpora.Previous techniques give good results, but fail to cope well with ambiguity or rare words.An algorithm, context distribution clustering (CDC), is presented which can be naturally extended to handle these problems.
This paper presents the first-ever results of applying statistical parsing models to the newly-available Chinese Treebank.We have employed two models, one extracted and adapted from BBN's SIFT System (Miller et al., 1998) and a TAGbased parsing model, adapted from (Chiang, 2000).On sentences with <40 words, the former model performs at 69% precision, 75% recall, and the latter at 77% precision and 78% recall.
This paper presents a method of Japanese dependency structure analysis based on Support Vector Machines (SVMs).Conventional parsing techniques based on Machine Learning framework, such as Decision Trees and Maximum Entropy Models, have difficulty in selecting useful features as well as finding appropriate combination of selected features.On the other hand, it is well-known that SVMs achieve high generalization performance even with input data of very high dimensional feature space.Furthermore, by introducing the Kernel principle, SVMs can carry out the training in high-dimensional spaces with a smaller computational cost independent of their dimensionality.We apply SVMs to Japanese dependency structure identification problem.Experimental results on Kyoto University corpus show that our sysachieves the 89.09% even with small training data (7958 sentences).
This paper presents results for a maximumentropy-based part of speech tagger, which achieves superior performance principally by enriching the information sources used for tagging.In particular, we get improved results by incorporating these features: (i) more extensive treatment of capitalization for unknown words; (ii) features for the disambiguation of the tense forms of verbs; (iii) features for disambiguating particles from prepositions and adverbs.The best resulting accuracy for the tagger on the Penn Treebank is 96.86% overall, and 86.91% on previously unseen words.
Certain generation applications may profit from the use of stochastic methods.In developing stochastic methods, it is crucial to be able to quickly assess the relative merits of different approaches or models.In this paper, we present several types of intrinsic (system internal) metrics which we have used for baseline quantitative assessment.This quantitative assessment should then be augmented to a fuller evaluation that examines qualitative aspects.To this end, we describe an experiment that tests correlation between the quantitative metrics and human qualitative judgment.The experiment confirms that intrinsic metrics cannot replace human evaluation, but some correlate significantly with human judgments of quality and understandability and can be used for evaluation during development.
natural language generation sysit often advantageous to have a separate component that deals purely with morphological processing.We present such a component: a fast and robust morphological generator for English based on finite-state techniques that generates a word form given a specification of the lemma, part-of-speech, and the type of inflection required.We describe how this morphological generator is used in a prototype system for automatic simplification of English newspaper text, and discuss practical morphological and orthographic issues we have encountered in generation of unrestricted text within this application.
Co-Training is a weakly supervised learning paradigm in which the redundancy of the learning task is captured by training two classifiers using separate views of the same data.This enables bootstrapping from a small set of labeled training data via a large set of unlabeled data.This study examines the learning behavior of co-training on natural language processing tasks that typically require large numbers of training instances to achieve usable performance levels.Using base noun phrase bracketing as a case study, we find that co-training reduces by 36% the difference in error between classifiers and supervised clastrained on a labeled version all available data.However, degradation in the quality of the bootstrapped data arises as an obstacle to further improvement.To address this, we propose a moderately supervised variant of cotraining in which a human corrects the mistakes made during automatic labeling.Our analysis suggests that corrected co-training and similar moderately supervised methods may help cotraining scale to large natural language learning tasks.
We are developing corpus-based techniques for identifying semantic relations at an intermediate level of description (more specific than those used in case frames, but more general than those used in traditional knowledge representation systems).In this paper we describe a classification algorithm for identifying relationships between two-word noun compounds.We find that a very simple approach using a machine learning algorithm and a domain-specific lexical hierarchy successfully generalizes from training instances, performing better on previously unseen words than a baseline consisting of training on the words themselves.
We seek a knowledge-free method for inducing multiword units from text corpora for use as machine-readable dictionary headwords.We provide two major evaluations of nine existing collocation-finders and illustrate the continuing need for improvement.We use Latent Semantic Analysis to make modest gains in performance, but we show the significant challenges encountered in trying this approach.
This paper describes a method for linear text segmentation that is more accurate or at least as accurate as state-of-the-art methods (Utiyama and Isahara, 2001; Choi, 2000a).Inter-sentence similarity is estimated by latent semantic analysis (LSA).Boundary locations are discovered by divisive clustering.Test results show LSA is a more accurate similarity measure than the
Most work in statistical parsing has focused on a single corpus: the Wall Street Journal portion of the Penn Treebank.While this has allowed for quantitative comparison of parsing techniques, it has left open the question of how other types of text might affect parser performance, and how portable parsing models are across corpora.We examine these questions by comparing results for the Brown and WSJ corpora, and also consider which parts of the parser's probability model are particularly tuned to the corpus on which it was trained.This leads us to a technique for pruning parameters to reduce the size of the parsing model.
We describe a procedure for arranging into a time-line the contents of news stories describing the development of some situation.We describe the parts of the system that deal with 1. breaking sentences into event-clauses and 2. resolving both explicit and implicit temporal references.Evaluations show a performance of 52%, compared to humans.
and Anne Anderson.1997.The reliability of a dialogue structure coding Linguistics 13-32.Giacomo Ferrari.1998.Preliminary steps toward the creation of a discourse and text In of the First International Conference on Language
NLTK, the Natural Language Toolkit, is a suite of open source program modules, tutorials and problem sets, providing ready-to-use computational linguistics courseware.NLTK covers symbolic and statistical natural language processing, and is interfaced to annotated corpora.Students augment and replace existing components, learn structured programming by example, and manipulate sophisticated models from the outset.
We explore the use of Support Vector Machines (SVMs) for biomedical named entity recognition.To make the SVM training with the available largest corpus – the GENIA corpus – tractable, we propose to split the non-entity class into sub-classes, using part-of-speech information.In addition, we explore new features such as word cache and the states of an HMM trained by unsupervised learning.Experiments on the GENIA corpus show that our class splitting technique not only enables the training with the GENIA corpus but also improves the accuracy.The proposed new features also contribute to improve the accuracy.We compare our SVMbased recognition system with a system using Maximum Entropy tagging method.
We present a transliteration algorithm based on sound and spelling mappings using finite state machines.The transliteration models can be trained on relatively small lists of names.We introduce a new spelling-based model that is much more accurate than state-of-the-art phonetic-based models and can be trained on easier-toobtain training data.We apply our transliteration algorithm to the transliteration of names from Arabic into English.We report on the accuracy of our algorithm based on exact-matching criterion and based on human-subjective evaluation.We also compare the accuracy of our system to the accuracy of human translators.
We present two methods for unsupervised segmentation of words into morphemelike units.The model utilized is especially suited for languages with a rich morphology, such as Finnish.The first method is based on the Minimum Description Length (MDL) principle and works online.In the second method, Maximum Likelihood (ML) optimization is used.The quality of the segmentations is measured using an evaluation method that compares the segmentations produced to an existing morphological analysis.Experiments on both Finnish and English corpora show that the presented methods perform well compared to a current stateof-the-art system.
Open Mind Word Expert is an implemented active learning system for collecting word sense tagging from the general public over the Web.It is available at http://teach-computers.org.We expect the system to yield a large volume of high-quality training data at a much lower cost than the traditional method of hiring lexicographers.We thus propose a Senseval-3 lexical sample activity where the training data is collected via Open Mind Word Expert.If successful, the collection process can be extended to create the definitive corpus of word sense information.
This paper presents work on the task of constructing a word-level translation lexicon purely from unrelated monolingual corpora.We combine various clues such as cognates, similar context, preservation of word similarity, and word frequency.Experimental results for the construction of a German-English noun lexicon are reported.Noun translation accuracy of 39% scored against a parallel test corpus could be achieved.
The use of semantic resources is comin modern but methods to extract lexical semantics have only recently begun to perform well enough for practical use.We evaluate existing and new similarity metrics for thesaurus extraction, and experiment with the tradeoff between extraction performance and efficiency.We propose an approximation based on attributes and coarseand fine-grained matching, that reduces the time complexity and execution time of thesaurus extraction with only a marginal performance penalty.
We describe new algorithms for train ing tagging models, as an alternativeto maximum-entropy models or conditional random elds (CRFs).The al gorithms rely on Viterbi decoding oftraining examples, combined with simple additive updates.We describe the ory justifying the algorithms througha modi cation of the proof of conver gence of the perceptron algorithm forclassi cation problems.We give experimental results on part-of-speech tag ging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger.
In this paper, we evaluate a variety of knowledge sources and supervised learning algorithms for word sense disambiguation on SENSEVAL-2 and SENSEVAL-1 data.Our knowledge sources include the part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations.The learning algorithms evaluated include Support Vector Machines (SVM), Naive Bayes, AdaBoost, and decision tree algorithms.We present empirical results showing the relative contribution of the component knowledge sources and the different learning algorithms.In particular, using all of these knowledge sources and SVM (i.e., a single learning algorithm) achieves accuracy higher than the best official scores on both SENSEVAL-2 and SENSEVAL-1 test data.
We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative.Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines.However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization.We conclude by examining factors that make the sentiment classification problem more challenging.
We present a joint probability model for statistical machine translation, which automatically learns word and phrase equivalents from bilingual corpora.Translations produced with parameters estimated using the joint model are more accurate than translations produced using IBM Model 4.1 Motivation Most of the noisy-channel-based models used in statistical machine translation (MT) (Brown et al., 1993) are conditional probability models.In the noisy-channel framework, each source sentence e in a parallel corpus is assumed to “generate” a target sentence f by means of a stochastic process, whose parameters are estimated using traditional EM techniques (Dempster et al., 1977).The generative model explains how source words are mapped into target words and how target words are re-ordered to yield well-formed target sentences.A variety of methods are used to account for the re-ordering stage: word-based (Brown et al., 1993), templatebased (Och et al., 1999), and syntax-based (Yamada and Knight, 2001), to name just a few.Although these models use different generative processes to explain how translated words are re-ordered in a target language, at the lexical level they are quite similar; all these models assume that source words are into target individual words may contain a non-existent element, called NULL.We suspect that MT researchers have so far chosen to automatically learn translation lexicons defined only over words for primarily pragmatic reasons.Large scale bilingual corpora with vocabularies in the range of hundreds of thousands yield very large translation lexicons.Tuning the probabilities associated with these large lexicons is a difficult enough task to deter one from trying to scale up to learning phrase-based lexicons.Unfortunately, trading space requirements and efficiency for explanatory power often yields non-intuitive results.Consider, for example, the parallel corpus of three sentence pairs shown in Figure 1.Intuitively, if we allow any Source words to be aligned to any Target words, the best alignment that we can come up with is the one in Figure 1.c.Sentence pair (S2, T2) offers strong evidence that “b c” in language S means the same thing as “x” in language T. On the basis of this evidence, we expect the system to also learn from sentence pair (S1, T1) that “a” in language S means the same thing as “y” in language T. Unfortunately, if one works with translation models that do not allow Target words to be aligned to more than one Source word — as it is the case in the IBM models (Brown et al., 1993) — it is impossible to learn that the phrase “b c” in language S means the same thing as word “x” in language T. The IBM Model 4 (Brown et al., 1993), for example, converges to the word alignments shown in Figure 1.b and learns the probabilities shown in Figure Since in the IBM model one cannot link a Target word to more than a Source word, the training procedure train the IBM-4 model, we used Giza (Al-Onaizan et al., 1999).IBM−4 T−Table IBM−4 Intuitive Joint Joint T−Table p(y  |a) = 1 p(x  |c) = 1 p(z  |b) = 0.98 p(x  |b) = 0.02 S1: a b c T1: x y S2: b c S1: a b c T1: x y S2: b c S1: a b c T1: x y S2: b c p(x y, a b c) = 0.32 p(x, b c) = 0.34 p(y, a) = 0.01 p(z, b) = 0.33 Corresponding Conditional Table T2: x T2: x T2: x p(x y  |a b c ) = 1 p(x  |b c) = 1 p(y  |a) = 1 p(z  |b) = 1 S3: b S3: b S3: b T3: z T3: z T3: z a) b) c) d) e) Figure 1: Alignments and probability distributions in IBM Model 4 and our joint phrase-based model. yields unintuitive translation probabilities.(Note that another good word-for-word model is one that assigns high probability to p(xb) and p(zb) and low probability to p(xc).)In this paper, we describe a translation model that assumes that lexical correspondences can be established not only at the word level, but at the phrase level as well.In constrast with many previous approaches (Brown et al., 1993; Och et al., 1999; Yamada and Knight, 2001), our model does not try to capture how Source sentences can be mapped into Target sentences, but rather how Source and Target sentences can be generated simultaneously.In other words, in the style of Melamed (2001), we estimate a joint probability model that can be easily marginalized in order to yield conditional probability models for both source-to-target and target-tosource machine translation applications.The main difference between our work and that of Melamed is that we learn joint probability models of translation equivalence not only between words but also between phrases and we show that these models can be used not only for the extraction of bilingual lexicons but also for the automatic translation of unseen sentences.In the rest of the paper, we first describe our model (Section 2) and explain how it can be implemented/trained (Section 3).We briefly describe a decoding algorithm that works in conjunction with our model (Section 4) and evaluate the performance of a translation system that uses the joint-probability model (Section 5).We end with a discussion of the strengths and weaknesses of our model as compared to other models proposed in the literature.2 A Phrase-Based Joint Probability Model 2.1 Model 1 In developing our joint probability model, we started out with a very simple generative story.We assume that each sentence pair in our corpus is generated by the following stochastic process: 1.Generate a bag of concepts.2.For each concept , generate a pair of phrases , according to the distribution contain at least one word.3.Order the phrases generated in each language so as to create two linear sequences of phrases; these sequences correspond to the sentence pairs in a bilingual corpus.For simplicity, we initially assume that the bag of concepts and the ordering of the generated phrases are modeled by uniform distributions.We do not assume that is a hidden variable that generates pair , but rather that .Under these assumptions, it follows that the probability of generating a sentence pair (E, F) using concepts given by the product of all phrase-tophrase translation probabilities, yield bags of phrases that can be ordered linearly so as to obtain the sentences E and F. For example, the sentence pair “a b c” — “x y” can be generated using two concepts, (“a b” : “y”) and (“c” : “x”); or one concept, (“a b c” : “x y”), because in both cases the phrases in each language can be arranged in a sequence that would yield the original sentence pair.However, the same sentence pair cannot be generated using the concepts (“a b” : “y”) and (“c” : “y”) because the sequence “x y” cannot be recreated from the two phrases “y” and “y”.Similarly, the pair cannot be generated using concepts (“a c” : “x”) and (“b” : “y”) because the sequence “a b c” cannot be created by catenating the phrases “a c” and “b”.We say that a set of concepts can be linearized into a sentence pair (E, F) if E and F can be obtained by permuting the phrasesandthat characterize concepts .We denote this property using the predicate .Under this model, the probability of a given sentence pair (E, F) can then be obtained by summing up over all possible ways of generating bags of concepts that can linearized to (E, F).(1) 2.2 Model 2 Although Model 1 is fairly unsophisticated, we have found that it produces in practice fairly good alignments.However, this model is clearly unsuited for translating unseen sentences as it imposes no constraints on the ordering of the phrases associated with a given concept.In order to account for this, we modify slightly the generative process in Model 1 so as to account for distortions.The generative story of Model 2 is this: 1.Generate a bag of concepts.2.Initialize E and F to empty sequences.3.Randomly take a concept and generate a pair of phrases , according to the distribution , whereandeach contain at least one word.Remove then from.4.Append phraseat the end of F. Letbe the start position ofin F. 5.Insert phraseat positionin E provided that no other phrase occupies any of the positions betweenand , wheregives length of the phrase.We hence create the alignment between the two phrasesand with probability is a position-based distortion distribution.6.Repeat steps 3 to 5 untilis empty.In Model 2, the probability to generate a sentence pair (E, F) is given by formula (2), where the position of wordof phrasein sen- F and denotes the position in tence E of the center of mass of phrase.(2) Model 2 implements an absolute position-based distortion model, in the style of IBM Model 3.We have tried many types of distortion models.We eventually settled for the model discussed here because it produces better translations during decoding.Since the number of factors involved in computing the probability of an alignment does not vary with the size of the Target phrases into which Source phrases are translated, this model is not predisposed to produce translations that are shorter than the Source sentences given as input.3 Training Training the models described in Section 2 is computationally challenging.Since there is an exponential number of alignments that can generate a sentence pair (E, F), it is clear that we cannot apply the 1.Determine high-frequency ngrams in the bilingual corpus.2.Initialize the t-distribution table.3.Apply EM training on the Viterbi alignments, while using smoothing.4.Generate conditional model probabilities.Figure 2: Training algorithm for the phrase-based joint probability model.EM training algorithm exhaustively.To estimate the parameters of our model, we apply the algorithm in Figure 2, whose steps are motivated and described below.3.1 Determine high-frequency n-grams in E and F If one assumes from the outset that any phrases can be generated from a cept , one would need a supercomputer in order to store in the memory a table that models the distribution.Since we don’t have access to computers with unlimited memory, we initially learn t distribution entries only for the phrases that occur often in the corpus and for unigrams.Then, through smoothing, we learn t distribution entries for the phrases that occur rarely as well.In order to be considered in step 2 of the algorithm, a phrase has to occur at least five times in the corpus.3.2 Initialize the t-distribution table Before the EM training procedure starts, one has no idea what word/phrase pairs are likely to share the same meaning.In other words, all alignments that can generate a sentence pair (E, F) can be assumed to have the same probability.Under these conditions, the evidence that a sentence pair (E, F) contributes to fact that are generated by the same cept is given by the number of alignments that can be built between (E, F) that have a concept that is linked to phrasein sentence E and phrase sentence F divided by the total number of alignments that can be built between the two sentences.Both these numbers can be easily approximated.Given a sentence E ofwords, there are ways in which thewords can be partitioned into sets/concepts, where is the ling number of second kind.There are also ways in which the words a sentence F can be partitioned into nonempty sets.Given that any words in E can be mapped to any words in F, it follows that there are alignments that can be built between two sentences (E, F) of lengthsand , respectively.When a concept generates two phrases of lengthand, respectively, there are only and words left to link.Hence, the absence of any other information, the probability that phrasesandare generated by the same concept is given by formula (4).Note that the fractional counts returned by equation (4) are only an approximation of the t distribution that we are interested in because the Stirling numbers of the second kind do not impose any restriction on the words that are associated with a given concept be consecutive.However, since formula (4) overestimates the numerator and denominator equally, the approximation works well in practice.In the second step of the algorithm, we apply equation (4) to collect fractional counts for all unigram and high-frequency n-gram pairs in the cartesian product defined over the phrases in each sentence pair (E, F) in a corpus.We sum over all these t-counts and we normalize to obtain an initial joint distribution.This step amounts to running the EM algorithm for one step over all possible alignments in the corpus.3.3 EM training on Viterbi alignments Given a non-uniform t distribution, phrase-to-phrase alignments have different weights and there are no other tricks one can apply to collect fractional counts over all possible alignments in polynomial time.Starting with step 3 of the algorithm in Figure 2, for each sentence pair in a corpus, we greedily produce an initial alignment by linking together phrases so as to create concepts that have high t probabilities.We then hillclimb towards the Viterbi alignment of highest probability by breaking and merging concepts, swapping words between concepts, and moving words across concepts.We compute the probabilities associated with all the alignments we generate during the hillclimbing process and collect t counts over all concepts in these alignments.We apply this Viterbi-based EM training procedure for a few iterations.The first iterations estimate the alignment probabilities using Model 1.The rest of the iterations estimate the alignment probabilities using Model 2.During training, we apply smoothing so we can associate non-zero values to phrase-pairs that do not occur often in the corpus.3.4 Derivation of conditional probability model At the end of the training procedure, we take marginals on the joint probability distributionsand .This yields conditional probability distributions and which we use for decoding.3.5 Discussion When we run the training procedure in Figure 2 on the corpus in Figure 1, after four Model 1 iterations we obtain the alignments in Figure 1.d and the joint and conditional probability distributions shown in Figure 1.e.At prima facie, the Viterbi alignment for the first sentence pair appears incorrect because we, as humans, have a natural tendency to build alignments between the smallest phrases possible.However, note that the choice made by our model is quite reasonable.After all, in the absence of additional information, the model can either assume that “a” and “y” mean the same thing or that phrases “a b c” and “x y” mean the same thing.The model chose to give more weight to the second hypothesis, while preserving some probability mass for the first one.Also note that although the joint distribution puts the second hypothesis at an advantage, the conditional distribution does not.The conditional distribution in Figure 1.e is consistent with our intuitions that tell us that it is reasonable both to translate “a b c” into “x y”, as well as “a” into “y”.The conditional distribution mirrors perfectly our intuitions.4 Decoding For decoding, we have implemented a greedy procedure similar to that proposed by Germann et al. (2001).Given a Foreign sentence F, we first produce a gloss of it by selecting phrases inthat the probability .We then tively hillclimb by modifying E and the alignment between E and F so as to maximize the formula .We hillclimb by modifying an existing alignment/translation through a set of operations that modify locally the aligment/translation built until a given time.These operations replace the English side of an alignment with phrases of different probabilities, merge and break existing concepts, and swap words across concepts.The probability p(E) is computed using a simple trigram language model that was trained using the CMU Language Modeling Toolkit (Clarkson and Rosenfeld, 1997).The language model is estimated at the word (not phrase) level.Figure 3 shows the steps taken by our decoder in order to find the translation of sentence “je vais me arrˆeter l`a .” Each intermediate translation in Figure 3 is preceded by its probability and succeded by the operation that changes it to yield a translation of higher probability.5 Evaluation To evaluate our system, we trained both Giza (IBM Model 4) (Al-Onaizan et al., 1999) and our joint probability model on a French-English parallel corpus of 100,000 sentence pairs from the Hansard corpus.The sentences in the corpus were at most 20 words long.The English side had a total of 1,073,480 words (21,484 unique tokens).The French side had a total of 1,177,143 words (28,132 unique tokens).We translated 500 unseen sentences, which were uniformly distributed across lengths 6, 8, 10, 15, and 20.For each group of 100 sentences, we manually determined the number of sentences translated perfectly by the IBM model decoder of Germann et (2001) and the decoder that uses the joint prob- Model Percent perfect translations IBM Bleu score Sentence length Sentence length 6 8 10 15 20 Avg.6 8 10 15 20 Avg.IBM 36 26 35 11 2 22 0.2076 0.2040 0.2414 0.2248 0.2011 0.2158 Phrase-based 43 37 33 19 6 28 0.2574 0.2181 0.2435 0.2407 0.2028 0.2325 Table 1: Comparison of IBM and Phrase-Based, Joint Probability Models on a translation task. je vais me arreter la . je vais me arreter la .9.46e−08 i am going to stop there .Figure 3: Example of phrase-based greedy decoding. ability model.We also evaluated the translations automatically, using the IBM-Bleu metric (Papineni et al., 2002).The results in Table 1 show that the phrased-based translation model proposed in this paper significantly outperforms IBM Model 4 on both the subjective and objective metrics.6 Discussion 6.1 Limitations The main shortcoming of the phrase-based model in this paper concerns the size of the t-table and the cost of the training procedure we currently apply.To keep the memory requirements manageable, we arbitrarily restricted the system to learning phrase translations of at most six words on each side.Also, the swap, break, and merge operations used during the Viterbi training are computationally expensive.We are currently investigating the applicability of dynamic programming techniques to increase the speed of the training procedure.Clearly, there are language pairs for which it would be helpful to allow concepts to be realized as non-contiguous phrases.The English word “not”, for example, is often translated into two French words, “ne” and “pas”.But “ne” and “pas” almost never occur in adjacent positions in French texts.At the outset of this work, we attempted to develop a translation model that enables concepts to be mapped into non-contiguous phrases.But we were not able to scale and train it on large amounts of data.The model described in this paper cannot learn that the English word “not” corresponds to the French words “ne” and “pas”.However, our model learns to deal with negation by memorizing longer phrase translation equivalents, such as (“ne est pas”, “is not”); (“est inadmissible”, “is not good enough”); and (“ne est pas ici”, “is not here”).6.2 Comparison with other work A number of researchers have already gone beyond word-level translations in various MT settings.For example, Melamed (2001) uses wordlevel alignments in order to learn translations of noncompositional compounds.Och and Ney (1999) learn phrase-to-phrase mappings involving word classes, which they call “templates”, and exploit them in a statistical machine translation system.And Marcu (2001) extracts phrase translations from automatically aligned corpora and uses them in conjunction with a word-for-word statistical translation system.However, none of these approaches learn simultaneously the translation of phrases/templates and the translation of words.As a consequence, there is a chance that the learning procedure will not discover phrase-level patterns that occur often in the je vais me arreter la .7.50e−11 FuseAndChangeTrans(&quot;la .&quot;, &quot;there .&quot;) i want me to that . je vais me arreter la .2.97e−10 ChangeWordTrans(&quot;arreter&quot;,&quot;stop&quot;) 7.75e−10 1.09e−09 i want me to there . je vais me arreter la . i want me stop there . je vais me arreter la . let me to stop there .FuseAndChange(&quot;je vais&quot;,&quot;let me&quot;) FuseAndChange(&quot;je vais me&quot;, &quot;i am going to&quot;) 1.28e−14 changeWordTrans(&quot;vais&quot;, &quot;want&quot;) i . me to that . data.In our approach, phrases are not treated differently from individual words, and as a consequence the likelihood of the EM algorithm converging to a better local maximum is increased.Working with phrase translations that are learned independent of a translation model can also affect the decoder performance.For example, in our previous work (Marcu, 2001), we have used a statistical translation memory of phrases in conjunction with a statistical translation model (Brown et al., 1993).The phrases in the translation memory were automatically extracted from the Viterbi alignments produced by Giza (Al-Onaizan et al., 1999) and reused in decoding.The decoder described in (Marcu, 2001) starts from a gloss that uses the translations in the translation memory and then tries to improve on the gloss translation by modifying it incrementally, in the style described in Section 4.However, because the decoder hill-climbs on a word-forword translation model probability, it often discards good phrasal translations in favour of word-for-word translations of higher probability.The decoder in Section 4 does not have this problem because it hillclimbs on translation model probabilities in which phrases play a crucial role.
graphs: An efficient interface between continous speech recognition and language understanding.International Conference on Acoustics, and Signal Processing, 2, pages 119-122, Minneapolis, MN, April.Stefan Ortmanns, Hermann Ney, and Xavier Aubert.1997.A word graph algorithm for large vocabcontinuous speech recognition. and Language, January.Christoph Tillmann and Hermann Ney.2000.Word re-ordering and DP-based search in statistical matranslation.In '00: The 18th Int.
This paper describes a bootstrapping algorithm called Basilisk that learns highquality semantic lexicons for multiple categories.Basilisk begins with an unannotated corpus and seed words for each semantic category, which are then bootstrapped to learn new words for each category.Basilisk hypothesizes the semantic class of a word based on collective information over a large body of extraction pattern contexts.We evaluate Basilisk on six semantic categories.The semantic lexicons produced by Basilisk have higher precision than those produced by previous techniques, with several categories showing substantial improvement.
There has been much interest in using phrasal movement to improve statistical machine translation.We explore how well phrases cohere across two languages, specifically English and French, and examine the particular conditions under which they do not.We demonstrate that while there are cases where coherence is poor, there are many regularities which can be exploited by a statistical machine translation system.We also compare three variant syntactic representations to determine which one has the best properties with respect to cohesion.
We present a broad coverage Japanese grammar written in the HPSG formalism with MRS semantics.The grammar is created for use in real world applications, such that robustness and performance issues play an important role.It is connected to a POS tagging and word segmentation tool.This grammar is being developed in a multilingual context, requiring MRS structures that are easily comparable across languages.
The grammar matrix is an open-source starter-kit for the development of broad- By using a type hierarchy to represent cross-linguistic generalizations and providing compatibility with other open-source tools for grammar engineering, evaluation, parsing and generation, it facilitates not only quick start-up but also rapid growth towards the wide coverage necessary for robust natural language processing and the precision parses and semantic representations necessary for natural language understanding.
We report on the Parallel Grammar (ParGram) project which uses the XLE parser and grammar development platform for six languages: English,
In this paper, we propose a new statistical Japanese dependency parser using a cascaded chunking model.Conventional Japanese statistical dependency parsers are mainly based on a probabilistic model, which is not always efficient or scalable.We propose a new method that is simple and efficient, since it parses a sentence deterministically only deciding whether the current segment modifies the segment on its immediate right hand side.Experiments using the Kyoto University Corpus show that the method outperforms previous systems as well as improves the parsing and training efficiency.
Conditional maximum entropy (ME) models provide a general purpose machine learning technique which has been successfully applied to fields as diverse as computer vision and econometrics, and which is used for a wide variety of classification problems in natural language processing.However, the flexibility of ME models is not without cost.While parameter estimation for ME models is conceptually straightforward, in practice ME models for typical natural language tasks are very large, and may well contain many thousands of free parameters.In this paper, we consider a number of algorithms for estimating the parameters of ME models, including iterative scaling, gradient ascent, conjugate gradient, and variable metric methods.Surprisingly, the standardly used iterative scaling algorithms perform quite poorly in comparison to the others, and for all of the test problems, a limitedmemory variable metric algorithm outperformed the other choices.
string date occurrence Local .125 context All .125 word distribution Narrow .125 .083 .083 .083 .125 Wide IDF RF Burstiness 1 exact-match accuracy 0.8 0.6 0.4 0.2 4 3 2 1 combo combo minus levenshtein 0 0 500 1000 1500 2000 2500 3000 3500 4000 4500 test words covered 0 500 1000 1500 2000 combo levenshtein only 2 1 0 500 1000 1500 2000 4 combo combo minus context 3 2 online + paper dictionary scoring online dictionary scoring 1 4 0 500 1000 1500 2000 3 combo combo minus date 2 1 0 500 1000 1500 2000 combo combo minus rfjdf,burstiness 4 3 2 1 4 3 0 500 1000 1500 2000 4 3 bulg+czech bulg+czech w/ retrained levenshtein bulg+czech w/ retrained levenshtein & context 2 1 0 500 1000 1500 2000 RANK CRIB.SCR.C'OMBINED STRING DATE-LOCAL WIDE-COS NARROW-C'OS BURSTINESS RF 1 0.18 protest/N (1) abhorrence/N break/V protest/V protest/N protest/N protest/N 2 0.19 opening/N (1) abomination/N resistance/N protest/N system/N reluctance/N port/N 3 0.24 break/N (1) allergy/N stress/V break/V break/V break/N opening/N 4 0.28 mouth/N (1) animosity/N protest/V hate/V protest/V kick/V stress/V 5 0.29 objection/N (1) antagonism/N escape/V opening/N antagonism/N protest/V protest/V 6 (1) antipathy/N protest/N escape/V hate/V escape/V escape/V 7 0.30 opposition/N (1) aperture/N opening/N stress/V dislike/V opposition/N resistance/N 8 0.33 reluctance/N (1) averse/J break/N system/N resentment/N mouth/N break/N 9 0.33 port/N (1) aversion/N kick/V defiance/N unit/N unit/N break/V 10 0.36 hole/N (1) bore/N system/N mouth/N disgust/V formation/N opposition/N 11 0.38 stress/N (1) bore/V opposition/N contradiction/N reluctance/N port/N unit/N 12 0.38 escape/N (1) boring/J kick/N kick/V formation/N stress/V hole/N 13 0.38 formation/N (1) boring/N formation/N resentment/N animosity/N objection/N kick/V 14 0.40 animosity/N (1) break/N punch/N dislike/V dislike/N protestation/N outlet/N 15 0.40 resentment/N (1) break/V unit/N reluctance/N escape/V hate/V column/N (1) resistance/N RANK CRIB.SCR.C'OMBINED STRING DATE-LOCAL WIDE-C'OS NARROW-COS BURSTINESS RF 1 (1) freedom/N independence/N independence/N independence/N evidence/V free/V 2 0.09 freedom/N relation/N single/J ease/N necessity/N cold/J 3 0.11 depend/V free/J cold/N irrelevant/J fair/J abandon/V 4 0.13 relation/N (4) irrelevance/N side/N side/N ease/V single/V importance/N 5 0.20 consequence/N (5) illegality/N importance/N independent/J applicability/N application/N ease/V 6 0.21 lift/V (5) illegitimacy/N depend/V consequence/N single/J independence/N licence/N 7 0.21 importance/N independent/J freedom/N disagreement/N currency/N lift/V 8 0.22 obligation/N single/J abandon/V lift/V free/V miss/N 9 0.23 ease/V life/N lack/V cold/N inadequacy/N green/N 10 0.23 independent/J freedom/N depend/V depend/V pride/N involvement/N 11 0.23 single/J irrelevant/N moment/N pride/N cold/J green/J 12 0.24 abandon/V miss/V importance/N side/N irrelevant/J consequence/N 13 0.24 integrity/N imperative/J relation/N realty/N side/V utility/N 14 0.24 necessity/N safety/N lack/N consequence/N disagreement/N lack/V 15 0.24 irrelevant/J obligation/N necessity/N drag/N independent/N independent/N (25)indpndnce/N RANK CRIB.SCR.COMBINED STRING DATE-LOCAL WIDE-COS NARROW-C'OS BURSTINESS RF 1 quarter/N currency/N currency/N exchange/V bless/V 2 0.43 chop/V (1) calibre/N good/J applaud/V praise/V making/N chop/V 3 0.45 bless/V (1) chop/N quality/N praise/N superior/J praise/N commend/V 4 0.48 applaud/V (1) chop/V class/N praise/V good/J class/N laud/V 5 0.49 exchange/V (1) class/N exchange/N good/J class/N currency/N making/N 6 (1) class/V compliment/N making/N good/N applaud/V applaud/V 7 0.56 commend/V (1) making/N superior/J bless/V quarter/N quarter/N superior/J 8 0.57 class/V (1) quality/J exchange/V superior/J quality/N superior/J praise/N 9 0.68 quarter/V (1) quality/N superior/N good/N biennial/J good/N superior/N 10 0.71 compliment/V (1) quarter/N praise/V exchange/V exchange/N quality/N compliment/N 11 0.81 scroll/V (1) quarter/V praise/N chop/V bless/V superior/N scroll/N 12 2.30 superior/J (12) applaud/V good/N exchange/N praise/N laud/V exchange/V 13 2.30 class/N (12) biennial/J bless/V quality/N exchange/V praise/V chop/N 14 2.34 quality/N (12) biennial/N currency/N class/N exchange/N good/N 15 2.35 making/N (12) bless/N caliber/N biennial/J bless/V calibre/N 17 32 (12) laud/V RANK CRIB.SCR.C'OMBINED STRING DATE-LOCAL WIDE-C'OS NARROW-COS BURSTINESS RF 1 rise/V bear/V widow/N stand/V horse/N 2 0.30 suffer/V (1) endure/V suffer/V stand/V stand/V raise/V expire/V 3 0.31 bear/V (1) expire/V stand/V leave/V leave/V suffer/V proceed/V 4 0.39 leave/V (1) leave/V limit/N suffer/V bear/V bear/V quantity/N 5 0.41 proceed/V (1) proceed/V raise/V endure/V boundary/N leave/V boundary/N 6 0.41 endure/V (1) raise/V bear/V limit/N endure/V rise/V limit/N 7 0.42 raise/V (1) rise/V leave/V raise/V limit/N proceed/V endure/V 8 0.44 rise/V (1) shallow/J horse/N quantity/N suffer/V endure/V widow/N 9 0.45 expire/V boundary/N proceed/V proceed/V limit/N bear/V 10 0.45 limit/N (1) suffer/V expire/V horse/N raise/V expire/V suffer/V 11 0.52 boundary/N (11) mischief/N quantity/N widow/N expire/V quantity/N stand/V 12 0.57 quantity/N (12) boundary/N proceed/V boundary/N rise/V widow/N mischief/N 13 0.61 widow/N (12) horse/N endure/V shallow/J horse/N horse/N raise/V 14 0.62 horse/N (12) limit/N widow/N rise/V quantity/N boundary/N shallow/J 15 0.72 shallow/J (12) quantity/N mischief/N expire/V shallow/J rise/V 5: tables show the performance of individual similarity measures as well as their combined choice, after model retraining.Correct translations are shown in bold.Note that in many cases the string-similarity-based orderings of the bridge candidates underperform individual non-string similarity measures, and they consistently underperform the weighted combiof all 8 similarity measures.Note that in the case of correct translation successfully above its quite closely related competitor almost every non-string-based similarity measure in isolation.This behavior (shown quantatively in Figure 8) illustrates the contribution of consensus modeling over this set of diverse similarity measures.Mann, G. and D. Yarowsky, 2001.Multipath translation induction via bridge languages.In
BiBr.EF.1 BiBr.EF.2 BiBr.EF.3 BiBr.EF.4 BiBr.EF.5 BiBr.EF.6 BiBr.EF.7 BiBr.EF.8 Limited Unlimited Unlimited Limited Unlimited Unlimited Limited Unlimited baseline of bilingual bracketing baseline of bilingual bracketing + English POS tagging baseline of bilingual bracketing + English POS tagging and base NP reverse direction of BiBr.EF.1 reverse direction of BiBr.EF.2 reverse direction of BiBr.EF.3 intersection of BiBr.EF.1 & BiBr.EF.3 intersection of BiBr.EF.3 & BiBr.EF.6 ProAlign.EF.1 Unlimited cohesion between source and target language + English parser + distributional similarity for English words Ralign.EF.1 Limited Giza (IBM Model 2) + recursive parallel segmentation UMD.EF.1 Limited IBM Model 2, trained with 1/20 of the corpus, distortion 2, iterations 4 XRCE.Base.EF.1 XRCE.Nolem.EF.2 XRCE.Nolem.EF.3 Limited GIZA++ (IBM Model 4) with English and French lemmatizer GIZA++ only (IBM Model 4), trained with 1/4 of the corpus GIZA++ only (IBM Model 4), trained with 1/2 of the corpus Table 2: Short description for English-French systems System Resources Description BiBr.RE.1 BiBr.RE.2 BiBr.RE.3 Limited Unlimited Unlimited baseline of bilingual bracketing baseline of bilingual bracketing + English POS tagging baseline of bilingual bracketing + English POS tagging and base NP
We explore the idea of creating a subjectivity classifier that uses lists of subjective nouns learned by bootstrapping algorithms.The goal of our research is to develop a system that can distinguish subjective sentences from objective sentences.First, we use two bootstrapping algorithms that exploit extraction patterns to learn sets of subjective nouns.Then we train a Naive Bayes classifier using the subjective nouns, discourse features, and subjectivity clues identified in prior research.The bootstrapping algorithms learned over 1000 subjective nouns, and the subjectivity classifier performed well, achieving 77% recall with 81% precision.
This paper presents a set of algorithms for distinguishing personal names with multiple real referents in text, based on little or no supervision.The approach utilizes an unsupervised clustering technique over a rich feature space of biographic facts, which are automatically extracted via a language-independent bootstrapping process.The induced clustering of named entities are then partitioned and linked to their real referents via the automatically extracted biographic data.Performance is evaluated based on both a test set of handlabeled multi-referent personal names and via automatically generated pseudonames.
This paper investigates booststrapping part-ofspeech taggers using co-training, in which two taggers are iteratively re-trained on each other’s output.Since the output of the taggers is noisy, there is a question of which newly labelled examples to add to the training set.We investigate selecting examples by directly maximising tagger agreement on unlabelled data, a method which has been theoretically and empirically motivated in the co-training literature.Our results show that agreement-based co-training can significantly improve tagging performance for small seed datasets.Further results show that this form of co-training considerably outperforms self-training.However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost.
Entity Recognition systems need to integrate a wide variety of information for optimal performance.This paper demonstrates that a maximum entropy tagger can effectively encode such information and identify named entities with very high accuracy.The tagger uses features which can be obtained for a variety of languages and works effectively not only for English, but also for other languages such as German and Dutch.
This paper presents a classifier-combination experimental framework for named entity recognition in which four diverse classifiers (robust linear classifier, maximum entropy, transformation-based learning, and hidden Markov model) are combined under different conditions.When no gazetteer or other additional training resources are used, the combined system attains a performance of 91.6F on the English development data; integrating name, location and person gazetteers, and named entity systems trained on additional, more general, data reduces the F-measure error by a factor of 15 to 21% on the English data.
and Abstracts for Nice Summaries, In Workon Automatic Philadelphia, PA, pp.9-14.Edmundson, H. (1969).“New methods in automatic of the 16(2).Grefenstett, G. (1998).Producing intelligent telegraphic text reduction to provide an audio scanning serfor the blind.In Notes of the AIII Spring on Intelligent Text Summarization,
We use deep linguistic features to predict semantic roles on syntactic arguments, and show that these perform considerably better than surface-oriented features.We also show that predicting labels from a “lightweight” parser that generates deep syntactic features performs comparably to using a full parser that generates only surface syntactic features.
We present a system for automatically identifying PropBank-style semantic roles based on the output of a statistical parser for Combinatory Categorial Grammar.This system performs at least as well as a system based on a traditional Treebank parser, and outperforms it on core argument roles.
We present a general framework for distributional similarity based on the concepts of precision and recall.Different parameter settings within this framework approximate different existing similarity measures as well as many more which have, until now, been unexplored.We show that optimal parameter settings outperform two existing state-of-the-art similarity measures on two evaluation tasks for high and low frequency nouns.
This paper presents a bootstrapping process that learns linguistically rich extraction patterns for subjective (opinionated) expressions.High-precision classifiers label unannotated data to automatically create a large training set, which is then given to an extraction pattern learning algorithm.The learned patterns are then used to identify more subjective sentences.The bootstrapping process learns many subjective patterns and increases recall while maintaining high precision.
Opinion question answering is a challenging task for natural language processing.In this paper, we discuss a necessary component for an opinion question answering system: separating opinions from fact, at both the document and sentence level.We present a Bayesian classifier for discriminating between documents with a preponderance of opinions such as editorials from regular news stories, and describe three unsupervised, statistical techniques for the significantly harder task of detecting opinions at the sentence level.We also present a first model for classifying opinion sentences as positive or negative in terms of the main perspective being expressed in the opinion.Results from a large collection of news stories and a human evaluation of 400 sentences are reported, indicating that we achieve very high performance in document classification (upwards of 97% precision and recall), and respectable performance in detecting opinions and classifying them at the sentence level as positive, negative, or neutral (up to 91% accuracy).
In this paper, experiments on automatic extraction of keywords from abstracts using a supervised machine learning algorithm are discussed.The main point of this paper is that by adding linguistic knowledge to the representation (such as syntactic features), rather than relying only on (such as term frequency and grams), a better result is obtained as measured by keywords previously assigned by professional indexers.In more detail, exgives a better precithan and by adding the tag(s) assigned to the term as a feature, a dramatic improvement of the results is obtained, independent of the term selection approach applied.
We address the problem of transliterating English names using Chinese orthography in support of cross-lingual speech and text processing applications.We demonstrate the application of statistical machine translation techniques to “translate” the phonemic representation of an English name, obtained by using an automatic text-to-speech system, to a sequence of initials and finals, commonly used subword units of pronunciation for Chinese.We then use another statistical translation model to map the initial/final sequence to Chinese characters.We also present an evaluation of this module in retrieval of Mandarin spoken documents from the TDT corpus using English text queries.
This paper presents the results from the ACL-SIGHAN-sponsored First International Chinese Word Segmentation Bakeoff held in 2003 and reported in conjunction with the Second SIGHAN Workshop on Chinese Language Processing, Sapporo, Japan.We give the motivation for having an international segmentation contest (given that there have been two within-China contests to date) and we report on the results of this first international contest, analyze these results, and make some recommendations for the future.
PK F-score ❳ c the set by the MEMM tagger that scans the input from left to right and the last column is the results after the Transformation- Based Learner is applied.The results show that using Transformation-Based learning only give rise to slight improvements.It seems that the bidirectional approach does not help much for the LMR tagging.Therefore, we only submitted the results of our leftto-right MEMM tagger, retrained on the entire training sets, as our official results.F-score MEMM MEMM+TBL AS 0.9595 0.9603 HK 0.9143 N/A PK 0.9391 0.9398 Table 2: F-score on development data The results on the official test data is similar to we have got on our except that the F-score on the Beijing Univ. corpus is over 2 lower in absolute accuracy than what we expected.The reason is that in the training data of Beijing University corpus, all the numbers are encoded in GBK, while in the test data many numbers are encoded in ASCII, which are unknown to our tagger.With this problem fixed, the results of the official test data are compatible with the results on However, we have withdrawn our segmentation results on the Beijing University corpus. corpus R P F AS 0.961 0.958 0.959 0.729 0.966 HK 0.917 0.915 0.916 0.670 0.936 Table 3: Official Bakeoff Outcome 4 Conclusions and Future Work Our closed track experiments on the first Sighan Bakeoff data show that the LMR algorithm produces promising results.Our system ranks the second when tested on the Academia Sinica corpus and third on the Hong Kong City University corpus.In the future, we will try to incorporate a large word list into our tagger to test its performance on open track experiments.Its high accuracy on makes it a good candidate as a general purpose segmenter.References E. Brill.1995.Transformation-based error-driven learning and natural language processing: A case study part-of-speech tagging.21(4):543–565.C. K. Fan and W. H. Tsai.1988.Automatic word identification in chinese sentences by the relaxation tech- Processing of Chinese and Oriental 4(1):33–56.Kok-Wee Gan, Martha Palmer, and Kim-Teng Lua.1996.A statistically emergent approach for language processing: Application to modeling context effects in chinese word boundary perception.Com- 22(4):531–53.J. Lafferty, A. McCallum, and F. Pereira.2001.Conditional random fields: Probabilistic models for stgmenand labeling sequence data.In of G. Ngai and R. Florian.2001.Transformation-based in the fast lane.In of NAACLpages 40–47.Adwait Ratnaparkhi.1996.A maximum entropy part-oftagger.In of the Empirical Methin Natural Language Processing University of Pennsylvania.L. Shen and A. K. Joshi.2003.A SNoW based supertagwith application to NP chunking.In R. Sproat, Chilin Shih, William Gale, and Nancy Chang.1996.A stochastic finite-state word-segmentation for chinese.22(3):377–404.H. van Halteren, J. Zavrel, and W. Daelmans.1998.Improving data driven wordclass tagging by system com- In of COLING-ACL Andi Wu.2003.Customizable segmentation of morderived words in chinese. and Chinese Language Nianwen Xue.2003.Chinese word segmentation as tagging.Linguistics and
This document presents the results from Inst. of Computing Tech., CAS in the ACL- SIGHAN-sponsored First International Chinese Word Segmentation Bakeoff.The authors introduce the unified HHMM-based frame of our Chinese lexical analyzer ICTCLAS and explain the operation of the six tracks.Then provide the evaluation results and give more analysis.Evaluation on ICTCLAS shows that its performance is competitive.Compared with other system, ICTCLAS has ranked top both in CTB and PK closed track.In PK open track, it ranks second position.ICTCLAS BIG5 version was transformed from GB version only in two days; however, it achieved well in two BIG5 closed tracks.Through the first bakeoff, we could learn more about the development in Chinese word segmentation and become more confident on our HHMM-based approach.At the same time, we really find our problems during the evaluation.The bakeoff is interesting and helpful.
This paper describes a distributional approach to the semantics of verb-particle (e.g.We report first on a framework for implementing and evaluating such models.We then go on to report on the implementation of some techniques for using statistical models acquired from corpus data to infer the meaning of verb-particle constructions.
We investigate the use of an automatically acquired thesaurus for measures designed to indicate the compositionality of candidate multiword verbs, specifically English phrasal verbs identified automatically using a robust parser.We examine various measures using the nearest neighbours of the phrasal verb, and in some cases the neighbours of the simplex counterpart and show that some of these correlate significantly with human rankings of compositionality on the test set.We also show that whilst the compositionality judgements correlate with some statistics commonly used for extracting multiwords, the relationship is not as strong as that using the automatically constructed thesaurus.
This paper presents a constructioninspecific model of multiword expression decomposability based on latent semantic analysis.We use latent semantic analysis to determine the similarity between a multiword expression and its constituent words, and claim that higher similarities indicate greater decomposability.We test the model over English noun-noun compounds and verb-particles, and evaluate its correlation with similarities and hyponymy values in WordNet.Based on mean hyponymy over partitions of data ranked on similarity, we furnish evidence for the calculated similarities being correlated with the semantic relational content of WordNet.
Deterministic dependency parsing is a robust and efficient approach to syntactic parsing of unrestricted natural language text.In this paper, we analyze its potential for incremental processing and conclude that strict incrementality is not achievable within this framework.However, we also show that it is possible to minimize the number of structures that require nonincremental processing by choosing an optimal parsing algorithm.This claim is substantiated with experimental evidence showing that the algorithm achieves incremental parsing for 68.9% of the input when tested on a random sample of Swedish text.When restricted to sentences that are accepted by the parser, the degree of incrementality increases to 87.9%.
The SENSEVAL-3 task to perform automatic labeling of semantic roles was designed to encourage research into and use of the FrameNet dataset.The task was based on the considerable expansion of the FrameNet data since the baseline study of automatic labeling of semantic roles by Gildea and Jurafsky.The FrameNet data provide an extensive body of “gold standard” data that can be used in lexical semantics research, as the basis for its further exploitation in NLP applications.Eight teams participated in the task, with a total of 20 runs.Discussions among participants during development of the task and the scoring of their runs contributed to a successful task.Participants used a wide variety of techniques, investigating many aspects of the FrameNet data.They achieved results showing considerable improvements from Gildea and Jurafsky’s baseline study.Importantly, their efforts have contributed considerably to making the complex FrameNet dataset more accessible.They have amply demonstrated that FrameNet is a substantial lexical resource that will permit extensive further research and exploitation in NLP applications in the future.
This paper presents the task definition, resources, participating systems, and comparative results for the English lexical sample task, which was orgaas part of the evaluation exercise.The task drew the participation of 27 teams from around the world, with a total of 47 systems.
merl.systeml .459 .447 IR,ST-DDD-09-U .729 .441 autoPS-U .490 .433 clr04-aw .506 .431 autoPSNVs-U .563 .354 merl.system2 .480 .352 DLSI-UA-all-Nosu .343 .275 Table 2: &quot;Without U&quot; scores, sorted by recall; a -S or -U suffix after the system name indicates that the system was reported as supervised or unsupervised, respectively.System Name Email Contact autoPS dianam©sussex.ac.uk autoPSNVs dianam©sussex.ac.uk clr04-aw ken©clres.com DFA-Unsup-AW david©lsi.uned.es DLSI-UA-Nosu montoyo©dlsi.ua.es GAMBL-AW bart.decadt©ua.ac.be IR ST-DDD-00 strappaKfitc.it IRST-DDD-09 strappaKfitc.it IRST-DDD-LSI strappaKfitc.it Koc University dyuret©ku.edu.tr KUNLP-Eng-All hcseoKM1p.korea.ac.kr LCCaw parker©languagecomputer.com Meaning lluism©lsi.upc.es Meaning simple lluism©lsi.upc.es merl.systeml bhiksha©merl.com merl.system2 bhiksha©merl.com merl.system3 bhiksha©merl.com R2D2: EAW montoyo©dlsi.ua.es SenseLearner rada©cs.unt.edu UJAEN mgarcia©ujaen.es USussex-Prob3 Judita.Preiss©cl.cam.ac.uk USussex-Prob4 Judita.Preiss©cl.cam.ac.uk USussex-Prob5 Judita.Preiss©cl.cam.ac.uk upv-shmm-eaw amolina©dsic.upv.es upv-CIAOSENSO amolina©dsic.upv.es upv-CIAOSENS02 amolina©dsic.upv.es Table 3: email contact for each system; sorted alphabetically. the unsupervised systems (Palmer et al., 2001).In fact, all of the seven systems reported as supervised scored higher than any of the nine systems reported as unsupervised in both precision and recall (using either of the two scoring criteria).The greatest difference between these results those of the all-words task is that a greater number of systems have now achieved scores at or above the baseline.While this result is encouraging, it seems that the best systems have a hit a wall in the 65- 70% range.This is not surprising given the typical inter-annotator agreement of 70-75% for this task.We believe that further significant progress must await the development of resources with coarser-grained sense distinctions and with glosses that draw explicit contrasts between the senses — resources more suitable for the task at hand.
for Recall-Oriented Understudy for Gisting Evaluation.It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans.The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans.This paper introduces four different included in the summarization evaluation package and their evaluations.Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST.
2 Data We describe a new corpus of over 180,000 handannotated dialog act tags and accompanying adjacency pair annotations for roughly 72 hours of speech from 75 naturally-occurring meetings.We provide a brief summary of the annotation system and labeling procedure, inter-annotator reliability statistics, overall distributional statistics, a description of auxiliary files distributed with the corpus, and information on how to obtain the data.
Given a collection of discrete random variables representing outcomes of learned local predictors in natural language, e.g., named entities and relations, we seek an optimal global assignment to the variables in the presence of general (non-sequential) constraints.Examples of these constraints include the type of arguments a relation can take, and the mutual activity of different relations, etc.We develop a linear programming formulation for this problem and evaluate it in the context of simultaneously learning named entities and relations.Our approach allows us to efficiently incorporate domain and task specific constraints at decision time, resulting in significant improvements in the accuracy and the “human-like” quality of the inferences.
This paper systematically compares unsupervised word sense discrimination techniques that cluster instances of a target word that occur in raw text using both vector and similarity spaces.The context of each instance is represented as a vector in a high dimensional feature space.Discrimination is achieved by clustering these context vectors directly in vector space and also by finding pairwise similarities among the vectors and then clustering in similarity space.We employ two different representations of the context in which a target word occurs.First order context vectors represent the context of each instance of a target word as a vector of features that occur in that context.Second order context vectors are an indirect representation of the context based on the average of vectors that represent the words that occur in the context.We evaluate the discriminated clusters by carrying out experiments ussense–tagged instances of 24 words and the well known corpora.
This paper reports the results of experimentsusing memory-based learning to guide a de terministic dependency parser for unrestricted natural language text.Using data from a small treebank of Swedish, memory-based classifiers for predicting the next action of the parser are constructed.The accuracy of a classifier as such is evaluated on held-out data derived from the treebank, and its performance as a parserguide is evaluated by parsing the held-out por tion of the treebank.The evaluation shows thatmemory-based learning gives a signficant im provement over a previous probabilistic modelbased on maximum conditional likelihood esti mation and that the inclusion of lexical features improves the accuracy even further.
Roles.In 28(3).Relation no.1 2 3 6 7 11 13 15 16 21 25 the rest 0.06103 0.11268 0.00939 0.04225 0.39437 0.01878 0.03286 0.25822 0.04694 0.01878 0.00469 0 Table 5: Sample row from the conditional probability table where the feature pair is entity-entity.The numbers in the top row identify the semantic relations (as in Table 4).Level Level 1 Level 2 Level 3 Level 4 Number of 9 52 70 122 features Number 9 46 47 47 features No. of feature pairs 57 out of 81 189 out of 2392 204 out of 3290 250 out of 5734 Number of 1 152 181 225 with only one relation Average number 2.7692 1.291 1.1765 1.144 non-zero relations per line Table 6: Statistics for the semantic class features by level of specialization.
This paper describes NomBank, a project that will provide argument structure for instances of common nouns in the Penn Treebank II corpus.NomBank is part of a larger effort to add additional layers of annotation to the Penn Treebank II corpus.The University of Pennsylvania’s PropBank, NomBank and other annotation projects taken together should lead to the creation of better tools for the automatic analysis of text.This paper describes the NomBank project in detail including its specifications and the process involved in creating the resource.
We describe an approach to two areas of biomedical information extraction, drug development and cancer genomics.We have developed a framework which includes corpus annotation integrated at multiple levels: a Treebank containing syntactic structure, a Propbank containing predicate-argument structure, and annotation of entities and relations among the entities.Crucial to this approach is the proper characterization of entities as relation components, which allows the integration of the entity annotation with the syntactic structure while retaining the capacity to annotate and extract more complex events.We are training statistical taggers using this annotation for such extraction as well as using them for improving the annotation process.
We present a novel discriminative approach to parsing inspired by the large-margin criterion underlying support vector machines.Our formulation uses a factorization analogous to the standard dynamic programs for parsing.In particular, it allows one to efficiently learn a model which discriminates among the entire space of parse trees, as opposed to reranking the top few candidates.Our models can condition on arbitrary features of input sentences, thus incorporating an important kind of lexical information without the added algorithmic complexity of modeling headedness.We provide an efficient algorithm for learning such models and show experimental evidence of the model’s improved performance over a natural baseline model and a lexicalized probabilistic context-free grammar.
Broad-coverage repositories of semantic relations between verbs could benefit many NLP tasks.We present a semi-automatic method for extracting fine-grained semantic relations between verbs.We detect similarity, strength, antonymy, enablement, and temporal happens-before relations between pairs of strongly associated verbs using lexicosyntactic patterns over the Web.On a set of 29,165 strongly associated verb pairs, our extraction algorithm yielded 65.5% accuracy.Analysis of types shows that on the relation achieved 75% accuracy.We provide the called for download at
Paraphrase recognition is a critical step for natural language interpretation.Accordingly, many NLP applications would benefit from high coverage knowledge bases of paraphrases.However, the scalability of state-of-the-art paraphrase acquisition approaches is still limited.We present a fully unsupervised learning algorithm for Web-based extraction an extended model of paraphrases.We focus on increased scalability and generality with respect to prior work, eventually aiming at a full scale knowledge base.Our current implementation of the algorithm takes as its input a verb lexicon and for each verb searches the Web for related syntactic entailment templates.Experiments show promising results with respect to the ultimate goal, achieving much better scalability than prior Web-based methods.
We describe how simple, commonly understood statistical models, such as statistical dependency parsers, probabilistic context-free grammars, and word-to-word translation models, can be effectively combined into a unified bilingual parser that jointly searches for the best English parse, Korean parse, and word alignment, where these hidden structures all constrain each other.The model used for parsing is completely factored into the two parsers and the TM, allowing separate parameter estimation.We evaluate our bilingual parser on the Penn Korean Treebank and against several baseline systems and show improvements parsing Korean with very limited labeled data.
We present a method capable of extracting parallel sentences from far more disparate “very-non-parallel corpora” than previous “comparable corpora” methods, by exploiting bootstrapping on top of IBM Model 4 EM.Step 1 of our method, like previous methods, uses similarity measures to find matching documents in a corpus first, and then extracts parallel sentences as well as new word translations from these documents.But unlike previous methods, we extend this with an iterative bootstrapping framework based on the principle of which claims documents found to contain one pair of parallel sentences must contain others even if the documents are judged to be of low similarity.We re-match documents based on extracted sentence pairs, and refine the mining process iteratively until convergence.This novel principle allows us to add parallel sentences from documents, to the baseline set.Experimental results show that our proposed method is nearly 50% more effective than the baseline method without iteration.We also show that our method is effective in boosting the performance of the IBM Model 4 EM lexical learner as the latter, though stronger than Model 1 used in previous work, does not perform well on data from very-non-parallel corpus.
This paper takes a critical look at the features used in the semantic role tagging literature and show that the information in the input, generally a syntactic parse tree, has yet to be fully exploited.We propose an additional set of features and our experiments show that these features lead to fairly significant improvements in the tasks we performed.We further show that different features are needed for different subtasks.Finally, we show that by using a Maximum Entropy classifier and fewer features, we achieved results comparable with the best previously reported results obtained with SVM models.We believe this is a clear indication that developing features that capture the right kind of information is crucial to advancing the stateof-the-art in semantic analysis.
We present an unsupervised method for labelling the arguments of verbs with their semantic roles.Our bootstrapping algorithm makes initial unambiguous role assignments, and then iteratively updates the probability model on which future assignments are based.A novel aspect of our approach is the use of verb, slot, and noun class information as the basis for backing off in our probability model.We achieve 50–65% reduction in the error rate over an informed baseline, indicating the potential of our approach for a task that has heretofore relied on large amounts of manually generated training data.
We apply statistical machine translation (SMT) tools to generate novel paraphrases of input sentences in the same language.The system is trained on large volumes of sentence pairs automatically extracted from clustered news articles available on the World Wide Web.Alignment Error Rate (AER) is measured to gauge the quality of the resulting corpus.A monotone phrasal decoder generates contextual replacements.Human evaluation shows that this system outperforms baseline paraphrase generation techniques and, in a departure from previous work, offers better coverage and scalability than the current best-of-breed paraphrasing approaches.
This paper presents Japanese morphological analysis based on conditional random fields (CRFs).Previous work in CRFs assumed that observation sequence (word) boundaries were fixed.However, word boundaries are not clear in Japanese, and hence a straightforward application of CRFs is not possible.We show how CRFs can be applied to situations where word boundary ambiguity exists.CRFs offer a solution to the long-standing problems in corpus-based or statistical Japanese morphological analysis.First, flexible feature designs for hierarchical tagsets become possible.Second, influences of label and length bias are minimized.We experiment CRFs on the standard testbed corpus used for Japanese morphological analysis, and evaluate our results using the same experimental dataset as the HMMs and MEMMs previously reported in this task.Our results confirm that CRFs not only solve the long-standing problems but also improve the performance over HMMs and MEMMs.
Chinese part-of-speech (POS) tagging assigns one POS tag to each word in a Chinese sentence.However, since words are not demarcated in a Chinese sentence, Chinese POS tagging requires word segmentation as a prerequisite.We could perform Chinese POS tagging strictly after word segmentation approach), or perform both word segmentation and POS tagging in a combined, single step simultaneously (all-atonce approach).Also, we could choose to assign POS tags on a word-by-word basis, making use of word features in the surrounding context (word-based), or on a character-by-character basis with character features (character-based).This paper presents an in-depth study on such issues of processing architecture and feature representation for Chinese POS tagging, within a maximum entropy framework.We found that while the all-at-once, characterbased approach is the best, the one-at-a-time, character-based approach is a worthwhile compromise, performing only slightly worse in terms of accuracy, but taking shorter time to train and run.As part of our investigation, we also built a state-of-the-art Chinese word segmenter, which outperforms the best SIGHAN 2003 word segmenters in the closed track on 3 out of 4 test corpora.
A novel technique for maximum “a posteriori” (MAP) adaptation of maximum entropy (MaxEnt) and maximum entropy Markov models (MEMM) is presented.The technique is applied to the problem of recovering the correct capitalization of uniformly cased text: a “background” capitalizer trained on 20Mwds of Wall Street Journal (WSJ) text from 1987 is adapted to two Broadcast News (BN) test sets — one containing ABC Primetime Live text and the other NPR Morning News/CNN Morning Edition text — from 1996.The “in-domain” performance of the WSJ capitalizer is 45% better than that of the 1-gram baseline, when evaluated on a test set drawn from WSJ 1994.When evaluating on the mismatched “out-ofdomain” test data, the 1-gram baseline is outperformed by 60%; the improvement brought by the adaptation technique using a very small amount of matched BN data — 25-70kwds — is about 20-25% relative.Overall, automatic capitalization error rate 1.4%is achieved on BN data.
The focus of research in text classification has expanded from simple topic identification to more challenging tasks such as opinion/modality identification.Unfortunately, the latter goals exceed the ability of the traditional bag-of-word representation approach, and a richer, more structural representation is required.Accordingly, learning algorithms must be created that can handle the structures observed in texts.In this paper, we propose a Boosting algorithm that captures sub-structures embedded in texts.The proposal consists of i) decision stumps that use subtrees as features and ii) the Boosting algorithm which employs the subtree-based decision stumps as weak learners.We also discuss the relation between our algorithm and SVMs with tree kernel.Two experiments on opinion/modality classification confirm that subtree features are important.
Multidocument extractive summarization relies on the concept of sentence centrality to identify the most important sentences in a document.Centrality is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence.We are now considering an approach for computing sentence importance based on the concept of eigenvector centrality (prestige) that we call LexPageRank.In this model, a sentence connectivity matrix is constructed based on cosine similarity.If the cosine similarity between two sentences exceeds a particular predefined threshold, a corresponding edge is added to the connectivity matrix.We provide an evaluation of our method on DUC 2004 data.The results show that our approach outperforms centroid-based summarization and is quite successful compared to other summarization systems.
If two translation systems differ differ in performance on a test set, can we trust that this indicates a difference in true system quality?To answer this question, we describe bootstrap resampling methods to compute statistical significance of test results, and validate them on the concrete example of the Even for small test sizes of only 300 sentences, our methods may give us assurances that test result differences are real.
In this paper, we introduce TextRank – a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications.In particular, we propose two innovative unsupervised methods for keyword and sentence extraction, and show that the results obtained compare favorably with previously published results on established benchmarks.
This paper introduces an approach to sentiment analysis which uses support vector machines (SVMs) to bring together diverse sources of potentially pertinent information, including several favorability measures for phrases and adjectives and, where available, knowledge of the topic of the text.Models using the features introduced are further combined with unigram models which have been shown to be effective in the past (Pang et al., 2002) and lemmatized versions of the unigram models.Experiments on movie review data from Epinions.com demonstrate that hybrid SVMs which combine unigram-style feature-based SVMs with those based on real-valued favorability measures obtain superior performance, producing the best results yet published using this data.Further experiments using a feature set enriched with topic information on a smaller dataset of music reviews handannotated for topic are also reported, the results of which suggest that incorporating topic information into such models may also yield improvement.
We introduce a learning semantic parser,SCISSOR, that maps natural-language sentences to a detailed, formal, meaning representation language.It first usesan integrated statistical parser to pro duce a semantically augmented parse tree, in which each non-terminal node has both a syntactic and a semantic label.A compositional-semantics procedure is then used to map the augmented parse tree into a final meaning representation.We evaluate the system in two domains, a natural-language database interface and an interpreter for coaching instructions in robotic soccer.We present experimentalresults demonstrating that SCISSOR produces more accurate semantic representa tions than several previous approaches.
result with joint inference on the development set.Overall results on the development and test sets are shown in Table 1.Table 2 shows the results of individual systems and the improvement gained by the joint inference on the development set.4 Conclusions We present an implementation of SRL system which composed of four stages—1) pruning, 2) argument identification, 3) argument classification, and 4) inference.The inference provides a natural way to take the output of multiple argument classifiers and combines them into a coherent predicate-argument output.Significant improvement in overall SRL performance through this inference is illustrated.Acknowledgments We are grateful to Dash Optimization for the free academic use of Xpress-MP.This research is sup
Automatic evaluation of machine translabased on computing similarity between system output and human reference translations, has revolutionized the development of MT systems.We explore the use of syntactic information, including constituent labels and head-modifier dependencies, in computing similarity between output and reference.Our results show that adding syntactic information to the evaluation metric improves both sentence-level and corpus-level correlation with human judgments.
This paper presents a knowledge-based method for measuring the semanticsimilarity of texts.While there is a large body of previous work focused on finding the semantic similarity of concepts and words, the application of these wordoriented methods to text similarity has not been yet explored.In this paper, we introduce a method that combines wordto-word similarity metrics into a text-totext metric, and we show that this method outperforms the traditional text similarity metrics based on lexical matching.
We discuss the relevance of k-best parsing torecent applications in natural language pro cessing, and develop efficient algorithms for k-best trees in the framework of hypergraphparsing.To demonstrate the efficiency, scal ability and accuracy of these algorithms, we present experiments on Bikel?s implementation of Collins?lexicalized PCFG model, and on Chiang?s CFG-based decoder for hierarchicalphrase-based translation.We show in particu lar how the improved output of our algorithms has the potential to improve results from parse reranking systems and other applications.
We present a classifier-based parser that produces constituent trees in linear time.The parser uses a basic bottom-up shiftreduce algorithm, but employs a classifier to determine parser actions instead of a grammar.This can be seen as an extension of the deterministic dependency parser of Nivre and Scholz (2004) to full constituent parsing.We show that, with an appropriate feature set used in classification, a very simple one-path greedy parser can perform at the same level of accuracy as more complex parsers.We evaluate our parser on section 23 of the WSJ section of the Penn Treebank, and obtain precision and recall of 87.54% and 87.61%, respectively.
This paper presents a method for identifying an opinion with its holder and topic, given a sentence from online news media texts.We introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective.This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using data from FrameNet.We decompose our task into three phases: identifying an opinion-bearing word, labeling semantic roles related to the word in the sentence, and then finding the holder and the topic of the opinion word among the labeled semantic roles.For a broader coverage, we also employ a clustering technique to predict the most probable frame for a word which is not defined in FrameNet.Our experimental results show that our system performs significantly better than the baseline.
Making use of latent semantic analysis, we explore the hypothesis that local linguistic context can serve to identify multi-word expressions that have noncompositional meanings.We propose that vector-similarity between distribution vectors associated with an MWE as a whole and those associated with its constitutent parts can serve as a good measure of the degree to which the MWE is compositional.We present experiments that show that low (cosine) similarity does, in fact, correlate with non-compositionality.
We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases.The SPMT models outperform a state of the art phrase-based baseline model by 2.64 Bleu points on the NIST 2003 Chinese-English test corpus and 0.28 points on a humanbased quality metric that ranks translations on a scale from 1 to 5.
We discuss different strategies for smoothing the phrasetable in Statistical MT, and give results over a range of translation settings.We show that any type of smoothing is a better idea than the relativefrequency estimates that are often used.The best smoothing techniques yield consistent gains of approximately 1% (absolute) according to the BLEU metric.
Discriminative learning methods are widely used in natural language processing.These methods work best when their training and test data are drawn from the same distribution.For many NLP tasks, however, we are confronted with new domains in which labeled data is scarce or non-existent.In such cases, we seek to adapt existing models from a resourcerich source domain to a resource-poor domain.We introduce learning automatically induce correspondences among features from different domains.We test our technique on part of speech tagging and show performance gains for varying amounts of source and target training data, as well as improvements in target domain parsing accuracy using our improved tagger.
Integer Linear Programming has recently been used for decoding in a number of probabilistic models in order to enforce global constraints.However, in certain applications, such as non-projective dependency parsing and machine translation, the complete formulation of the decoding problem as an integer linear program renders solving intractable.We present an approach which solves the problem incrementally, thus we avoid creating intractable integer linear programs.This approach is applied to Dutch dependency parsing and we show how the addition of linguistically motivated constraints can yield a significant improvement over stateof-the-art.
We investigate whether one can determine from the transcripts of U.S. Congressional floor debates whether the speeches represent support of or opposition to proposed legislation.To address this problem, we exploit the fact that these speeches occur as part of a discussion; this allows us to use sources of information regarding relationships between discourse segments, such as whether a given utterance indicates agreement with the opinion expressed by another.We find that the incorporation of such information yields substantial improvements over classifying speeches in isolation.
This paper proposes an unsupervised lexicon building method for the detecof which convey positive or negative aspects in a specific domain.The lexical entries to be acare called the minimum human-understandable syntactic structures that specify the polarity of clauses.As a clue to obtain candidate atoms, we use the tendency for same polarities to appear successively in contexts.Using the overall density and precision of coherency in the corpus, the statistical estimation picks up appropriate polar atoms among candidates, without any manual tuning of the threshold values.The experimental results show that the precision of polarity assignment with the automatically acquired lexicon was 94% on average, and our method is robust for corpora in diverse domains and for the size of the initial lexicon.
We present an approach for the joint extraction of entities and relations in the con text of opinion recognition and analysis.We identify two types of opinion-related entities ? expressions of opinions andsources of opinions ? along with the linking relation that exists between them.In spired by Roth and Yih (2004), we employ an integer linear programming approach to solve the joint opinion recognition task,and show that global, constraint-based inference can significantly boost the perfor mance of both relation extraction and theextraction of opinion-related entities.Performance further improves when a seman tic role labeling system is incorporated.The resulting system achieves F-measuresof 79 and 69 for entity and relation extrac tion, respectively, improving substantially over prior results in the area.
In this paper we approach word sense disambiguation and information extraction as a unified tagging problem.The task consists of annotating text with the tagset defined by the 41 Wordnet supersense classes for nouns and verbs.Since the tagset is directly related to Wordnet synsets, the tagger returns partial word sense disambiguation.Furthermore, since the noun tags include the standard named entity detection classes – person, location, organization, time, etc.– the tagger, as a by-product, returns extended named entity information.We cast the problem of supersense tagging as a sequential labeling task and investigate it empirically with a discriminatively-trained Hidden Markov Model.Experimental evaluation on the main sense-annotated datasets available, i.e., Semcor and Senseval, shows considerable improvements over the best known “first-sense” baseline.
In this paper, we introduce a WordNetbased measure of semantic relatedness by combining the structure and content of WordNet with co–occurrence information derived from raw text.We use the co–occurrence information along with the definitions to build vectors corresponding to each concept in Word- Net.Numeric scores of relatedness are assigned to a pair of concepts by measuring the cosine of the angle between their respective gloss vectors.We show that this measure compares favorably to other measures with respect to human judgments of semantic relatedness, and that it performs well when used in a word sense disambiguation algorithm that relies on semantic relatedness.This measure is flexible in that it can make comparisons between any two concepts without regard to their part of speech.In addition, it can be adapted to different domains, since any plain text corpus can be used to derive the co–occurrence information.
In this paper we investigate a new problem identifying the which a document is written.By perspective we mean a point of view, for example, from the perspective of Democrats or Republicans.Can computers learn to identify the perspective of a document?Not every sentence is written strongly from a perspective.Can computers learn to identify which sentences strongly convey a particular perspective?We develop statistical models to capture how perspectives are expressed at the document and sentence levels, and evaluate the proposed models on articles about the Israeli-Palestinian conflict.The results show that the proposed models successfully learn how perspectives are reflected in word usage and can identify the perspective of a document with high accuracy.
Each year the Conference on Computational Natural Language Learning features a shared task, in which participants train and test their systems on exactly the same data sets, in order to better compare systems.The tenth CoNLL (CoNLL-X) saw a shared task on Multilingual Dependency Parsing.In this paper, we describe how treebanks for 13 languages were converted into the same dependency format and how parsing performance was measured.We also give an overview of the parsing approaches that participants took and the results that they achieved.Finally, we try to draw general conclusions about multi-lingual parsing: What makes a particular language, treebank or annotation scheme easier or harder to parse and which phenomena are challenging for any dependency parser?Acknowledgement Many thanks to Amit Dubey and Yuval Krymolowski, the other two organizers of the shared task, for discussions, converting treebanks, writing and helping with the also to Alexander Yeh for additional help with the paper reviews.His work was made possible by the MITRE Cor
We use SVM classifiers to predict the next action of a deterministic parser that builds labeled projective dependency graphs in an incremental fashion.Non-projective dependencies are captured indirectly by projectivizing the training data for the classifiers and applying an inverse transformation to the output of the parser.We present evaluation results and an error analysis focusing on Swedish and Turkish.
We present translation results on the shared task ”Exploiting Parallel Texts for Statistical Machine Translation” generated by a chart parsing decoder operating on phrase tables augmented and generalized with target language syntactic categories.We use a target language parser to generate parse trees for each sentence on the target side of the bilingual training corpus, matching them with phrase table lattices built for the corresponding source sentence.Considering phrases that correspond to syntactic categories in the parse trees we develop techniques to augment (declare a syntactically motivated category for a phrase pair) and generalize (form mixed terminal and nonterminal phrases) the phrase table into a synchronous bilingual grammar.We present results on the French-to-English task for this workshop, representing significant improvements over the workshop’s baseline system.Our translation system is available open-source under the GNU General
We present a graph-based semi-supervised learning algorithm to address the sentiment analysis task of rating inference.Given a set of documents (e.g., movie reviews) and accompanying ratings (e.g., “4 stars”), the task calls for inferring numerical ratings for unlabeled documents based on the perceived sentiment expressed by their text.In particular, we are interested in the situation where labeled data is scarce.We place this task in the semi-supervised setting and demonstrate that considering unlabeled reviews in the learning process can improve ratinginference performance.We do so by creating a graph on both labeled and unlabeled data to encode certain assumptions for this task.We then solve an optimization problem to obtain a smooth rating function over the whole graph.When only limited labeled data is available, this method achieves significantly better predictive accuracy over other methods that ignore the unlabeled examples during training.
We introduce Chinese Whispers, a randomized graph-clustering algorithm, which is time-linear in the number of edges.After a detailed definition of the algorithm and a discussion of its strengths and weaknesses, the performance of Chinese Whispers is measured on Natural Language Processing (NLP) problems as diverse as language separation, acquisition of syntactic word classes and word sense disambiguation.At this, the fact is employed that the small-world property holds for many graphs in NLP.
We present a phrasal inversion transduction grammar as an alternative to joint phrasal translation models.This syntactic model is similar to its flatstring phrasal predecessors, but admits polynomial-time algorithms for Viterbi alignment and EM training.We demonstrate that the consistency constraints that allow flat phrasal models to scale also help ITG algorithms, producing an 80-times faster inside-outside algorithm.We also show that the phrasal translation tables produced by the ITG are superior to those of the flat joint phrasal model, producing up to a 2.5 point improvement in BLEU score.Finally, we explore, for the first time, the utility of a joint phrasal translation model as a word alignment method.
Combinatorial Categorial Grammar (CCG) supertags present phrase-based machine translation with an opportunity to access rich syntactic information at a word level.The challenge is incorporating this informa tion into the translation process.Factoredtranslation models allow the inclusion of supertags as a factor in the source or target language.We show that this results in an im provement in the quality of translation and that the value of syntactic supertags in flat structured phrase-based models is largely due to better local reorderings.
We describe a mixture-model approach to adapting a Statistical Machine Translation System for new domains, using weights that depend on text distances to mixture components.We investigate a number of variants on this approach, including cross-domain versus dynamic adaptation; linear versus loglinear mixtures; language and translation model adaptation; different methods of assigning weights; and granularity of the source unit being adapted to.The best methods achieve gains of approximately one BLEU percentage point over a state-of-the art non-adapted baseline system.
j schroeder ed ac uk Abstract This paper evaluates the translation quality of machine translation systems for 8 language pairs: translating French, German, Spanish, and Czech to English and back.We carried out an extensive human evaluation which allowed us not only to rank the different MT systems, but also to perform higher-level analysis of the evaluation process.We measured timing and intraand inter-annotator agreement for three types of subjective evaluation.We measured the correlation of automatic evaluation metrics with human judgments.This meta-evaluation reveals surprising facts about the most commonly used methodologies.
2: Test set performance of our systems: and output/reference length ratio.4.3 Training and decoding parameters We tried to improve performance by increasing some of the limits imposed on the training and decoding setup.During training, long sentences are removed from the training data to speed up the GIZA++ word alignment process.Traditionally, we worked with a sentence length limit of 40.We found that increasing this limit to about 80 gave better results without causing undue problems with running the word alignment (GIZA++ increasingly fails and runs much slower with long sentences).We also tried to increase beam sizes and the limit on the number of translation options per coverage span (ttable-limit).This has shown to be successful in our experiments with Arabic–English and Chinese–English systems.Surprisingly, increasing the maximum stack size to 1000 (from 200) and ttable-limit to 100 (from 20) has barely any efon translation performance.The changed only by less than 0.05, and often worsened.4.4 German–English system The German–English language pair is especially challenging due to the large differences in word order.Collins et al. (2005) suggest a method to reorder the German input before translating using a set of manually crafted rules.In our German–English submissions, this is done both to the training data and the input to the machine translation system.5 Conclusions Our submission to the WMT 2007 shared task is a fairly straight-forward use of the Moses MT system using default parameters.In a sense, we submitted baseline performance of this system. for all our systems on the test sets are displayed in Table 2.Compared to other submitted systems, these are very good scores, often the best or second highest scores for these tasks.We made a special effort in two areas: We explored domain adaptation methods for the News- Commentary test sets and we used reordering rules for the German–English language pair.
an automatic metric for Machine Translation evaluation which has been demonstrated to have high levels of correlation with human judgments of translation quality, significantly outperforming the more used It is one of several automatic metrics used in this year’s shared task within the ACL WMT-07 workshop.This paper recaps the technical details underlying the metric and describes recent improvements in the metric.The latest release includes improved metric parameters and extends the metric to support evaluation of MT output in Spanish, French and
We would like to thank the people and organizations that made these sources available for the challenge.In addition, we thank Idan Szpektor and Roy Bar Haim from Bar-Ilan University for their assistance and advice, and Valentina Bruseghini CELCT for managing the RTE-3 We would also like to acknowledge the people and organizations involved in creating and annotating the data: Pamela Forner, Errol Hayman, Cameron Fordyce from CELCT and Courtenay Hendricks, Adam Savel and Annika Hamalainen This work was supported in part by the IST Programme of the European Community, under the Network of IST-2002- 506778.We wish to thank the managers of the PASCAL challenges program, Michele Sebag and Florence d’Alche-Buc, for their efforts and support, which made this challenge possible.We also thank David Askey, who helped manage the RTE 3 website.
This paper presents ongoing work on the detection of preposition errors of non-native speakers of English.Since prepositions account for a substantial proportion of all grammatical errors by ESL (English as a Second Language) learners, developing an NLP application that can reliably detect these types of errors will provide an invaluable learning resource to ESL students.To address this problem, we use a maximum entropy classifier combined with rule-based filters to detect preposition errors in a corpus of student essays.Although our work is preliminary, we achieve a precision of 0.8 with a recall of 0.3.
The goal of this task is to allow for comparison across sense-induction and discrim ination systems, and also to compare thesesystems to other supervised and knowledgebased systems.In total there were 6 participating systems.We reused the SemEval 2007 English lexical sample subtask of task17, and set up both clustering-style unsuper vised evaluation (using OntoNotes senses as gold-standard) and a supervised evaluation (using the part of the dataset for mapping).We provide a comparison to the results ofthe systems participating in the lexical sam ple subtask of task 17.
This paper presents the coarse-grained En glish all-words task at SemEval-2007.We describe our experience in producing acoarse version of the WordNet sense inven tory and preparing the sense-tagged corpusfor the task.We present the results of participating systems and discuss future direc tions.
In this paper we describe the English Lexical Substitution task for SemEval.In the task, annotators and systems find an alternative substitute word or phrase for a target word in context.The task involves both finding the synonyms and disambiguating the context.Participating systems are free to use any lexical resource.There is a subtask which re quires identifying cases where the word isfunctioning as part of a multiword in the sen tence and detecting what that multiword is.
This paper presents the task definition, resources, participation, and comparative re sults for the Web People Search task, which was organized as part of the SemEval-2007 evaluation exercise.This task consists of clustering a set of documents that mention an ambiguous person name according to the actual entities referred to using that name.
The TempEval task proposes a simple way to evaluate automatic extraction of temporalrelations.It avoids the pitfalls of evaluating a graph of inter-related labels by defining three sub tasks that allow pairwise eval uation of temporal relations.The task not only allows straightforward evaluation, italso avoids the complexities of full tempo ral parsing.
This paper describes our experience in preparing the data and evaluating the results for three subtasks of SemEval-2007 Task-17 ? Lexical Sample, Semantic Role Labeling(SRL) and All-Words respectively.We tab ulate and analyze the results of participating systems.
This task consists of recognizing words and phrases that evoke semantic frames as defined in the FrameNet project (http: //framenet.icsi.berkeley.edu), and their semantic dependents, which are usually, but not always, their syntacticdependents (including subjects).The train ing data was FN annotated sentences.In testing, participants automatically annotated three previously unseen texts to match goldstandard (human) annotation, including pre dicting previously unseen frames and roles.Precision and recall were measured both for matching of labels of frames and FEs and for matching of semantic dependency trees based on the annotation.
In this paper we investigate several nonprojective parsing algorithms for dependency parsing, providing novel polynomial time solutions under the assumption that each dependency decision is independent of all the others, called here the edge-factored model.We also investigate algorithms for non-projective parsing that account for nonlocal information, and present several hardness results.This suggests that it is unlikely that exact non-projective dependency parsing is tractable for any model richer than the edge-factored model.
j schroeder ed ac uk Abstract This paper analyzes the translation quality of machine translation systems for 10 language pairs translating between Czech, English, French, German, Hungarian, and Spanish.We report the translation quality of over 30 diverse translation systems based on a large-scale manual evaluation involving hundreds of hours of effort.We use the human judgments of the systems to analyze automatic evaluation metrics for translation quality, and we report the strength of the correlation with human judgments at both the system-level and at the sentence-level.We validate our manual evaluation methodology by measuring intraand inter-annotator agreement, and collecting timing information.
Previous work has shown that Chinese word segmentation is useful for machine translation to English, yet the way different segmentation strategies affect MT is still poorly understood.In this paper, we demonstrate that optimizing segmentation for an existing segmentation standard does not always yield better MT performance.We find that other factors such as segmentation consistency and granularity of Chinese “words” can be more important for machine translation.Based on these findings, we implement methods inside a conditional random field segmenter that directly optimize segmentation granularity with respect to the MT task, providing an improvement of 0.73 BLEU.We also show that improving segmentation consistency using external lexicon and proper noun features yields a 0.32 BLEU increase.
Training word alignment models on large corpora is a very time-consuming processes.This paper describes two parallel implementations of GIZA++ that accelerate this word alignment process.One of the implementations runs on computer clusters, the other runs on multi-processor system using multi-threading technology.Results show a near-linear speedup according to the number of CPUs used, and alignment quality is preserved.
This paper examines the Stanford typed dependencies representation, which was designed to provide a straightforward description of grammatical relations for any user who could benefit from automatic text understanding.For such purposes, we argue that dependency schemes must follow a simple design and provide semantically contentful information, as well as offer an automatic procedure to extract the relations.We consider the underlying design principles of the Stanford scheme from this perspective, and compare it to the GR and PARC representations.Finally, we address the question of the suitability of the Stanford scheme for parser evaluation.
We describe a parsing approach that makes use of the perceptron algorithm, in conjunction with dynamic programming methods, to recover full constituent-based parse trees.The formalism allows a rich set of parse-tree features, including PCFGbased features, bigram and trigram dependency features, and surface features.A severe challenge in applying such an approach to full syntactic parsing is the efficiency of the parsing algorithms involved.We show that efficient training is feasible, using a Tree Adjoining Grammar (TAG) based parsing formalism.A lower-order dependency parsing model is used to restrict the search space of the full model, thereby making it efficient.Experiments on the Penn WSJ treebank show that the model achieves state-of-the-art performance, for both constituent and dependency accuracy.
The Conference on Computational Natural Language Learning is accompanied every year by a shared task whose purpose is to promote natural language processing applications and evaluate them in a standard setting.In 2008 the shared task was dedicated to the joint parsing of syntactic and semantic dependencies.This shared task not only unifies the shared tasks of the previous four years under a unique dependency-based formalism, but also extends them significantly: this year’s syntactic dependencies include more information such as named-entity boundaries; the semantic dependencies model roles of both verbal and nominal predicates.In this paper, we define the shared task and describe how the data sets were created.Furthermore, we report and analyze the results and describe the approaches of the participating systems.
j schroeder ed ac uk Abstract This paper presents the results of the WMT09 shared tasks, which included a translation task, a system combination task, and an evaluation task.We conducted a large-scale manual evaluation of 87 machine translation systems and 22 system combination entries.We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality, for more than 20 metrics.We present a new evaluation technique whereby system output is edited and judged for correctness.
describe an open source toolkit for statistical machine translation.Joshua implements all of the algorithms required for synchronous context grammars (SCFGs): chart-parsing, gram language model integration, beamcube-pruning, and extraction.The toolkit also implements suffix-array grammar extraction and minimum error rate training.It uses parallel and distributed computing techniques for scalability.We demonstrate that the toolkit achieves state of the art translation performance on the WMT09 French-English translation task.
Domain adaptation has recently gained interest in statistical machine translation to cope with the performance drop observed when testing conditions deviate from training conditions.The basic idea is that in-domain training data can be exploited to adapt all components of an already developed system.Previous work showed small performance gains by adapting from limited in-domain bilingual data.Here, we aim instead at significant performance gains by exploiting large but cheap monolingual in-domain data, either in the source or in the target language.We propose to synthesize a bilingual corpus by translating the monolingual adaptation data into the counterpart language.Investigations were conducted on a stateof-the-art phrase-based system trained on the Spanish–English part of the UN corpus, and adapted on the corresponding Europarl data.Translation, re-ordering, and language models were estimated after translating in-domain texts with the baseline.By optimizing the interpolation of these models on a development set the BLEU score was improved from 22.60% to 28.10% on a test set.
Automatic Machine Translation (MT) evaluation metrics have traditionally been evaluated by the correlation of the scores they assign to MT output with human judgments of translation performance.Different types of human judgments, such as Fluency, Adequacy, and HTER, measure varying aspects of MT performance that can be captured by automatic MT metrics.We explore these differences through the use of a new tunable MT metric: TER-Plus, which extends the Translation Edit Rate evaluation metric with tunable parameters and the incorporation of morphology, synonymy and paraphrases.TER-Plus was shown to be one of the top metrics in NIST’s Metrics MATR 2008 Challenge, having the highest average rank in terms of Pearson and Spearman correlation.Optimizing TER-Plus to different types of human judgments yields significantly improved correlations and meaningful changes in the weight of different types of edits, demonstrating significant differences between the types of human judgments.
Finding negation signals and their scope in text is an important subtask in information extraction.In this paper we present a machine learning system that finds the scope of negation in biomedical texts.The system combines several classifiers and works in two phases.To investigate the robustness of the approach, the system is tested on the three subcorpora of the BioScope corpus representing different text types.It achieves the best results to date for this task, with an error reduction of 32.07% compared to current state of the art results.
We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system.In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system.In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an system that achieves 90.8 on the CoNLL-2003 NER shared task, the best reported result for this dataset.
Identifying hedged information in biomedical literature is an important subtask in information extraction because it would be misleading to extract speculative information as factual information.In this paper we present a machine learning system that finds the scope of hedge cues in biomedical texts.The system is based on a similar system that finds the scope of negation cues.We show that the same scope finding approach can be applied to both negation and hedging.To investigate the robustness of the approach, the system is tested on the three subcorpora of the BioScope corpus that represent different text types.
The paper presents the design and implementation of the BioNLP’09 Shared Task, and reports the final results with analysis.The shared task consists of three sub-tasks, each of which addresses bio-molecular event extraction at a different level of specificity.The data was developed based on the GENIA event corpus.The shared task was run over 12 weeks, drawing initial interest from 42 teams.Of these teams, 24 submitted final results.The evaluation results are encouraging, indicating that state-of-the-art performance is approaching a practically applicable level and revealing some remaining challenges.
Even though considerable attention has been given to semantic orientation of words and the creation of large polarity lexicons, research in emotion analysis has had to rely on limited and small emotion lexicons.In this paper, we show how we create a high-quality, moderate-sized emotion lexicon using Mechanical Turk.In addition to questions about emotions evoked by terms, we show how the inclusion of a word choice question can discourage malicious data entry, help identify instances where the annotator may not be familiar with the target term (allowing us to reject such annotations), and help obtain annotations at sense level (rather than at word level).We perform an extensive analysis of the annotations to better understand the distribution of emotions evoked by terms of different parts of speech.We identify which emotions tend to be evoked simultaneously by the same term and show that certain emotions indeed go hand in hand.
In this paper we give an introduction to using Amazon?s Mechanical Turk crowdsourc ing platform for the purpose of collecting data for human language technologies.Wesurvey the papers published in the NAACL 2010 Workshop.24 researchers participated in the workshop?s shared task to create data for speech and language applications with $100.
This paper presents the results of the WMT10 and MetricsMATR10 shared which included a translation task, a system combination task, and an evaluation task.We conducted a large-scale manual evaluation of 104 machine translation systems and 41 system combination entries.We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 26 metrics.This year we also investigated increasing the number of human judgments by hiring non-expert annotators through Amazon’s
In this paper we explore the computational modelling of compositionality in distributional models of semantics.In particular, we model the semantic composition of pairs of adjacent English Adjecand Nouns from the National We build a vector-based semantic space from a lemmatised version of the BNC, where the most frequent A-N lemma pairs are treated as single tokens.We then extrapolate three different models of compositionality: a simple additive model, a pointwise-multiplicative model and a Partial Least Squares Regression (PLSR) model.We propose two evaluation methods for the implemented models.Our study leads to the conclusion that regression-based models of compositionality generally out-perform additive and multiplicative approaches, and also show a number of advantages that make them very promising for future research.
Current approaches to semantic parsing, the task of converting text to a formal meaning representation, rely on annotated training data mapping sentences to logical forms.Providing this supervision is a major bottleneck in scaling semantic parsers.This paper presents a new learning paradigm aimed at alleviating the supervision burden.We develop two novel learning algorithms capable of predicting complex structures which only rely on a binary feedback signal based on the context of an external world.In addition we reformulate the semantic parsing problem to reduce the dependency of the model on syntactic patterns, thus allowing our parser to scale better using less supervision.Our results surprisingly show that without using any annotated meaning representations learning with a weak feedback signal is capable of producing a parser that is competitive with fully supervised parsers.
The CoNLL-2010 Shared Task was dedicated to the detection of uncertainty cues and their linguistic scope in natural language texts.The motivation behind this task was that distinguishing factual and uncertain information in texts is of essential importance in information extraction.This paper provides a general overview of the shared task, including the annotation protocols of the training and evaluation datasets, the exact task definitions, the evaluation metrics employed and the overall results.The paper concludes with an analysis of the prominent approaches and an overview of the systems submitted to the shared task.
We examine sentiment analysis on Twitter data.The contributions of this paper are: (1) We introduce POS-specific prior polarity features.(2) We explore the use of a tree kernel to obviate the need for tedious feature engineering.The new features (in conjunction with previously proposed features) and the tree kernel perform approximately at the same level, both outperforming the state-of-the-art baseline.
The BioNLP Shared Task 2011, an information extraction task held over 6 months up to March 2011, met with community-wide participation, receiving 46 final submissions from 24 teams.Five main tasks and three supporting tasks were arranged, and their results show advances in the state of the art in fine-grained biomedical domain information extraction and demonstrate that extraction methods successfully generalize in various aspects.
The Genia event task, a bio-molecular event extraction task, is arranged as one of the main tasks of BioNLP Shared Task 2011.As its second time to be arranged for community-wide focused efforts, it aimed to measure the advance of the community since 2009, and to evaluate generalization of the technology to full text papers.After a 3-month system development period, 15 teams submitted their performance results on test cases.The results show the community has made a significant advancement in terms of both performance improvement and generalization.
The CoNLL-2011 shared task involved predicting coreference using OntoNotes data.Resources in this field have tended to be limited to noun phrase coreference, often on a set of entities, such as entities.OntoNotes provides a large-scale corpus of general anaphoric coreference not restricted to noun phrases or to a specified set of entity types.OntoNotes also provides additional layers of integrated annotation, capturing additional shallow semantic structure.This paper briefly describes the OntoNotes annotation (coreference and other layers) and then describes the parameters of the shared task including the format, pre-processing information, and evaluation criteria, and presents and discusses the results achieved by the participating systems.Having a standard test set and evaluation parameters, all based on a new resource that provides multiple integrated annotation layers (parses, semantic roles, word senses, named entities and coreference) that could support joint models, should help to energize ongoing research in the task of entity and event coreference.
This paper details the coreference resolution system submitted by Stanford at the CoNLL- 2011 shared task.Our system is a collection of deterministic coreference resolution models that incorporate lexical, syntactic, semantic, and discourse information.All these models use global document-level information by sharing mention attributes, such as gender and number, across mentions in the same cluster.We participated in both the open and closed tracks and submitted results using both predicted and gold mentions.Our system was ranked first in both tracks, with a score of 57.8 in the closed track and 58.3 in the open track.
This paper presents the results of the WMT11 shared tasks, which included a translation task, a system combination task, and a task for machine translation evaluation metrics.We conducted a large-scale manual evaluation of 148 machine translation systems and 41 system combination entries.We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 21 evaluation metrics.This year featured a Haitian Creole to English task translating SMS messages sent to an emergency response service in the aftermath of the Haitian earthquake.We also conducted a pilot ‘tunable metrics’ task to test whether optimizing a fixed system to different metrics would result in perceptibly different translation quality.
This paper describes Meteor 1.3, our submission to the 2011 EMNLP Workshop on Statistical Machine Translation automatic evaluation metric tasks.New metric features include improved text normalization, higher-precision paraphrase matching, and discrimination between content and function words.We include Ranking and Adequacy versions of the metric shown to have high correlation with human judgments of translation quality as well as a more balanced Tuning version shown to outperform BLEU in minimum error rate training for a phrase-based Urdu-English system.
This paper presents the results of the WMT12 shared tasks, which included a translation task, a task for machine translation evaluation metrics, and a task for run-time estimation of machine translation quality.We conducted a large-scale manual evaluation of 103 machine translation systems submitted by 34 teams.We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 12 evaluation metrics.We introduced a new quality estimation task this year, and evaluated submissions from 11 teams.
We have developed a new program called aligning parallel text, text such as the Canadian Hansards that are available in two or more languages.The program takes the of 1993), a robust alternative to sentence-based alignment programs, and applies word-level constraints usa version of Brown Model 2 (Brown et al., 1993), modified and extended to deal robustness issues. tested on a subset of Canadian Hansards supplied by Simard (Simard et al., 1992).The combination of word_align plus char_align reduces the variance (average square error) by a factor of over More importantly, because word_align and char_align were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at AT&T Language Line Services, a commercial translation service, to help them with difficult terminology.
I survey some recent applications-oriented NL generation systems, and claim that despite very different theoretical backgrounds, these systems have a remarkably similar architecture in terms of the modules they divide the generation process into, the computations these modules perform, and the way the modules interact with each other.I also compare this 'consensus architecture' among applied NLG systems with psycholinguistic knowledge about how humans speak, and argue that at least some aspects of the consensus architecture seem to be in agreement with what is known about human language production, despite the fact that psycholinguistic plausibility was not in general a goal of the developers of the surveyed systems.
In this paper we describe an unsupervised learning algorithm for automatically training a rule-based part of speech tagger without using a manually tagged corpus.We compare this algorithm to the Baum-Welch algorithm, used for unsupervised training of stochastic taggers.Next, we show a method for combining unsupervised and supervised rule-based training algorithms to create a highly accurate tagger using only a small amount of manually tagged text.
Recent work has considered corpus-based or statistical approaches to the problem of prepositional attachment ambiguity.Typically, ambiguous verb phrases of the form v p np2 through a model which considers values of the four head words (v, nl, paper shows that the problem is analogous to n-gram language models in speech recognition, and that one of the most common methods for language modeling, the backed-off estimate, is applicable.Results on Wall Street Journal data of 84.5% accuracy are obtained using this method.
Word groupings useful for language processing tasks are increasingly available, as thesauri appear on-line, and as distributional word clustering techniques improve.However, for many tasks, one is in relationships among word words.This paper presents a method for automatic sense disambiguation of nouns appearing within sets of related nouns — the kind of data one finds in on-line thesauri, or as the output of distributional clustering algorithms.Disambiguation is performed with respect to WordNet senses, which are fairly fine-grained; however, the method also permits the assignment of higher-level WordNet categories rather than sense labels.The method is illustrated primarily by example, though results of a more rigorous evaluation are also presented.
Eric Brill introduced transformation-based learning and showed that it can do part-ofspeech tagging with fairly high accuracy.The same method can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive &quot;baseNP&quot; chunks.For this purpose, it is convenient to view chunking as a tagging problem by encoding the chunk structure in new tags attached to each word.In automatic tests using Treebank-derived data, this technique achieved recall and precision rates of roughly 92% for baseNP chunks and 88% for somewhat more complex chunks that partition the sentence.Some interesting adaptations to the transformation-based learning approach are also suggested by this application.
This paper shows how to induce an N-best translation lexicon from a bilingual text corpus using statistical properties of the corpus together with four external knowledge sources.The knowledge sources are cast as filters, so that any subset of them can be cascaded in a uniform framework.A new objective evaluation measure is used to compare the quality of lexicons induced with different filter cascades.The best filter cascades improve lexicon quality by up to 137% over the plain vanilla statistical method, and approach human performance.Drastically reducing the size of the training corpus has a much smaller impact on lexicon quality when these knowledge sources are used.This makes it practical to train on small hand-built corpora for language pairs where large bilingual corpora are unavailable.Moreover, three of the four filters prove useful even when used with large training corpora.
We introduce a memory-based approach to part of speech tagging.Memory-based learning is a form of supervised learning based on similarity-based reasoning.The part of speech tag of a word in a particular context is extrapolated from the most similar cases held in memory.Supervised learning approaches are useful when a tagged corpus is available as an example of the desired output of the tagger.Based on such a corpus, the tagger-generator automatically builds a tagger which is able to tag new text the same way, diminishing development time for the construction of a tagger considerably.Memory-based tagging shares this advantage with other statistical or machine learning approaches.Additional advantages specific to a memory-based approach include (i) the relatively small tagged corpus size sufficient for training, (ii) incremental learning, (iii) explanation capabilities, (iv) flexible integration of information in case representations, (v) its non-parametric nature, (vi) reasonably good results on unknown words without morphological analysis, and (vii) fast learning and tagging.In this paper we show that a large-scale application of the memory-based approach is feasible: we obtain a tagging accuracy that is on a par with that of known statistical approaches, and with attractive and time complexity properties when using tree-based formalism for indexing and searching huge case bases.The use of IGTree has as additional advantage that optimal context size for disambiguation is dynamically computed.
This paper describes an experimental comparison of seven different learning algorithms on the problem of learning to disambiguate the meaning of a word from context.The algorithms tested include statistical, neural-network, decision-tree, rule-based, and case-based classification techniques.The specific problem tested involves disambiguating six senses of the word &quot;line&quot; using the words in the current and proceeding sentence as context.The statistical and neural-network methods perform the best on this particular problem and we discuss a potential reason for this obdifference.We also discuss the role of in machine learning and its importance in explaining performance differences observed on specific problems.
This paper presents a statistical model which trains from a corpus annotated with Part-Of- Speech tags and assigns them to previously unseen text with state-of-the-art accuracy(96.6%).The can be classified as a Entropy model and simultaneously uses many contextual &quot;features&quot; to predict the POS tag.Furthermore, this paper demonstrates the use of specialized features to model difficult tagging decisions, discusses the corpus consistency problems discovered during the implementation of these features, and proposes a training strategy that mitigates these problems.
Excellent results have been reported for Data- Oriented Parsing (DOP) of natural language texts (Bod, 1993c).Unfortunately, existing algorithms are both computationally intensive and difficult to implement.Previous algorithms are expensive due to two factors: the exponential number of rules that must be generated and the use of a Monte Carlo parsing algorithm.In this paper we solve the first problem by a novel reduction of the DOP model to,a small, equivalent probabilistic context-free grammar.We solve the second problem by a novel deterministic parsing strategy that maximizes the expected number of correct constituents, rather than the probability of a correct parse tree.Using the optimizations, experiments yield a 97% crossing brackets rate and 88% zero crossing brackets rate.This differs significantly from the results reported by Bod, and is comparable to results from a duplication of Pereira and Schabes's (1992) experiment on the same data.We show that Bod's results are at least partially due to an extremely fortuitous choice of test data, and partially due to using cleaner data than other researchers.
This paper deals with two important ambiguities of natural language: prepositional phrase attachment and word sense ambiguity.We propose a new supervised learning method for PPattachment based on a semantically tagged corpus.Because any sufficiently big sense-tagged corpus does not exist, we also propose a new unsupervised context based word sense disambiguation algorithm which amends the training corpus for the PP attachment by word sense tags.We present the results of our approach and evaluate the achieved PP attachment accuracy in comparison with other methods.
We present a statistical word feature, the Word Relation Matrix, which can be used to find translated pairs of words and terms from non-parallel corpora, across language groups.Online dictionary entries are used as seed words to generate Word Relation Matrices for the unknown words according to correlation measures.Word Relation Matrices are then mapped across the corpora to find translation pairs.Translation accuracies are around 30% when only the top candidate is counted.Nevertheless, top 20 candidate output give a 50.9% average increase in accuracy on human translator performance.
absence of is a real problem for corpus-based approaches to sense disambiguation, one that is unlikely to be solved soon.Selectional preference is traditionally connected with sense ambiguity; this paper explores how a statistical model of selectional preference, requiring neither manual annotation of selection restrictions nor supervised training, can be used in sense disambiguation.
3.A search heuristic which attempts to find the highest scoring parse tree for a given input sentence.Abstract This paper presents a statistical parser for natural language that obtains a parsing accuracy—roughly 87% precision and 86% recall—which surpasses the best previously published results on the Wall St. Journal domain.The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework. running time of the parser on test sentence linear with respect to the sentence length.Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring a dramatically higher accuracy of 93% precision and recall.
We present a variation on classic beam thresholding techniques that is up to an order of magnitude faster than the traditional method, at the same performance level.We also present a new thresholding technique, global thresholding, which, combined with the new beam thresholding, gives an additional factor of two improvement, and a novel technique, multiple pass parsing, that can be combined with the others to yield yet another 50% improvement.We use a new search algorithm to simultaneously optimize the thresholding parameters of the various algorithms.
Automatic segmentation of text into minimal content-bearing units is an unsolved problem even for languages like English.Spaces between words offer an easy first approximation, but this approximation is not good enough for machine translation (MT), where many word sequences are not translated word-for-word.This paper presents an efficient automatic method for discovering sequences of words that are translated as a unit.The method proceeds by comparing pairs of statistical translation models induced from parallel texts in two languages.It can discover hundreds of noncompositional compounds on each iteration, and constructs longer compounds out of shorter ones.Objective evaluation on a simple machine translation task has shown the method's potential to improve the quality of MT output.The method makes few assumptions about the data, so it can be applied to parallel data other than parallel texts, such as word spellings and pronunciations.
Semantic knowledge can be a great asset to natural language processing systems, but it is usually hand-coded for each application.Although some semantic information is available in general-purpose knowledge bases such as WordNet and Cyc, many applications require domain-specific lexicons that represent words and categories for a particular topic.In this paper, we present a corpus-based method that can be used to build semantic lexicons for specific categories.The input to the system is a small set of seed words for a category and a representative text corpus.The output is a ranked list of words that are associated with the category.A user then reviews the top-ranked words and decides which ones should be entered in the semantic lexicon.In experiments with five categories, users typically found about 60 words per category in 10-15 minutes to build a core semantic lexicon.
This paper describes an experimental comparison of three unsupervised learning algorithms that distinguish the sense of an ambiguous word in untagged text.The methods described in this paper, McQuitty's similarity analysis, Ward's minimum—variance method, and the EM algorithm, assign each instance of an ambiguous word to a known sense definition based solely on the values of automatically identifiable features in text.These methods and feature sets are found to be more successful in disambiguating nouns rather than adjectives or verbs.Overall, the most accurate of these procedures is McQuitty's similarity analysis in combination with a high dimensional feature set.
We describe experiments that show that the concepts of rhetorical analysis and nucleanty can be used effectively for deternulling the most important units in a text We show how these concepts can be implemented and we discuss results that we obtained with a discourse-based summanzation program 1 Motivation The evaluation of automatic summanzers has always been a thorny problem most papers on summarization describe the approach that they use and give some &quot;consamples of the output In very few cases, output of a summarization program with a human-made summary or evaluated with the help of human subjects, usually, the results are modest Unfortunately, evaluating the results of a particular implementation does not enable one to determine what part of the failure is due to the implementation itself and what part to its underlying assumptions The position that we take in this paper is that, in order to build high-quality summarization programs, one needs to evaluate not only a representative set of automatically generated outputs (a highly difficult problem by itself), but also the adequacy of the assumptions that these programs use That way, one is able to distinguish the problems that pertain to a particular implementation from those that pertain to the underlying theoretical framework and explore new ways to improve each With few exceptions, automatic approaches to summarization have primarily addressed possible ways to determine the most important parts of a text (see Paice (1990) for an excellent overview) Determining the salient parts is considered to be achievable because one or more of the following assumptions hold (i) important sentences in a text contain words that are used frequently (Lahn, 1958, Edmundson, 1968), (n) important sentences contain words that are used in the tide and section headings (Edmundson, 1968), (in) important sentences are located at the beginning or end of paragraphs (Baxendale, 1958), (Iv) important sentences are located at posilions in a text that are genre dependent these positions can be determined automatically, through training techniques (Lin and Hovy, 1997), (v) important sentences use words as &quot;greatest&quot; and &quot;significant&quot; or indiphrases as &quot;the main aim of this paper&quot; and &quot;the purpose of this article&quot;, while non-important senuse words as &quot;impossible&quot; (Edmundson, 1968, Rush, Salvador, and Zamora, (vi) important sentences and concepts highest connected entities in elaborate semantic structures (Skorochodko, 1971, Lin, 1995, Barzilay and Elhadad, 1997), and (vn) imponant and non-important sentences are derivable from a discourse representation of the text (Sparck Jones, 1993, Ono, Surmta, and Mike, 1994) In determinmg the words that occur most frequently in a text or the sentences that use words that occur in the headings of sections, computers are accurate tools Flowever, in determining the concepts that are semantically related or the discourse structure of a text, computers are no longer so accurate, rather, they are highly dependent on the coverage of the linguistic resources that they use and the quality of the algondims that they implement Although it is plausible that elaborate cohesionand coherence-based structures can be used effectively in summarization, we believe that before building summarization programs, we should determine the extent to which these assumptions hold In this paper, we describe experiments that show that concepts of rhetorical analysis and nucleanty used effectively for determining the most important units in a text We show how these concepts were implemented and discuss results that we obtained with a discoursebased summarization program 2 From discourse trees to summaries — an empirical view
We present the lexical-semantic net for German &quot;GermaNet&quot; which integrates conceptual ontological information with lexical semantics, within and across word classes.It is compatible with the Princeton WordNet but integrates principlebased modifications on the constructional and organizational level as well as on the level of lexical and conceptual relations.GermaNet includes a new treatment of regular polysemy, artificial concepts and of particle verbs.It furthermore encodes cross-classification and basic syntactic information, constituting an interesting tool in exploring the interacof syntax and development of such a large scale resource is particularly important as German up to now lacks basic online tools for the semantic exploration of very large corpora.
This paper presents a high precision pronoun resolution system that is capable of greater than 90% precision with 60% and better recall for some pronouns.It is suggested that the system is resolving a sub-set of anaphors that do not require general world knowledge or sophisticated linguistic processing for successful resolution.The system does this by being very sensitive to ambiguity, and only resolving pronouns when very high confidence rules have been satisfied.The system is capable of 'noticing' ambiguity because it requires that there be a unique antecedent within a salience ranking, and the salience rankings are not total orders, i.e. two or more antecedents can be equally salient.Given the nature of the systems rules, it is very likely that they are largely domain independent and that they reflect processing strategies used by humans for general language comprehension.The system has been evaluated in two distinct experiments which support the overall validity of the approach.
tem: Experiments in Automatic Document Pro- M. Sanderson.1994.Word sense disambiguation information retrieval.In of 17th International Conference on Research and Development in Information Retrieval.A.F.Smeaton and A. Quigley.1996.Experiments on using semantic distances between words in imcaption retrieval.Proceedings of the International Conference on Research and Development in IR.A. Smeaton, F. Kelledy, and R. O'Donnell.1995.TREC-4 experiments at dublin city university: Thresolding posting lists, query expansion with and POS tagging of spanish.In Proceedings of TREC-4.M. Voorhees.1994.Query relations.In of the 17th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval.
TIME 52 process eliminates the need for a human to assign roles to the extraction patterns by hand, as had been necessary when using AutoSlog or AutoSlog-TS by themselves.For example, the pattern &quot;machinegunned <direct-obj>&quot; had strong semantic preferences for CIVILIAN, LOCATION, so was expanded to have three conceptual roles with four selectional restrictions.The expanded extraction pattern for &quot;machinegunned <direct-obj>&quot; is: &quot;machinegunned <direct-obj>&quot; -+ VEHICLE Only semantic categories that were associated with a pattern are included as selectional restric- For example, the also represents possible terrorism victims, but it was not strongly associated with the pattern.Our rationale is that an individual pattern may have a strong preference for only a subset of the categories that can be associated with a role.For example, the pattern &quot;<subject> was ambushed&quot; showed a preference for but not which makes sense because it is hard to imagine ama building.Including only as selectional restriction for targets might help eliminate incorrect building extractions.One could argue that this pattern is not likely to find building extractions anyway so the selectional restriction will not matter, but the selectional restriction might help filter out incorrect extractions due to misparses or (e.g., &quot;The White House by reporters.&quot;).Ultimately, it is an empirical question whether it is better to include all of the semantic categories associated with a conceptual role or not.Finally, we merge the expanded extraction patterns into multi-slot case frames.All extraction patterns that share the same trigger word and compatible syntactic constraints are merged into a single structure.For example, we would merge all patterns triggered by a specific verb in its passive voice.For example, the patterns &quot;<subject> was kidnapped&quot;, &quot;was kidnapped by <noun-phrase>&quot;, and &quot;was kidnapped in <noun-phrase>&quot; would be merged into a single case frame.Similarly, we would merge all patterns triggered by a specific verb in its active voice.For example, we would merge patterns for the active form of &quot;destroyed&quot; that extract the subject of &quot;destroyed&quot;, its direct object, and any prepositional phrases that are associated with it.We also merge syntactically compatible patterns that are triggered by the same noun (e.g., &quot;assassination&quot;) or by the same infinitive verb structure (e.g., &quot;to kill&quot;).When merge extraction patterns into a case frame, of the slots are simply unioned together.4 Examples In this section, we show several examples of case frames that were generated automatically by our system.Figure 5 shows a simple case frame triggered by active forms of the verb &quot;ambushed&quot;.The subject extracted as a has a selectional of direct object is exas a has a selectional restriction that the case frame does not contain even though it is theoretically possible to ambush people.During training, the &quot;ambushed <direct-obj>&quot; pattern extracted 13 people, 11 of were recognized as Since our domain roles only list civilians and government as legitimate terrorism a victim slot was not created.This example shows how the case frames are tailored for the domain empirically.Caseframe: (active_verb ambushed) VEHICLE Figure 5: Case frame for active forms of &quot;ambushed&quot; Figure 6 shows a case frame triggered by active of &quot;blew_up&quot; This case frame extracts information from an entire sentence into a single struc- The subject object (tara prepositional phrase location) all be extracted together.Caseframe: (active_verb blew_up) subject VEHICLE Figure 6: Case frame for active forms of &quot;blew_up&quot; The case frame in Figure 7 illustrates how a semantic category can show up in multiple places.This case frame will handle phrases like &quot;the guerrillas detonated a bomb&quot;, as well as &quot;the bomb detonated&quot;.Both constructions are very common in the training corpus so the system added slots for both possibilities.It would be easy for a human to overlook some of these variations when creating case frames by hand.The case frame in Figure 8 is activated by the noun &quot;attack&quot; and includes slots for a variety of prepositional phrases.The same preposition can recognize different types of information (e.g., &quot;on&quot; can victims, locations, the same role can be filled by different prepositions military victims were classified as military incidents, not terrorism, according to the MUC-4 guidelines. represent lexicalized expressions in our phrasal lexicon.53 Caseframe: (active_verb detonated) subject instrument subject WEAPON Figure 7: Case frame for active forms of &quot;detonated&quot; be extracted from &quot;on&quot;, &quot;against&quot;, or &quot;at&quot;).This example again shows the power of corpus-based methods to identify common constructions empirically.Anticipating all of these prepositional arguments would be difficult for a person.Caseframe: (noun attack) VEHICLE CIVILIAN GOVOFFICIAL BUILDING CIVILIAN locationpp(at) Figure 8: Case frame for noun forms of &quot;attack&quot; A disadvantage of this automated method is that inappropriate slots sometimes end up in the case frames.For example, Figure 9 shows a case frame that is activated by passive forms of the verb &quot;killed&quot;.Some of the slots are correct: the subis assigned to the and objects of the preposition &quot;by&quot; are assigned to the perpetrator and However, the remaining slots do sense.The is the result of polysemy; many person names are also location names, as &quot;Flores&quot;.The was produced by inparses of date expressions.The and (by)) slots were caused by incorrect role assignments.The list of domain roles assumes that terrorists are always perpetrators and civilians are always victims, but of course this is not true.Terrorists can be killed and civilians can be killers. killed) subject pp(by) pp(by) Figure 9: Case frame for passive forms of &quot;killed&quot; The previous example illustrates some of the problems that can occur when generating case frames automatically.Currently, we are assuming that each semantic category will be uniquely associated with a conceptual role, which may be an unrealistic assumption for some domains.One avenue for future work is to develop more sophisticated methods for mapping semantic preferences to conceptual roles.One could also have a human review the case frames and manually remove inappropriate slots.For now, we chose to avoid additional human interaction and used the case frames exactly as they were generated.The purpose of the selectional restrictions is to constrain the types of information that can be instantiated by each slot.Consequently, we hoped that the case frames would be more reliably instantiated than the extraction patterns, thereby producing fewer false hits.To evaluate the case frames, we used the same corpus and evaluation metrics as previous experiments with AutoSlog and AutoSlog- TS (Riloff, 1996b) so that we can draw comparisons between them.For training, we used the 1500 MUC- 4 development texts to generate the extraction patterns and the semantic lexicon.AutoSlog-TS generated 44,013 extraction patterns in its first pass.After discarding the patterns that occurred only once, the remaining 11,517 patterns were applied to the corpus for the second pass and ranked for manual We reviewed the top 2168 and kept 306 extraction patterns for the final dictionary.We built a semantic lexicon for nine categories aswith terrorism: CIVILIAN, GOV- OFFICIAL, MILITARYPEOPLE, LOCATION, TERROR- DATE, VEHICLE, WEAPON. reviewed the top 500 words for each category.It takes about 30 minutes to review a category assuming that the reviewer is familiar with the domain.Our final semantic dictionary contained 494 words.In total, the review process required approximately 6 person-hours: 1.5 hours to review the extraction patterns plus 4.5 hours to review the words for 9 semantic categories.From the extraction patterns and semantic lexicon, our system generated 137 conceptual case frames. important question is how to deal with unknown words during extraction.This is especially important in the terrorism domain because many of extracted items are proper names, which cannot be expected to be in the semantic lexicon.We allowed unknown words to fill all eligible slots and then used a precedence scheme so that each item was instantiated by only one slot.Precedence was based on the order of the roles shown in Figure 4.This is not a very satisfying solution and one of the weaknesses of our current approach.Handling unknown words more intelligently is an important direction for future research.We compared AutoSlog-TS' extraction patterns decided to review the top but continued down the list until there were no more ties.54 Slot cor mis mlb dup spu R P Perp 25 31 10 18 84 .45 .31 Victim 44 23 16 24 62 .66 .47 Target 31 22 17 23 66 .58 .39 Instr 16 15 7 17 23 .52 .52 Total 116 91 50 82 235 .56 .41 Table 1: AutoSlog-TS results the case frames using 100 blind from the MUC-4 test set.The MUC-4 answer keys were used to score the output.Each extracted item was scored either mislabeled, duplicate, spurious. item was it matched against the answer An item was it matched against the answer keys but was extracted as the wrong type of object (e.g., if a victim was extracted as a perpe- An item it was coreferent with an item in the answer keys.Correct items extracted more than once were scored as duplicates, as well as correct but underspecified extractions such as instead of &quot;John F. An item it did not appear in the answer keys.All items extracted from irrelevant texts were spurious.Finally, items in the answer keys that were not were counted as Correct + missthe total number of items in the answer 1 shows the for AutoSlog-TS' extraction patterns, and Table 2 shows the results for case frames.We computed (R) cor- I (correct + missing), (P) (correct duplicate) I (correct + duplicate + misla- + spurious). extraction patterns and case frames achieved similar recall results, although the case frames missed seven correct extractions.However the case frames produced substantially fewer false hits, producing 82 fewer spurious extractions.Note that perpetrators exhibited by far the lowest precision.The reason is that the perpetrator slot received highest precedence among competing slots for unknown words.Changing the precedence relevant texts and 25 irrelevant texts from each of the TST3 and TST4 test sets. rationale for scoring coreferent phrases as duplicates instead of spurious is that the extraction pattern or case frame was instantiated with a reference to the correct answer.In other words, the pattern (or case frame) did the right thing.Resolving coreferent phrases to produce the best answer is a problem for subsequent discourse analysis, which is not addressed by the work presented here. caveat is that the MUC-4 answer keys contain some &quot;optional&quot; answers.We scored these as correct if they were extracted but they were never scored as missing, which is how the &quot;optional&quot; items were scored in MUC-4.Note that the number of possible extractions can vary depending on the output of the system. reimplemented AutoSlog-TS to use a different sentence analyzer, so these results are slightly different from those reported in (Riloff, 19966).Slot cor mis mlb dup spu R P Perp 26 30 4 17 71 .46 .36 Victim 38 28 24 12 26 .58 .50 Target 28 25 3 29 48 .53 .53 Instr 17 14 2 19 8 .55 .78 Total 109 97 33 77 153 .53 .50 Table 2: Case frame results scheme produces a bubble effect where many incorrect extractions shift to the primary default category.The case frames therefore have the potential for even higher precision if the unknown words are handled better.Expanding the semantic lexicon is one option, and additional work may suggest ways to choose slots for unknown words more intelligently.6 Conclusions We have shown that conceptual case frames can be generated automatically using unannotated text as input, coupled with a few hours of manual review.Our results for the terrorism domain show that the case frames achieve similar recall levels as the extraction patterns, but with substantially fewer false hits.Our results are not directly comparable to the MUC-4 results because the MUC-4 systems contained additional components, such as domainspecific discourse analyzers that resolved coreferent noun phrases, merged event descriptions, and filtered out irrelevant information.The work presented here only addresses the initial stage of information extraction.However, in previous work we showed that AutoSlog-TS achieved performance comparable to AutoSlog (Riloff, 1996b), which performed very well in the MUC-4 evaluation (Lehnert et al., 1992b).Since the conceptual case frames achieved comparable recall and higher precision than AutoSlog-TS' extraction patterns, our results suggest that the case frames performed well relative to previous work on this domain.Several other systems learn extraction patterns that can also be viewed as conceptual case frames with selectional restrictions (e.g., PALKA (Kim and Moldovan, 1993) and CRYSTAL (Soderland et al., 1995)).The case frames learned by our system are not necessarily more powerful then those generated by other systems.The advantage of our approach is that it requires no special training resources.Our technique requires only preclassified training texts and a few hours of manual filtering to build the intermediate dictionaries.Given preclassified texts, it is possible to build a dictionary of conceptual case frames for a new domain in one day.Another advantage of our approach is its highly empirical nature; a corpus often reveals important patterns in a domain that are not necessarily intuitive to people.By using corpus-based methods to generate all of the intermediate dictionaries and 55 the final case frame structures, the most important words, role assignments, and semantic preferences are less likely to be missed.Our empirical approach aims to exploit the text corpus to automatically acquire the syntactic and semantic role assignments that are necessary to achieve good performance in the domain.
Best-first probabilistic chart parsing attempts to parse efficiently by working on edges that are judged &quot;best&quot; by some probabilistic figure of merit (FOM).Recent work has used probabilistic context-free grammars (PCFGs) to assign probabilities to constituents, and to use these probabilities as the starting point for the FOM.This paper extends this approach to using a probabilistic FOM to judge edges (incomplete constituents), thereby giving a much finergrained control over parsing effort.We show how this can be accomplished in a particularly simple way using the common idea of binarizing the PCFG.The results obtained are about a facof twenty improvement over the best results — that is, our parser achieves equivalent results using one twentieth the number of edges.Furthermore we show that this improvement is obtained with parsing precision and recall levels superior to those achieved by exhaustive parsing.
This paper describes a novel statistical namedentity (i.e.&quot;proper name&quot;) recognition system built around a maximum entity framework.By working within the framework of maximum entropy theory and utilizing a flexible object-based architecture, the system is able to make use of an extraordinarily diverse range of knowledge sources in making its tagging decisions.These knowledge sources include capitalization features, lexical features, features indicating the current section of text (i.e. headline or main body), and dictionaries of single or multi-word terms.The purely statistical system contains no hand-generated patterns and achieves a result comparable with the best statistical systems.However, when combined with other handcoded systems, the system achieves scores that exceed the highest comparable scores thus-far published.
This paper presents an algorithm for identifying pronominal anaphora and two experiments based upon this algorithm.We incorporate multiple anaphora resolution factors into a statistical framework — specifically the distance between the pronoun and the proposed antecedent, gender/number/animaticity of the proposed antecedent, governing head information and noun phrase repetition.We combine into a single probability that enables to identify the referent.Our first experiment shows the relative contribution of each source of information and demonstrates a success rate 82.9% for all sources combined. experiment investigates a method for unsupervised learning of gender/number/animaticity information.We present some experiments illustrating the accuracy of the method and note that with this information added, our pronoun resolution method achieves 84.2% accuracy.
Marcu has characterised an important and difficult problem in text planning: given a set of facts to convey and a set of rhetorical relations that can be used to link them together, how can one arrange this material so as to yield the best possible text?We describe experiments with a number of heuristic search methods for this task.
Montemagni and Structural Patterns vs string patterns for extracting semantic from dictionaries In of '92, pp 546-552 Y Ravin Disambiguating and interpreting verb def
PP — 31.5 In all experiments, we use the following three error criteria: • WER (word error rate): The WER is computed as the minimum number of substitution, insertion and deletion operations that have to be performed to convert the generated string into the target string.This performance criterion is widely used in speech recognition.• PER (position-independent word error rate): A shortcoming of the WER is the fact that it requires a perfect word order.This is 26 2: for Text and Speech Input: error rate (WER), positionindependent word error rate (PER) and subjective sentence error rate (SSER) with/without preprocessing (147 sentences = 1 968 words of the Verbmobil task).Input Preproc.WER[%] PER[Vo] SSER[%] Single-Word Based Approach Text No 53.4 38.3 35.7 Yes 56.0 41.2 35.3 Speech No 67.8 50.1 54.8 Yes 67.8 51.4 52.7 Alignnient Templates Text No 49.5 35.3 31.5 Yes 48.3 35.1 27.2 Speech No 63.5 45.6 52.4 Yes 62.8 45.6 50.3 particularly a problem for the Verbmobil task, where the word order of the German- English sentence pair can be quite different.As a result, the word order of the automatically generated target sentence can be different from that of the target sentence, but nevertheless acceptable so that the WER measure alone could be misleading.In order to overcome this problem, we introduce as additional measure the positionindependent word error rate (PER).This measure compares the words in the two senthe word order into account.Words that have no matching counterparts are counted as substitution errors.Depending on whether the translated sentence is longer or shorter than the target translation, the remaining words result in either insertion or deletion errors in addition to substitution errors.The PER is guaranteed to be less than or equal to the WER.• SSER (subjective sentence error rate): For a more detailed analysis, subjective judgments by test persons are necessary.Each translated sentence was judged by a human examiner according to an error scale from 0.0 to 1.0.A score of 0.0 means that the translation is semantically and syntactically correct, a score of 0.5 means that a sentence is semantically correct but syntactically wrong and a score of 1.0 means that the sentence is semantically wrong.The human examiner was offered the translated sentences of the two approaches at the same As a result we expect a better possibility of reproduction.The results of the translation experiments using the single-word based approach and the alignment template approach on text input and on speech input are summarized in Table 2.The results are shown with and without the use of domain-specific preprocessing.The alignment template approach produces better translation results than the single-word based approach.From this we draw the conclusion that it is important to model word groups in source and target language.Considering the recognition word error rate of 31% the degradation of about 20% by speech input can be expected.The average translation time on an Alpha workstation for a single sentence is about one second for the alignment template appreach and 30 seconds for the single-word based search procedure.Within the Verbmobil project other translation modules based on rule-based, examplebased and dialogue-act-based translation are used.We are not able to present results with these methods using our test corpus.But in the current Verbmobil prototype the preliminary evaluations show that the statistical methods produce comparable or better results than other systems.An advantage of the systhat it is robust and always produces a translation result even if the input of the speech recognizer is quite incorrect.5 Summary We have described two approaches to perform statistical machine translation which extend the baseline alignment models.The single-word 27 based approach allows for the the possibility of one-to-many alignments.The alignment template approach uses two different alignment levels: a phrase level alignment between phrases and a word level alignment between single words.As a result the context of words has a greater influence and the changes in word order from source to target language can be learned explicitly.An advantage of both methods is that they learn fully automatically by using a bilingual training corpus and are capable of achieving better translation results on a limited-domain task than other example-based or rule-based translation systems.Acknowledgment This work has been partially supported as part of the Verbmobil project (contract number 01 IV 701 T4) by the German Federal Ministry of Education, Science, Research and Technology and as part of the EuTrans project by the by the European Community (ESPRIT project number 30268).
This paper introduces a new, unsupervised algorithm for noun phrase coreference resolution.It differs from existing methods in that it views coreference resolution as a clustering task.In an evaluation on the MUC-6 coreference resolution corpus, the algorithm achieves an F-measure of 53.6%, placing it firmly between the worst (40%) and best (65%) systems in the MUC-6 evaluation.More importantly, the clustering approach outperforms the only MUC-6 system to treat coreference resolution as a learning problem.The clustering algorithm appears to provide a flexible mechanism for coordinating the application of context-independent and context-dependent constraints and preferences for accurate partitioning of noun phrases into coreference equivalence classes.
Identifying and classifying personal, geographic, institutional or other names in a text is an important task for numerous applications.This paper describes and evaluates a language-independent bootstrapping algorithm based on iterative learning and re-estimation of contextual and morphological patterns captured in hierarchically smoothed trie models.The algorithm learns from unannotated text and achieves competitive performance when trained on a very short labelled name list with no other required language-specific information, tokenizers or tools.
We present a new composite similarity metric that combines information from multiple linguistic indicators to measure semantic distance between pairs of small textual units.Several potential features are investigated and an optimal combination is selected via machine learning.We discuss a more restrictive definition of similarity than traditional, document-level and information retrieval-oriented, notions of similarity, and motivate it by showing its relevance to the multi-document text summarization problem.Results from our system are evaluated against standard information retrieval techniques, establishing that the new method is more effective in identifying closely related textual units.1 Research Goals In this paper, we focus on the problem of detecttwo small textual units (paragraphor sentence-sized) contain common information, as a necessary step towards extracting such common information and constructing thematic groups of text units across multiple documents. pieces of text has many applications (e.g., summarization, information retrieval, text clustering).Most research in this area has centered on detecting similarity between documents [Willet 1988], similarity between a query and a document [Salton 1989] or between a query and a segment of a document [Callan 1994].While effective techniques have been developed for document clustering and classification which depend on inter-document similarity measures, these techniques mostly on shared words, or occasionally collocations of words [Smeaton 1992].When larger units of text are compared, overlap may be sufficient to detect similarity; but when the units of text are small, simple surface matching of words and phrases is less likely to succeed since the number of potential matches is smaller.Our task differs from typical text matching applications not only in the smaller size of the text units compared, but also in its overall goal.Our notion of similarity is more restrictive than topical similarity—we provide a detailed definition in the next section.We aim to recover small textual units from a of documents so that each text unit within a set describes the same action. syswhich is fully implemented, is motivated by the need for determining similarity between small pieces of text across documents that potentially span different topics during multi-document summarization.It serves as the first component of a domain-independent multisummarization system which generates a through reformulation [Barzilay al. by combining information from these similar text passages.We address concerns of sparse data and the narrower than topical definition of similarity by exploring several linguistic features, in addition to shared words or collocations, as indicators of similarity.Our include linked noun phrases, WordNet synonyms, and similar verbs.We also define comover pairs of features. provide an effective method for aggregating the feature values into a similarity measure using machine learning, and present results 203 on a manually annotated corpus of 10,345 pairs of compared paragraphs.Our new features, and especially the composite ones, are shown to outperform traditional techniques such as TF*IDF [Buckley 1985; Salton 1989] for determining similarity over small text units.2 Definition of Similarity Similarity is a complex concept which has been widely discussed in the linguistic, philosophical, and information theory communities.For example, Frawley [1992] discusses all semantic typing in terms of two mechanisms: the detection of similarity and difference.Jackendoff [1983] argues that standard semantic relations such as synonymy, paraphrase, redundancy, and entailment all result from judgments of likeness whereas antonymy, contradiction, and inconsistency derive from judgments of difference.Losee [1998] reviews notions of similarity and their impact on information retrieval techniques.For our task, we define two text units as similar if they share the same focus on a common concept, actor, object, or action.In addition, the common actor or object must perform or be subjected to the same action, or be the subject of the same description.For example, Figure 1 shows three input text fragments (paragraphs) taken from the TDT pilot corpus (see Section 5.1), all from the same topic on the forced landing of a U.S. helicopter in North Korea.We consider units (a) and (b) in Figure 1 to be similar, because they both focus on the same event (loss of contact) with the same primary participant (the helicopter).On the other hand, unit (c) in Figure 1 is not similar to either (a) or (b).Although all three refer to a helicopter, the primary focus in (c) is on the emergency landing rather than the loss of contact.We discuss an experimental validation of our similarity definition in Section 5.2, after we introduce the corpus we use in our experiments.3 Related Work Although there is related empirical research on determining text similarity, primarily in the information retrieval community, there are two major differences between the goals of this earlier work and the problem we address in this (a) An OH-58 helicopter, carrying a crew of two, was on a routine training orientation when contact was lost at about 11:30 a.m. Saturday (9:30 p.m. EST Friday).(b) &quot;There were two people on board,&quot; said Bacon.&quot;We lost radar contact with the helicopter about 9:15 EST (0215 GMT).&quot; (c) An OH-58 U.S. military scout helicopter made an emergency landing in North Korea at about 9.15 p.m. EST Friday (0215 GMT Saturday), the Defense Department said.Figure 1: Input text units (from the TDT pilot corpus, topic 11). paper.First, the notion of similarity as defined in the previous section is more restrictive than the traditional definition of similarity [Anderberg 1973; Willet 1988].Standard notions of similarity generally involve the creation of a vector or profile of characteristics of a text fragment, and then computing on the basis of frequencies the distance between vectors to determine conceptual distance [Salton and Buckley 1988; Salton 19891.Features typically include stemmed words although sometimes multi-word units and collocations have been used [Smeaton 1992], as well as typological characteristics, such as thesaural features.The distance between vectors for one text (usually a query) and another (usually a document) then determines closeness or similarity [van Rijsbergen 1979].In some cases, the texts are represented as vectors of sparse n-grams of word occurrences and learning is applied over those vectors [Schapire and Singer 1999].But since our definition of similarity is oriented to the small-segment goal, we make more fine-grained distinctions.Thus, a set of passages that would probably go into the same class by standard IR criteria would be further separated by our methods.Second, we have developed a method that functions over pairs of small units of text, so the size of the input text to be compared is different.This differs from document-to-document 204 or query-to-document comparison.A closely related problem is that of matching a query to the relevant segment from a longer document [Callan 1994; Kaszkiel and Zobel 1998], which primarily involves determining which segment of a longer document is relevant to a query, whereas our focus is on which segments are similar to each other.In both cases, we have less data to compare, and thus have to explore additional or more informative indicators of similarity.4 Methodology compute a feature vector over a pair of texunits, where features are either of one characteristic, or consisting of pairs of primitive features.4.1 Primitive Features draw on a number of linguistic approaches to text analysis, and are based on both single words and simplex noun phrases (head preceded by optional premodifiers with no embedded recursion).Each of these syntactic, and semantic several variations.We thus consider following potential matches between text units: Word co-occurrence, sharing a single word between text units.Variations of this feature restrict matching to cases where the parts of speech of the words also match, or relax it to cases where just the stems of the two words are identical.Matching noun phrases. the LINKIT tool [Wacholder 1998] to identify simplex noun phrases and match those that share the same head.WordNet synonyms. provides sense information, placing in sets of synonyms match words that appear in the same synset.Variations on this feature restrict the words considered to a specific part-of-speech class.• Common semantic classes for verbs.Levin's [1993] semantic classes for verbs have been found to be useful for determining document type and text similarity [Klavans and Kan 1998].We match two verbs that share the same semantic class.Shared proper nouns. nouns are using the set [Abal.Variations on proper noun matching include restricting the proper noun type to a person, place, or an organization subcategories are also extracted entity finder).In order to normalize for text length and frequency effects, we experimented with two types of optional normalization of feature values.The first is for text length (measured in words), where each feature value is normalized by the of the textual units in the pair. for of textual units feature values are divided by: length(A) x length(B) (1) This operation removes potential bias in favor longer text The second type of normalization we examined was based on the relative frequency of occurrence of each primitive.This is motivated the fact that infrequently primiare likely to have higher impact on similarity than primitives which match more frequently.We perform this normalization in manner similar to the IDF part of Every primitive element is associated with a value which is the number of textual units in which the primitive appeared in the corpus.For a primitive element which compares single words, this is the number of textual units which contain that word in the corpus; for a noun phrase, this is the number of textual units that contain noun phrases that share the same head; and similarly for other primitive types.We multiply each feature's value by: number of textual units (2) Number of textual units containing this primitive Since each normalization is optional, there are four variations for each primitive feature.4.2 Composite Features addition to the above that compare single items from each text unit, we which combine pairs of primitive features.Composite features are defined by placing different types of restrictions on the participating primitive features: 205 Figure 2: A composite feature over word primitives with a restriction on order would count the pair &quot;two&quot; and &quot;contact&quot; as a match because they occur with the same relative order in both textual units.An 011-58 helicopter, carrying a crew of orientation when c ntac as lost (9:30 p.m. EST Friday).(a) was on a routine training out 11:30 a.m. Saturday (b) &quot;There were[twolpeople on board,&quot; said Bacon.&quot;We lost radar with the helicopter about 9:15 EST (0215 GMT).&quot; Figure 3: A composite feature over word primitives with a restriction on distance would match on the pair &quot;lost&quot; and &quot;contact&quot; because they occur within two words of each other in both textual units.:0}145theligoPterjearryinga crew of two, was on a routine training (a) rientation when contact was ft t about 11:30 a.m. Saturday ('10 p.m. EST Friday).(b) &quot;T ere were two people on board,&quot; said Bacon.&quot;Wetradar contact with th.elielico ter bout 9:15 EST (0215 GMT).&quot; Figure 4: A composite feature with restrictions on the primitives' type.One primitive must be a matching simplex noun phrase (in this case, a helicopter), while the other primitive must be a matching verb (in this case, &quot;lost&quot;.)The example shows a pair of textual units where this composite feature detects a valid match.An 011-58 helicopter, carrying a crew of two, was on a routine training (a) orientation when vas Li t about 11:30 am.Saturday (9:30 p.m. EST Friday).(b) &quot;There were two people on board,&quot; said Bacon.&quot;W with the helicopter about 9:15 EST (0215 GMT).&quot; Ordering. pairs of primitive elements are required to have the same relative order in both textual units (see Figure 2).Distance. pairs of primitive elements are required to occur within a certain distance in both textual units (see Figure 3).The maximum distance between the primitive elements can vary as an additional parameter.A distance of one matches rigid collocations whereas a distance of five captures related primitives within a region of the text unit [Smeaton 1992; Smadja 1993].Primitive. element of the pair of primitive elements can be restricted to a specific primitive, allowing more expressiveness in the composite features.For example, we can restrict one of the primitive features to be a simplex noun phrase and the other to be a verb; then, two noun phrases, one from each text unit, must match according to the rule for matching simplex noun phrases (i.e., sharing the same head), and two verbs must match according to the rule for verbs (i.e., sharthe same semantic class); see Figure This particular combination loosely approximates grammatical relations, e.g., matching subject-verb pairs.'Verbs can also be matched by the first (and more reof Section 4.1, namely requiring that their stemmed forms be identical.206 Since these restrictions can be combined, many different composite features can be defined, although our empirical results indicate that the most successful tend to include a distance constraint.As we put more restrictions on a composite feature, the fewer times it occurs in the corpus; however, some of the more restrictive features are most effective in determining similarity.Hence, there is a balance between the discriminatory power of these features and applicability to number of cases. features are normalized features are (i.e., for text unit length and for frequency of occurrence).This type of normalization also uses equation (2) but averages the normalization values of each primitive in the composite feature.4.3 Learning a Classifier For each pair of text units, we compute a vector of primitive and composite feature values.To determine whether the units match overall, we employ a machine learning algorithm, RIP- PER [Cohen 1996], a widely used and effective rule induction system.RIPPER is trained over a corpus of manually marked pairs of units; we discuss the specifics of our corpus and of the annotation process in the next session.We experwith varying RIPPER's ratio, measures the cost of a false positive relative to that of a false negative (where we view &quot;similar&quot; as the positive class), and thus controls the relative weight of precision versus recall.This is an important step in dealing with the sparse data problem; most text units are not similar, given our restrictive definition, and thus positive instances are rare.5 Results 5.1 The Evaluation Corpus For evaluation, we use a set of articles already classified into topical subsets which we obtained from the Reuters part of the 1997 pilot Topic Detection and Tracking (TDT) corpus.The TDT corpus, developed by NIST and DARPA, is a collection of 16,000 news articles from Reuters and CNN where many of the articles and transcripts have been manually grouped into 25 categories each of which corresponds a single event (see //morph. ldc edu/Catalog/LDC98T25 .html). the Reuters part of the corpus, we selected five of the larger categories and extracted all articles assigned to them from several randomly chosen days, for a total of 30 articles.Since paragraphs in news stories tend to be short—typically one or two sentences—in this study we use paragraphs as our small text units, although sentences would also be a possibility.In total, we have 264 text units and 10,345 comparisons between units.As comparisons are made between all pairs of paragraphs from the same topic, the total number of comparisons is equal to 2 the number of paragraphs in all selected articles from topical category i.Training of our machine learning component was done by three-fold cross-validation, ransplitting the pairs paragraphs into three (almost) equally-sized subsets.In each of the three runs, two of these subsets were used for training and one for testing.To create a reference standard, the entire collection of 10,345 paragraph pairs was marked for by two reviewers who were given definition and detailed instructions.Each reindependently marked each paragraphs as similar or not similar.Subsequently, the two reviewers jointly examined cases where was disagreement, discussed reasons, reconciled the differences.5.2 Experimental Validation of the In order to independently validate our definiof similarity, we performed additional experiments.In the first, we asked three addijudges to determine a ransample 40 paragraph pairs.High agreement between judges would indicate that our definition of similarity reflects an objective reality and can be mapped unambiguously to an operational procedure for marking text units as similar or not.At the same time, it would also validate the judgments between text units that we use for our experiments (see Section 5.1). this task, judges were given opportuprovide reasons for claiming similarity or dissimilarity, and comments on the task were for future analysis. three additional 207 judges agreed with the manually marked and standardized corpus on 97.6% of the comparisons.Unfortunately, approximately 97% (depending on the specific experiment) of the comparisons in both our model and the subsequent validation experiment receive the value &quot;not similar&quot;.This large percentage is due to our finegrained notion of similarity, and is parallel to happens in randomly sampled collections, since in that case most documents will not be relevant to any given query.Nevertheless, we can account for the high probability of inter-reviewer agreement expected by chance, 0.97.0.97+ (1 —0.97)- (1-0.97) --- 0.9418, by referring to the kappa statistic [Cohen 1960; Carletta 1996].The kappa statistic is defined as PA PO K — the probability that two reviewers agree in practice, and Po is the probability that they would agree solely by chance.In our case, 0.9418, and = indicating that the observed agreement by the is indeed If Po is estimated from the particular sample used in this experiment rather than from our entire corpus, it would be only 0.9, producing a value of 0.76 In addition to this validation experiment that used randomly sampled pairs of paragraphs (and reflected the disproportionate rate of occurrence of dissimilar pairs), we performed a balanced experiment by randomly selecting 50 of the dissimilar pairs and 50 of the similar pairs, in a manner that guaranteed generation an independent Pairs in this subset were rated for similarity by two additional independent reviewers, who agreed on their decisions 91% of the time, versus 50% expected chance; in this case, = Thus, we feel confident in the reliability of our annotation is always between 0 and 1, with 0 indicating no better agreement than expected by chance and 1 indicating perfect agreement. guarantee independence, pairs of paragraphs were randomly selected for inclusion in the sample a pair (A, immediately rejected if there were paragraphs Xi, , X.n. for n > 0 such that all (X1, X2), ..., , 13) already been included in the sample. process, and can use the annotated corpus to assess the performance of our similarity measure and compare it to measures proposed earlier in the information retrieval literature.5.3 Performance Comparisons We compare the performance of our system to three other methods.First, we use standard TF*IDF, a method that with various alterations, remains at the core of many information retrieval and text matching systems [Salton and Buckley 1988; Salton 1989].We compute the total frequency (TF) of words in each text unit.We also compute the number of units each word appears in in our training set (DF, or document frequency).Then each text unit is represented as a vector of TF*IDF scores calculated as Similarity between text units is measured by the cosine of the angle between the corresponding two vectors (i.e., the normalized inner product of the two vectors).A further cutoff point is selected to convert similarities to hard decisions of &quot;similar&quot; or &quot;not similar&quot;; different cutoffs result in different tradeoffs between recall and precision.Second, we compare our method against a standard, widely available information retrieval system developed at Cornell University, [Buckley SMART utilizes a modified TF*IDF measure (ATC) plus stemming and a fairly sizable stopword list.Third, we use as a baseline method the default selection of the most frequent category, i.e., &quot;not similar&quot;.While this last method cannot be effectively used to identify similar paragraphs, it offers a baseline for the overall accuracy of any more sophisticated technique for this task.5.4 Experimental Results Our system was able to recover 36.6% of the similar paragraphs with 60.5% precision, as shown in Table 1.In comparison, the unmodiobtained only 32.6% precision when recall is 39.1%, i.e., close to our system's recall; and only 20.8% recall at precision of 62.2%, comparable to our classifier's used version 11.0 of SMART, released in July 1992.• log number of units 208 Recall Precision Accuracy Machine learning over linguistic indicators 36.6% 60.5% 98.8% TF*IDF 30.0% 47.4% 97.2% SMART 29.1% 48.3% 97.1% Default choice (baseline) 0% undefined 97.5% Table 1: Experimental results for different similarity metrics.For comparison purposes, we list the average recall, precision, and accuracy obtained by TF*IDF and SMART at the two points in the precision-recall curve identified for each method in the text (i.e., the point where the method's precision is most similar to ours, and the point where its recall is most similar to ours). precision.SMART (in its default configuration) offered only a small improvement over the base TF*IDF implementation, and significantly underperformed our method, obtaining 34.1% precision at recall of 36.7%, and 21.5% recall at 62.4% precision.The default method of always marking a pair as dissimilar obtains of course 0% recall and undefined precision.Figure 5 illustrates the difference between our system and straight TF*IDF at different points of the precision-recall spectrum.When overall accuracy (total percentage of correct answers over both categories of similar and non-similar pairs) is considered, the numbers are much closer together: 98.8% for our approach; 96.6% and 97.8% for TF*IDF on the two P-R points mentioned for that method 96.5% and for SMART, again at the two P-R points mentioned for SMART and 97.5% for the default Nevertheless, since the challenge of identifying sparsely occurring similar small text units is our goal, the accuracy measure and the baseline technique of classifying everything as not similar are included only for reference but do tests of significance cannot be performed for cmnparing these values, since paragraphs appear in multiple comparisons and consequently the comparisons are not independent.Figure 5: Precision-recall graph comparing our using line with squares) versus TF*IDF (dotted line with triangles). not reflect our task.6 Analysis and Discussion of Feature Performance We computed statistics on how much each feature helps in identifying similarity, summarized in Table 2.Primitive features are named acto the type of the feature (e.g., the feature that counts the number of matching verbs according to exact matches).Composite feature names indicate the restrictions applied to primitives.For example, the composite fea- < a pair of matching primitives to occur within a relative distance of four words.If the composite feature also restricts the types of the primitives in the pair, the name of the restricting primitive feature is added to the composite feature name.For exthe feature named Distance < 5 requires one member of the pair to be a verb and the relative distance between the primitives to be at most five.The second column in Table 2 shows whether the feature value has been normalized accordto its overall while the third column indicates the actual threshold used in decisions assuming that only this feature is used for clas- The fourth column shows the applicathat feature, that is, the percentage of results reported in Table 2 include our first norstep that accounts for the difference in length of text units.209 Feature Name Normalized?Threshold Applicability Recall Precision Any word Yes 0.360 2.2% 31.4% 41.8% 0.505 16.7% 75.4% Noun Yes 0.150 8.1% 43.2% 15.9% 0.275 1.5% 20.9% 37.0% Proper noun Yes 0.200 0.2% 2.0% 30.8% Verb No 0.775 ' 10.6% 19.7% 1.6% Simplex NP Yes 0.150 5.7% 35.5% 18.6% 0.275 2.7% 10.1% 44.6% 0.350 0.7% 3.7% 69.2% Semantic class of verbs No 0.875 0.1% 2.0% 3.4% WordNet Yes 0.250 5,4% 4.1% 2.3% Distance < 2 Yes 0.075 4.7% 24.9% 15.7% Distance < 3 Yes 0.250 0.5% 10.2% 55.6% Distance < 4 Yes 0.275 1.9% 14.6% 50.0% Distance < 5 Yes 0.200 1.9% 22.4% 53.4% Order Distance < 5 Yes 0.200 1.5% 20.4% 40.7% Noun Distance < 5 Yes 0.175 1.9% 21.2% 31.9% Verb Distance < 5 Yes 0.200 0.3% 7.3% 66.7% No 0.850 0.6% 11.0% 56.3% Table 2: Statistics for a selected subset of features.Performance measures are occasionally given multiple times for the same feature and normalization option, highlighting the effect of different decision thresholds. paragraph pairs for which this feature would apply (i.e., have a value over the specified threshold).Finally, the fifth and sixth columns show the recall and precision on identifying similar paragraphs for each independent feature.Note that some features have low applicability over the entire corpus, but target the hard-to-find similar pairs, resulting in significant gains in recall and precision.Table 2 presents a selected subset of primitive and composite features in order to demonstrate our results.For example, it was not surprising to observe that the most effective primitive feain determining similarity are word, NP, other primitives as not as effective independently.This is to be expected since nouns name objects, entities, and concepts, and frequently exhibit more sense constancy.In contrast, verbs are functions and tend to shift senses in a more fluid fashion depending on context.Furthermore, our technique does not label phrasal verbs (e.g. look up, look out, look over, look for, etc.), which are a major source of verbal ambiguity in English.Whereas primitive features viewed independently might not have a directly visible effect on identifying similarity, when used in composite features they lead to some novel results.The pronounced case of this is for the composite feature Distance < can help identify similarity effectively, as seen in Table 2.This composite feature approximates verb-argument and verb-collocation relations, which are strong indicators of similarity.At the same time, the more restrictive a feature is, the fewer occurrences of that feature appear in the training set.This suggests that we could consider adding additional features suggested by current results in order to further refine and improve our similarity identification algorithm.7 Conclusion and Future Work We have presented a new method to detect similarity between small textual units, which combines primitive and composite features using machine learning.We validated our similarity definition using human judges, applied 210 our method to a substantial number of paragraph pairs from news articles, and compared results to baseline and standard information retrieval techniques.Our results indicate that our method outperforms the standard techniques for detecting similarity, and the system has been successfully integrated into a larger multipledocument summarization system [McKeown et We are currently working on incorporating a clustering algorithm in order to give as output a set of textual units which are mutually similar rather than just pairwise similar.Future work includes testing on textual units of different size, comparing with additional techniques proposed for document similarity in the information retrieval and computational linguistics literature, and extending the feature set to incorporate other types of linguistic information in the statistical learning method.Acknowledgments We are grateful to Regina Barzilay, Hongyan Jing, Kathy McKeown, Shimei Pan, and Yoram Singer for numerous discussions of earlier versions of this paper and for their help with setting up and running RIPPER and SMART.This research has been supported in part by an NSF STIMULATE grant, IRI-96-1879.Any opinions, findings, and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of the National Science Foundation.
In this paper we discuss cascaded Memory- Based grammatical relations assignment.In the first stages of the cascade, we find chunks of several types (NP,VP,ADJP,ADVP,PP) and label them with their adverbial function (e.g. local, temporal).In the last stage, we assign grammatical relations to pairs of chunks.We studied the effect of adding several levels to this cascaded classifier and we found that even the less performing chunkers enhanced the performance of the relation finder.

We present an unsupervised method for detecting grammatical errors by inferring negative evidence from edited textual corpora.The system was developed and tested using essay-length responses to prompts on the Test of English as a Foreign Language (TOEFL).The errorrecognition system, ALEK, performs with about 80% precision and 20% recall.
Keh- J iann Chen Sh ing- l luan Liu Institute of lnfl~rmation Science Academia Sinica Chinese sentences are composed with string of characters without blanks to mark words.However the basic unit for sentence parsing and understanding is word.Therefore the first step of processing Chinese sentences i to identify the words.The difficulties of identifying words include (l) the identification of com- plex words, such as Determinative-Measure, redupli- cations, derived words etc., (2) the identification of proper names,(3) resolving the ambiguous segmenta- tions.In this paper, we propose the possible solutions for the above difficulties.We adopt a matching algo- rithm with 6 different heuristic rules to resolve the am- biguities and achieve an 99.77% of the success rate.The statistical data supports that the maximal match- ing algorithm is the most effective heuristics.
This paper investigates why the HMMs es timated by Expectation-Maximization (EM) produce such poor results as Part-of-Speech(POS) taggers.We find that the HMMs es timated by EM generally assign a roughlyequal number of word tokens to each hid den state, while the empirical distribution of tokens to POS tags is highly skewed.This motivates a Bayesian approach using a sparse prior to bias the estimator toward such a skewed distribution.We investigate Gibbs Sampling (GS) and Variational Bayes(VB) estimators and show that VB con verges faster than GS for this task and thatVB significantly improves 1-to-1 tagging ac curacy over EM.We also show that EM does nearly as well as VB when the number of hidden HMM states is dramatically reduced.We also point out the high variance in all of these estimators, and that they requiremany more iterations to approach conver gence than usually thought.
We present V-measure, an external entropybased cluster evaluation measure.V measure provides an elegant solution tomany problems that affect previously defined cluster evaluation measures includ ing 1) dependence on clustering algorithm or data set, 2) the ?problem of matching?, where the clustering of only a portion of datapoints are evaluated and 3) accurate evaluation and combination of two desirable aspects of clustering, homogeneity and completeness.We compare V-measure to a num ber of popular cluster evaluation measuresand demonstrate that it satisfies several desirable properties of clustering solutions, us ing simulated clustering results.Finally, we use V-measure to evaluate two clustering tasks: document clustering and pitch accent type clustering.
Minimum-error-rate training (MERT) is a bottleneck for current development in statistical machine translation because it is limited in the number of weights it can reliably optimize.Building on the work of Watanabe et al., we explore the use of the MIRA algorithm of Crammer et al. as an alternative to MERT.We first show that by parallel processing and exploiting more of the parse forest, we can obtain results using MIRA that match or surpass MERT in terms of both translation quality and computational cost.We then test the method on two classes of features that address deficiencies in the Hiero hierarchical phrasebased model: first, we simultaneously train a large number of Marton and Resnik’s soft syntactic constraints, and, second, we introduce a novel structural distortion model.In both cases we obtain significant improvements in translation performance.Optimizing them in combination, for a total of 56 feature weights, improve performance by 2.6 a subset of the NIST 2006 Arabic-English evaluation data.
While phrase-based statistical machine translation systems currently deliver state-of-the art performance, they remain weak on word order changes.Current phrase reordering models can properly handle swaps between adjacent phrases, but they typically lack theability to perform the kind of long-distance re orderings possible with syntax-based systems.In this paper, we present a novel hierarchical phrase reordering model aimed at improvingnon-local reorderings, which seamlessly in tegrates with a standard phrase-based system with little loss of computational efficiency.Weshow that this model can successfully han dle the key examples often used to motivate syntax-based systems, such as the rotation of a prepositional phrase around a noun phrase.We contrast our model with reordering models commonly used in phrase-based systems, and show that our approach provides statistically significant BLEU point gains for two language pairs: Chinese-English (+0.53 on MT05 and +0.71 on MT08) and Arabic-English (+0.55 on MT05).
We address the creation of cross-lingual textual entailment corpora by means of crowdsourcing.Our goal is to define a cheap and replicable data collection methodology that minimizes the manual work done by expert annotators, without resorting to preprocessing tools or already annotated monolingual datasets.In line with recent works emphasizing the need of large-scale annotation efforts for textual entailment, our work aims to: the scarcity of data available to train evaluate systems, and the recourse to crowdsourcing as an effective way to reduce the costs of data collection without sacrificing quality.We show that a complex data creation task, for which even experts usually feature low agreement scores, can be effectively decomposed into simple subtasks assigned to non-expert annotators.The resulting dataset, obtained from a pipeline of different jobs routed to Amazon Mechanical Turk, contains more than 1,600 aligned pairs for each combination of texts-hypotheses in English, Italian and German.
We present a practical co-training method for bootstrapping statistical parsers using a small amount of manually parsed training material and a much larger pool of raw sentences.Experimental results show that unlabelled sentences can be used to improve the performance of statistical parsers.In addition, we consider the problem of bootstrapping parsers when the manually parsed training material is in a different domain to either the raw sentences or the testing material.We show that bootstrapping continues to be useful, even though no manually produced parses from the target domain are used.
Many of the tasks required for semantic tagging of phrases and texts rely on a list of words annotated with some semanticfeatures.We present a method for ex tracting sentiment-bearing adjectives fromWordNet using the Sentiment Tag Extrac tion Program (STEP).We did 58 STEP runs on unique non-intersecting seed lists drawn from manually annotated list ofpositive and negative adjectives and evaluated the results against other manually annotated lists.The 58 runs were then col lapsed into a single set of 7, 813 unique words.For each word we computed a Net Overlap Score by subtracting the totalnumber of runs assigning this word a neg ative sentiment from the total of the runs that consider it positive.We demonstrate that Net Overlap Score can be used as ameasure of the words degree of member ship in the fuzzy category of sentiment:the core adjectives, which had the high est Net Overlap scores, were identifiedmost accurately both by STEP and by hu man annotators, while the words on the periphery of the category had the lowest scores and were associated with low rates of inter-annotator agreement.
the last few years, so called in general, and morphology in particular, have become widely accepted as paradigms for the computational treatment of morphology.Finite-state morphology appeals to the notion of a finite-state transducer, which is simply a classical finite-state automaton whose transitions are labeled with pairs, rather than with single symbols.The automaton operates on a pair of tapes and advances over a given transition if the current symbols on the tapes match the pair on the transition.One member of the pair of symbols on a transition can be the designated null symbol, which we will write c. When this appears, the corresponding tape is not examined, and it does not advance as the machine moves to the next state.Finite-state morphology originally arose out of a desire to provide ways of analyzing surface forms using grammars expressed in terms of systems of ordered rewriting rules.Kaplan and Kay (in preparation) observed, that finite-state transducers could be used to mimic a large class of rewriting rules, possibly including all those for phonology.The importance of came from two considerations.First, transducers are indifferent as to the direction in which they are applied.In other words, they can be used with equal facility to translate between tapes, in either direction, to accept or reject pairs of tapes, or to generate pairs of tapes.Second, a pair of transducers with one tape in common is equivalent to a single transducer operating on the remaining pair of tapes.A simple algorithm exists for constructing the transition diagram for composite machine given those of the original pair.By repeated application of this algorithm, it is therefore possible to reduce a cascade of transducers, each linked to the next by a common tape, to a single transducer which accepts exactly the same pair of tapes as was accepted by the original cascade as a whole.From these two facts together, it follows that an arbitrary ordered set of rewriting rules can be modeled by a finite-state transducer which can be automatically constructed from them and which serves as well for analyzing surface forms as for generating them from underlying lexical strings.A transducer obtained from an ordered set of in the way just outlined is a level the sense that mediates directly between lexical and surface forms without ever the intermediate forms would arise in the course of applying the original rules by one.The term morphology, is used a more restricted way, to apply to a system in which no intermediate forms are posited, even in the original grammatical formalism.The writer of a grammar using a two-level formalism never needs to think in terms of any representations other than the lexical and the surface ones.What he does is to specify, using one formalism or another, a set of transducers, each of which mediates directly between these forms and each of which restricts the allowable pairs of strings in some way.The pairs that the system as a whole accepts are those are those that are rejected by none of the component transducers, modulo certain assumptions about way in they interact, whose details need not concern us.Once again, there is a formal procedure that can be used to combine set transducers that make up such a system 2 into a single automaton with the same overall so that the final result indistinguishable form that obtained from a set of ordered rules.However it is an advantage of parallel machines that they can be used with very little loss of efficiency without combining them in this way.While it is not the purpose of this paper to explore the formal properties of finite-state transducers, a brief excursion may be in order at this point to forestall a possible objection to the claim that a parallel configuration of transducers can be combined into a single one.On the face of it, this cannot generally be so because there is generally no finite-state transducer that will accept the intersection of the sets of tape pairs accepted by an arbitrary set of transducers.It is, for example, easy to design a transducer that will map a string of x's onto the same number of y's followed by an arbitrary number of z's.It is equally easy to design one that maps a string of x's onto the same number of z's preceded by an arbitrary number of x's.The intersection of these sets contains those pairs with some number of x's on one tape, and that same number of y's followed by the same number of z's on the other tape.The set of second tapes therefore contains a context-free language which it is clearly not within the power of any finite-state device to generate.Koskenniemi overcame this objection in his original work by adopting the view that all the transducers in the parallel configuration should share the same pair or read-write heads.The effect of this is to insist that they not only accept the same pairs of tapes, but that they agree on the particular sequence of symbol pairs that must be rehearsed in the course of accepting each of them.Kaplan has been able to put a more formal construction on this in the following way Let the empty symbols appearing in the pairs labeling any transition in the transducers be replaced by some ordinary symbol not otherwise part of the alphabet.The new set of transducers derived in way clearly do not accept the same tapes as the original ones did, but there is an algorithm for constructing a single finite-state transducer that will accept the intersection of the pairs they all accept.Suppose, now, that this configuration of parallel transducers is put in series with two other standard transducers, one which carries the real empty symbol onto its surrogate, and everything else onto itself, and another transducer that carries the surrogate onto the real empty symbol, then the resulting configuration accepts just the desired set of languages, all of which are also acceptable by single transducers that can be algorithmically derived form the originals.It may well appear that the systems we have been considering properly belong to finite-state phonology or graphology, and not to morphology, properly construed.Computational linguists have indeed often been guilty of some carelessness in their use of this terminology.But it is not hard to see how it could have arisen.The first step in any process that treats natural text is to recognize the words it contains, and this generally involves analyzing each of them in terms of a constituent set of formatives of some kind.Most important among the difficulties that this entails are those having to do with the different shapes that formatives assume in different environments.In other words, the principal difficulties of morphological analysis are in fact phonological or graphological.The inventor of two-level morphology, Kimmo Koskenniemi, is fact provided a finite-state account not just of morphophonemics (or morphographemics), but also of morphotactics.He took it that the allowable set of words simply constituted a regular set of morheme sequences.This is probably the more controversial part of his proposal, but it is also the less technically elaborate, and therefore the one that has attracted less attention.As a result, the term &quot;two-level morphology&quot; has come to be commonly accepted as applying to any system of word recognition that involves two-level, finite-state, phonology or graphology.The approach to nonconcatenative morphology to be outlined in this paper will provide a more unified treatment of morphophonemics and morphotactics than has been usual 3 I shall attempt to show how a two-level account might be given of nonconcatenative morphological phenomena, particularly those exhibited in the Semitic languages.The approach I intend to take is inspired, not only by finite-state morphology, broadly construed, but equally by autosegmental phonology as proposed by Goldsmith (1979) and the autosegmental morphology of McCarthy (1979) All the data that I have used in this work is taken from McCarthy (1979) and my debt to him will be clear throughout. forms that can be constructed on the basis of each of the stems shown.However, there is every reason to suppose that, though longer and greatly more complex in detail, that enterprise would not require essentially different mechanisms from the ones I shall describe.The overall principles on which the material Table is are clear from a fairly cursory inspection Each form contains the letters &quot;ktb&quot; somewhere in it.This is the root of the verb meaning &quot;write&quot;.By replacing these three letters with other appropriately chosen Perfective Imperfective Participle Active Passive Active I katab kutib aktub II kattab kuttib ukattib III kaatab kuutib ukaatib IV ?aktab ?uktib u?aktib V takattab tukuttib atakattab VI takaatab tukuutib atakaatab VII nkatab nkutib ankatib VIII ktatab ktutib aktatib IX ktabab aktabib X staktab stuktib astaktib XI ktaabab aktaabib XII ktawtab aktawtib XIII ktawwab aktawwib XIV ktanbab aktanbib XV ktanbay aktanbiy Passive maktuub mukattab mukaatab mu?aktab mutakattab mutakaatab munkatab muktatab muktabib mustaktab muktaabib muktawtib muktawwib muktanbib muktanbiy Table I I take it as my task to describe how the members of a paradigm like the one in Table I might be generated and recognized effectively and efficiently, and in such a way as to capture profit from the linguistic generalizations inherent in it.Now this is a slightly artificial problem because the forms in Table I are not in words, but only verb stems.To get the verb forms that would be found in Arabic text, we should have to expand the table very considerably to show the inflected sequences of three consonants, we would obtain corresponding paradigms for other roots.With some notable exceptions, the columns of the table contain stems with the same sequence of vowels. of these is known as a as the headings of the columns show, these can serve to distinguish perfect from imperfective, active from passive, and the like.Each row of the table is characterized by a particular pattern according to which the vowels and consonants alternate.In other words, it is characteristic of a given row 4 that the vowel in a particular position is long or short, or that a consonant is simple or geminate, or that material in one syllable is repeated in the following one.McCarthy refers to each of these as a template, term which I shall take over.Each of them adds a particular semantic component to the basic verb, making it reflexive, causative, or whatever.Our problem, will therefore involve designing an abstract device capable of combining components of these three kinds into a single sequence.Our solution will take the form of a set of one or more finite-state transducers that will work in parallel like those of Koskenniemmi(1983), but on four tapes rather than just two.There will not be space, in this paper, to give a detailed account, even of all the material in Table I, not to mention problems that would arise if we were to consider the full range of Arabic roots.What I do hope to do, however, is to establish a theoretical framework within which solutions to all of these problems could be developed.We must presumably expect the transducers we construct to account for the Arabic data to have transition functions from states and quadruples of symbols to states.In other words, we will be able to describe them with transition diagrams whose edges are labeled with a vector of four symbols.When the automaton moves from one state to another, each of the four tapes will advance over the symbol corresponding to it on the transition that sanctions the move. shall allow myself extensions to this basic scheme which will enhance the perspicuity and economy of the formalism without changing its essential character.In particular, these extensions will leave us clearly within the domain of finite-state devices.The extensions have to do with separating the process of reading or writing a symbol on a tape, from advancing the tape to the next position.The quadruples that label the transitions in the transducers we shall be constructing will be elements each consisting of two parts, a symbol, and an instruction concerning the movement of the tape I shall use the following notation for this.A unadorned symbol will be read in the traditional way, namely, as requiring the tape on which that symbol appears to move to the next position as soon as it has been read or written.If the symbol is shown in brackets, on the other hand, the tape will not advance, and the quadruple specifying the next following transition will therefore clearly have to be one that specifies the same symbol for that tape, since the symbol will still be under the read-write head when that transition is taken.With this convention, it is natural to dispense with the e symbol in favor of the notation &quot;[1&quot;, that is, an unspecified symbol over which the corresponding tape does not advance.A symbol can also be written in braces, in which case the corresponding tape will move if the symbol under the read-write head is the last one on the tape.This is intended to capture the of autosegmental morphology, that is, the principal according to which the last item in a string may be reused when required to fill several positions. particular set of quadruples, or made up of symbols, with or without brackets or will constitute the the automata, and the &quot;useful&quot; alphabet must be the same for all the automata because none of them can move from one state to another unless the make an exactly parallel Not surprisingly, a considerable amount of information about the language is contained just in the constitution of the alphabet.Indeed, a single machine with one state which all transitions both leave and enter will generate a nontrivial subset of the material in Table I.An example of the steps involved in generating a form that depends only minimally on information in a transducer is given in table The eight step are labeled (a) - (h).For each one, a box is shown enclosing the symbols currently under the read-write heads.The tapes move under the heads from the right and then continue to the left No symbols are shown to the right on the bottom tape, because we are assuming that the operation chronicled in these diagrams is one in which a surface form is being 5 (a) V t b [1 (e) k t {b} CCVVCVC V VCCV V C C a a {al a [1 a ak t a (b) t b t [] V a a VVCVC [] VCCVC V V a ak tab (c) (g) V C VC VC VCCVC a a [] a •■■• [] ak t a b (d) k t [] (h) k t b V C C V a C VC V VCCVCVC a a i a k t a a ak t ab i b Table II generated.The bottom tape—the one containing the surface form—is therefore being written and it is for this reason that nothing appears to the right.The other three tapes, in the order shown, contain the root, the prosodic template, and the vocalism.To the right of the tapes, the frame is shown which sanctions the move that will be made to advance from that position to the next.No such frame is given for the last configuration for the obvious reason that this represents the end of the process. move from (a) to (b) sanctioned by a frame in which the root consonant is ignored.There must be a &quot;V&quot; on the template tape and an &quot;a&quot; in the current position of the vocalism.However, the vocalism tape will not move when the automata move to their next states.Finally, there will be an &quot;a&quot; on the tape containing the surface form.In summary, given that the prosodie template calls for a vowel, the next vowel in the vocalism has been copied to the surface.Nondeterministically, the device predicts that this same contribution from the vocalism will also be required to fill a later position.The move from (b) to (c) is sanctioned by a in which the is ignored.The template requires a consonant and the frame accordingly specifies the same consonant on both the root and the surface tapes, advancing both of them.A parallel move, differing only in the identity of the consonant, is made from (c) to (d).The move from (d) to (e) is similar to that from (a) to (b) except that, this time, the vocalism tape does advance.The nondeterministic prediction that is being made in this case is that there will be no further slots for the &quot;a&quot; to fill.Just what it is that makes this the &quot;right&quot; move is a matter to which we shall return.The move from (e) to (f) 6 differs from the previous two moves over root consonants in that the &quot;b&quot; is being &quot;spread&quot;.In other words, the root tape does not move, and this possibility is allowed on the specific grounds that it is the last symbol on the tape.Once again, the automata are making a nondeterministic decision, this time that there will be another consonant called for later by the prosodic template and which it will be possible to fill only if this last entry on the root tape does not move away.The moves from (f) to (g) and from (g) to (h) are like those from (d) to (e) and (b) to (c) respectively.Just what is the force of the remark, made from time to time in this commentary, that a move is made These are all situations in which some other move was, in fact, open to the transducers but where the one displayed was carefully chosen to be the one that would lead to the correct result.Suppose that, instead of leaving the root tape stationary in the move from (e) to (f), it had been allowed to advance using a frame parallel to the one used in the moves from (b) to (c) and (c) to (d), a frame which it is only reasonable to assume must exist for all consonants, including &quot;b&quot;.The move from (f) to (g) could still have been made in the same way, but this would have led to a configuration in which a consonant was required by the prosodic template, but none was available from the root.A derivation cannot be allowed to count as complete until all tapes are exhausted, so the automata have reached impasse.We must assume that, when this happens, the automata are able to return to a preceding situation in which an essentially arbitrarily choice was made, and try a different alternative.Indeed, we must assume that a general backtracking strategy is in effect, which ensures that all allowable sequences of choices are explored.Now consider the nondeterministic choice that was made in the move from (a) to (b), as contrasted with the one made under essentially circumstances from (d) to the vocalism tape had advanced in the first of these situations, but not in the second, we should presumably have been able to generate the putative form &quot;aktibib&quot;, which does not exist.This can be excluded only if we assume that there is a transducer that disallows this sequence of events, or if the frames available for &quot;i&quot; are not the same as those for &quot;a&quot;.We are, in fact, making the latter assumption, on the grounds that the vowel &quot;i&quot; occurs only in the final position of Arabic verb stems. now, the forms in rows V of table I.In each of these, the middle consonant of the root is geminate in the surface.This is not a result of spreading as we have described it, spreading occurs with the last consonant of a root.If the prosodic template for row II is &quot;CVCCVC&quot;, how is that we do not get forms like &quot;katbab&quot; and &quot;kutbib&quot; beside the ones shown?This is a problem that is overcome in McCarthy's autosegmental account only at considerable cost.Indeed, is is a deficiency of that formalism that the only mechanisms available in it to account for gemination are as complex as they are, given how common the phenomenon is.Within the framework proposed here, gemination is provided for in a very natural way.Consider the following pair of frame schemata, in and arbitrary consonant: [c] [1 [1 First of these the one that was used for the consonants in the above example except in the situation for the first occurrence of &quot;b&quot;, where is was being spread into the final two consonantal positions of the form.The second frame differs from this is two respects.First, the prosodic template contains the hitherto unused symbol &quot;G&quot;. for &quot;geminate&quot;, and second, the root tape is not advanced.Suppose, now, that the the prosodic template for forms like &quot;kattab&quot; is not &quot;CVCCVC&quot;, but &quot;CVGCVC&quot;.It will be possible to discharge the &quot;G&quot; only if the root template does not advance, so that the following &quot;C&quot; in the template can only cause the same consonant to be into the word time.The sequence &quot;GC&quot; in a prosodic template is therefore an idiom for consonant gemination.7 Needless to say, McCarthy's work, on which this paper is based, is not interesting simply for the fact that he is able to achieve an adequate description of the data in table I, but also for the claims he makes about the way that account extends to a wider class of phenomena, thus achieving a measure of explanatory power.In particular, he claims that it extends to roots with two and four consonants.Consider, in particular, the following sets of forms: ktanbab dhanraj kattab dahraj takattab tadahraj Those in the second column are based on the root /dhrj/.In the first column are the corresponding forms of /ktb/.The similarity in the sets of corresponding forms is unmistakable.They exhibit the same patterns of consonants and vowels, differing only in that, whereas some consonant appears twice in the forms in column one, the consonantal slots are all occupied by different segments in the forms on the right.For these purposes, the &quot;n&quot; of the first pair of forms should be ignored since it is contributed by the prosodic template, and not by the root. consonantal slot in the prosodic template only in the case of the shorter form.The structure of the second and third forms is equally straighforward, but it is less easy to see how our machinery could account for them.Once again, the template calls for four root consonants and, where only three are provided, one must do double duty.But in this case, the effect is achieved through gemination rather than spreading so that the gemination mechanism just outlined is presumably in play.That mechanism makes no provision for gemination to be invoked only when needed to fill slots in the prosodic template that would otherwise remain empty.If the mechanism were as just described, and the triliteral forms were &quot;CVGCVC&quot; and &quot;tVCVGCVC&quot; respectively, then the quadriliteral forms would have to be generated on a different base.It is in cases like this, of which there in fact many, that the finite-state transducers play a substantive role.What is required in this case is a transducer that allows the root tape to remain stationary while the template tape moves over a &quot;G&quot;, provided no spreading will be allowed to occur later to fill consonantal slots that would Fig.1 a triliteral and a quadriliteral root, otherwise he unclaimed. extra consonants spread not geminate G-geminate 0-simple no spread no spread no spread the first pair are exactly as one would expect—the final root consonant is spread to fill the final required, then the first priority must he to let them occupy the slots marked with a &quot;0&quot; in the 8 template.Fig.1 shows a schema for the transition diagram of a transducer that has this effect.I call it a &quot;schema&quot; only because each of the edges shown does duty for a number of actual transitions.The machine begins in the &quot;start&quot; state and continues to return there so long as no frame is encountered involving a &quot;G&quot; on the template tape.A &quot;G&quot; transition causes a nondeterministic choice.If the root tape moves at the same time as the &quot;G&quot; is scanned, the transducer goes into its &quot;no-spread&quot; state, to which it continues to return so long as every move over a &quot;C&quot; on the prosodic tape is accompanied by a move over a consonant on the root tape.In other words, it must be possible to complete the process without spreading consonants.The other alternative is that the transducer should enter the &quot;geminate&quot; state over a transition over a in the template with the root tape remaining stationary.The transitions at the &quot;geminate&quot; state allow both spreading and nonspreading transitions.In summary, spreading can occur only if the transducer never leaves the &quot;start&quot; state and there is no &quot;G&quot; in the template, or there is a &quot;G&quot; on the template which does not trigger gemination.A &quot;G&quot; can fail to trigger gemination only when the root contains enough consonants to fill all the requirements that the template makes for them.One quadriliteral case remains to be accounted for, namely the following: ktaabab dharjaj According to the strategy just elaborated, we should have expected the quadriliteral form to been &quot;dhaaraj&quot;.But, this form contains a slot that is used for vowel lengthening with triliteral roots, and as consonantal position for quadriliterals.We must therefore presumably take it that the prosodic template for this form is something like &quot;CCVXCVC&quot; where &quot;X&quot; is a segment, but not specified as either vocalic or consonantal.This much is in line with the proposal that McCarthy himself makes The question is, when should be filled by a vowel, and when by a consonant?The data in Table I is, of course, insufficient to answer question. but a plausible answer that strongly suggests itself is that the &quot;X&quot; slot prefers a consonantal filler that would result in gemination.If this is true, then it is another case where the notion of gemination, though not actually exemplified in the form, plays a central role.Supposing that the analysis is correct, the next question is, how is it to be implemented.The most appealing answer would be to make &quot;X&quot; the exact obverse of &quot;G&quot;, when filled with a consonant.In other words, when a root consonant fills such a slot, the root tape must advance so that the same consonant will no longer be to fill the next position.The that the next root consonant would simply be a repetition of the current one would be excluded if we were to take over from autosegmental phonology and morphology, some version of th Contour Principle (OCP) 1979) which disallows repeated segments except in the prosodic template and in the surface string.McCarthy points out the roots like /smnn/, which appear to violate the OCP can invariably be reanalyzed as biliteral roots like /sm/ and, if this is done, our analysis, like his, goes through.The OCP does seem likely to cause some trouble when we come to treat one of the principal remaining problems, namely that of the forms in row I of table I.It turns out that the vowel that appears in the second syllable of these forms is not provided by the vocalism, but by the root.The vowel that appears in the perfect is generally different from the one that appears in the imperfect, and four different pairs are possible. pair that is used with given root is an idiosyncratic property of that root.One possibility is, therefore, that we treat the traditional triliteral roots as consisting not simply of three consonants, but as three consonants with a vowel intervening between the second and third, for a total of four segments.This flies in the face of traditional wisdom.It also counter to one of the intuitions autosegmental phonology which would have it that particular phonological features can be represented on at most one lexical tier, or tape.The intuition is that these tiers or tapes each contain a record or a particular kind of 9 gesture; the hearer's point of view, it is as though they contained a record of the signal received from a receptor that was attuned only to certain features.If we wish to maintain this model, there are presumably two alternatives open to us.Both involve assuming that roots are represented on at least two tapes in parallel, with the consonants separate from the vowel.According to one alternative, the root vowel would be written on the same tape as the vocalism; according to the other, it would be on a tape of its own.Unfortunately, neither alternative makes for a particularly happy solution.No problem arises from the proposal that a given morpheme should, in general, be represented on more than one lexical tape.However, the idea that the vocalic material associated with a root should appear on a special tape, reserved for it alone, breaks the clean lines of the system as so far presented in two ways.First, it spearates material onto two tapes, specifically the new one and the vocalism, on purely lexical grounds, having nothing to do with their phonetic or phonological constitution, and this runs counter to the idea of tapes as records of activity on phonetically specialized receptors.It is also at least slightly troublesome in that that newly introduced tape fills no function except in the generation of the first row of the table.Neither of these arguments is conclusive, and they could diminish considerably in force as a wider range of data was considered.Representing the vocalic contribution of the root on the same tape as the vacalism would avoid both of these objections, but would require that vocalic contribution to be recorded either before or after the vocalism itself.Since the root vowel affects the latter part of the root, it seems reasonable that it should be positioned to the right.Notice, however, that this is the only instance in which we have had to make any assumptions about the relative ordering of the morphemes that contribute to a stem.Once again, it may be possible to assemble further evidence reflecting on some such ordering, but I do not see it in these data.It is only right that I should point out the difficulty of accounting satisfactorily for the vocalic contribution of verbal roots.It is only right that I should also point out that the autosegmental solution fares no better on this score, resorting, as it must, to rules that access essentially non-phonological properties of the morphemes involved.By insisting that what I called the a morpheme should by, by definition, be its only contribution to phonological processes, I have cut myself off from such ex machina.Linguists in general, and computational linguists in particular, do well to employ finite-state devices wherever possible.They are theoretically appealing because they are computational weak and best understood from a mathematical point of view.They are computationally appealing because they make for simple, elegant, and highly efficient implementaions.In this paper, I hope I have shown how they can be applied to a problem in nonconcatenative morphology which seems initially to require heavier machinary.
The paper proposes a Constrained Entity Alignment F-Measure (CEAF) for evaluatingcoreference resolution.The metric is com puted by aligning reference and system entities (or coreference chains) with the constraint that a system (reference) entity is aligned with at most one reference (system) entity.We show that the best alignment is a maximum bipartite matching problem which can be solved by theKuhn-Munkres algorithm.Comparative experiments are conducted to show that the widely known MUC F-measure has serious flaws in evaluating a coreference system.The proposed metric is also compared with the ACE-Value, the official evaluation metric in the AutomaticContent Extraction (ACE) task, and we con clude that the proposed metric possesses someproperties such as symmetry and better inter pretability missing in the ACE-Value.
Consumers are often forced to wade through many on-line reviews inorder to make an informed prod uct choice.This paper introducesOPINE, an unsupervised informationextraction system which mines reviews in order to build a model of important product features, their evalu ation by reviewers, and their relative quality across products.Compared to previous work, OPINE achieves 22% higher precision (with only 3% lower recall) on the feature extraction task.OPINE?s novel use ofrelaxation labeling for finding the semantic orientation of words in con text leads to strong performance on the tasks of finding opinion phrases and their polarity.
This study reports the results of using minimum description length (MDL) analysis to model unsupervised learning of the morphological segmentation of European languages, using corpora ranging in size from 5,000 words to 500,000 words.We develop a set of heuristics that rapidly develop a probabilistic morphological grammar, and use MDL as our primary tool to determine whether the modifications proposed by the heuristics will be adopted or not.The resulting grammar matches well the analysis that would be developed by a human morphologist.In the final section, we discuss the relationship of this style of MDL grammatical analysis to the notion of evaluation metric in early generative grammar.
M. University of Pennsylvania This article documents a large set of heretofore unpublished details Collins used in his parser, such that, along with Collins’ (1999) thesis, this article contains all information necessary to duplicate Collins’ benchmark results.Indeed, these as-yet-unpublished details account for an 11% relative increase in error from an implementation including all details to a clean-room implementation of Collins’ model.We also show a cleaner and equally well-performing method for the handling of punctuation and conjunction and reveal certain other probabilistic oddities about Collins’ parser.We not only analyze the effect of the unpublished details, but also reanalyze the effect of certain well-known details, revealing that bilexical dependencies are barely used by the model and that head choice is not nearly as important to overall parsing performance as once thought.Finally, we perform experiments that show that the true discriminative power of lexicalization appears to lie in the fact that unlexicalized syntactic structures are generated conditioning on the headword and its part of speech.
This article considers approaches which rerank the output of an existing probabilistic parser.The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses.A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence.The strength of our approach is that it allows a tree to be represented as an arbitrary set offeatures, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account.We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998).We apply the boosting method to parsing the Wall Street Journal treebank.The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the model.The new model achieved 89.75% a 13% relative decrease in measure error over the baseline model’s score of 88.2%.The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data.Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach.We argue that the method is an appealing alternative—in terms of both simplicity and efficiency—to work on feature selection methods within log-linear (maximum-entropy) models.Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation.
The first issue to consider is what the analysis will be used for and what constraints this places on its form.The corpus analysis literature contains a variety of proposals, ranging from part-of-speech tagging to assignment of a unique, sophisticated syntactic analysis.Our eventual goal is to recover a semantically and pragmatically appropriate syntactic analysis capable of supporting semantic interpretation.Two stringent requirements follow immediately: firstly, the analyses assigned must determinately represent the syntactic relations that hold between all constituents in the input; secondly, they be drawn from an priori well-formed set of possible syntactic analyses (such as the set defined by a generative grammar).Otherwise, semantic interpretation of the resultant analyses cannot be guaranteed to be (structurally) unambiguous, and the semantic operations defined (over syntactic configurations) cannot be guaranteed to match and yield an interpretation.These requirements immediately suggest that approaches that recover only lexical tags (e.g. de Rose 1988) or a syntactic analysis that is the 'closest fit' to some previously defined set of possible analyses (e.g.Sampson, Haigh, and Atwell 1989), are inadequate (taken alone).Pioneering approaches to corpus analysis proceeded on the assumption that computationally tractable generative grammars of sufficiently general coverage could not be developed (see, for example, papers in Garside, Leech, and Sampson 1987).However, the development of wide-coverage declarative and computationally tractable grammars makes this assumption questionable.For example, the ANLT word and sentence grammar (Grover et al. 1989; Carroll and Grover 1989) consists of an English lexicon of approximately 40,000 lexemes and a 'compiled' fixed-arity term unification grammar containing around 700 phrase structure rules.Taylor, Grover, and Briscoe (1989) demonstrate that an earlier version of this grammar was capable of assigning the correct analysis to 96.8% of a corpus of 10,000 noun phrases extracted (without regard for their internal form) from a variety of corpora.However, although Taylor, and show that the ANLT grammar very wide coverage, they abstract away from issues of lexical idiosyncrasy by formimg equivalence classes of noun phrases and parsing a single token of each class, and they do not address the issues of 1) tuning a grammar to a particular corpus or sublanguage 2) selecting the correct analysis from the set licensed by the grammar and 3) providing reliable analyses of input outside the coverage of the grammar.Firstly, it is clear that vocabulary, idiom, and conventionalized constructions used in, say, legal language and dictionary definitions, will differ both in terms of the range and frequency of words and constructions deployed.Secondly, Church and Patil (1982) demonstrate that for a realistic grammar parsing realistic input, the set of possible analyses licensed by the grammar can be in the thousands.Finally, it is extremely unlikely that any generative grammar will ever be capable of correctly analyzing all naturally occurring input, even when tuned for a particular corpus or sublanguage (if only because of the synchronic idealization implicit in the assumption that the set of grammatical sentences of a language is well formed.) this paper, we describe our to the first and second problems and make some preliminary remarks concerning the third (far harder) problem.Our apto grammar tuning is based on a semi-automatic parsing phase which additions to the grammar are made manually and statistical information concerning the frequency of use of grammar rules is acquired.Using this statistical information and modified grammar, a breadth-first probabilistic parser is constructed.The latter is capable of ranking the possible parses identified by the grammar in a useful (and efficient) manner.However, (unseen) sentences whose correct analysis is outside the coverage of the grammar reri:ain a problem.The feasibility and usefulness of our approach has been investigated in a preliminary way by analyzing a small corpus of 26 Ted Briscoe and John Carroll Generalized Probabilistic LR Parsing definitions drawn from the Dictionary of Contemporary English (Procter 1978).This corpus was chosen because the vocabulary employed is restricted (to approximately 2,000 morphemes), average definition length is about 10 words (with a maximum of around 30), and each definition is independent, allowing us to ignore phenomena such as ellipsis.In addition, the language of definitions represents a recognizable sublanguage, allowing us to explore the task of tuning a general purpose grammar.The results reported below suggest that probabilistic information concerning the frequency of occurrence of syntactic rules correlates in a useful (though not absolute) way with the semantically and pragmatically most plausible analysis.In Section 2, we briefly review extant work on probabilistic approaches to corpus analysis and parsing and argue the need for a more refined probabilistic model to distinguish distinct derivations.Section 3 discusses work on LR parsing of natural language and presents our technique for automatic construction of LR parsers for unification-based grammars.Section 4 presents the method and results for constructing a LALR(1) parse table for the ANLT grammar and discusses these in the light of both computational complexity and other empirical results concerning parse table size and construction time.Section 5 motivates our interactive and incremental approach to semi-automatic production of a disambiguated training corpus and describes the variant of the LR parser used for this task.Section 6 describes our implementation of a breadth-first LR parser and compares its performance empirically to a highly optimized chart parser for the same grammar, suggesting that (optimized) LR parsing is more efficient in practice for the ANLT grammar despite exponential worst case complexity results.Section 7 explains the technique we employ for deriving a probabilistic version of the LR parse table from the training corpus, and demonstrates that this leads to a more refined and parse-context—dependent probabilistic model capable of distinguishing derivations that in a probabilistic context-free model would be equally probable.Section 8 describes and presents the results of our first experiment parsing LDOCE noun definitions, and Section 9 draws some preliminary conclusions and outlines ways in which the work described should be modified and extended.2.Probabilistic Approaches to Parsing In the field of speech recognition, statistical techniques based on hidden Markov mod
In this paper we present some experiments on the use of a probabilistic model to tag English text, i.e. to assign to each word the correct tag (part of speech) in the context of the sentence.The main novelty of these experiments is the use of untagged text in the training of the model.We have used a simple triclass Markov model and are looking for the best way to estimate the parameters of this model, depending on the kind and amount of training data provided.Two approaches in particular are compared and combined: • using text that has been tagged by hand and computing relative frequency counts, • using text without tags and training the model as a hidden Markov process, according to a Maximum Likelihood principle.Experiments show that the best training is obtained by using as much tagged text as possible.They also show that Maximum Likelihood training, the procedure that is routinely used to estimate hidden Markov models parameters from training data, will not necessarily improve the tagging accuracy.In fact, it will generally degrade this accuracy, except when only a limited amount of hand-tagged text is available.
It is challenging to translate names and technical terms across languages with different alphabets and sound inventories.These items are commonly transliterated, i.e., replaced with approximate phonetic equivalents.For example, &quot;computer&quot; in English comes out as &quot;konpyuutaa&quot; in Japanese.Translating such items from Japanese back to English is even more challenging, and of practical interest, as transliterated items make up the bulk of text phrases not found in bilingual dictionaries.We describe and evaluate a method for performing backwards transliterations by machine.This method uses a generative model, incorporating several distinct stages in the transliteration process.
Semantic inference is a key component for advanced natural language understanding.However, existing collections of automatically acquired inference rules have shown disappointing results when used in applications such as textual entailment and question answering.This papresents a collection of methods for automatically learning admissible argument values to which an inference rule be applied, which we call and methods for filtering out incorrect inferences.We present empirical evidence of its effectiveness.
We present a novel deterministic dependency parsing algorithm that attempts to create the easiest arcs in the dependency structure first in a non-directional manner.Traditional deterministic parsing algorithms are based on a shift-reduce framework: they traverse the sentence from left-to-right and, at each step, perform one of a possible set of actions, until a complete tree is built.A drawback of this approach is that it is extremely local: while decisions can be based on complex structures on the left, they can look only at a few words to the right.In contrast, our algorithm builds a dependency tree by iteratively selecting the best pair of neighbours to connect at each parsing step.This allows incorporation of features from already built structures both to the left and to the right of the attachment point.The parser learns both the attachment preferences and the order in which they should be performed.The result is a determinbest-first, which is significantly more accurate than best-first transition based parsers, and nears the performance of globally optimized parsing models.
Continuous space language models have recently demonstrated outstanding results across a variety of tasks.In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights.We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset.This allows vector-oriented reasoning based on the offsets between words.For example, the male/female relationship is automatically learned, and with the induced vector representations, “King - Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions.We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions.Remarkably, this method outperforms the best previous systems.
Broad-coverage corpora annotated with semantic role, or argument structure, information are becoming available for the first time.Statistical systems have been trained to automatically label semantic roles from the output of statistical parsers on unannotated text.In this paper, we quantify the effect of parser accuracy on these systems' performance, and examine the question of whether a flatter &quot;chunked&quot; representation of the input can be as effective for the purposes of semantic role identification.
This paper compares a number of generative probability models for a widecoverage Combinatory Categorial Gram mar (CCG) parser.These models are trained and tested on a corpus obtained by translating the Penn Treebank trees into CCG normal-form derivations.According to an evaluation of unlabeled word-word dependencies, our best model achieves a performance of 89.9%, comparable to thefigures given by Collins (1999) for a lin guistically less expressive grammar.Incontrast to Gildea (2001), we find a significant improvement from modeling word word dependencies.
This paper describes algorithms which rerank the top N hypotheses from a maximum-entropy tagger, the application being the recovery of named-entity boundaries in a corpus of web data.The first approach uses a boosting algorithm for ranking problems.The second approach uses the voted perceptron algorithm.Both algorithms give comparable, significant improvements over the maximum-entropy baseline.The voted perceptron algorithm can be considerably more efficient to train, at some cost in computation on test examples.
In this paper we propose a competition learning approach to coreference resolution.Traditionally, supervised machine learning approaches adopt the singlecandidate model.Nevertheless the preference relationship between the antecedent candidates cannot be determined accurately in this model.By contrast, our approach adopts a twin-candidate learning model.Such a model can present the competition criterion for antecedent candidates reliably, and ensure that the most preferred candidate is selected.Furthermore, our approach applies a candidate filter to reduce the computational cost and data noises during training and resolution.The experimental results on MUC-6 and MUC-7 data set show that our approach can outperform those based on the singlecandidate model.
We present a domain-independent topic segmentation algorithm for multi-party speech.Our feature-based algorithm comknowledge about a text-based algorithm as a feature and linguistic and acoustic cues about topic shifts extracted from speech.This segmentation algorithm uses automatically induced decision rules to combine the different features.The embedded text-based algorithm builds on lexical cohesion and has performance comparable to state-of-the-art algorithms based on lexical information.A significant error reduction is obtained by combining the two knowledge sources.
In this paper we have designed and experimented novel convolution kernels for automatic classification of predicate arguments.Their main property is the ability to process structured representations.Support Vector Machines (SVMs), using a combination of such kernels and the flat feature kernel, classify Prop- Bank predicate arguments with accuracy higher the current argument classification state- Additionally, experiments on FrameNet data have shown that SVMs are appealing for the classification of semantic roles even if the proposed kernels do not produce any improvement.
In this paper, we propose a multi-criteriabased active learning approach and effectively apply it to named entity recognition.Active learning targets to minimize the human annotation efforts by selecting examples for labeling.To maximize the contribution of the selected examples, we the multiple criteria: informativepropose measures to quantify them.More comprehensively, we incorporate all the criteria using two selection strategies, both of which result in less labeling cost than single-criterion-based method.The results of the named entity recognition in both MUC-6 and GENIA show that the labeling cost can be reduced by at least 80% without degrading the performance.
We describe a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation.This method requires a source-language dependency parser, target language word segmentation and an unsupervised word alignment component.We align a parallel corpus, project the source dependency parse onto the target sentence, extract dependency treelet translation pairs, and train a tree-based ordering model.We describe an efficient decoder and show that using these treebased models in combination with conventional SMT models provides a promising approach that incorporates the power of phrasal SMT with the linguistic generality available in a parser.
Entity relation detection is a form of information extraction that finds predefined relations between pairs of entities in text.This paper describes a relation detection approach that combines clues from different levels of syntactic processing using kernel methods.Information from three different levels of processing is considered: tokenization, sentence parsing and deep dependency analysis.Each source of information is represented by kernel functions.Then composite kernels are developed to integrate and extend individual kernels so that processing errors occurring at one level can be overcome by information from other levels.We present an evaluation of these methods on the 2004 ACE relation detection task, using Support Vector Machines, and show that each level of syntactic processing contributes useful information for this task.When evaluated on the official test data, our approach produced very competitive ACE value scores.We also compare the SVM with KNN on different kernels.
We present an approach to using a morphological analyzer for tokenizing and morphologically tagging (including partof-speech tagging) Arabic words in one process.We learn classifiers for individual morphological features, as well as ways of using these classifiers to choose among entries from the output of the analyzer.We obtain accuracy rates on all tasks in the high nineties.
Developing better methods for segmenting continuous text into words is important for improving the processing of Asian languages, and may shed light on how humans learn to segment speech.We propose two new Bayesian word segmentation methods that assume unigram and bigram models of word dependencies respectively.The bigram model greatly outperforms the unigram model (and previous probabilistic models), demonstrating the importance of such dependencies for word segmentation.We also show that previous probabilistic models rely crucially on suboptimal search procedures.
We propose a novel algorithm for inducing semantic taxonomies.Previous algorithms for taxonomy induction have typically focused on independent classifiers for discovering new single relationships based on hand-constructed or automatically discovered textual patterns.By contrast, our algorithm flexibly incorporates evidence from multiple classifiers over heterogenous relationships to optimize the entire structure of the taxonomy, using knowledge of a word’s coordinate terms to help in determining its hypernyms, and vice versa.We apply our algorithm on the problem of sense-disambiguated noun hyponym acquisition, where we combine the predictions of hypernym and coordinate term classifiers with the knowledge in a preexisting semantaxonomy (WordNet 2.1).We add to WordNet 2.1 at a relaerror reduction of a non-joint algorithm using the same component classifiers.Finally, we show that a taxonomy built using our algorithm shows a 23% relative F-score improvement over WordNet 2.1 on an independent testset of hypernym pairs.
We present algorithms for higher-order dependency parsing that are “third-order” in the sense that they can evaluate substructures containing three dependencies, and “efficient” in the sense that they reonly Importantly, our new parsers can utilize both sibling-style and grandchild-style interactions.We evaluate our parsers on the Penn Treebank and Prague Dependency Treebank, achieving unlabeled attachment scores of 93.04% and 87.38%, respectively.
Conditional Random Fields (CRFs) are a widely-used approach for supervised sequence labelling, notably due to their ability to handle large description spaces and to integrate structural dependency between labels.Even for the simple linearchain model, taking structure into account implies a number of parameters and a computational effort that grows quadratically with the cardinality of the label set.In this paper, we address the issue of training very large CRFs, containing up to hundreds output labels and several billion features.Efficiency stems here from the sparinduced by the use of a term.Based on our own implementation, we compare three recent proposals for implementing this regularization strategy.Our experiments demonstrate that very large CRFs can be trained efficiently and that very large models are able to improve the accuracy, while delivering compact parameter sets.
The research focus of computational coreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade.This paper surveys the major milestones in supervised coreference research since its inception fifteen years ago.
We describe our experience with automatic alignment of sentences in parallel English-Chinese texts.Our report concerns three related topics: (1) progress on the HKUST English-Chinese Parallel Bilingual Corpus; (2) experiments addressing applicability of Gale (1991) lengthbased statistical method to the task of alignment involving a non-Indo-European language; and (3) an improved statistical method that also incorporates domain-specific lexical cues.
We present a natural language interface system which is based entirely on trained statistical models.The system consists of three stages of processing: parsing, semantic interpretation, and discourse.Each of these stages is modeled as a statistical process.The models are fully integrated, resulting in an end-to-end system that maps input utterances into meaning representation frames.
Most previous corpus-based algorithms disambiguate a word with a classifier trained from previous usages of the same word.Separate classifiers have to be trained for different words.We present an algorithm that uses the same knowledge sources to disambiguate different words.The algorithm does not require a sense-tagged corpus and exploits the fact that two different words are likely to have similar meanings if they occur in identical local contexts.
This paper considers statistical parsing of Czech, which differs radically from English in at least two (1) it is a inflected and it has relatively word order. differences are likely to pose new problems for techniques that have been developed on English.We describe our experience in building on the parsing model of (Collins 97).Our final results — 80% dependency accuracy — represent good progress towards the 91% accuracy of the parser on English (Wall Street Journal) text.
We discuss two named-entity recognition models which use characters and character -grams either exclusively or as an important part of their data representation.The first model is a character-level HMM with minimal context information, and the second model is a maximum-entropy conditional markov model with substantially richer context features.Our best model achieves an overall F of 86.07% on the English test data (92.31% on the development data).This number represents a 25% error reduction over the same model without word-internal (substring) features.
We explore the use of speculative language in MEDLINE abstracts.Results from a manual annotation experiment suggest that the notion of speculative sentence can be reliably annotated by humans.In addition, an experiment with automated methods also suggest that reliable automated methods might also be developed.Distributional observations are also presented as well as a discussion of possible uses for a system that can recognize speculative language.
We investigate why weights from generative models underperform heuristic estimates in phrasebased machine translation.We first propose a simple generative, phrase-based model and verify that its estimates are inferior to those given by surface statistics.The performance gap stems primarily from the addition of a hidden segmentation variable, which increases the capacity for overfitting during maximum likelihood training with EM.In particular, while word level models benefit greatly from re-estimation, phrase-level models do not: the crucial difference is that distinct word alignments cannot all be correct, while distinct segmentations can.Alternate segmentations rather than alternate alignments compete, resulting in increased determinization of the phrase table, decreased generalization, and decreased final BLEU score.We also show that interpolation of the two methods can result in a modest increase in BLEU score.
We present discriminative reordering models for phrase-based statistical machine translation.The models are trained using the maximum entropy principle.We use several types of features: based on words, based on word classes, based on the local context.We evaluate the overall performance of the reordering models as well as the contribution of the individual feature types on a word-aligned corpus.Additionally, we show improved translation performance using these reordering models compared to a state-of-the-art baseline system.
SD translation schema (synchronous grammar) (string relation) A syntax-directed translator first parses the source-language input into a parsetree, and then recursively converts the tree into a string in the target-language.We model this conversion by an extended treeto-string transducer that have multi-level trees on the source-side, which gives our system more expressive power and flexibility.We also define a direct probability model and use a linear-time dynamic programming algorithm to search for the best derivation.The model is then extended to the general log-linear framework in order to rescore with other fealike language models.We devise a simple-yet-effective algorithm to non-duplicate translations rescoring.Initial experimental results on English-to-Chinese translation are presented.

In this paper we describe a methodologyfor detecting preposition errors in the writ ing of non-native English speakers.Our system performs at 84% precision andclose to 19% recall on a large set of stu dent essays.In addition, we address the problem of annotation and evaluation inthis domain by showing how current ap proaches of using only one rater can skew system evaluation.We present a sampling approach to circumvent some of the issuesthat complicate evaluation of error detec tion systems.
In this paper, we consider sentence sim plification as a special form of translation with the complex sentence as the source and the simple sentence as the target.We propose a Tree-based Simplification Model (TSM), which, to our knowledge, is the first statistical simplification model covering splitting, dropping, reorderingand substitution integrally.We also de scribe an efficient method to train our model with a large-scale parallel dataset obtained from the Wikipedia and Simple Wikipedia.The evaluation shows that our model achieves better readability scores than a set of baseline systems.
Automated identification of diverse sen timent types can be beneficial for manyNLP systems such as review summariza tion and public media analysis.In some ofthese systems there is an option of assign ing a sentiment value to a single sentence or a very short text.In this paper we propose a supervised sentiment classification framework whichis based on data from Twitter, a popu lar microblogging service.By utilizing50 Twitter tags and 15 smileys as sen timent labels, this framework avoids theneed for labor intensive manual annotation, allowing identification and classifi cation of diverse sentiment types of shorttexts.We evaluate the contribution of different feature types for sentiment classification and show that our framework successfully identifies sentiment types of untagged sentences.The quality of the senti ment identification was also confirmed byhuman judges.We also explore dependencies and overlap between different sen timent types represented by smileys and Twitter hashtags.
We present an algorithm for anaphora res- olutkm which is a modified and extended version of that developed by (Lappin and Leass,/994).In contrast to that work, our al- gorithm does not require in-depth, full, syn..tactic parsing of text.Instead, with minimal compromise in output quality, the modifica- tions enable the resolution process to work from tile output of a part of speech tag- ge~; enriched only with annotations of gram- matica\] functkm of lexical items in the in- put text stream.Evaluation of the results of our in-tplementation demonstrates that ac- curate anaphora resolution can be realized within natural anguage processing fl'ame- works which do not--~,)r cannot- employ ro- bust and rcqiable parsing components.
In this paper, we describe a new model for word alignment in statistical trans- lation and present experimental results.The idea of the model is to make the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions.To achieve this goal, the approach us- es a first-order Hidden Markov model (HMM) for the word alignment problem as they are used successfully in speech recognition for the time alignment prob- lem.The difference to the time align- ment HMM is that there is no monotony constraint for the possible word order- ings.We describe the details of the mod- el and test the model on several bilingual corpora.
The Conference on Computational Natural Language Learning features a shared task, inwhich participants train and test their learn ing systems on the same data sets.In 2007, as in 2006, the shared task has been devoted to dependency parsing, this year with both a multilingual track and a domain adaptation track.In this paper, we define the tasks of the different tracks and describe how the data sets were created from existing treebanks for ten languages.In addition, we characterize the different approaches of the participating systems, report the test results, and provide a first analysis of these results.
We formulate dependency parsing as a graphical model with the novel ingredient of global constraints.We show how to apply loopy belief propagation (BP), a simple and tool for and inference.As a parsing algorithm, BP is both asymptotically and empirically efficient.Even with second-order features or latent variables, which would make exact parsing considerslower or NP-hard, BP needs only with a small constant factor.Furthermore, such features significantly improve parse accuracy over exact first-order methods.Incorporating additional features would increase the runtime additively rather than multiplicatively.
Minimum-error-rate training (MERT) is a bottleneck for current development in statistical machine translation because it is limited in the number of weights it can reliably optimize.Building on the work of Watanabe et al., we explore the use of the MIRA algorithm of Crammer et al. as an alternative to MERT.We first show that by parallel processing and exploiting more of the parse forest, we can obtain results using MIRA that match or surpass MERT in terms of both translation quality and computational cost.We then test the method on two classes of features that address deficiencies in the Hiero hierarchical phrasebased model: first, we simultaneously train a large number of Marton and Resnik’s soft syntactic constraints, and, second, we introduce a novel structural distortion model.In both cases we obtain significant improvements in translation performance.Optimizing them in combination, for a total of 56 feature weights, improve performance by 2.6 a subset of the NIST 2006 Arabic-English evaluation data.
This paper describes a novel Bayesian approach to unsupervised topic segmentation.Unsupervised systems for this task are driven the tendency of wellformed segments to induce a compact and consistent lexical distribution.We show that lexical cohesion can be placed in a Bayesian context by modeling the words in each topic segment as draws from a multinomial language model associated with the segment; maximizing the observation likelihood in such a model yields a lexically-cohesive segmentation.This contrasts with previous approaches, which relied on hand-crafted cohesion metrics.The Bayesian framework provides a principled way to incorporate additional features such as cue phrases, a powerful indicator of discourse structure that has not been previously used in unsupervised segmentation systems.Our model yields consistent improvements over an array of state-of-the-art systems on both text and speech datasets.We also show that both an entropy-based analysis and a well-known previous technique can be de
Many statistical translation models can be regarded as weighted logical deduction.Under this paradigm, we use weights from the expectation semiring (Eisner, 2002), to compute first-order statistics (e.g., the expected hypothesis length or feature counts) over packed forests of translations (lattices or hypergraphs).We then introduce novel semiring, which computes second-order statistics (e.g., the variance of the hypothesis length or the gradient of entropy).This second-order semiring is essential for many interesting training paradigms such as minimum risk, deterministic annealing, active learning, and semi-supervised learning, where gradient descent optimization requires computing the gradient of entropy or risk.We use these semirings in an open-source machine translation toolkit, enabling minimum-risk training a benefit of up to 1.0
We address the creation of cross-lingual textual entailment corpora by means of crowdsourcing.Our goal is to define a cheap and replicable data collection methodology that minimizes the manual work done by expert annotators, without resorting to preprocessing tools or already annotated monolingual datasets.In line with recent works emphasizing the need of large-scale annotation efforts for textual entailment, our work aims to: the scarcity of data available to train evaluate systems, and the recourse to crowdsourcing as an effective way to reduce the costs of data collection without sacrificing quality.We show that a complex data creation task, for which even experts usually feature low agreement scores, can be effectively decomposed into simple subtasks assigned to non-expert annotators.The resulting dataset, obtained from a pipeline of different jobs routed to Amazon Mechanical Turk, contains more than 1,600 aligned pairs for each combination of texts-hypotheses in English, Italian and German.
People tweet more than 100 Million times daily, yielding a noisy, informal, but sometimes informative corpus of 140-character messages that mirrors the zeitgeist in an unprecedented manner.The performance of standard NLP tools is severely degraded on tweets.This paper addresses this issue by re-building the NLP pipeline beginning with part-of-speech tagging, through chunking, to recognition.Our novel doubles compared with the NER system. the redundancy inherent in tweets to achieve this performance, using LabeledLDA to exploit Freebase dictionaries as a source of distant supervision.LabeledLDA outperforms coincreasing 25% over ten common entity types.NLP tools are available at:
Most current dependency parsers presuppose that input words have been morphologically disambiguated using a part-of-speech tagger before parsing begins.We present a transitionbased system for joint part-of-speech tagging and labeled dependency parsing with nonprojective trees.Experimental evaluation on Chinese, Czech, English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of-theart results for all languages.
We present a model for sentence compression that uses a discriminative largemargin learning framework coupled with a novel feature set defined on compressed bigrams as well as deep syntactic representations provided by auxiliary dependency and phrase-structure parsers.The parsers are trained out-of-domain and contain a significant amount of noise.We argue that the discriminative nature of the learning algorithm allows the model to learn weights relative to any noise in the feature set to optimize compression accuracy directly.This differs from current state-of-the-art models (Knight and Marcu, 2000) that treat noisy parse trees, for both compressed and uncompressed sentences, as gold standard when calculating model parameters.
In this paper we propose a new graphbased method that uses the knowledge in a LKB (based on WordNet) in order to perform unsupervised Word Sense Disambiguation.Our algorithm uses the full graph of the LKB efficiently, performing better than previous approaches in English all-words datasets.We also show that the algorithm can be easily ported to other languages with good results, with the only requirement of having a wordnet.In addition, we make an analysis of the performance of the algorithm, showing that it is efficient and that it could be tuned to be faster.
In statistical natural language processing we always face the problem of sparse data.One way to reduce this problem is to group words into equivalence classes which is a standard method in statistical language modeling.In this paper we describe a method to determine bilingual word classes suitable for statistical machine translation.We develop an optimization criterion based on a maximumlikelihood approach and describe a clustering algorithm.We will show that the usage of the bilingual word classes we get can improve statistical machine translation.
We formalize weighted dependency pars ing as searching for maximum spanning trees (MSTs) in directed graphs.Using this representation, the parsing algorithmof Eisner (1996) is sufficient for search ing over all projective trees in O(n3) time.More surprisingly, the representation isextended naturally to non-projective pars ing using Chu-Liu-Edmonds (Chu andLiu, 1965; Edmonds, 1967) MST algorithm, yielding an O(n2) parsing al gorithm.We evaluate these methodson the Prague Dependency Treebank using online large-margin learning tech niques (Crammer et al, 2003; McDonald et al, 2005) and show that MST parsingincreases efficiency and accuracy for lan guages with non-projective dependencies.
Linguistically annotated corpus based on texts in biomedical domain has been constructed to tune natural language processing (NLP) tools for bio textmining.As the focus of information extraction is shifting from "nominal" information such as named entity to "verbal" information such as function and interaction of substances, applica tion of parsers has become one of the key technologies and thus the corpus annotated for syntactic structure of sen tences is in demand.A subset of the GENIA corpus consisting of 500 MEDLINE abstracts has been annotated for syntactic structure in an XML based format based on Penn Treebank II (PTB) scheme.Inter-annotator agreement test indicated that the writ ing style rather than the contents of the research abstracts is the source of the difficulty in tree annotation, and that annotation can be stably done by linguists without much knowledge of bi ology with appropriate guidelines regarding to linguistic phenomena par ticular to scientific texts.
In statistical machine translation, correspondences between the words in the source and the target language are learned from parallel corpora, and often little or no linguistic knowledge is used to structure the underlying models.In particular, existing statistical systems for machine translation often treat different inflectedforms of the same lemma as if they were independent ofone another.The bilingual training data can be better exploited by explicitly taking into account the interdependencies of related inflected forms.We propose the construction of hierarchical lexicon models on the basis of equivalence classes of words.In addition, we introduce sentence-level restructuring transformations which aim at the assimilation of word order in related sentences.We have systematically investigated the amount of bilingual training data required to maintain an acceptable quality of machine translation.The combination of the suggested methods for improving translation quality in frameworks with scarce resources has been successfully tested: We were able to reduce the amount of bilingual training data to less than 10% of the original corpus, while losing only 1.6% in translation quality.The improvement of the translation results is demonstrated on two German-English corpora taken from the Uerbmobil task and the Nespole! task.
Many approaches to automatic sentiment analysis begin with a large lexicon of words marked with their prior polarity (also called semantic orientation).However, the contextual polarity of the phrase in which a particular instance of a word appears may be quite different from the word’s prior polarity.Positive words are used in phrases expressing negative sentiments, or vice versa.Also, quite often words that are positive or negative out of context are neutral in context, meaning they are not even being used to express a sentiment.The goal of this work is to automatically distinguish between prior and contextual polarity, with a focus on understanding which features are important for this task.Because an important aspect of the problem is identifying when polar terms are being used in neutral contexts, features for distinguishing between neutral and polar instances are evaluated, as well as features for distinguishing between positive and negative contextual polarity.The evaluation includes assessing the performance of features across multiple machine learning algorithms.For all learning algorithms except one, the combination of all features together gives the best performance.Another facet of the evaluation considers how the presence of neutral instances affects the performance offeatures for distinguishing between positive and negative polarity.These experiments show that the presence of neutral instances greatly degrades the performance of these features, and that perhaps the best way to improve performance across all polarity classes is to improve the system’s ability to identify when an instance is neutral.
The kinds of tree representations used in a treebank corpus can have a dramatic effect on performance of a parser based on the PCFG estimated from that corpus, causing the estimated likelihood of a tree to differ substantially from its frequency in the training corpus.This paper points out that the Penn II treebank representations are of the kind predicted to have such an effect, and describes a simple node relabeling transformation that improves a treebank PCFG-based parser's average precision and recall by around 8%, or approximately half of the performance difference between a simple PCFG model and the best broad-coverage parsers available today.This performance variation comes about because any PCFG, and hence the corpus of trees from which the PCFG is induced, embodies independence assumptions about the distribution of words and phrases.The particular independence assumptions implicit in a tree representation can be studied theoretically and investigated empirically by means of a tree transformation / detransformation process.
We describe a syntax-based algorithm that automatically builds Finite State Automata (word lattices) from semantically equivalent translation sets.These FSAs are good representations of paraphrases.They can be used to extract lexical and syntactic paraphrase pairs and to generate new, unseen sentences that express the same meaning as the sentences in the input sets.Our FSAs can also predict the correctness of alternative semantic renderings, which may be used to evaluate the quality of translations.
This paper describes the application of discriminative reranking techniques to the problem of machine translation.For each sentence in the source language, we obtain from a baseline statistical machine translation system, a ranked best list of candidate translations in the target language.We introduce two novel perceptroninspired reranking algorithms that improve on the quality of machine translation over the baseline system based on evaluation using the BLEU metric.We provide experimental results on the NIST 2003 Chinese-English large data track evaluation.We also provide theoretical analysis of our algorithms and experiments that verify that our algorithms provide state-of-theart performance in machine translation.
With the increasing use of research paper search engines, such as CiteSeer, for both literature search and hiring decisions, the accuracy of such systems is of paramount importance.This paper employs Conditional Random Fields (CRFs) for the task of extracting various common fields from the headers and citation of research papers.The basic theory of CRFs is becoming well-understood, but best-practices for applying them to real-world data requires additional exploration.This paper makes an empirical exploration of several factors, including variations on Gaussian, expoand priors for improved regularization, and several classes of features and Markov order.On a standard benchmark data set, we achieve new state-of-the-art performance, reducing error in average F1 by 36%, and word error rate by 78% in comparison with the previous best SVM results.Accuracy compares even more favorably against HMMs.
We present an exploration of generative probabilistic models for multi-document summarization.Beginning with a simple word frequency based model (Nenkova and Vanderwende, 2005), we construct a sequence of models each injecting more structure into the representation of document set content and exhibiting ROUGE gains along the way.Our model, utilizes a hierarchical LDA-style model (Blei et al., 2004) to represent content specificity as a hierarchy of topic vocabulary distributions.At the task of producing generic DUC-style summaries, state-of-the-art ROUGE performance and in pairwise user evaluation strongly outperforms Toutanova et al. (2007)’s state-of-the-art discriminative system.We explore capacity to produce multiple ‘topical summaries’ in order to facilitate content discovery and navigation.
A good decoding algorithm is critical to the success of any statistical machine translation system.The decoder’s job is to find the translation that is most likely according to set of previously learned parameters (and a formula for combining them).Since the space of possible translations is extremely large, typical decoding algorithms are only able to examine a portion of it, thus risking to miss good solutions.In this paper, we compare the speed and output quality of a traditional stack-based decoding algorithm with two new decoders: a fast greedy decoder and a slow but optimal decoder that treats decoding as an integer-programming optimization problem.
algebraic path problem (shortest paths; matrix inver- 34(3):191–219.Richard Sproat and Michael Riley.1996.Compilation of weighted finite-state transducers from decision trees. of the 34th Annual Meeting of the Andreas Stolcke and Stephen M. Omohundro.1994.Best-first model merging for hidden Markov model induction.Tech.Report ICSI TR-94-003, Berkeley, CA.Robert Endre Tarjan.1981a.A unified approach to path of the 28(3):577–593, July.Robert Endre Tarjan.1981b.Fast algorithms for solving problems. of the 28(3):594–614, July.G. van Noord and D. Gerdemann.2001.An extendible regular expression compiler for finite-state approaches natural language processing.In Impleno.22 in Springer Lecture Notes in CS.
peg,. pyscropyrssexa pa...Inn/bee(Esrey:,we6onee nocnemere)seressartba 43csrassartsmeuroto-ssoprbonourseLproronparserrna flaw) sror spe6yer — ,r1 Transducerloacleo a erkirlE.FOL.*I PD.021 fP21111“enwialitneI r Figure 2: Unicode text in Gate2 witnessed on the software scene with the emergence of Unicode as a universal standard for representing textual data.GATE supports multilingual data processing using Unicode as its default text encoding.It also provides a means of entering text in various languages, using virtual keyboards where the language is not supported by the underlying operating platform.(Note that although Java represents characters as Unicode, it doesn't support input in many of the languages covered by Unicode.)Currently 28 languages are supported, and more are planned for future releases.Because GATE is an open architecture, new virtual keyboards can be defined by the users and added to the system as needed.For displaying the text, GATE relies on the rendering facilities offered by the Java implementation for the platform it runs on.Figure 2 gives an example of text in various languages displayed by GATE.The ability to handle Unicode data, along with the separation between data and implementation, allows LE systems based on GATE to be ported to new languages with no additional overhead apart from the development of the resources needed for the specific language.These facilities have been developed as part of the EMILLE project (McEnery et al., 2000), which focuses on the construction a 63 million word electronic corpus of South Asian languages.3 Applications One of GATE's strengths is that it is flexible and robust enough to enable the development of a wide range of applications within its framework.In this section, we describe briefly some of the NLP applications we have developed using the GATE architecture.3.1 MUSE The MUSE system (Maynard et al., 2001) is a multi-purpose Named Entity recognition system which is capable of processing texts from widely different domains and genres, thereby aiming to reduce the need for costly and time-consuming adaptation of existing resources to new applications and domains.The system aims to identify the parameters relevant to the creation of a name recognition system across different types of variability such as changes in domain, genre and media.For example, less formal texts may not follow standard capitalisation, punctuation and spelling formats, which can be a problem for many generic NE systems.Current evaluations with this system average around 93% precision and 95% recall across a variety of text types.3.2 ACE The MUSE system has also been adapted to take part in the current ACE (Automatic Content Extraction) program run by NIST.This requires systems to perform recognition and tracking tasks of named, nominal and pronominal entities and their mentions across three types of clean news text (newswire, broadcast news and newspaper) and two types of degraded news text (OCR output and ASR output).3.3 MUMIS The MUMIS (MUltiMedia Indexing and Searching environment) system uses Information Extraction components developed within GATE to produce formal annotations about essential events in football video programme material.This IE system comprises versions of the tokenisation, sentence detection, POS-tagging, and semantic tagging modules developed as part of GATE's standard resources, but also includes morphological analysis, full syntactic parsing and discourse interpretation modules, thereby enabling the production of annotations over text encoding structural, lexical, syntactic and semantic information.The semantic tagging module currently achieves around 91% precision and 76% recall, a significant improvement on a baseline named entity recognition system evaluated against it.4 Processing Resources Provided with GATE is a set of reusable processing resources for common NLP tasks.(None of them are definitive, and the user can replace and/or extend them as necessary.)These are packaged together to form ANNIE, A Nearly- New IE system, but can also be used individually or coupled together with new modules in order to create new applications.For example, many other NLP tasks might require a sentence splitter and POS tagger, but would not necessarily require resources more specific to IE tasks such as a named entity transducer.The system is in use for a variety of IE and other tasks, sometimes in combination with other sets of application-specific modules.ANNIE consists of the following main processing resources: tokeniser, sentence splitter, POS tagger, gazetteer, finite state transducer (based on GATE's built-in regular expressions over annotations language (Cunningham et al., 2002)), orthomatcher and coreference resolver.The resources communicate via GATE's annotation API, which is a directed graph of arcs bearing arbitrary feature/value data, and nodes rooting this data into document content (in this case text). text into simple tokens, such as numbers, punctuation, symbols, and words of different types (e.g. with an initial capital, all upper case, etc.).The aim is to limit the work of the tokeniser to maximise efficiency, and enable greater flexibility by placing the burden of analysis on the grammars.This means that the tokeniser does not need to be modified for different applications or text types. splitter a cascade of finitestate transducers which segments the text into sentences.This module is required for the tagger.Both the splitter and tagger are domainand application-independent. a modified version of the Brill tagger, which produces a part-of-speech tag as an annotation on each word or symbol.Neither the splitter nor the tagger are a mandatory part of the NE system, but the annotations they produce can be used by the grammar (described below), in order to increase its power and coverage. of lists such as cities, organisations, days of the week, etc.It not only consists of entities, but also of names of useful as typical company designators (e.g.'Ltd:), titles, etc.The gazetteer lists are compiled into finite state machines, which can match text tokens. tagger of handcrafted rules written in the JAPE (Java Annotations Pattern Engine) language (Cunningham et al., 2002), which describe patterns to match and annotations to be created as a result.JAPE is a version of CPSL (Common Pattern Specification Language) (Appelt, 1996), which provides finite state transduction over annotations based on regular expressions.A JAPE grammar consists of a set of phases, each of which consists of a set of pattern/action rules, and which run sequentially.Patterns can be specified by describing a specific text string, or annotations previously created by modules such as the tokeniser, gazetteer, or document format analysis.Rule prioritisation (if activated) prevents multiple assignment of annotations to the same text string. another optional module for the IE system.Its primary objective is to perform co-reference, or entity tracking, by recognising relations between entities.It also has a secondary role in improving named entity recognition by assigning annotations to previously unclassified names, based on relations with existing entities. identity relations between entities in the text.For more details see (Dimitrov, 2002).4.1 Implementation The implementation of the processing resources is centred on robustness, usability and the clear distinction between declarative data representations and finite state algorithms The behaviour of all the processors is completely controlled by external resources such as grammars or rule sets, which makes them easily modifiable by users who do not need to be familiar with programming languages.The fact that all processing resources use finite-state transducer technology makes them quite performant in terms of execution times.Our initial experiments show that the full named entity recognition system is capable of processing around 2.5KB/s on a PITT 450 with 256 MB RAM (independently of the size of the input file; the processing requirement is linear in relation to the text size).Scalability was tested by running the ANNIE modules over a randomly chosen part of the British National Corpus (10% of all documents), which contained documents of up to 17MB in size.5 Language Resource Creation Since many NLP algorithms require annotated corpora for training, GATE's development environment provides easy-to-use and extendable facilities for text annotation.In order to test their usability in practice, we used these facilities to build corpora of named entity annotated texts for the MUSE, ACE, and MUMIS applications.The annotation can be done manually by the user or semi-automatically by running some processing resources over the corpus and then correcting/adding new annotations manually.Depending on the information that needs to be annotated, some ANNIE modules can be used or adapted to bootstrap the corpus annotation task.For example, users from the humanities created a gazetteer list with 18th century place names in London, which when supplied to the ANNIE gazetteer, allows the automatic annotation of place information in a large collection of 18th century court reports from the Old Bailey in London.Since manual annotation is a difficult and error-prone task, GATE tries to make it simple to use and yet keep it flexible.To add a new annotation, one selects the text with the mouse (e.g., &quot;Mr. Clever&quot;) and then clicks on the desired annotation type (e.g., Person), which is shown in the list of types on the right-handside of the document viewer (see Figure 1).If however the desired annotation type does not already appear there or the user wants to associate more detailed information with the annotation (not just its type), then an annotation editing dialogue can be used.6 Evaluation A vital part of any language engineering application is the evaluation of its performance, and a development environment for this purpose would not be complete without some mechanisms for its measurement in a large number of test cases.GATE contains two such mechanisms: an evaluation tool (AnnotationDiff) which enables automated performance measurement and visualisation of the results, and a benchmarking tool, which enables the tracking of a system's progress and regression testing.6.1 The AnnotationDiff Tool Gate's AnnotationDiff tool enables two sets of annotations on a document to be compared, in order to either compare a system-annotated text with a reference (hand-annotated) text, or to compare the output of two different versions of the system (or two different systems).For each annotation type, figures are generated for precision, recall, F-measure and false positives.The AnnotationDiff viewer displays the two sets of annotations, marked with different colours (similar to 'visual diff' implementations such as in the MKS Toolkit or TkDiff).Annotations in the key set have two possible colours depending on their state: white for annotations which have a compatible (or partially compatible) annotation in the response set, and orange for annotations which are missing in the response set.Annotations in the response set have three possible colours: green if they are compatible with the key annotation, blue if they Figure 3: Fragment of results from benchmark tool are partially compatible, and red if they are spurious.In the viewer, two annotations will be positioned on the same row if they are co-extensive, and on different rows if not.6.2 Benchmarking tool GATE's benchmarking tool differs from the AnnotationDiff in that it enables evaluation to be carried out over a whole corpus rather than a single document.It also enables tracking of the system's performance over time.The tool requires a clean version of a corpus (with no annotations) and an annotated reference corpus.First of all, the tool is run in generation mode to produce a set of texts annotated by the system.These texts are stored for future use.The tool can then be run in three ways: 1.Comparing the annotated set with the reference set; 2.Comparing the annotated set with the set produced by a more recent version of the system resources (the latest set); 3.Comparing the latest set with the reference set.In each case, performance statistics will be provided for each text in the set, and overall statistics for the entire set, in comparison with the reference set.In case 2, information is also provided about whether the figures have increased or decreased in comparison with the annotated set.The annotated set can be updated at any time by rerunning the tool in generation mode with the latest version of the system resources.Furthermore, the system can be run in verbose mode, where for each figure below a certain threshold (set by the user), the non-coextensive annotations (and their corresponding text) will be displayed.The output of the tool is written to an HTML file in tabular form, as shown in Figure 3.Current evaluations for the MUSE NE system are producing average figures of 90-95% Precision and Recall on a selection of different text types (spoken transcriptions, emails etc.).The default ANNIE system produces figures of between 80-90% Precision and Recall on news texts.This figure is lower than for the MUSE system, because the resources have not been tuned to a specific text type or application, but are intended to be adapted as necessary.Work on resolution of anaphora is currently averaging 63% Precision and 45% Recall, although this work is still very much in progress, and we expect these figures to improve in the near future.7 Related Work GATE draws from a large pool of previous work on infrastructures, architectures and development environments for representing and processing language resources, corpora, and annotations.Due to space limitations here we will discuss only a small subset.For a detailed review and its use for deriving the desiderata for this architecture see (Cunningham, 2000).Work on standard ways to deal with XML data is relevant here, such as the LT XML work at Edinburgh (Thompson and McKelvie, 1997), as is work on managing collections of documents and their formats, e.g.(Brugman et al., 1998; Grishman, 1997; Zajac, 1998).We have also drawn from work on representing information about text and speech, e.g.(Brugman et al., 1998; Mikheev and Finch, 1997; Zajac, 1998; Young et al., 1999), as well as annotation standards, such as the ATLAS project (an architecture for linguistic annotation) at LDC (Bird et kirlactunerkterripNerl ABC19980430.1830.0858.sgm Annotation tope., GPE RecallIncreaseon Atonaltmarked iron 06371426571426571la10 type Organization 1.0 increaseon hurnan-markedto 1 0 09444444444444444 Mug 07, limEll.ncreaseon tom0345to 07, 14153ING ANNOTATIONSIt To automatetetteABC Ir.NNOTATKMISinTo embroil,bath PARTIALLYCORRECT Pl4071&quot;ATIC*15nhe automateIDA, PratotationType Annotation type Person Precision increase on human-marked from 08947368421052632 lc 09444444444444444 03444444444444444 al., 2000).Our approach is also related to work on user interfaces to architectural facilities such as development environments, e.g.(Brugman et al., 1998) and to work on comparing different versions of information, e.g.(Sparck-Jones and Galliers, 1996; Paggio, 1998).This work is particularly novel in that it addresses the complete range of issues in NLP application development in a flexible and extensible way, rather than focusing just on some particular aspect of the development process.In addition, it promotes robustness, re-usability, and scalability as important principles that help with the construction of practical NLP systems.8 Conclusions In this paper we have described an infrastructure for language engineering software which aims to assist the develeopment of robust tools and resources for NLP.One future direction is the integration of processing resources which learn in the background while the user is annotating corpora in GATE's visual environment.Currently, statistical models can be integrated but need to be trained separately.We are also extending the system to handle language generation modules, in order to enable the construction of applications which require language production in addition to analysis, e.g. intelligent report generation from IE data.
We consider the task of unsupervised lecture segmentation.We formalize segmentation as a graph-partitioning task that optimizes the normalized cut criterion.Our approach moves beyond localized comparisons and takes into account longrange cohesion dependencies.Our results demonstrate that global analysis improves the segmentation accuracy and is robust in the presence of speech recognition errors.
We present a new approach to relation extraction that requires only a handful of training examples.Given a few pairs of named entities known to exhibit or not exhibit a particular relation, bags of sentences containing the pairs are extracted from the web.We extend an existing relation extraction method to handle this weaker form of supervision, and present experimental results demonstrating that our approach can reliably extract relations from web documents.
A central problem in grounded language acquisition is learning the correspondences between a rich world state and a stream of text which references that world state.To deal with the high degree of ambiguity present in this setting, we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state.We show that our model generalizes across three domains of increasing difficulty—Robocup sportscasting, weather forecasts (a new domain), and NFL recaps.
Paraphrase generation (PG) is important in plenty of NLP applications.However, the research of PG is far from enough.In this paper, we propose a novel method for statistical paraphrase generation (SPG), which can (1) achieve various applications based on a uniform statistical model, and (2) naturally combine multiple resources to enhance the PG performance.In our experiments, we use the proposed method to generate paraphrases for three different applications.The results show that the method can be easily transformed from one application to another and generate valuable and interesting paraphrases.
Modern models of relation extraction for tasks like ACE are based on supervised learning of relations from small hand-labeled corpora.We investigate an alternative paradigm that does not require labeled corpora, avoiding the domain dependence of ACEstyle algorithms, and allowing the use of corpora of any size.Our experiments use Freebase, a large semantic database of several thousand relations, to For each pair of entities that appears in some Freebase relation, we find all sentences containing those entities in a large unlabeled corpus and extract textual features to train a relation classifier.Our algorithm combines the advantages of supervised IE (combining 400,000 noisy pattern features in a probabilistic classifier) and unsupervised IE (extracting large numbers of relations from large corpora of any domain).Our model is able to extract 10,000 instances of 102 relations at a precision of 67.6%.We also analyze feature performance, showing that syntactic parse features are particularly helpful for relations that are ambiguous or lexically distant in their expression.
We address the problem of selecting nondomain-specific language model training data to build auxiliary language models for use in tasks such as machine translation.Our approach is based on comparing the cross-entropy, according to domainspecific and non-domain-specifc language models, for each sentence of the text source used to produce the latter language model.We show that this produces better language models, trained on less data, than both random data selection and two other previously proposed methods.
In this paper we present a formalization of the centering approach to modeling attentional structure in discourse and use it as the basis for an algorithm to track discourse context and bind pronouns.As described in [GJW86], the process of centering attention on entities in the discourse gives rise to the intersentential states of, retaining shiftpropose an extension to these states which handles some additional cases of multiple ambiguous pronouns.The algorithm has been implemented in an HPSG natural language system which serves as the interface to a database query application.
This paper describes an implemented program that takes a raw, untagged text corpus as its only input (no open-class dictionary) and generates a partial list of verbs occurring in the text and the subcategorization frames (SFs) in which they occur.Verbs are detected by a novel technique based on the Case Filter of Rouvret and Vergnaud (1980).The completeness of the output list increases monotonically with the total number of occurrences of each verb in the corpus.False positive rates are one to three percent of observations.Five SFs are currently detected and more are planned.Ultimately, I expect to provide a large SF dictionary to the NLP community and to train dictionaries for specific corpora.
In this paper we present a method to group adjectives according to their meaning, as a first step towards the automatic identification of adjectival scales.We discuss the properties of adjectival scales and of groups of semantically related adjectives and how they imply sources of linguistic knowledge in text corpora.We describe how our system exploits this linguistic knowledge to compute a measure of similarity between two adjectives, using statistical techniques and without having access to any semantic information about the adjectives.We also show how a clustering algorithm can use these similarities to produce the groups of adjectives, and we present results produced by our system for a sample set of adjectives.We conclude by presenting evaluation methods for the task at hand, and analyzing the significance of the results obtained.
This paper presents a trainable rule-based algorithm for performing word segmentation.The algorithm provides a simple, language-independent alternative to large-scale lexical-based segmenters requiring large amounts of knowledge engineering.As a stand-alone segmenter, we show our algorithm to produce high performance Chinese segmentation.In addition, we show the transformation-based algorithm to be effective in improving the output of several existing word segmentation algorithms in three different languages.
We are developing corpus-based techniques for identifying semantic relations at an intermediate level of description (more specific than those used in case frames, but more general than those used in traditional knowledge representation systems).In this paper we describe a classification algorithm for identifying relationships between two-word noun compounds.We find that a very simple approach using a machine learning algorithm and a domain-specific lexical hierarchy successfully generalizes from training instances, performing better on previously unseen words than a baseline consisting of training on the words themselves.
We explore the use of Support Vector Machines (SVMs) for biomedical named entity recognition.To make the SVM training with the available largest corpus – the GENIA corpus – tractable, we propose to split the non-entity class into sub-classes, using part-of-speech information.In addition, we explore new features such as word cache and the states of an HMM trained by unsupervised learning.Experiments on the GENIA corpus show that our class splitting technique not only enables the training with the GENIA corpus but also improves the accuracy.The proposed new features also contribute to improve the accuracy.We compare our SVMbased recognition system with a system using Maximum Entropy tagging method.
We describe new algorithms for train ing tagging models, as an alternativeto maximum-entropy models or conditional random elds (CRFs).The al gorithms rely on Viterbi decoding oftraining examples, combined with simple additive updates.We describe the ory justifying the algorithms througha modi cation of the proof of conver gence of the perceptron algorithm forclassi cation problems.We give experimental results on part-of-speech tag ging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger.
We present a joint probability model for statistical machine translation, which automatically learns word and phrase equivalents from bilingual corpora.Translations produced with parameters estimated using the joint model are more accurate than translations produced using IBM Model 4.1 Motivation Most of the noisy-channel-based models used in statistical machine translation (MT) (Brown et al., 1993) are conditional probability models.In the noisy-channel framework, each source sentence e in a parallel corpus is assumed to “generate” a target sentence f by means of a stochastic process, whose parameters are estimated using traditional EM techniques (Dempster et al., 1977).The generative model explains how source words are mapped into target words and how target words are re-ordered to yield well-formed target sentences.A variety of methods are used to account for the re-ordering stage: word-based (Brown et al., 1993), templatebased (Och et al., 1999), and syntax-based (Yamada and Knight, 2001), to name just a few.Although these models use different generative processes to explain how translated words are re-ordered in a target language, at the lexical level they are quite similar; all these models assume that source words are into target individual words may contain a non-existent element, called NULL.We suspect that MT researchers have so far chosen to automatically learn translation lexicons defined only over words for primarily pragmatic reasons.Large scale bilingual corpora with vocabularies in the range of hundreds of thousands yield very large translation lexicons.Tuning the probabilities associated with these large lexicons is a difficult enough task to deter one from trying to scale up to learning phrase-based lexicons.Unfortunately, trading space requirements and efficiency for explanatory power often yields non-intuitive results.Consider, for example, the parallel corpus of three sentence pairs shown in Figure 1.Intuitively, if we allow any Source words to be aligned to any Target words, the best alignment that we can come up with is the one in Figure 1.c.Sentence pair (S2, T2) offers strong evidence that “b c” in language S means the same thing as “x” in language T. On the basis of this evidence, we expect the system to also learn from sentence pair (S1, T1) that “a” in language S means the same thing as “y” in language T. Unfortunately, if one works with translation models that do not allow Target words to be aligned to more than one Source word — as it is the case in the IBM models (Brown et al., 1993) — it is impossible to learn that the phrase “b c” in language S means the same thing as word “x” in language T. The IBM Model 4 (Brown et al., 1993), for example, converges to the word alignments shown in Figure 1.b and learns the probabilities shown in Figure Since in the IBM model one cannot link a Target word to more than a Source word, the training procedure train the IBM-4 model, we used Giza (Al-Onaizan et al., 1999).IBM−4 T−Table IBM−4 Intuitive Joint Joint T−Table p(y  |a) = 1 p(x  |c) = 1 p(z  |b) = 0.98 p(x  |b) = 0.02 S1: a b c T1: x y S2: b c S1: a b c T1: x y S2: b c S1: a b c T1: x y S2: b c p(x y, a b c) = 0.32 p(x, b c) = 0.34 p(y, a) = 0.01 p(z, b) = 0.33 Corresponding Conditional Table T2: x T2: x T2: x p(x y  |a b c ) = 1 p(x  |b c) = 1 p(y  |a) = 1 p(z  |b) = 1 S3: b S3: b S3: b T3: z T3: z T3: z a) b) c) d) e) Figure 1: Alignments and probability distributions in IBM Model 4 and our joint phrase-based model. yields unintuitive translation probabilities.(Note that another good word-for-word model is one that assigns high probability to p(xb) and p(zb) and low probability to p(xc).)In this paper, we describe a translation model that assumes that lexical correspondences can be established not only at the word level, but at the phrase level as well.In constrast with many previous approaches (Brown et al., 1993; Och et al., 1999; Yamada and Knight, 2001), our model does not try to capture how Source sentences can be mapped into Target sentences, but rather how Source and Target sentences can be generated simultaneously.In other words, in the style of Melamed (2001), we estimate a joint probability model that can be easily marginalized in order to yield conditional probability models for both source-to-target and target-tosource machine translation applications.The main difference between our work and that of Melamed is that we learn joint probability models of translation equivalence not only between words but also between phrases and we show that these models can be used not only for the extraction of bilingual lexicons but also for the automatic translation of unseen sentences.In the rest of the paper, we first describe our model (Section 2) and explain how it can be implemented/trained (Section 3).We briefly describe a decoding algorithm that works in conjunction with our model (Section 4) and evaluate the performance of a translation system that uses the joint-probability model (Section 5).We end with a discussion of the strengths and weaknesses of our model as compared to other models proposed in the literature.2 A Phrase-Based Joint Probability Model 2.1 Model 1 In developing our joint probability model, we started out with a very simple generative story.We assume that each sentence pair in our corpus is generated by the following stochastic process: 1.Generate a bag of concepts.2.For each concept , generate a pair of phrases , according to the distribution contain at least one word.3.Order the phrases generated in each language so as to create two linear sequences of phrases; these sequences correspond to the sentence pairs in a bilingual corpus.For simplicity, we initially assume that the bag of concepts and the ordering of the generated phrases are modeled by uniform distributions.We do not assume that is a hidden variable that generates pair , but rather that .Under these assumptions, it follows that the probability of generating a sentence pair (E, F) using concepts given by the product of all phrase-tophrase translation probabilities, yield bags of phrases that can be ordered linearly so as to obtain the sentences E and F. For example, the sentence pair “a b c” — “x y” can be generated using two concepts, (“a b” : “y”) and (“c” : “x”); or one concept, (“a b c” : “x y”), because in both cases the phrases in each language can be arranged in a sequence that would yield the original sentence pair.However, the same sentence pair cannot be generated using the concepts (“a b” : “y”) and (“c” : “y”) because the sequence “x y” cannot be recreated from the two phrases “y” and “y”.Similarly, the pair cannot be generated using concepts (“a c” : “x”) and (“b” : “y”) because the sequence “a b c” cannot be created by catenating the phrases “a c” and “b”.We say that a set of concepts can be linearized into a sentence pair (E, F) if E and F can be obtained by permuting the phrasesandthat characterize concepts .We denote this property using the predicate .Under this model, the probability of a given sentence pair (E, F) can then be obtained by summing up over all possible ways of generating bags of concepts that can linearized to (E, F).(1) 2.2 Model 2 Although Model 1 is fairly unsophisticated, we have found that it produces in practice fairly good alignments.However, this model is clearly unsuited for translating unseen sentences as it imposes no constraints on the ordering of the phrases associated with a given concept.In order to account for this, we modify slightly the generative process in Model 1 so as to account for distortions.The generative story of Model 2 is this: 1.Generate a bag of concepts.2.Initialize E and F to empty sequences.3.Randomly take a concept and generate a pair of phrases , according to the distribution , whereandeach contain at least one word.Remove then from.4.Append phraseat the end of F. Letbe the start position ofin F. 5.Insert phraseat positionin E provided that no other phrase occupies any of the positions betweenand , wheregives length of the phrase.We hence create the alignment between the two phrasesand with probability is a position-based distortion distribution.6.Repeat steps 3 to 5 untilis empty.In Model 2, the probability to generate a sentence pair (E, F) is given by formula (2), where the position of wordof phrasein sen- F and denotes the position in tence E of the center of mass of phrase.(2) Model 2 implements an absolute position-based distortion model, in the style of IBM Model 3.We have tried many types of distortion models.We eventually settled for the model discussed here because it produces better translations during decoding.Since the number of factors involved in computing the probability of an alignment does not vary with the size of the Target phrases into which Source phrases are translated, this model is not predisposed to produce translations that are shorter than the Source sentences given as input.3 Training Training the models described in Section 2 is computationally challenging.Since there is an exponential number of alignments that can generate a sentence pair (E, F), it is clear that we cannot apply the 1.Determine high-frequency ngrams in the bilingual corpus.2.Initialize the t-distribution table.3.Apply EM training on the Viterbi alignments, while using smoothing.4.Generate conditional model probabilities.Figure 2: Training algorithm for the phrase-based joint probability model.EM training algorithm exhaustively.To estimate the parameters of our model, we apply the algorithm in Figure 2, whose steps are motivated and described below.3.1 Determine high-frequency n-grams in E and F If one assumes from the outset that any phrases can be generated from a cept , one would need a supercomputer in order to store in the memory a table that models the distribution.Since we don’t have access to computers with unlimited memory, we initially learn t distribution entries only for the phrases that occur often in the corpus and for unigrams.Then, through smoothing, we learn t distribution entries for the phrases that occur rarely as well.In order to be considered in step 2 of the algorithm, a phrase has to occur at least five times in the corpus.3.2 Initialize the t-distribution table Before the EM training procedure starts, one has no idea what word/phrase pairs are likely to share the same meaning.In other words, all alignments that can generate a sentence pair (E, F) can be assumed to have the same probability.Under these conditions, the evidence that a sentence pair (E, F) contributes to fact that are generated by the same cept is given by the number of alignments that can be built between (E, F) that have a concept that is linked to phrasein sentence E and phrase sentence F divided by the total number of alignments that can be built between the two sentences.Both these numbers can be easily approximated.Given a sentence E ofwords, there are ways in which thewords can be partitioned into sets/concepts, where is the ling number of second kind.There are also ways in which the words a sentence F can be partitioned into nonempty sets.Given that any words in E can be mapped to any words in F, it follows that there are alignments that can be built between two sentences (E, F) of lengthsand , respectively.When a concept generates two phrases of lengthand, respectively, there are only and words left to link.Hence, the absence of any other information, the probability that phrasesandare generated by the same concept is given by formula (4).Note that the fractional counts returned by equation (4) are only an approximation of the t distribution that we are interested in because the Stirling numbers of the second kind do not impose any restriction on the words that are associated with a given concept be consecutive.However, since formula (4) overestimates the numerator and denominator equally, the approximation works well in practice.In the second step of the algorithm, we apply equation (4) to collect fractional counts for all unigram and high-frequency n-gram pairs in the cartesian product defined over the phrases in each sentence pair (E, F) in a corpus.We sum over all these t-counts and we normalize to obtain an initial joint distribution.This step amounts to running the EM algorithm for one step over all possible alignments in the corpus.3.3 EM training on Viterbi alignments Given a non-uniform t distribution, phrase-to-phrase alignments have different weights and there are no other tricks one can apply to collect fractional counts over all possible alignments in polynomial time.Starting with step 3 of the algorithm in Figure 2, for each sentence pair in a corpus, we greedily produce an initial alignment by linking together phrases so as to create concepts that have high t probabilities.We then hillclimb towards the Viterbi alignment of highest probability by breaking and merging concepts, swapping words between concepts, and moving words across concepts.We compute the probabilities associated with all the alignments we generate during the hillclimbing process and collect t counts over all concepts in these alignments.We apply this Viterbi-based EM training procedure for a few iterations.The first iterations estimate the alignment probabilities using Model 1.The rest of the iterations estimate the alignment probabilities using Model 2.During training, we apply smoothing so we can associate non-zero values to phrase-pairs that do not occur often in the corpus.3.4 Derivation of conditional probability model At the end of the training procedure, we take marginals on the joint probability distributionsand .This yields conditional probability distributions and which we use for decoding.3.5 Discussion When we run the training procedure in Figure 2 on the corpus in Figure 1, after four Model 1 iterations we obtain the alignments in Figure 1.d and the joint and conditional probability distributions shown in Figure 1.e.At prima facie, the Viterbi alignment for the first sentence pair appears incorrect because we, as humans, have a natural tendency to build alignments between the smallest phrases possible.However, note that the choice made by our model is quite reasonable.After all, in the absence of additional information, the model can either assume that “a” and “y” mean the same thing or that phrases “a b c” and “x y” mean the same thing.The model chose to give more weight to the second hypothesis, while preserving some probability mass for the first one.Also note that although the joint distribution puts the second hypothesis at an advantage, the conditional distribution does not.The conditional distribution in Figure 1.e is consistent with our intuitions that tell us that it is reasonable both to translate “a b c” into “x y”, as well as “a” into “y”.The conditional distribution mirrors perfectly our intuitions.4 Decoding For decoding, we have implemented a greedy procedure similar to that proposed by Germann et al. (2001).Given a Foreign sentence F, we first produce a gloss of it by selecting phrases inthat the probability .We then tively hillclimb by modifying E and the alignment between E and F so as to maximize the formula .We hillclimb by modifying an existing alignment/translation through a set of operations that modify locally the aligment/translation built until a given time.These operations replace the English side of an alignment with phrases of different probabilities, merge and break existing concepts, and swap words across concepts.The probability p(E) is computed using a simple trigram language model that was trained using the CMU Language Modeling Toolkit (Clarkson and Rosenfeld, 1997).The language model is estimated at the word (not phrase) level.Figure 3 shows the steps taken by our decoder in order to find the translation of sentence “je vais me arrˆeter l`a .” Each intermediate translation in Figure 3 is preceded by its probability and succeded by the operation that changes it to yield a translation of higher probability.5 Evaluation To evaluate our system, we trained both Giza (IBM Model 4) (Al-Onaizan et al., 1999) and our joint probability model on a French-English parallel corpus of 100,000 sentence pairs from the Hansard corpus.The sentences in the corpus were at most 20 words long.The English side had a total of 1,073,480 words (21,484 unique tokens).The French side had a total of 1,177,143 words (28,132 unique tokens).We translated 500 unseen sentences, which were uniformly distributed across lengths 6, 8, 10, 15, and 20.For each group of 100 sentences, we manually determined the number of sentences translated perfectly by the IBM model decoder of Germann et (2001) and the decoder that uses the joint prob- Model Percent perfect translations IBM Bleu score Sentence length Sentence length 6 8 10 15 20 Avg.6 8 10 15 20 Avg.IBM 36 26 35 11 2 22 0.2076 0.2040 0.2414 0.2248 0.2011 0.2158 Phrase-based 43 37 33 19 6 28 0.2574 0.2181 0.2435 0.2407 0.2028 0.2325 Table 1: Comparison of IBM and Phrase-Based, Joint Probability Models on a translation task. je vais me arreter la . je vais me arreter la .9.46e−08 i am going to stop there .Figure 3: Example of phrase-based greedy decoding. ability model.We also evaluated the translations automatically, using the IBM-Bleu metric (Papineni et al., 2002).The results in Table 1 show that the phrased-based translation model proposed in this paper significantly outperforms IBM Model 4 on both the subjective and objective metrics.6 Discussion 6.1 Limitations The main shortcoming of the phrase-based model in this paper concerns the size of the t-table and the cost of the training procedure we currently apply.To keep the memory requirements manageable, we arbitrarily restricted the system to learning phrase translations of at most six words on each side.Also, the swap, break, and merge operations used during the Viterbi training are computationally expensive.We are currently investigating the applicability of dynamic programming techniques to increase the speed of the training procedure.Clearly, there are language pairs for which it would be helpful to allow concepts to be realized as non-contiguous phrases.The English word “not”, for example, is often translated into two French words, “ne” and “pas”.But “ne” and “pas” almost never occur in adjacent positions in French texts.At the outset of this work, we attempted to develop a translation model that enables concepts to be mapped into non-contiguous phrases.But we were not able to scale and train it on large amounts of data.The model described in this paper cannot learn that the English word “not” corresponds to the French words “ne” and “pas”.However, our model learns to deal with negation by memorizing longer phrase translation equivalents, such as (“ne est pas”, “is not”); (“est inadmissible”, “is not good enough”); and (“ne est pas ici”, “is not here”).6.2 Comparison with other work A number of researchers have already gone beyond word-level translations in various MT settings.For example, Melamed (2001) uses wordlevel alignments in order to learn translations of noncompositional compounds.Och and Ney (1999) learn phrase-to-phrase mappings involving word classes, which they call “templates”, and exploit them in a statistical machine translation system.And Marcu (2001) extracts phrase translations from automatically aligned corpora and uses them in conjunction with a word-for-word statistical translation system.However, none of these approaches learn simultaneously the translation of phrases/templates and the translation of words.As a consequence, there is a chance that the learning procedure will not discover phrase-level patterns that occur often in the je vais me arreter la .7.50e−11 FuseAndChangeTrans(&quot;la .&quot;, &quot;there .&quot;) i want me to that . je vais me arreter la .2.97e−10 ChangeWordTrans(&quot;arreter&quot;,&quot;stop&quot;) 7.75e−10 1.09e−09 i want me to there . je vais me arreter la . i want me stop there . je vais me arreter la . let me to stop there .FuseAndChange(&quot;je vais&quot;,&quot;let me&quot;) FuseAndChange(&quot;je vais me&quot;, &quot;i am going to&quot;) 1.28e−14 changeWordTrans(&quot;vais&quot;, &quot;want&quot;) i . me to that . data.In our approach, phrases are not treated differently from individual words, and as a consequence the likelihood of the EM algorithm converging to a better local maximum is increased.Working with phrase translations that are learned independent of a translation model can also affect the decoder performance.For example, in our previous work (Marcu, 2001), we have used a statistical translation memory of phrases in conjunction with a statistical translation model (Brown et al., 1993).The phrases in the translation memory were automatically extracted from the Viterbi alignments produced by Giza (Al-Onaizan et al., 1999) and reused in decoding.The decoder described in (Marcu, 2001) starts from a gloss that uses the translations in the translation memory and then tries to improve on the gloss translation by modifying it incrementally, in the style described in Section 4.However, because the decoder hill-climbs on a word-forword translation model probability, it often discards good phrasal translations in favour of word-for-word translations of higher probability.The decoder in Section 4 does not have this problem because it hillclimbs on translation model probabilities in which phrases play a crucial role.
This paper describes a bootstrapping algorithm called Basilisk that learns highquality semantic lexicons for multiple categories.Basilisk begins with an unannotated corpus and seed words for each semantic category, which are then bootstrapped to learn new words for each category.Basilisk hypothesizes the semantic class of a word based on collective information over a large body of extraction pattern contexts.We evaluate Basilisk on six semantic categories.The semantic lexicons produced by Basilisk have higher precision than those produced by previous techniques, with several categories showing substantial improvement.
In this paper, we propose a new statistical Japanese dependency parser using a cascaded chunking model.Conventional Japanese statistical dependency parsers are mainly based on a probabilistic model, which is not always efficient or scalable.We propose a new method that is simple and efficient, since it parses a sentence deterministically only deciding whether the current segment modifies the segment on its immediate right hand side.Experiments using the Kyoto University Corpus show that the method outperforms previous systems as well as improves the parsing and training efficiency.
We present a system for automatically identifying PropBank-style semantic roles based on the output of a statistical parser for Combinatory Categorial Grammar.This system performs at least as well as a system based on a traditional Treebank parser, and outperforms it on core argument roles.
We present a graph-based semi-supervised learning algorithm to address the sentiment analysis task of rating inference.Given a set of documents (e.g., movie reviews) and accompanying ratings (e.g., “4 stars”), the task calls for inferring numerical ratings for unlabeled documents based on the perceived sentiment expressed by their text.In particular, we are interested in the situation where labeled data is scarce.We place this task in the semi-supervised setting and demonstrate that considering unlabeled reviews in the learning process can improve ratinginference performance.We do so by creating a graph on both labeled and unlabeled data to encode certain assumptions for this task.We then solve an optimization problem to obtain a smooth rating function over the whole graph.When only limited labeled data is available, this method achieves significantly better predictive accuracy over other methods that ignore the unlabeled examples during training.
This paper presents ongoing work on the detection of preposition errors of non-native speakers of English.Since prepositions account for a substantial proportion of all grammatical errors by ESL (English as a Second Language) learners, developing an NLP application that can reliably detect these types of errors will provide an invaluable learning resource to ESL students.To address this problem, we use a maximum entropy classifier combined with rule-based filters to detect preposition errors in a corpus of student essays.Although our work is preliminary, we achieve a precision of 0.8 with a recall of 0.3.
Eric Brill introduced transformation-based learning and showed that it can do part-ofspeech tagging with fairly high accuracy.The same method can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive &quot;baseNP&quot; chunks.For this purpose, it is convenient to view chunking as a tagging problem by encoding the chunk structure in new tags attached to each word.In automatic tests using Treebank-derived data, this technique achieved recall and precision rates of roughly 92% for baseNP chunks and 88% for somewhat more complex chunks that partition the sentence.Some interesting adaptations to the transformation-based learning approach are also suggested by this application.
This paper presents a high precision pronoun resolution system that is capable of greater than 90% precision with 60% and better recall for some pronouns.It is suggested that the system is resolving a sub-set of anaphors that do not require general world knowledge or sophisticated linguistic processing for successful resolution.The system does this by being very sensitive to ambiguity, and only resolving pronouns when very high confidence rules have been satisfied.The system is capable of 'noticing' ambiguity because it requires that there be a unique antecedent within a salience ranking, and the salience rankings are not total orders, i.e. two or more antecedents can be equally salient.Given the nature of the systems rules, it is very likely that they are largely domain independent and that they reflect processing strategies used by humans for general language comprehension.The system has been evaluated in two distinct experiments which support the overall validity of the approach.
PP — 31.5 In all experiments, we use the following three error criteria: • WER (word error rate): The WER is computed as the minimum number of substitution, insertion and deletion operations that have to be performed to convert the generated string into the target string.This performance criterion is widely used in speech recognition.• PER (position-independent word error rate): A shortcoming of the WER is the fact that it requires a perfect word order.This is 26 2: for Text and Speech Input: error rate (WER), positionindependent word error rate (PER) and subjective sentence error rate (SSER) with/without preprocessing (147 sentences = 1 968 words of the Verbmobil task).Input Preproc.WER[%] PER[Vo] SSER[%] Single-Word Based Approach Text No 53.4 38.3 35.7 Yes 56.0 41.2 35.3 Speech No 67.8 50.1 54.8 Yes 67.8 51.4 52.7 Alignnient Templates Text No 49.5 35.3 31.5 Yes 48.3 35.1 27.2 Speech No 63.5 45.6 52.4 Yes 62.8 45.6 50.3 particularly a problem for the Verbmobil task, where the word order of the German- English sentence pair can be quite different.As a result, the word order of the automatically generated target sentence can be different from that of the target sentence, but nevertheless acceptable so that the WER measure alone could be misleading.In order to overcome this problem, we introduce as additional measure the positionindependent word error rate (PER).This measure compares the words in the two senthe word order into account.Words that have no matching counterparts are counted as substitution errors.Depending on whether the translated sentence is longer or shorter than the target translation, the remaining words result in either insertion or deletion errors in addition to substitution errors.The PER is guaranteed to be less than or equal to the WER.• SSER (subjective sentence error rate): For a more detailed analysis, subjective judgments by test persons are necessary.Each translated sentence was judged by a human examiner according to an error scale from 0.0 to 1.0.A score of 0.0 means that the translation is semantically and syntactically correct, a score of 0.5 means that a sentence is semantically correct but syntactically wrong and a score of 1.0 means that the sentence is semantically wrong.The human examiner was offered the translated sentences of the two approaches at the same As a result we expect a better possibility of reproduction.The results of the translation experiments using the single-word based approach and the alignment template approach on text input and on speech input are summarized in Table 2.The results are shown with and without the use of domain-specific preprocessing.The alignment template approach produces better translation results than the single-word based approach.From this we draw the conclusion that it is important to model word groups in source and target language.Considering the recognition word error rate of 31% the degradation of about 20% by speech input can be expected.The average translation time on an Alpha workstation for a single sentence is about one second for the alignment template appreach and 30 seconds for the single-word based search procedure.Within the Verbmobil project other translation modules based on rule-based, examplebased and dialogue-act-based translation are used.We are not able to present results with these methods using our test corpus.But in the current Verbmobil prototype the preliminary evaluations show that the statistical methods produce comparable or better results than other systems.An advantage of the systhat it is robust and always produces a translation result even if the input of the speech recognizer is quite incorrect.5 Summary We have described two approaches to perform statistical machine translation which extend the baseline alignment models.The single-word 27 based approach allows for the the possibility of one-to-many alignments.The alignment template approach uses two different alignment levels: a phrase level alignment between phrases and a word level alignment between single words.As a result the context of words has a greater influence and the changes in word order from source to target language can be learned explicitly.An advantage of both methods is that they learn fully automatically by using a bilingual training corpus and are capable of achieving better translation results on a limited-domain task than other example-based or rule-based translation systems.Acknowledgment This work has been partially supported as part of the Verbmobil project (contract number 01 IV 701 T4) by the German Federal Ministry of Education, Science, Research and Technology and as part of the EuTrans project by the by the European Community (ESPRIT project number 30268).

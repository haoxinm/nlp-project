Figure 2: Sample sentence and parse tree we have an input sentence (ABCDEhas a parse tree shown in Figure 2. a human reduces the sentence to which can be translated to a series of decisions made along edges in the sentence parse tree as shown in Figure 3.The symbol &quot;y&quot; along an edge means the node it points to will be kept, and &quot;n&quot; means the node will be removed.Suppose the program reduces sentence to can be translated similarly to the annotated tree shown in Figure 4.Figure 4: Reduced form by the program We can see that along five edges (they are D—)T, D—*G, B-4A, B—>C), both the human and the program made decisions.Two out of the five decisions agree (they are D--÷B and D—>E), so the rate is 2/5 (40%).The rate defined as: man and the program have made the same decision success rate = the total # of edges along which both the human and the progam have made decisions 313 Note that the edges along which only the human or the program has made a decision (e.g., G--F and G—.>F in Figure 3 and Figure 4) are not considered in the computation of success rate, since there is no agreement issue in such cases.3.2 Evaluation result In the evaluation, we used 400 sentences in the corpus to compute the probabilities that a phrase is removed, reduced, or unchanged.We tested the program on the rest 100 sentences.Using five-fold validation (i.e., chose different 100 sentences for testing each time and repeating the experiment five times), The program achieved an average success rate of 81.3%.If we consider the baseline as removing all the prepositional phrases, clauses, to-infinitives and gerunds, the baseline performance is 43.2%.We also computed the success rate of program's decisions on particular types of phrases.For the decisions on removing or keeping a clause, the system has a success rate of 78.1%; for the decisions on removing or keeping a to-infinitive, the system has a success rate of 85.2%.We found out that the system has a low success rate on removing adjectives of noun phrases or removing adverbs of a sentence or a verb phrase.One reason for this is that our probability model can hardly capture the dependencies between a particular adjective and the head noun since the training corpus is not large enough, while the other sources of information, including grammar or context information, provide little evidence on whether an adjective or an adverb should be removed.Given that whether or not an adjective or an adverb is removed does not affect the conciseness of the sentence significantly and the system lacks of reliability in making such decisions, we decide not to remove adjectives and adverbs.On average, the system reduced the length of the 500 sentence by 32.7% (based on the number of words), while humans reduced it by 41.8%.The probabilities we computed from the training corpus covered 58% of instances in the test corpus.When the corpus probability is absent for a case, the system makes decisions based on the other two sources of knowledge.Some of the errors made by the system result from the errors by the syntactic parser.We randomly checked 50 sentences, and found that 8% of the errors made by the system are due to parsing errors.There are two main reasons responsible for this relative low percentage of errors resulted from mistakes in parsing.One reason is that we have taken some special measures to avoid errors introduced by mistakes in parsing.For example, PP attachment is a difficult problem in parsing and it is not rare that a PP is wrongly attached.Therefore, we take this into account when marking the obligatory components using subcategorization knowledge from the lexicon (step 2) — we not only look at the PPs that are attached to a verb phrase, but also PPs that are next to the verb phrase but not attached, in case it is part of the verb phrase.We also wrote a preprocessor to deal with particular structures that the parser often has problems with, such as appositions.The other reason is that parsing errors do not always result in reduction errors.For example, given a sentence &quot;The spokesperson of the University said that ...&quot;, although that-clause in the sentence may have a complicated structure and the parser gets it wrong, the reduction system is not necessarily affected since it may decide in this case to keep that-clause as it is, as humans often do, so the parsing errors will not matter in this example.4 Discussion and related work The reduction algorithm we present assumes generic summarization; that is, we want to generate a summary that includes the most important information in an article.We can tailor the reduction system to queries-based summarization.In that case, the task of the reduction is not to remove phrases that are extraneous in terms of the main topic of an article, but phrases that are not very relevant to users' queries.We extended our sentence reduction program to query-based summarization by adding another step in the algorithm to measure the relevance of users' queries to phrases in the sentence.In the last step of reduction when the system makes the final decision, the relevance of a phrase to the query is taken into account, together with syntactic, context, and corpus information.Ideally, the sentence reduction module should interact with other modules in a summarization system.It should be able to send feedback to the extraction module if it finds that a sentence selected by the extraction module may be inappropriate (for example, having a very low context importance score).It should also be able to interact with the modules that run after it, such as the sentence combination module, so that it can revise reduction decisions according to the feedback from these modules.Some researchers suggested removing phrases or clauses from sentences for certain applications.(Grefenstette, 1998) proposed to remove phrases in sentences to produce a telegraphic text that can be used to provide audio scanning service for the blind.(Corston-Oliver and Dolan, 1999) proposed to remove clauses in sentences before indexing documents for Information Retrieval.Both studies removed phrases based only on their syntactic categories, while the focus of our system is on deciding when it is appropriate to remove a phrase. researchers worked on the text simplifica- 314 tion problem, which usually involves in simplifying text but not removing any phrases.For example, (Carroll et al., 1998) discussed simplifying newspaper text by replacing uncommon words with common words, or replacing complicated syntactic structures with simpler structures to assist people with reading disabilities.(Chandrasekar et al., 1996) discussed text simplification in general.The difference between these studies on text simplification and our system is that a text simplification system usually not from an original sentence, although it may change its structure or words, but our system removes extraneous phrases from the extracted sentences.5 Conclusions and future work We present a novel sentence reduction system which removes extraneous phrases from sentences that are extracted from an article in text summarization.The deleted phrases can be prepositional phrases, clauses, to-infinitives, or gerunds, and multiple phrases can be removed form a single sentence.The focus of this work is on determining, for a sentence in a particular context, which phrases in the sentence are less important and can be removed.Our system makes intelligent reduction decisions based on multiple sources of knowledge, including syntactic knowledge, context, and probabilities computed from corpus analysis.We also created a corpus consisting of 500 sentences and their reduced forms produced by human professionals, and used this corpus for training and testing the system.The evaluation shows that 81.3% of reduction decisions made by the system agreed with those of humans.In the future, we would like to integrate our sentence reduction system with extraction-based summarization systems other than the one we have developed, improve the performance of the system further by introducing other sources of knowledge necessary for reduction, and explore other interesting applications of the reduction system.Acknowledgment This material is based upon work supported by the National Science Foundation under Grant No.IRI 96-19124 and IRI 96-18797.Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not
In part of speech tagging by Hidden Markov Model, a statistical model is used to assign grammatical categories to words in a text.Early work in the field relied on a corpus which had been tagged by a human annotator to train the model.recently, Cutting al. suggest that training can be achieved with a minimal lexicon and a limited amount priori about probabilities, by using an Baum-Welch re-estimation to automatically refine the model.In this paper, I report two experiments designed to determine how much manual training information is needed.The first experiment suggests that initial biasing of either lexical or transition probabilities is essential to achieve a good accuracy.The second experiment reveals that there are three distinct patterns of Baum-Welch reestimation.In two of the patterns, the re-estimation ultimately reduces the accuracy of the tagging rather than improving it.The pattern which is applicable can be predicted from the quality of the initial model and the similarity between the tagged training corpus (if any) and the corpus to be tagged.Heuristics for deciding how to use re-estimation in an effective manner are given.The conclusions are broadly in agreement with those of Merialdo (1994), but give greater detail about the contributions of different parts of the model.1 Background Part-of-speech tagging is the process of assigning grammatical categories to individual words in a corpus.One widely used approach makes use of a statistical technique called a Hidden Markov Model (HMM).The model is defined by two collections of the probabilities, express the probability that a tag follows the preceding (or two for a second order model); and the the probability that a word has a given tag without regard to words on either side of it.To tag a text, the tags with non-zero probability are hypothesised for each word, and the most probable sequence of tags given the sequence of words is determined from the probabilities.Two algorithms are commonly used, known as the Forward-Backward (FB) and Viterbi algorithms.FB assigns a probability to every tag on every word, while Viterbi prunes tags which cannot be chosen because their probability is lower than the ones of competing hypotheses, with a corresponding gain in computational ef

Comparisons of automatic evaluation metrics for machine translation are usually conducted on corpus level using correlation statistics such as Pearson?s product moment correlation coefficient or Spearman?s rank order correlation coefficient between human scores and automatic scores.However, such comparisons rely on human judgments of translation qualities such as adequacy and fluency.Unfortunately, these judgments are often inconsistent and very expensive to acquire.In this paper, we introduce a new evaluation method, ORANGE, for evaluating automatic machine translation evaluation metrics automatically without extra human involvement other than using a set of reference translations.We also show the results of comparing several existing automatic metrics and three new automatic metrics using ORANGE.
Most work on unsupervised entailment rule acquisition focused on rules between templates with two variables, ignoring unary rules - entailment rules betweentemplates with a single variable.In this paper we investigate two approaches for unsupervised learning of such rules and com pare the proposed methods with a binary rule learning method.The results show that the learned unary rule-sets outperform the binary rule-set.In addition, a novel directional similarity measure for learning entailment, termed Balanced-Inclusion, is the best performing measure.
In this paper we describe a methodologyfor detecting preposition errors in the writ ing of non-native English speakers.Our system performs at 84% precision andclose to 19% recall on a large set of stu dent essays.In addition, we address the problem of annotation and evaluation inthis domain by showing how current ap proaches of using only one rater can skew system evaluation.We present a sampling approach to circumvent some of the issuesthat complicate evaluation of error detec tion systems.
In this paper, we describe a new model for word alignment in statistical trans- lation and present experimental results.The idea of the model is to make the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions.To achieve this goal, the approach us- es a first-order Hidden Markov model (HMM) for the word alignment problem as they are used successfully in speech recognition for the time alignment prob- lem.The difference to the time align- ment HMM is that there is no monotony constraint for the possible word order- ings.We describe the details of the mod- el and test the model on several bilingual corpora.
We consider the problem of learning toparse sentences to lambda-calculus repre sentations of their underlying semantics and present an algorithm that learns a weighted combinatory categorial grammar (CCG).A key idea is to introduce non-standard CCG combinators that relax certain parts of the grammar?for example allowing flexible word order, or insertion of lexical items?with learned costs.We also present a new, online algorithm for inducing a weighted CCG.Results for the approach on ATIS data show 86% F-measure in recovering fully correct semantic analyses and 95.9% F-measure by a partial-match criterion, a more than 5% improvement over the 90.3% partial-match figure reported by He and Young (2006).
This paper proposes a tree kernel with contextsensitive structured parse tree information for re lation extraction.It resolves two critical problems in previous tree kernels for relation extraction in two ways.First, it automatically determines a dynamic context-sensitive tree span for relation ex traction by extending the widely-used Shortest Path-enclosed Tree (SPT) to include necessary context information outside SPT.Second, it pro poses a context-sensitive convolution tree kernel, which enumerates both context-free and context sensitive sub-trees by considering their ancestor node paths as their contexts.Moreover, this paper evaluates the complementary nature between our tree kernel and a state-of-the-art linear kernel.Evaluation on the ACE RDC corpora shows that our dynamic context-sensitive tree span is much more suitable for relation extraction than SPT and our tree kernel outperforms the state-of-the-art Collins and Duffy?s convolution tree kernel.It also shows that our tree kernel achieves much bet ter performance than the state-of-the-art linear kernels . Finally, it shows that feature-based and tree kernel-based methods much complement each other and the composite kernel can well integrate both flat and structured features.
We formulate dependency parsing as a graphical model with the novel ingredient of global constraints.We show how to apply loopy belief propagation (BP), a simple and tool for and inference.As a parsing algorithm, BP is both asymptotically and empirically efficient.Even with second-order features or latent variables, which would make exact parsing considerslower or NP-hard, BP needs only with a small constant factor.Furthermore, such features significantly improve parse accuracy over exact first-order methods.Incorporating additional features would increase the runtime additively rather than multiplicatively.
Manual evaluation of translation quality is generally thought to be excessively time consuming and expensive.We explore a fast and inexpensive way of doing it using Amazon’s Mechanical Turk to pay small sums to a large number of non-expert annotators.For $10 we redundantly recreate judgments from a WMT08 translation task.We find that when combined non-expert judgments have a high-level of agreement with the existing gold-standard judgments of machine translation quality, and correlate more strongly with expert judgments than Bleu does.We go on to show that Mechanical Turk can be used to calculate human-mediated translation edit rate (HTER), to conduct reading comprehension experiments with machine translation, and to create high quality reference translations.
We connect two scenarios in structured parser trained on one corpus to another annotation style, and annotations from one to another.We propose quasigrammar features for these structured learning tasks.That is, we score a aligned pair of source and target trees based on local features of the trees and the alignment.Our quasi-synchronous model assigns positive probability to any alignment of any trees, in contrast to a synchronous grammar, which would insist on some form of structural parallelism.In monolingual dependency parser adaptation, we achieve high accuracy in translating among multiple annotation styles for the same sentence.On the more difficult problem of cross-lingual parser projection, we learn a dependency parser for a target language by using bilingual text, an English parser, and automatic word alignments.Our experiments show that unsupervised QG projection improves on parses trained using only highprecision projected annotations and far outperforms, by more than 35% absolute dependency accuracy, learning an unsupervised parser from raw target-language text alone.When a few target-language parse trees are available, projection gives a boost equivalent to doubling the number of target-language trees. first author would like to thank the Center for Intelligent Information Retrieval at UMass Amherst.We would also like to thank Noah Smith and Rebecca Hwa for helpful discussions and the anonymous reviewers for their suggestions for improving the paper.
Coreference systems are driven by syntactic, semantic, and discourse constraints.We present a simple approach which completely modularizes these three aspects.In contrast to much current work, which focuses on learning and on the discourse component, our system is deterministic and is driven entirely by syntactic and semantic compatibility as learned from a large, unlabeled corpus.Despite its simplicity and discourse naivete, our system substantially outperforms all unsupervised systems and most supervised ones.Primary contributions include (1) the presentation of a simpleto-reproduce, high-performing baseline and (2) the demonstration that most remaining errors can be attributed to syntactic and semantic factors external to the coreference phenomenon (and perhaps best addressed by non-coreference systems).
This paper addresses the problem of learning to map sentences to logical form, given training data consisting of natural language sentences paired with logical representations of their meaning.Previous approaches have been designed for particular natural languages or specific meaning representations; here we present a more general method.The approach induces a probabilistic CCG grammar that represents the meaning of individual words and defines how these meanings can be combined to analyze complete sentences.We use higher-order unification to define a hypothesis space containing all grammars consistent with the training data, and develop an online learning algorithm that efficiently searches this space while simultaneously estimating the parameters of a log-linear parsing model.Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations.
mining a recent subdiscipline of computational linguistics which is concerned not with the topic a document is about, but with the opinion it expresses.To aid the extraction of opinions from text, recent work has tackled the issue determining the “subjective” terms contained in text, i.e. deciding whether a term that carries opinionated content has a positive or a negative connotation.This is believed to be of key importance for identifying the orientation of documents, i.e. determining whether a document expresses a positive or negative opinion about its subject matter.We contend that the plain determination of the orientation of terms is not a realistic problem, since it starts from the nonrealistic assumption that we already know whether a term is subjective or not; this would imply that a linguistic resource that marks terms as “subjective” or “objective” is available, which is usually not the case.In this paper we confront the task of deciding whether a given term has a positive or a negative connotation, no subjective connotation at this problem thus subsumes the problem of desubjectivity problem of determining orientation.We tackle this problem by testing three different variants of a semi-supervised method previously proposed for orientation detection.Our results show that determining subjectivity is a much harder problem than determining orientation alone.
We investigate the lexical and syntactic flexibility of a class of idiomatic expressions.We develop measures that draw on such linguistic properties, and demonstrate that these statistical, corpus-based measures can be successfully used for distinguishing idiomatic combinations from non-idiomatic ones.We also propose a means for automatically determining which syntactic forms a particular idiom can appear in, and hence should be included in its lexical representation.
We propose an approach for extracting relations between entities from biomedical literature based solely on shallow linguistic information.We use a combination of kernel functions to integrate two different information sources: (i) the whole sentence where the relation appears, and (ii) the local contexts around the interacting entities.We performed experiments on extracting gene and protein interactions from two different data sets.The results show that our approach outperforms most of the previous methods based on syntactic and semantic information.
Sense induction seeks to automatically identify word senses directly from a corpus.A key assumption underlying previous work is that the context surrounding an ambiguous word is indicative of its meaning.Sense induction is thus typically viewed as an unsupervised clustering problem where the aim is to partition a word’s contexts into different classes, each representing a word sense.Our work places sense induction in a Bayesian context by modeling the contexts of the ambiguous word as samples from a multinomial distribution over senses which are in turn characterized as distributions over words.The Bayesian framework provides a principled way to incorporate a wide range of features beyond lexical cooccurrences and to systematically assess their utility on the sense induction task.The proposed approach yields improvements over state-of-the-art systems on a benchmark dataset.
Chinese is written without using spaces or other word delimiters.Although a text may be thought of as a corresponding sequence of words, there is considerable ambiguity in the placement of boundaries.Interpreting a text as a sequence of words is beneficial for some information retrieval and storage tasks: for example, full-text search, word-based compression, and key phrase extraction.We describe a scheme that infers appropriate positions for word boundaries using an adaptive language model that is standard in text compression.It is trained on a corpus of presegmented text, and when applied to new text, interpolates word boundaries so as to maximize the compression obtained.This simple and general method performs well with respect to specialized schemes for Chinese language segmentation.
present a statistical machine translation model that uses that contain subphrases.The model is formally a synchronous context-free grammar but is learned from a parallel text without any syntactic annotations.Thus it can be seen as combining fundamental ideas from both syntax-based translation and phrase-based translation.We describe our system’s training and decoding methods in detail, and evaluate it for translation speed and translation accuracy.Using BLEU as a metric of translation accuracy, we find that our system performs significantly better than the Alignment Template System, a state-of-the-art phrasebased system.
We propose an account of indirect forms of speech acts to request and inform based on the hypothesis that language users can recognize actions being performed by others, infer goals being sought, and cooperate in their achievement.This cooperative behaviour is independently motivated and may or may not be intended by speakers.If the hearer believes it is intended, he or she can recognize the speech act as indirect; otherwise it is interpreted directly.Heuristics are suggested to decide among the interpretations.
In this paper, I consider a range of English expressions and show that their context-dependency can be characterized in terms of two properties: 1.They specify entities in an evolving model of the discourse that the listener is constructing; 2.The particular entity specified depends on another entity in that part of the evolving &quot;discourse model&quot; that the listener is currently attending to. expressions have been called show how tensed clauses share these characteristics, usually just attributed to anaphoric noun phrases.This not only allows us to capture in a simple way the but difficult-to-prove intuition that is anaphoric, also contributes to our knowledge of what is needed for understanding narrative text.
We propose that many ambiguous prepositional phrase attachments can be resolved on the basis of the relative strength of association of the preposition with verbal and nominal heads, estimated on the basis of distribution in an automatically parsed corpus.This suggests that a distributional approach can provide an approximate solution to parsing problems that, in the worst case, call for complex reasoning.
Natural languages are full of collocations, recurrent combinations of words that co-occur more often than expected by chance and that correspond to arbitrary word usages.Recent work in lexicography indicates that collocations are pervasive in English; apparently, they are common in all types of writing, including both technical and nontechnical genres.Several approaches have been proposed to retrieve various types of collocations from the analysis of large samples of textual data.These techniques automatically produce large numbers of collocations along with statistical figures intended to reflect the relevance of the associations.However, none of these techniques provides functional information along with the collocation.Also, the results produced often contained improper word associations reflecting some spurious aspect of the training corpus that did not stand for true collocations.In this paper, we describe a set of techniques based on statistical methods for retrieving and identifying collocations from large textual corpora.These techniques produce a wide range of collocations and are based on some original filtering methods that allow the production of richer and higher-precision output.These techniques have been implemented and resulted in a tool, techniques are described and some results are presented on a 10 corpus of stock market news reports.A lexicographic evaluation of a retrieval tool has been made, and the estimated precision of 80%.
This paper presents a set of mathematical and computational tools for manipulating and reasoning about regular languages and regular relations and argues that they provide a solid basis for computational phonology.It shows in detail how this framework applies to ordered sets of context-sensitive rewriting rules and also to grammars in Koskenniemi's two-level formalism.This analysis provides a common representation of phonological constraints that supports efficient generation and recognition by a single simple interpreter.
surface text patterns for a question answering system. of the 40th Annual Meeting of the As
The quality of a statistical machine translation (SMT) system is heavily dependent upon the amount of parallel sentences used in training.In recent years, there have been several approaches developed for obtaining parallel sentences from non-parallel, or comparable data, such as news articles published within the same time period (Munteanu and Marcu, 2005), or web pages with a similar structure (Resnik and Smith, 2003).One resource not yet thoroughly explored is Wikipedia, an online encyclopedia containing linked articles in many languages.We advance the state of the art in parallel sentence extraction by modeling the document level alignment, motivated by the observation that parallel sentence pairs are often found in close proximity.We also include features which make use of the additional annotation given by Wikipedia, and features using an automatically induced lexicon model.Results for both accuracy in sentence extraction and downstream improvement in an SMT system are presented.
It has been established that incorporating word cluster features derived from large unlabeled corpora can significantly improve prediction of linguistic structure.While previous work has focused primarily on English, we extend these results to other languages along two dimensions.First, we show that these results hold true for a number of languages across families.Second, and more interestingly, we provide an algorithm for inducing cross-lingual clusters and we show that features derived from these clusters significantly improve the accuracy of cross-lingual structure prediction.Specifically, we show that by augmenting direct-transfer systems with cross-lingual cluster features, the relative error of delexicalized dependency parsers, trained on English treebanks and transferred to foreign languages, can be reduced by up to 13%.When applying the same method to direct transfer of named-entity recognizers, we observe relative improvements of up to 26%.
In this paper, we present and compare various single-word based alignment models for statistical machine translation.We discuss the five IBM alignment models, the Hidden- Markov alignment model, smoothing techniques and various modifications.We present different methods to combine alignments.As evaluation criterion we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment.We show that models with a first-order dependence and a fertility model lead to significantly better results than the simple models IBM-1 or IBM-2, which are not able to go beyond zero-order dependencies.
In statistical machine translation, the generation of a translation hypothesis is computationally expensive.If arbitrary wordreorderings are permitted, the search problem is NP-hard.On the other hand, if we restrict the possible word-reorderings in an appropriate way, we obtain a polynomial-time search algorithm.In this paper, we compare two different reordering constraints, namely the ITG constraints and the IBM constraints.This comparison includes a theoretical discussion on the permitted number of reorderings for each of these constraints.We show a connection between the ITG constraints and the since 1870 known We evaluate these constraints on two tasks: the Verbmobil task and the Canadian Hansards task.The evaluation consists of two parts: First, we check how many of the Viterbi alignments of the training corpus satisfy each of these constraints.Second, we restrict the search to each of these constraints and compare the resulting translation hypotheses.The experiments will show that the baseline ITG constraints are not sufficient on the Canadian Hansards task.Therefore, we present an extension to the ITG constraints.These extended ITG constraints increase the alignment coverage from about 87% to 96%.
We extend previous work on tree kernels to estimate the similarity between the dependency trees of sentences.Using this kernel within a Support Vector Machine, we detect and classify relations between entities in the Automatic Content Extraction (ACE) corpus of news articles.We examine the utility of different features such as Wordnet hypernyms, parts of speech, and entity types, and find that the dependency tree kernel achieves a 20% F1 improvement over a “bag-of-words” kernel.
We describe a statistical approach for modeling agreements and disagreements in conversational interaction.Our approach first identifies adjacency pairs using maximum entropy ranking based on a set of lexical, durational, and structural features that look both forward and backward in the discourse.We then classify utterances as agreement or disagreement using these adjacency pairs and features that represent various pragmatic influences of previous agreement or disagreement on the current utterance.Our approach achieves 86.9% accuracy, a 4.9% increase over previous work.
address the wherein rather than simply decide whether a review is “thumbs up” or “thumbs down”, as in previous sentiment analysis work, one must determine an author’s evaluation with respect to a multi-point scale (e.g., one to five “stars”).This task represents an interesting twist on standard multi-class text categorization because there are several different degrees of similarity between class labels; for example, “three stars” is intuitively closer to “four stars” than to “one star”.We first evaluate human performance at task.Then, we apply a based on a labeling formulation of the problem, that alters a given-ary classifier’s output in an explicit attempt to ensure that similar items receive similar labels.We show that the meta-algorithm can provide significant improvements over both multi-class and regression versions of SVMs when we employ a novel similarity measure appropriate to the problem.
We propose a method for extracting semantic orientations of words: desirable or undesirable.Regarding semantic orientations as spins of electrons, we use the mean field approximation to compute the approximate probability function of the system instead of the intractable actual probability function.We also propose a criterion for parameter selection on the basis of magnetization.Given only a small number of seed words, the proposed method extracts semantic orientations with high accuracy in the experiments on English lexicon.The result is comparable to the best value ever reported.
Entity relation detection is a form of information extraction that finds predefined relations between pairs of entities in text.This paper describes a relation detection approach that combines clues from different levels of syntactic processing using kernel methods.Information from three different levels of processing is considered: tokenization, sentence parsing and deep dependency analysis.Each source of information is represented by kernel functions.Then composite kernels are developed to integrate and extend individual kernels so that processing errors occurring at one level can be overcome by information from other levels.We present an evaluation of these methods on the 2004 ACE relation detection task, using Support Vector Machines, and show that each level of syntactic processing contributes useful information for this task.When evaluated on the official test data, our approach produced very competitive ACE value scores.We also compare the SVM with KNN on different kernels.
This paper proposes a novel composite kernel for relation extraction.The composite kernel consists of two individual kernels: an entity kernel that allows for entity-related features and a convolution parse tree kernel that models syntactic information of relation examples.The motivation of our method is to fully utilize the nice properties of kernel methods to explore diverse knowledge for relation extraction.Our study illustrates that the composite kernel can effectively capture both flat and structured features without the need for extensive feature engineering, and can also easily scale to include more features.Evaluation on the ACE corpus shows that our method outperforms the previous best-reported methods and significantly outperforms previous two dependency tree kernels for relation extraction.
10 restarts 1 restart 793 Optimization Procedure labeled dependency acc.[%] Slovenian Bulgarian Dutch Max. like.27.78 47.23 36.78 Min. error 22.52 54.72 36.78 Ann. min. risk 31.16 54.66 36.71 2: dependency accuracy on parsing 200sentence test corpora, after training 10 experts on 1000 senand fitting their weights 200 more.For Slovenian, minimum risk annealing is significantly better than the other training methods, while minimum error is significantly worse.For Bulgarian, both minimum error and annealed minimum risk training achieve significant gains over maximum likelihood, but are indistinguishable from each other.For Dutch, the three methods are indistinguishable. than either and in some cases significantly helped.Note, however, that annealed minimum risk training results in a deterministic classifier just as these other training procedures do.The orthogonal of Bayes risk decoding achieved gains on parsing (Goodman, 1996) and machine translation (Kumar and Byrne, 2004).In speech recognition, researchers have improved decoding by smoothing probability estimates numerically on heldout data in a manner reminiscent of annealing (Goel and Byrne, 2000).We are interested in applying our techniques for approximating nonlinear loss functions to MBR by performing the risk minimization inside the dynamic programming or other decoder.Another training approach that incorporates arloss functions is found in the in the margin-based-learning community (Taskar et al., 2004; Crammer et al., 2004).Like other max-margin techniques, these attempt to make the best hypothesis far away from the inferior ones.The distinction is in using a loss function to calculate the required margins.8 Conclusions Despite the challenging shape of the error surface, we have seen that it is practical to optimize task-specific error measures rather than optimizing likelihood—it produces lower-error systems.Different methods can be used to attempt this global, non-convex optimization.We showed that for MT, and sometimes for dependency parsing, an annealed minimum risk approach to optimization performs significantly better than a previous line-search method that does not smooth the error surface.It never does significantly worse.With such improved methods for minimizing error, we can hope to make better use of task-specific training criteria in NLP.References L. R. Bahl, P. F. Brown, P. V. de Souza, and R. L. Mercer.1988.A new algorithm for the estimation of hidden model parameters.In pages 493–496.E. Charniak and M. Johnson.2005.Coarse-to-fine n-best and maxent discriminative reranking.In pages 173–180.S. F. Chen and R. Rosenfeld.1999.A gaussian prior for smoothing maximum entropy models.Technical report, CS Dept., Carnegie Mellon University.K. Crammer, R. McDonald, and F. Pereira.2004.New large algorithms for structured prediction.In Structured Outputs M. Dreyer, D. A. Smith, and N. A. Smith.2006.Vine parsing and minimum risk reranking for speed and precision.In G. Elidan and N. Friedman.2005.Learning hidden variable The information bottleneck approach.6:81–127.V. Goel and W. J. Byrne.2000.Minimum Bayes-Risk auspeech recognition.Speech and Lan- 14(2):115–135.J. T. Goodman.1996.Parsing algorithms and metrics.In pages 177–183.Hinton.1999.Products of experts.In of volume 1, pages 1–6.K.-U.Hoffgen, H.-U.Simon, and K. S. Van Horn.1995. trainability of single neurons. of Computer and 50(1):114–125.D. S. Johnson and F. P. Preparata.1978.The densest hemiproblem.Comp.6(93–107).S. Katagiri, B.-H. Juang, and C.-H. Lee.1998.Pattern recognition using a family of design algorithms based upon the probabilistic descent method.86(11):2345–2373, November.P. Koehn, F. J. Och, and D. Marcu.2003.Statistical phrasetranslation.In pages 48–54.S. Kumar and W. Byrne.2004.Minimum bayes-risk decodfor statistical machine translation.In J. Lafferty, A. McCallum, and F. C. N. Pereira.2001.Conditional random fields: Probabilistic models for segmenting labeling sequence data.In F. J. Och.2003.Minimum error rate training in statistical translation.In pages 160–167.K. Papineni, S. Roukos, T. Ward, and W.-J.Zhu.2002.A method for automatic evaluation of machine In pages 311–318.K. A. Papineni.1999.Discriminative training via linear In A. Rao and K. Rose.2001.Deterministically annealed deof Hidden Markov Model speech recognizers. on Speech and Audio 9(2):111–126.K. Rose.1998.Deterministic annealing for clustering, compression, classification, regression, and related optimizaproblems.86(11):2210–2239.N. A. Smith and J. Eisner.2004.Annealing techniques for statistical language learning.In pages 486–493.
This paper describes an incremental parser and an unsupervised learning algorithm for inducing this parser from plain text.The parser uses a representation for syntactic structure similar to dependency links which is well-suited for incremental parsing.In contrast to previous unsupervised parsers, the parser does not use part-of-speech tags and both learning and parsing are local and fast, requiring no explicit clustering or global optimization.The parser is evaluated by converting its output into equivalent bracketing and improves on previously published results for unsupervised parsing from plain text.
Unsupervised learning of linguistic structure is a difficult problem.A common approach is to define a generative model and maximize the probability of the hidden structure given the observed data.Typically, this is done using maximum-likelihood estimation (MLE) of the model parameters.We show using part-of-speech tagging that a fully Bayesian approach can greatly improve performance.Rather than estimating a single set of parameters, the Bayesian approach integrates over all possible parameter values.This difference ensures that the learned structure will have high probability over a range of possible parameters, and permits the use of priors favoring the sparse distributions that are typical of natural language.Our model has the structure of a standard trigram HMM, yet its accuracy is closer to that of a state-of-the-art discriminative model (Smith and Eisner, 2005), up to 14 percentage points better than MLE.We find improvements both when training from data alone, and using a tagging dictionary.
We present an unsupervised, nonparametric Bayesian approach to coreference resolution which models both global entity identity across a corpus as well as the sequential anaphoric structure within each document.While most existing coreference work is driven by pairwise decisions, our model is fully generative, producing each mention from a combination of global entity properties and local attentional state.Despite being unsupervised, our system achieves a 70.3 on the MUC-6 test set, broadly in the range of some recent supervised results.
present an open source framework for decoding, aligning with, and training a number of statistical machine translation models, including word-based models, phrase-based models, and models based on synchronous context-free grammars.Using a single unified internal representation for translation forests, the decoder strictly separates model-specific translation logic from general rescoring, pruning, and inference algorithms.From this unified representation, the decoder can not only the 1or translations, but also alignments to a reference, or the quantities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques.Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders.
The relations are used as plans; their intended effects are interpreted as the goals they achieve.In other words, in order to bring about the state which both speaker and hearer know that the purpose of know that they both know it, etc.), the structurer uses Purpose as a plan and tries to satisfy its constraints.In this system, constraints and goals are interfor example, in the event that believed not by the satellite constraint of the resimply becomes the goal to achieve (BMB (RESULT Similarly, the propo- Ow s SCAN-1 ?ACT-2)) (DMB (08.1 are interpreted as the goal to find some element that could legitimately take place of In order to enable the relations to nest recursome relations' nucleuses and satellites contain requirements that specify additional relations, such as examples, contrasts, etc.Of course, these additional requirements may only be included if such material can coherently follow the content of the nucleus or satellite.The question of ordering such additional constituents is still under investigation.The question of whether such additional material should be included at all is not addressed; the structurer tries to say everything it is given.The structurer produces all coherent paragraphs (that is, coherent as defined by the relations) that satisfy the given goal(s) for any set of input elements.For example, paragraph (b) is produced to the initial goal S H (SEQUENCE goal is produced by PEA, together with the appropriate representation ele- (ASK-I. in response to the does the system a program?.Different initial goals will result in different paragraphs.Each paragraph is represented as a tree in which branch points are RST relations and leaves are input elements.Figure 1 is the tree for para- (b).It contains the relations (signalled by 'then' and 'finally&quot;), Elaboration (&quot;in particular&quot;), and Purpose (&quot;in order to&quot;).In the corresponding paragraph produced by Penman, the relations' characteristic words or phrases (boldfaced below) appear between the blocks of text they relate: [The system asks the user to tell it the characteristic of the program to be system applies to the progran2.](0 system scans the proo order to opportunities to apply transformations to the system resolves [It confirms the enhancewith the [it performs the enhancement.ho 166 input sentence generator --ot update agenda choose final plan get next bud RST relations expand bud grow tree Figure 2: Hierarchical Planning Structurer 6-The Structurer As stated above, the structurer is a simplified top-down hierarchical expansion planner (see Figure 2).It operates as follows: given one or more communicative goals, it finds RST relations whose intended effects match (some of) these goals; it then inspects which of the input elements match the nucleus and subgoal constraints for each relation.Unmatched constraints become subgoals which are posted on an agenda for the next level of planning.The tree can be expanded in either depth-first or breadth-first fashion.Eventually, process bottoms out when either: (a) all input elements have been used and unsatisfied subgoals remain (in which case the structurer could request more input with desired properties from the encapsulating system); or (b) all goals are satisfied.If more than one plan (i.e., paragraph tree structure) is produced, the results are ordered by preferring trees with the minimum unused number of input elements and the minimum number of remaining unsatisfied subgoals.The best tree is then traversed in left-to-right order; leaves provide input to Penman to be generated in English and relations at branch points provide typical interclausal relation words or phrases.In this way the structurer performs top-down goal refinement down to the level of the input elements. and Further Work This work is also being tested in a completely separate domain: the generation of text in a multimedia system that answers database queries.Penman produces the following description of the ship Knox (where CTG 070.10 designates a group of ships): Knox is en route in rendezvous with CTG 070.10, arriving in Pearl Harbor on 4/24, for port visit until 4/30.In this text, each clause (en route, rendezvous, arrive, visit) is a separate input element; the structurer linked them using the relations Sequence and Purpose (the same Purpose as shown above; it is signalled by &quot;in order to&quot;).However, Penman can also be made to produce (d).Knox is en route in order to rendezvous with CTG 070.10.It will arrive in Pearl Harbor on 4/24.It will be on port visit until 4/30.The problem is clear: how should sentences in the paragraph be scoped?At present, avoiding any claims about a theory, the structurer can feed 167 Penman either extreme: make everything one sentence, or make each input element a separate sentence.However, neither extreme is satisfactory; as is clear from paragraph (b), &quot;short&quot; spans of text can be linked and &quot;long&quot; ones left separate.A simple way to implement this is to count the number of leaves under each branch (nucleus or satellite) in the paragraph structure tree.Another shortcoming is the treatment of input elements as indivisible entities.This shortcoming is a result of factoring out the problem of aggregation as a separate text planning task.Chunking together input elements (to eliminate detail) or taking them apart (to be more detailed) has received scant mention — see Ellovy 871, and for the related problem of paraphrase see [Schank 75] — but this task should interact with text structuring in order to provide text that is both optimally detailed and coherent.At the present time, only about 20% of the RST relations have been formalized to the extent that they can be used by the structurer.This formalization process is difficult, because it goes handin-hand with the development of terms with which to characterize the relations' goals/constraints.Though the formalization can never be completely finalized — who can hope to represent something like motivation or justification complete with all ramifications?— the hope is that, by having the requirements stated in rather basic terms, the relations will be easily adaptable to any new representation scheme and domain.(It should be noted, of course, that, to be useful, these formalizations need only be as specific and as detailed as the domain model and representation requires.)In addition, the availability of a set of communicative goals more detailed than just say or ask (for example), should make it easier for programs that require output text to interface with the generator.This is one focus of current text planning work at ISI.
In order to take steps towards establishing a methodology for evaluating Natural Language systems, we conducted a case study.We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues.We present the quantitative results of handsimulating these algorithms, but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general.We illustrate the general difficulties encountered with quantitative evaluation.These are problems with: (a) allowing for underlying assumptions, (b) determining how to handle underspecifications, and (c) evaluating the contribution of false positives and error chaining.
Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text, as is evidenced by their poor performance on domains like the Wall Street Journal, and by the movement away from parsing-based approaches to textprocessing in general.In this paper, I describe SPATTER, a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result.This work is based on the following premises: (1) grammars are too complex and detailed to develop manually for most interesting domains; (2) parsing models must rely heavily on lexical and contextual information to analyze sentences accurately; and (3) existing n-grain modeling techniques are inadequate for parsing models.In experiments comparing SPATTER with IBM's computer manuals parser, SPATTER significantly outperforms the grammar-based parser.Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures, SPATachieves 86% precision, and 1.3 crossing brackets per sentence for sentences of 40 words or less, and 91% precision, 90% recall, and 0.5 crossing brackets for sentences between 10 and 20 words in length.
This paper presents the first round of the on Textual Entailment for organized within SemEval-2012.The task was designed to promote research on semantic inference over texts written in different languages, targeting at the same time a real application scenario.Participants were presented with datasets for different language pairs, where multi-directional entailment relations (“forward”, “backward”, “bidirectional”, “no entailment”) had to be identified.We report on the training and test data used for evaluation, the process of their creation, the participating systems (10 teams, 92 runs), the approaches adopted and the results achieved.
and Anne Anderson.1997.The reliability of a dialogue structure coding Linguistics 13-32.Giacomo Ferrari.1998.Preliminary steps toward the creation of a discourse and text In of the First International Conference on Language
There has been much interest in using phrasal movement to improve statistical machine translation.We explore how well phrases cohere across two languages, specifically English and French, and examine the particular conditions under which they do not.We demonstrate that while there are cases where coherence is poor, there are many regularities which can be exploited by a statistical machine translation system.We also compare three variant syntactic representations to determine which one has the best properties with respect to cohesion.
We report on the Parallel Grammar (ParGram) project which uses the XLE parser and grammar development platform for six languages: English,
BiBr.EF.1 BiBr.EF.2 BiBr.EF.3 BiBr.EF.4 BiBr.EF.5 BiBr.EF.6 BiBr.EF.7 BiBr.EF.8 Limited Unlimited Unlimited Limited Unlimited Unlimited Limited Unlimited baseline of bilingual bracketing baseline of bilingual bracketing + English POS tagging baseline of bilingual bracketing + English POS tagging and base NP reverse direction of BiBr.EF.1 reverse direction of BiBr.EF.2 reverse direction of BiBr.EF.3 intersection of BiBr.EF.1 & BiBr.EF.3 intersection of BiBr.EF.3 & BiBr.EF.6 ProAlign.EF.1 Unlimited cohesion between source and target language + English parser + distributional similarity for English words Ralign.EF.1 Limited Giza (IBM Model 2) + recursive parallel segmentation UMD.EF.1 Limited IBM Model 2, trained with 1/20 of the corpus, distortion 2, iterations 4 XRCE.Base.EF.1 XRCE.Nolem.EF.2 XRCE.Nolem.EF.3 Limited GIZA++ (IBM Model 4) with English and French lemmatizer GIZA++ only (IBM Model 4), trained with 1/4 of the corpus GIZA++ only (IBM Model 4), trained with 1/2 of the corpus Table 2: Short description for English-French systems System Resources Description BiBr.RE.1 BiBr.RE.2 BiBr.RE.3 Limited Unlimited Unlimited baseline of bilingual bracketing baseline of bilingual bracketing + English POS tagging baseline of bilingual bracketing + English POS tagging and base NP
This paper presents the task definition, resources, participating systems, and comparative results for the English lexical sample task, which was orgaas part of the evaluation exercise.The task drew the participation of 27 teams from around the world, with a total of 47 systems.
Even though considerable attention has been given to semantic orientation of words and the creation of large polarity lexicons, research in emotion analysis has had to rely on limited and small emotion lexicons.In this paper, we show how we create a high-quality, moderate-sized emotion lexicon using Mechanical Turk.In addition to questions about emotions evoked by terms, we show how the inclusion of a word choice question can discourage malicious data entry, help identify instances where the annotator may not be familiar with the target term (allowing us to reject such annotations), and help obtain annotations at sense level (rather than at word level).We perform an extensive analysis of the annotations to better understand the distribution of emotions evoked by terms of different parts of speech.We identify which emotions tend to be evoked simultaneously by the same term and show that certain emotions indeed go hand in hand.
We examine sentiment analysis on Twitter data.The contributions of this paper are: (1) We introduce POS-specific prior polarity features.(2) We explore the use of a tree kernel to obviate the need for tedious feature engineering.The new features (in conjunction with previously proposed features) and the tree kernel perform approximately at the same level, both outperforming the state-of-the-art baseline.
Word groupings useful for language processing tasks are increasingly available, as thesauri appear on-line, and as distributional word clustering techniques improve.However, for many tasks, one is in relationships among word words.This paper presents a method for automatic sense disambiguation of nouns appearing within sets of related nouns — the kind of data one finds in on-line thesauri, or as the output of distributional clustering algorithms.Disambiguation is performed with respect to WordNet senses, which are fairly fine-grained; however, the method also permits the assignment of higher-level WordNet categories rather than sense labels.The method is illustrated primarily by example, though results of a more rigorous evaluation are also presented.
In this paper we discuss cascaded Memory- Based grammatical relations assignment.In the first stages of the cascade, we find chunks of several types (NP,VP,ADJP,ADVP,PP) and label them with their adverbial function (e.g. local, temporal).In the last stage, we assign grammatical relations to pairs of chunks.We studied the effect of adding several levels to this cascaded classifier and we found that even the less performing chunkers enhanced the performance of the relation finder.
